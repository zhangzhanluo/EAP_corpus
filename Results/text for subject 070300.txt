Library-Based LAMMPS Implementation of High-Dimensional Neural Network Potentials
ABSTRACT: Neural networks and other machine learning approaches have been successfully used to accurately represent atomic interaction potentials derived from computationally demanding electronic structure calculations. Due to their low computational cost, such representations open the possibility for large scale reactive molecular dynamics simulations of processes with bonding situations that cannot be described accurately with traditional empirical force fields. Here, we present a library of functions developed for the implementation of neural network potentials. Written in C++, this library incorporates several strategies resulting in a very high efficiency of neural network potential-energy and force evaluations. Based on this library, we have developed an implementation of the neural network potential within the molecular dynamics package LAMMPS and demonstrate its performance using liquid water as a test system.
1. INTRODUCTION
Over the past decades, molecular dynamics (MD) simulations have become a central tool for understanding how the properties of materials result from the interactions and dynamics of atoms. Ideally, the forces needed in MD simulations to integrate the equations of motion are determined by solving the electronic Schrödinger equation of the electrons in the felid of the nuclei. Such an ab initio approach, however, is computationally very costly, and consequently forces are often computed from simple empirical potentials postulated based on physical considerations with parameters tuned to reproduce key properties of the material under study. While empirical potentials are available for many types of systems and can dramatically extend the time and length scales accessible to MD simulations, they may introduce systematic inaccuracies as a result of the approximate functional form. Complex chemical environments or the dynamic formation and breaking of covalent bonds are in particular difficult to model within this framework. In recent years, machine learning potentials have become a new promising tool to bridge the gap between accurate and flexible but computationally expensive electronic structure methods and approximate but computationally convenient empirical force fields. Specifically, machine learning potentials have been used to predict physical properties of inorganic solids or complex molecular systems with ab initio accuracy and to model chemical dynamics, such as proton transfer or dissociative chemisorption The general idea of this approach is to use statistical learning techniques to devise potentials with a very flexible functional form with many parameters and to train them using ab initio reference data. While some recent machine learning potentials such as the Gaussian approximation potential (GAP), Coulomb-matrix based method or the spectral neighbor analysis potential (SNAP) are based on kernel methods, artificial neural networks have been used for a longer time, as first applications date back to 1995 (see refs 13 and 14 for a comprehensive list of applications). In an approach proposed in 2007 by Behler and Parrinello, the potential energy surface is represented using high-dimensional neural networks, which are trained to reproduce complex ab initio reference potential energy surfaces and allowed to perform simulations with large numbers of atoms. While during the training process the expensive calculation of energies and forces from the reference method is required, the final neural network potential (NNP) can predict the training data points and interpolate in between them at a fraction of the computational cost. This ability can be used to drive a fast MD simulation with ab initio accuracy.
From a practical point of view, the NNP approach requires software to accomplish two central tasks. First, a training program is needed that adjusts the neural network parameters in such a way that the reference data is correctly reproduced. Once the training is done and the parametrization is ready, a computer code is needed to execute the neural networks and calculate the forces needed in the MD simulation. While in principle these two pieces of code can be developed independently from each other, this would possibly result in many lines of duplicated code and additional maintenance work. Thus, it is beneficial to pursue a library-based programming paradigm such that common functionalities and data structures can be collected and combined as needed by different applications. Following this approach, we have developed a C++ library for high-dimensional neural network potentials (HDNNPs). The present work focuses on the basic building blocks of the library and how they are implemented in order to optimize performance. In addition to the NNP core library we present its first application, an implementation of the method in the popular MD package LAMMPS, and provide benchmark results for several systems simulated on a modern HPC cluster. Although similar implementations of the method by Behler and Parrinello written in Fortran, Python/Fortran, C++, r as an extension to Tensorflow have been developed recently, a clear illustration of the expected performance when applying the method is generally not provided. However, for a prospective user it is of optimization and parallelization, how fast MD simulations can be run in practice. Unfortunately, a comparison of all available software is currently unfeasible as the implementations often differ in available features and technical details.
The C++ library and the LAMMPS interface are designed to work in conjunction with existing parametrizations created with the RuNNer software. Further applications based on the presented library, such as an HDNNP training program, are also available and will be described in an upcoming publication. The NNP library and the LAMMPS implementation are available freely online as part of the n2p2 neural network potential package at http://github.com/CompPhysVienna/n2p2.
The remaining article is organized as follows: in Section 2 the neural network potential method is shortly outlined Section 3 focuses on the implementation and features of the NNP library as well as the LAMMPS interface. Finally, we present benchmarks for multiple test cases in Section 4 before we offer some conclusions in Section 5.
2. The High-Dimensional Neural Network Potential Method
This section is intended as a concise summary of the HDNNP method and does not provide a detailed introduction, for which we refer the reader to refs (41and42).
2.1. Atomic Neural Networks
The high-dimensional neural network potential method introduced by Behler and Parrinello is based on the assumption that the total potential energy E pot can be written as the sum of individual atomic contributions
where Nis the number of atoms. The atomic energies Ej, which depend on the local environment of the atoms, are calculated from feed-forward neural networks as depicted in Figure 1.
Figure 1. A simple feed-forward neural network with two hidden layers. Information about each atom’s surroundings is entered in the input layer neurons on the very left. The neural network processes the data by successively computing the neurons in the hidden layers and presents the resulting atomic energy contribution in the single output layer neuron. Note that here we omitted the atom index j of the input and output layer label.
These networks consist of multiple layers of nodes, also called neurons, which are interconnected by weights representing the fitting parameters of the neural network. The neurons of the input layer of the neural networks are populated with sets of real numbers Gj = {Gj,1,...,Gj,Njs}, called symmetry functions, that characterize the local environment of atom j (see Section 2.2). The neural network then propagates the data to neurons in the hidden layers with the simple rule
where yik denotes the value of neuron i in layer k, bik is the associated bias, ajilk is the weight connecting yik with yjl, and nl is the number of neurons in layer l = k–1. Thus, neuron values are computed from a weighted sum of all incoming connected neurons in the previous layer. In addition the bias is added to allow for a shift independent from input neurons. The activation function fa is usually a nonlinear function (e.g., a hyperbolic tangent or the logistic function), which enables the neural network as a whole to reproduce complex nonlinear potential energy surfaces after weights and biases have been correctly adapted in a training stage. After processing the input data through the hidden layers, the recursion of eq 2 will ultimately result in the atomic energy contribution presented as the value of the single output neuron. Note that separate neural networks and typically also separate symmetry function setups are used to account for different chemical elements. For a given chemical element the symmetry function parameters, the weight parameter values, and the neural network architecture are constrained to be identical. This construction ensures permutation symmetry because atoms of the same chemical element are treated equally.
To obtain analytical expressions for the atomic forces, one simply applies the chain rule to the total potential energy expressed in eq1
Where is the gradient with respect to the Cartesian coordinates of atom i. Note that the individual atomic energy contributions Ej do not have any physical meaning attached to them as they are not observables but merely a vehicle of the method to construct the total potential energy. In particular, only Epot is typically known from a reference method and can therefore be used during training. However, splitting up the total energy as shown in eq 1 comes with a major benefit: scalability. A neural network potential trained for small system sizes is capable of  predicting the potential energy surface for larger configurations by simply adding new summands in eq 1, as long as the atomic environments are properly included in the training set. Training a neural network potential, i.e., optimizing the weight and bias parameters, requires a data set of structures and corresponding potential energies, as well as optionally the forces, computed using the reference method of choice (for instance density functional theory). The procedure to generate appropriate training sets and the optimization algorithm itself are out of scope of this work and have been described elsewhere.
2.2. Symmetry Functions
To provide the atomic neural networks with information about each atom’s surroundings, sets of atom-centered symmetry functions (ACSFs) are computed from the Cartesian coordinates of the atoms. Here, the term “symmetry” refers to the invariance of these functions with respect to global translation, rotation, and index permutation. The use of such symmetry functions ensures that by construction also the total energy is invariant with respect to these operations as required. A variety of symmetry functions have been proposed in the past for the prediction of potential energy surfaces  or structure detection. Note that symmetry functions are only one set of possible descriptors of the local atomic environment used in machine learning approaches.
Symmetry functions can be organized in two groups: Radial symmetry functions are computed only from distances to neighbor atoms, whereas angular symmetry functions also involve angles between triplets of atoms (one central and two neighboring atoms). However, we note that in general symmetry functions for high-dimensional NNPs are many-body functions depending simultaneously on a large number of degrees of freedom in the atomic environments. Here, we recapitulate three frequently used basic types of symmetry functions, one radial variant (Gradial) and two angular variants (Gang.n. and Gang.w.) proposed by Behler in 2011.
The calculation of the radial symmetry function G radial for a given atom iinvolves summing over all neighboring atoms
where rij = |r⃗i – r⃗j| is the distance between atoms i and j, fc(r) is a cutoff function, and η and rs are free parameters determining the spatial shape of the symmetry function. The functional form of angular symmetry functions is similar but requires a double sum over all neighbors with corresponding exponential and cutoff function terms, and the angle θijk = ∠(j,i,k) enters as central term to distinguish different angular environments. The angular symmetry function comes in two flavors, the “narrow” version is defined as
whereas the “wide” variant
omits the factor e–ηrjk2fc(rjk) and thus effectively reduces the damping for large angles and neighbor atoms far from each other but not from the central atom. Both angular symmetry functions depend on the free parameters η, ζ, and λ.
As apparent in eq 3, calculating atomic forces from the neural network potential energy surface requires computation of the gradient of symmetry functions with respect to the Cartesian coordinates. Analytic expressions of symmetry function gradients are presented in the Supporting Information. The cutoff functions fc mentioned above ensure a smooth decay of symmetry functions and their derivatives when neighbor atoms reach the cutoff radius rc. Different functional forms have been proposed in the literature, e.g. involving the cosine
or the hyperbolic tangent
See Section 3.3 for additional choices based on polynomials.
3. Implementation
In general, extending an existing MD software with a new interaction type can be accomplished following two different strategies. First, one could implement all new functionality within the given framework, i.e. add new functions or classes to the code which calculate energies and forces for this particular interaction. In this way no computational overhead and no new dependencies will be added to the MD software. For interactions represented by simple functional forms with a small number of tunable parameters (e.g., pairwise potentials) this is the obvious design choice. For technically more complex interactions a library approach seems preferable. Here, all functions, data types, and variables related to the interaction are gathered in an external library, which provides an interface for the MD program to pass on required information, e.g. atomic positions or neighbor lists. Energies and forces are then computed within the library and returned back to the calling MD code. While this scheme introduces some overhead, it also comes with major advantages. The functionality introduced by the library is not restricted to a single MD software anymore but may be interfaced by multiple codes. The library as common code basis then ensures that changes made to internal routines (e.g., bug fixes, performance improvements, etc.) are automatically carried over to all interfacing programs. With numerous required computational steps (e.g., symmetry function calculation, neural network execution) and parameters (e.g., symmetry function types and parameters, neural network weights and topology) the neural network potential method clearly qualifies as a complex interaction, and a library strategy is suitable. The following sections introduce our NNP library implementation and present the LAMMPS interface.
3.1. NNP Library
The neural network potential library (libnnp) constitutes the foundation of our implementation. It is written in C++ adhering to the C++98 standard and utilizes features from the Standard Template Library (STL). Also, there are no dependencies to third-party libraries which simplifies its portability to different environments. The library provides classes supporting documentation generation from source code via doxygen.(47) They can be easily extended, or new classes can be derived to introduce additional functionality in the future. Although all classes are bundled together in the NNP library, it is still helpful to imagine them as part of two groups with different purpose.
First, some classes are intended for storing the data of the atomic configurations:
Structure class: Holds information for one configuration, i.e., contains the geometry of the periodic box and the positions of all atoms (a vector of type Atom) as well as member functions to compute neighbor lists.
Atom class: Stores the coordinates of an individual atom, its neighbor list (a vector of type Atom::Neighbor), and temporary results from symmetry function calculation.
Atom::Neighbor class: Within the scope of the Atom class this nested class contains the relative location of an atom’s neighbor, temporary symmetry function results, and the cutoff function cache (see Section 3.3).
The other group of classes organizes the neural network potential calculation depending on the large variety of possible user settings. The general layout of these classes is schematically shown in Figure 2.
Mode class: The top-level class of the NNP library containing the complete information required to calculate energies and forces for configurations of type Structure. Existing and future applications should use its interface or inherit from here.
Element class: Since the HDNNP method allows different neural network topologies and symmetry function setups for each element, all per-element components are grouped within this class. The Mode class contains as many instances of Element as there are elements in the system.
NeuralNetwork class: A simple implementation of a feed-forward neural network with selectable number of hidden layers and nodes per hidden layer. Linear, hyperbolic tangent, logistic, and softplus activation functions are available.
SymmetryFunction class: The base class for all symmetry functions providing common member variables and functions. Actual symmetry function implementations are inherited from here. New symmetry functions can be easily added by copying, renaming, and modifying existing child classes.
SymmetryFunctionGroup class: Similar to SymmetryFunction this is the base class for all symmetry function groups (see Section 3.2).
CutoffFunction class: A collection of different cutoff functions.
SymmetryFunctionStatistics class: This class collects statistics of symmetry functions during runtime.
Settings class: A map collecting all user-provided settings in the form of keyword-string pairs.
Log class: All NNP library output intended for the computer screen is passed on to this simple logging class. From there it is possible to redirect all information to multiple targets (screen, C-style file pointers, or C++ file streams) or even completely silence the output.
ElementMap class: This class provides a convenient mapping from atomic numbers to element strings and vice versa for all used atom types. Often required to construct other classes.
Figure 2. Top-level class layout of the neural network potential library libnnp: The Mode class contains all parts required to calculate the HDNNP energy and forces for a given configuration. Neural networks and symmetry functions (boxes labeled with Gi) are gathered per element. Symmetry functions with similar parameters are collected in symmetry function groups.
The NNP library features some basic thread-level parallelization via OpenMP in the Mode and Structure classes. Their respective member functions for neighbor list, symmetry function, and force calculation automatically distribute the workload when running on a multicore processor. Although OpenMP support is only provided for a few functions in the library, these cover the most time-consuming computations. Hence, applications will benefit immediately from this simple form of parallelization.
3.2. Symmetry Function Groups
The procedure to compute the HDNNP energy for a given atomic configuration is easily outlined: the outermost loop is running over all atoms in the structure, as required by eq 1. For each atom, the symmetry function set is calculated and subsequently used as an input for the corresponding neural network. The atomic energy contributions delivered by the neural networks are then summed up to yield the total potential energy. Computing analytic forces complicates this procedure slightly, but the computation of symmetry functions and the evaluation of the neural network remain as the two main computational building blocks. It turns out that the calculation of symmetry functions for a given atom is usually by far more time-consuming than the evaluation of the attached neural network. In particular, HDNNP setups with numerous angular symmetry functions are computationally dominated by the double loop structure in eqs 5 and 6. Naturally, in our effort to create an efficient HDNNP implementation we focused our optimizations on the corresponding parts of the code. Taking into consideration the symmetry function setup of existing HDNNPs,(3,7,33,36,38,48−50) possible strategies to restructure naive loop implementations and utilize temporary memory for intermediate results, we decided for a simple but effective method to reduce the computational cost via symmetry function groups. Here, we will now outline their construction and explain how performance benefits from their usage.
The basic idea is to avoid unnecessary loops over the same neighbor lists for symmetry functions with different parameters. As an example, consider a partial list (Table 1) of angular symmetry functions for oxygen atoms used in a NNP developed for water and ice.(3)
It enumerates different neighbor atom combinations and parameters η, λ, ζ, and rc. With the functional form in eq 5 in mind, two observations can be exploited for performance gains. First, all symmetry functions share the same cutoff rc and thus the same list of neighboring atoms. It would be inefficient to calculate the cutoff function values fc(rij), fc(rik), and fc(rjk) as well as their derivatives repeatedly for each symmetry function. Instead, it is beneficial to reorder the lines in Table 1 and group entries together according to their neighbor species signature and cutoff value. Each of the (e1,e2,rc)-combinations now forms a symmetry function group whose members correspond to the remaining parameter variations (here η, λ, and ζ). Table 2 shows the arrangement of the example symmetry functions in three groups.
The procedure to compute HDNNP energies and forces described at the beginning of this section is now modified as follows: while the outermost loop is still running over all atoms, the symmetry function loop is replaced by a loop over symmetry function groups. For each group, the correct neighbors are extracted from the neighbor list, and the corresponding cutoff function terms are calculated. Only then, in the innermost loop, the distinct parts of individual symmetry functions, i.e., the terms (1 + λ cos θijk)ζ and e–η(rij2+rik2+rjk2), are computed. At second glance another optimization step for angular symmetry functions is possible since it is common that multiple symmetry functions with equal η but different λ and ζ parameters are included. Thus, once calculated for a given η, the term e–η(rij2+rik2+rjk2) can be reused for multiple symmetry functions.
Grouping of symmetry functions works similarly for eqs 4 and 6. The NNP library implements all three basic symmetry function groups and uses them per default. Note that the concept behind the optimizations described in this section is likely to be applicable also to future symmetry functions or other descriptors of local atomic environment since common features will usually imply more efficient computation pathways than the sequential calculation.
3.3. Cutoff Functions and Caching
As described in Section 3.2, in a naive implementation of an NNP cutoff functions for identical distances are repeatedly calculated for each symmetry function. The introduction of symmetry function groups reduces the number of cutoff function calls significantly since temporary results are reused to compute multiple symmetry functions in a row. Still, different groups with equal cutoff function radius rc may loop over similar neighbor combinations and thus require the re-evaluation of cutoff functions. This computational overhead is avoided by the NNP library because the Atom::Neighbor class is equipped with additional storage for cutoff function values and their derivatives. During the symmetry function calculation the first symmetry function group encountering a particular ‘‘central to neighbor atom situation’’ will calculate fc and  and store the results. Other groups will then later retrieve the values from the cache instead of recalculating them.
The NNP library implements several cutoff functions, most of which (except the hyperbolic tangent in eq 11) also support the definition of an inner cutoff radius rci := αrc via a parameter α ∈ [0,1). The cutoff function returns 1 up to the inner cutoff and then falls off to 0 up to the actual cutoff rc with the desired functional form
3.4. LAMMPS Interface
After having presented the structure and capabilities of our NNP library in the preceding sections, we will now move on with the details of the LAMMPS implementation. LAMMPS is a widely used molecular dynamics software, which supports numerous potentials and simulation techniques. The documentation is comprehensive, it uses a spatial decomposition algorithm together with message passing interface (MPI) for parallelization, and the modular design allows for an easy extension with new features. For these reasons, LAMMPS is an ideal framework for the implementation of the neural network potential method.
The LAMMPS developer guide(17) provides a starting point for any extension and describes the code design and top-level classes. The authors state that nonbonded interactions are implemented as subclasses of the Pair base class. The name does not imply a restriction to pairwise interactions; also many-body potentials are included. Thus, we introduce the derived class PairNNP which makes the neural network functionality available in the LAMMPS scripting language via the keywords pair style nnp. As mentioned in the developer guide, during runtime at each time step the PairNNP compute method is called, and lists of atoms and their neighbors are provided. If the simulation is run in parallel, each MPI process is handling only a group of “local” atoms for which the contributions to potential energy and atomic forces are calculated. They are accompanied by surrounding “ghost” atoms which are required for the interaction computation but are local to other processes. At each time step the information about per-process available atomic configurations needs to be rearranged into data structures compatible with the NNP library. The atomic energy and force computation is then performed internally by the NNP library, and the results are returned back to the LAMMPS PairNNP class. Technically this scheme is implemented as follows: the class Interface Lammps is derived from the NNP top-level class Mode described in Section 3.1, and an instance is added as a member of PairNNP. In contrast to its base class, Interface Lammps contains an additional private Structure member and provides public functions to transfer atomic configurations and neighbor information from LAMMPS to this internal data storage. Once the data has been transferred, symmetry functions and atomic networks are computed via methods inherited from the Mode class. Finally the atomic energies and forces from local atoms are reported back to LAMMPS. See Figure 4 for a schematic representation of the class inheritance.
Special care has to be taken to correctly update the LAMMPS force arrays. Contributions of the form  (see eq 3), where i is a ghost atom and j is a local atom, must not be communicated and summed up during the interaction computation. Instead they are stored in force arrays of ghost atoms and later processed in a separate LAMMPS communication step. This ensures that the virial is correctly calculated by LAMMPS.
4. Benchmarks
In the preceding sections we described the building blocks of the NNP library and explained how they are combined in order to implement the method in the molecular dynamics package LAMMPS. Here, we proceed with an overview of the efficiency obtained for our NNP implementation in a modern high-performance computing (HPC) environment. To provide a useful performance measurement we decided not to benchmark individual parts of the library but rather collect timing results for real-world application of different NN potentials in MD simulations with LAMMPS (version 16Mar18).
All calculations discussed below were performed on the Vienna Scientific Cluster (VSC), an HPC system equipped with dual-processor nodes (2x Intel Xeon E5-2650v2, 2.6 GHz, 8 cores per processor) interconnected via the Intel QDR-80 InfiniBand fabric. All source code was compiled with the Intel C++ compiler and linked to the Intel MPI library, both part of the Intel Parallel Studio XE 2017 Update 7. Compilation was performed with a small set of optimization flags (-O3 -xHost -ipo).
4.1. Neural Network Parametrizations
Two neural network potentials were used for benchmarking, both modeling water and ice but with different number of atomic species. The first one is a model for liquid water and multiple ice phases reproducing the potential energy surface of density functional theory with the RPBE functional including van der Waals corrections according to the D3 method. This NNP has been used to investigate the importance of van der Waals corrections to explain anomalous properties of water and for the determination of the line of maximum density at negative pressures. This model was trained with the RuNNer software and will be abbreviated as “nnp-H2O” in the remaining work. The other neural network potential (“nnp-mW”) was developed purely for benchmarking purposes as it is not based on a computationally expensive ab initio method but rather on the coarse-grained monatomic model for water developed by Molinero and Moore, also known as mW water. In this approach, water molecules are represented by single particles favoring tetrahedral coordination structures via the Stillinger-Weber interaction, which includes terms depending on the angle between triplets of atoms. This system is ideal for our testing purposes, because it requires the use of angular symmetry functions to properly describe the atomic environments just like in the case of complex ab initio potential energy surfaces. Moreover, the mW model is implemented in LAMMPS, which makes it very easy to generate reference data for the training procedure. Note that naturally the applicability of this neural network potential is limited to benchmarking since the performance of the mW implementation exceeds that of the neural network representation by far due to its simpler functional form. However, as an effective one-component system the mW model is suitable for our purpose here, which is to demonstrate the achievable performance for single-component systems (in contrast to the nnp-H2O model with a more complex symmetry function setup for two chemical elements).
General settings for both neural network potentials are compared in Table 3, and symmetry function parameters for nnp-H2O, previously reported in ref (3), and resulting symmetry function group definitions are provided as Supporting Information. Note that since in the mW model water molecules are represented by single particles, only one corresponding “atomic” neural network for this atom type is required.
For the mW model we chose a mixture of all three symmetry function types from Section 2.2 with parameters chosen along the guidelines given in ref (42) and listed in Table 4. Note that there are many choices of symmetry function parameters equally suited to describe the local atomic environment, e.g. systematic approaches recently presented in refs. The performance is of course mostly determined by the number of symmetry functions (and their type) and not by the selected parameters. The corresponding symmetry function groups are listed in the Supporting Information. The training data set consists of 1991 configurations, where initially 1300 structures, each containing 128 mW particles, were taken directly from MD simulations of water and ice at different temperatures and pressures. The remaining configurations were obtained via artificial modifications (shearing and scaling of the simulation box, random displacements, and deletions of individual atoms) of the initial configurations. Training of the nnp-mW potential was performed with a training application based on the NNP library, which implements parallel Kalman filter based neural network training. We excluded roughly 10% of the entire data set from training. The configurations in this “test” set are used to detect overfitting and assess the overall quality of the fit. Only if the root-mean-square errors (RMSEs) of both the training and the test set are sufficiently low and do not deviate much from each other is the neural network capable of reproducing the potential energy surface at and in between the sample points in the training data set. Excellent training results were achieved for the mW model: the RMSE of energies in the training/test set converged to 0.24/0.27 meV/atom, and the RMSE of training/test forces converged to 15.5/15.7 meV/Å.
4.2. Comparison of Performance Optimization Strategies
In order to compare the performance gains achieved through the performance optimization strategies described in Sections 3.2 and 3.3, we set up MD simulation runs under realistic conditions on the VSC. Starting configurations of liquid water with almost cubic box geometry were prepared for both models. For nnp-H2O a snapshot containing 2880 water molecules from an equilibration run at 300 K and 1 bar was used. For the nnp-mW model, two configurations, one with 128 and another one with 8192 mW particles, were prepared at 298 K and zero pressure. These three test systems were propagated in time for 1000 time steps with the velocity Verlet algorithm, which yields trajectories consistent with the microcanonical ensemble. The time steps were 0.5 and 10 fs for nnp-H2O and nnp-mW, respectively.
Four LAMMPS binaries were created with different NNP-library optimizations enabled at compile time: the basic version without any optimization (“Basic”), one with the cutoff function cache enabled (“CFC”), another one making use of symmetry function groups (“SFG”), and finally one with both optimizations activated (‘‘SFG+CFC’’). For each binary the benchmark simulations were carried out on different numbers of VSC nodes starting from serial runs on a single node up to runs with 512 MPI tasks on 32 nodes. LAMMPS binaries automatically collected timing results, which were parsed from the output files.
Figure 5 shows the performance in units of time steps per second for all three test cases and each binary as a function of the number of MPI tasks. As is to be expected, in general the performance increases with the number of cores, but the relative speedup decreases for each additional core added due to parallelization overhead. Clearly, the naive implementation without any optimization yields the worst timing results. Caching cutoff function values gives a small speed gain, while the introduction of symmetry function groups significantly boosts the performance. Combining both optimization methods is only marginally better than the SFG-only version, yet a consistent improvement over all simulations is clearly visible. As it turns out, the relative performance gain over the basic version is comparable for different numbers of MPI tasks. Thus, it is practical to specify average values for each optimization method which are provided in Table 5.
Summarizing the results from Figure 5 and Table 5, we find that the LAMMPS implementation based on our NNP library performs best when both the cutoff function caching and symmetry function groups are enabled as on average the performance is increased by approximately a factor of 3. Naturally, the default compile options for the NNP library are set accordingly. Depending on neural network parametrization, system size, and number of neighbors within the cutoff sphere, the efficiency of the MPI parallelization may vary. Figure 6 shows the speedup and the parallel efficiency of the SFG+CFC version for all three test cases considered here.
4.3. MPI/OpenMP Hybrid Parallelization Performance
The parallel performance results obtained so far were solely based on the MPI parallelization strategy of LAMMPS. However, the NNP library offers an additional layer of thread-level parallelism via OpenMP (see Section 3.1) which can be used to run hybrid MPI/OpenMP simulations with LAMMPS. This approach allows for the combining of the capabilities of MPI (internode communication) and OpenMP (intranode shared memory access) to reflect the hierarchical hardware setups found on many cluster systems (interconnected nodes with multicore processors). Typically, in such a hybrid simulation run each MPI process dynamically spawns multiple OpenMP threads and distributes the local workload to cores within each node. The total number of cores used is therefore the product of MPI processes and OpenMP threads. In comparison to a pure MPI run with the same number of cores, the potential benefit is a reduction of MPI communication overhead and a better scaling behavior. However, the actual performance depends strongly on the hardware setup and the implementation details.
In Figure 7 we present benchmark results for the nnp-H2O model and a system size of 2880 molecules. A binary with all optimizations (SFG+CFC) and additional OpenMP support was used, and timing information was collected for hybrid MPI/OpenMP runs with 1, 2, and 4 OpenMP threads per MPI process. While the communication overhead drops as expected with multiple threads, an overall speedup is unfortunately not observed. Instead, up to about 128 total cores used there is even a clear decrease in performance of about 5% and 15% for 2 and 4 threads, respectively. Nevertheless, the situation changes above 256 cores as the performance approaches that of the pure MPI run. Ultimately, for the maximum number of cores (1024) there is even a small speedup of about 7%. A potential explanation for the observed behavior is the incomplete parallelization at the OpenMP level competing against the communication overhead. Due to technical reasons the LAMMPS interface parallelizes only the symmetry function calculation via OpenMP while additional energy and force computation parts remain serially executed by each MPI process. Thus, for small numbers of total cores this disadvantage dominates. Only with many MPI processes involved, the communication reduction benefit outweighs the parallel performance loss due to serial code parts.
The results demonstrate that pure MPI simulations using the LAMMPS interface already scale well to many cores. However, some additional performance gain can be achieved using a MPI/OpenMP hybrid parallelization strategy if many MPI processes are used and the communication time would otherwise be substantial. With some additional programming efforts in the future, the NNP library may be tuned to reduce the amount of serial computations and further improve the hybrid parallelization performance. For the remainder of this Article we discuss only results obtained with pure MPI and without OpenMP parallelization.
4.4. Dependence of Performance on Neural Network Potential Settings
In the previous section we showed that both optimization strategies presented in Section 3 significantly increase the performance when compared to a naive implementation. Here, we address two more detailed questions on how the performance is affected by NN parametrization specifics:
1. Is it possible to further increase the efficiency of the NNP implementation speed by making use of different cutoff functions, i.e. switching from a transcendental functional form to cutoff functions based on computationally less demanding polynomials?
2. How big is the performance loss if for a given NN parametrization additional symmetry functions are introduced? This question usually arises in the creation of a new neural network potential when training results indicate that the spatial resolution is not sufficient and additional symmetry functions should be used. More radial symmetry functions will usually not influence performance, but the number of angular symmetry functions will require a balance between accuracy and speed.
In order to answer these questions we set up three additional NN parametrizations based on the nnp-mW model and ran benchmarks with the 8192 mW particle system. Regarding the first question, the modification ‘‘nnp-mW-fc poly2’’ uses the same NN settings as the original model with only one exception: the cosine cutoff function f cos of all symmetry functions is replaced by the polynomial function f poly2 (leaving α = 0 unchanged) as described in Section 3.3. To tackle the second question, two variants with additional angular symmetry functions were constructed from the original nnp-mW model. First, the variant “nnp-mW-η” adds new symmetry functions of type Gang.n. and Gang.w. with η = 0.001. The remaining parameters are set following the scheme from the original model, i.e. λ = ±1, ζ = 1,3 and rc = 12.0 bohr, see Table 6 for a list of new symmetry functions. Similarly, the variation “nnp-mW-ζ” extends the original model by new symmetry functions with ζ = 9, see Table 7 for a list of additional symmetry function entries.
The three variants of the nnp-mW model were trained with the same data set as described in Section 4.1, and a comparable fit quality was achieved. Benchmark MD simulations were run with the 8192 mW particle system under the same conditions as in the previous section with all four LAMMPS binaries. Finally, the timing data were compared to those of the unmodified nnp-mW model, and the performance gain or loss was averaged over runs with different number of MPI tasks.
Table 8 shows the results for all variants and binaries in percent with respect to the performance of the nnp-mW model. With the results in the first line of the table, we are able to answer our first question posed. While the basic implementation would still benefit from the use of a computationally less expensive cutoff function, the advantage almost disappears when all NNP library optimizations are enabled. This is because the cutoff function calls are drastically reduced by caching and by the use of symmetry function groups. Regarding the second question about the performance loss caused by additional symmetry functions we find that the usage of symmetry function groups (binaries SFG and SFG+CFC) is clearly favorable. If symmetry function calls are executed sequentially without any reuse of intermediate results (binaries Basic and CFC), the expected maximum decrease in performance corresponds to the number of additional symmetry functions since the calculation of radial symmetry functions and the execution of other parts of the code can be neglected. In the case of 8 additional entries for the nnp-mW-η model, this amounts to 25%. Likewise for nnp-mW-ζ a maximum drop of 37.5% is to be expected. Indeed the observations presented in Table 8 closely match these expectations. On the other hand, the performance drop experienced with symmetry function groups enabled (columns SFG and SFG+CFC) is less pronounced as the corresponding numbers in Table 8 are consistently lower. This behavior is easily explained with the construction of symmetry function groups (see Section 3.2) in mind: With the given choice of parameters the additional symmetry functions can be appended to the list of symmetry function group members of the original nnp-mW model (see the Supporting Information) to form the symmetry function groups of the nnp-mW-η or nnp-mW-ζ model. Therefore, the additionally required computation steps involve only those that are not already part of calculations common to all group members, which effectively reduces the cost of the extra symmetry functions.
The three variants of the nnp-mW model were trained with the same data set as described in Section 4.1, and a comparable fit quality was achieved. Benchmark MD simulations were run with the 8192 mW particle system under the same conditions as in the previous section with all four LAMMPS binaries. Finally, the timing data were compared to those of the unmodified nnp-mW model, and the performance gain or loss was averaged over runs with different number of MPI tasks.
Table 8 shows the results for all variants and binaries in percent with respect to the performance of the nnp-mW model. With the results in the first line of the table, we are able to answer our first question posed. While the basic implementation would still benefit from the use of a computationally less expensive cutoff function, the advantage almost disappears when all NNP library optimizations are enabled. This is because the cutoff function calls are drastically reduced by caching and by the use of symmetry function groups. Regarding the second question about the performance loss caused by additional symmetry functions we find that the usage of symmetry function groups (binaries SFG and SFG+CFC) is clearly favorable. If symmetry function calls are executed sequentially without any reuse of intermediate results (binaries Basic and CFC), the expected maximum decrease in performance corresponds to the number of additional symmetry functions since the calculation of radial symmetry functions and the execution of other parts of the code can be neglected. In the case of 8 additional entries for the nnp-mW-η model, this amounts to 25%. Likewise for nnp-mW-ζ a maximum drop of 37.5% is to be expected. Indeed the observations presented in Table 8 closely match these expectations. On the other hand, the performance drop experienced with symmetry function groups enabled (columns SFG and SFG+CFC) is less pronounced as the corresponding numbers in Table 8 are consistently lower. This behavior is easily explained with the construction of symmetry function groups (see Section 3.2) in mind: With the given choice of parameters the additional symmetry functions can be appended to the list of symmetry function group members of the original nnp-mW model (see the Supporting Information) to form the symmetry function groups of the nnp-mW-η or nnp-mW-ζ model. Therefore, the additionally required computation steps involve only those that are not already part of calculations common to all group members, which effectively reduces the cost of the extra symmetry functions.
5. Conclusion
In this work, we have presented a neural network potential library that serves as an efficient foundation for the application of Behler-Parrinello type neural network potentials. With its data types and class structure, the library provides everything necessary to develop new standalone programs as well as interfaces to external software. As an example, we have integrated our neural network potential implementation into the molecular dynamics program LAMMPS, making the diverse molecular dynamics techniques included in this powerful package available for NNP simulations. Due to the library approach, the advantages of code optimizations within the library are automatically passed on to the interfacing software. We presented how the LAMMPS interface benefits from two optimization strategies: cutoff function caching and symmetry function groups. With both of these optimization options enabled we find that a speedup by a factor of 3 can be achieved on a modern HPC system in highly parallelized simulations.
With our LAMMPS implementation the possibility of the neural network potential method to simulate large systems over long time periods with the accuracy of the underlying ab initio potential energy surface can be fully exploited. As we demonstrated, it is feasible to run massively parallelized MD simulations of 2880 water molecules with a DFT neural network potential parametrization at rates of about 100 time steps per second. As the NNP performance is independent of the reference electronic structure method and the number of training data points, the same performance could be reached for high-level reference data from quantum chemical methods. With an appropriate time step this amounts to around 4 ns per day using 512 cores on a current HPC system.
 
Realistic Atomistic Structure of Amorphous Silicon from Machine-Learning-Driven Molecular Dynamics
Abstract: Amorphous silicon (a-Si) is a widely studied noncrystalline material, and yet the subtle details of its atomistic structure are still unclear. Here, we show that accurate structural models of a-Si can be obtained using a machine-learning-based interatomic potential. Our best a-Si network is obtained by simulated cooling from the melt at a rate of 1011 K/s (that is, on the 10 ns time scale), contains less than 2% defects, and agrees with experiments regarding excess energies, diffraction data, and 29Si NMR chemical shifts. We show that this level of quality is impossible to achieve with faster quench simulations. We then generate a 4096-atom system that correctly reproduces the magnitude of the first sharp diffraction peak (FSDP) in the structure factor, achieving the closest agreement with experiments to date. Our study demonstrates the broader impact of machine-learning potentials for elucidating structures and properties of technologically important amorphous materials.
Amorphous silicon (a-Si) is a fundamental and widely studied noncrystalline material, with applications ranging from photovoltaics and thin-film transistors to electrodes in batteries. Its atomic-scale structure is traditionally approximated in a Zachariasen-like picture with all atoms in locally “crystal-like”, tetrahedral environments, but without long-range order.  However, the real material contains a nonzero amount of coordination defects, colloquially referred to as “dangling bonds” (under-coordinated sites) and “floating bonds” (overcoordinated sites). Knowing the properties and abundance of such defects is important, as they can control electronic and other macroscopic properties. We note at the outset that, although defect sites in a-Si may be passivated by hydrogenation (to give “a-Si:H”) in some synthetic conditions, we here focus on the archetypical, hydrogen-free material as made in ion-implantation or sputter-deposition experiments.
Even the most advanced experimental approaches do not directly allow the observation of the bulk atomic structure in amorphous materials. Despite significant advances, including in situ NMR techniques and “inverse” approaches such as Reverse Monte Carlo (RMC) modeling of diffraction data, only indirect knowledge can be gained about the local atomic environments, and that only in a statistical sense. For almost three decades, molecular-dynamics (MD) simulations have therefore played a crucial and complementary role, with a-Si being a prominent example These simulations either use density-functional theory (DFT) or classical force fields. DFT-MD describes a system with quantum-mechanical accuracy and can largely correctly capture the structural and bonding subtleties of liquid and amorphous matter. However, it is computationally expensive, and therefore allows only limited system sizes (a few hundred atoms at most) and time scales to be simulated. Indeed, the cooling rates in previous DFT simulations of a-Si (≈ 1014 K/s) are orders of magnitude faster than those in experiments. Classical force fields require much less computational effort, giving access to nanometer-scale (“device-size”) structural models, both for a-Si and for multicomponent systems derived from it (see ref (25) for but one example). However, they are rarely accurate enough to fully describe the structural variations present in the amorphous state.
Capitalizing on today’s “big-data” revolution, machine-learning (ML) algorithms are increasingly used to generate interatomic potentials for atomistic simulations. By “learning from” (or rather, fitting to) quantum-mechanically computed reference data for energies and forces, ML-based interatomic potentials can enable simulations with an accuracy that is largely comparable to DFT, but with a computational cost that is orders of magnitude lower, and with linear (order-N) scaling behavior. Comparison to experimental observables is thereby the ultimate benchmark and means of validation for the quality of any ML-based interatomic potential, as we stress that no experimental but only DFT-computed data enter the “learning” process.
We believe that such ML potentials are particularly promising for disordered and amorphous materials, which must be represented by nanometer-scale structural models containing several hundreds or thousands of atoms. A landmark example has been the development of an artificial neural-network potential for the phase-change material GeTe, enabling simulation of the crystallization properties including entire nanowires. We recently introduced a ML potential for amorphous carbon, based on the Gaussian approximation potential (GAP) framework and the Smooth Overlap of Atomic Positions (SOAP) atomic similarity kernel, which captures the intricate structural, mechanical, and surface properties of the material and, more recently, has enabled accurate large-scale simulations of the growth mechanism. Very recent work using neural-network potentials allowed for the atomistic modeling of amorphous LixSi phases relevant in battery applications. Finally, such potentials were used in seminal studies to describe the complex phase transitions between polymorphs of crystalline Si.
In this Letter, we show how realistic atomistic modeling of a-Si can be enabled by a ML-based interatomic potential, again using SOAP and GAP. We first report on melt–quench simulations with cooling rates much slower (that is, better) than what can be achieved in quantum-mechanical-based simulations, and we show how this leads to a higher-quality and lower-energy structure of a-Si. Our structural models show excellent agreement with experiments probing local structure, including 29Si NMR shifts and diffraction data for high-quality samples, and open the door for future combined modeling and experimental studies on disordered and amorphous materials.
Simulated quenching from the melt is a widely used technique for generating amorphous model networks. In this, one starts with a liquid and progressively lowers the temperature, “freezing in” an amorphous structure. However, for silicon, this approach is not trivial, due to the change in local environments between the high-coordination metallic liquid and the tetrahedral-like amorphous state. We decided to perform a set of variable-volume and constant-pressure (NPT) quench simulations, in which we varied the quench rate, and thus the run-time, by several orders of magnitude. These were carried out using LAMMPS; details are in the Supporting Information. For the moment, we focus on a system size of 512 atoms in the cell and perform a single simulation at each quench rate. This system size is significantly larger than what has so far been accessible to DFT (64–216 atoms), but smaller than what is possible for empirical potentials; this will be addressed directly later on. Our fastest quench rate (1014 K/s) corresponds to early, seminal DFT studies, whereas our simulations at 1012 K/s mirror the limit of what is presently possible for DFT-quality MD. By contrast, we here use a recently developed GAP model which allows us to increase the simulation time 10-fold beyond that, namely, decreasing the quench rate to 1011 K/s, while retaining similar accuracy.
While an increase in simulation time by 1 order of magnitude may seem incremental at first sight, the full power of ML potentials becomes apparent when looking at the overall computational effort required (Figure 1a). For demonstration, we performed a brief DFT-MD simulation on a 512-atom a-Si network and use the timing information for a rough extrapolation (Supporting Information). Quenching with a rate of 1011 K/s would thus require around 16 million core hours, or current nominal costs of $185 000 on the UK national supercomputer. In contrast, the same quench rate in GAP-MD required below 40 000 core hours, equivalent to nominal costs below $500. Using GAP, it would hence be possible to decrease the quench rate even further, but given the results obtained at 1011 K/s, we subsequently chose to increase the system size instead (see below).
The slow quench rate of 1011 K/s, “unlocked” here using GAP, is indeed required to generate reliable structural models of a-Si. This is seen in Figure 1b: we took structural snapshots at various increments of the simulations, optimized them into local minima, and plotted their energy (relative to the thermodynamically stable form, diamond-type c-Si) as a function of how far the quench has progressed in time from the liquid to the final a-Si structure. The right-hand side shows the experimental sample stability with increasing annealing and thus ordering, based on calorimetry (as is common, we approximate ΔE ≅ ΔH when comparing theory and experiment). Intermediate quench rates lead to a-Si networks that are as stable as freshly deposited or partially annealed samples (ΔE ≈ 0.17–0.20 eV/atom). By contrast, our slowest quench at 1011 K/s yields a structure whose stability matches the experimental result for a well-annealed sample from ref (12) (ΔE ≈ 0.14 eV/atom). The GAP-computed bulk moduli for these a-Si networks range from 62–69 GPa and increase with slower quenching (the material becoming “harder”); the computed Young’s moduli increase from 73 to 98 GPa; see Supporting Information.
The benefit of slow quenching is further seen in two of the most common structural indicators used for amorphous solids. In a-Si, the bond angles are distributed around the ideal tetrahedral value (109.5°; Figure 1c). Fitting Gaussian distributions to these data allows us to determine the full width at half-maximum (fwhm), which decreases gradually from 30° to 22° with increasingly slower quenching. The experimental value for the bond-angle deviation of ≈11° (ref (10)) is consistent with the half width at half-maximum (HWHM) for our slowest quench. Moreover, the medium-range structural order is important in covalent amorphous networks, and we quantify it here using shortest-path ring statistics. In diamond-type c-Si, all atoms are in six-membered ring (cyclohexane-like; m = 6) configurations, whereas a-Si also contains a large number of five- and seven-membered rings, and a lesser amount of smaller and larger ones. All rings with m ≠ 6 depart from the reference crystalline state, and as such are a measure of disorder, but we here distinguish them further as follows. Five- and seven-membered rings are still expected to be energetically viable (supported by their abundance in a-Si), whereas, for example, four-membered rings will be clearly under strain. We therefore label rings with m < 5 as “small-ring defects”, and rings with m > 7 as “large-ring defects” (Figure 1d).
In Figures 2a–b, we show computed structure factors, S(Q), which can be compared to diffraction experiments. The third peak (at ≈5–7 Å–1) gradually splits into two well-defined subpeaks when moving from the 1014 K/s (yellow) to the 1011 K/s data (purple). This is qualitatively consistent with experimental observations: as-deposited samples show a fairly featureless third peak, whereas annealed ones (and also our 1011 K/s result) exhibit a clear splitting into subpeaks. Even better agreement with the experimental structure factor can be achieved for a larger structural model containing 4096 atoms, which we will show below.
We furthermore computed solid-state 29Si NMR chemical shifts, δ, for all atoms in the unit cells, thereby characterizing each atomic environment individually. We use established DFT-based algorithms and reference all δ values to tetramethylsilane (TMS), analogous to experiments. The results for the different GAP structures are shown in Figure 2c (histograms). Furthermore, due to the broad distribution of δ values in the amorphous state, we fit Gaussian profile functions to these data (lines), as detailed in the Supporting Information. We compare the output of these computations to experiments for pure a-Si prepared by sputter deposition.  The latter samples were analyzed via secondary-ion mass spectrometry (SIMS), showing no measurable oxygen contamination and ≈0.2 atom % hydrogen in the samples. This low level of impurity is thought to have little or no impact on the 29Si NMR results, enabling direct comparison to our simulations. In addition to the numerical values reported in ref (44), we fit a Gaussian profile to the experimental data for the sample annealed at 520 °C (before the onset of crystallization at higher temperature). We perform this fit using the same procedure as for our DFT data (Table 1). This yields numerical quality criteria that can be used to assess any given structural model.
Clearly, simulations using the two fastest quench rates (yellow and orange) lead to structures with very large scatter in the computed NMR shifts, as a direct consequence of their distorted atomic environments (and thus large fwhm for the Gaussian fits). The network generated at a slower quench rate, 1012 K/s (red), agrees more appreciably with experiment with regard to both the broadness and the center of mass for the Gaussian fit (δDFT = −37 ppm); the latter can be compared to δexp = −38.3 ppm for as-deposited a-Si, and δexp = −42.9 ppm for a sample annealed at 580 °C. Hence, there is a progressive shift to lower frequency in the experimental data with increasing structural ordering, and this is reproduced by our quenched structure at 1011 K/s (δDFT = −51 ppm), both qualitatively and quantitatively (to within a few ppm). The results for the 1011 K/s quench also compare well with those for a DFT-optimized Wooten–Winer–Weaire (WWW) network of a-Si (δDFT = −53 ppm; structure taken from ref (48)), while those for the faster quenches do not (Table 1).
We now place our melt–quench simulations into a wider context, as there are several different ways of modeling a-Si. First, we survey results of RMC modeling, which is an established means of extracting structural information from diffraction data. Recent work by some of us showed that reasonable restraints can improve the RMC modeling of a-Si. In particular, the SOAP similarity measure, initially developed to encode atomic structure in ML potentials, proved useful for this purpose. SOAP-RMC output, subsequently relaxed using DFT, has thus been shown to provide a high-quality structural model of a-Si. We now take the same structures but anneal them further using GAP: heating to 1100 K, holding, and cooling back to 300 K, for a total simulation time of 50 ps. This relatively short annealing is thought to be appropriate, as a recent DFT-MD study showed that annealing a quenched structure at 10 ps versus 20 ps had no appreciable effect on the outcome. We also performed the same annealing procedure for the DFT-optimized WWW model from ref (48); a somewhat similar strategy has been followed before, based on a tight-binding model and a system size of 216 atoms.  Finally, we include a state-of-the-art 216-atom structure that was carefully generated in a recent work, by slow quenching using the empirical Tersoff potential and subsequent multistep optimization using DFT (here labeled “Tsf+DFT”). 
We compare these structures in Figure 3 using three types of quality indicators. First, we report the number of coordination defects (Figure 3a), counting 3- and 5-fold bonded atoms with a bond-length cutoff of 2.85 Å. We then measure the distortion from ideal tetrahedral coordination environments: by fitting Gaussians to the angle distributions and determining their fwhm, and by using a numerical order parameter that was employed earlier for tetrahedral environments in liquid water and chalcogenide glasses  (Figure 3b). Beyond nearest-neighbor environments, we quantify the medium-range order using shortest-path ring statistics, as above, again considering as “defects” any rings with fewer than five or more than seven members (Figure 3c). In all cases, the GAP-quenched structure with the slowest quench rate (1011 K/s) exhibits very good figures of merit. Interestingly, the count of five-membered rings (m = 5) decreases continuously in progressively more ordered GAP-quenched structures, but that of seven-membered rings (m = 7) increases instead, as shown on the far right of Figure 3c.
Finally, we prepared a larger a-Si structural model containing 4096 atoms (Figure 4a), using GAP-MD and a variable quench rate between 1011 and 1013 K/s, as detailed in the Supporting Information. This system size is in reach for ML-based interatomic potentials, as they scale linearly with system size due to their finite cutoff radius (cf. Figure 1a). Having access to ab initio quality structural models on the 4 nm length scale allows us to study the medium-range order more closely. This fundamental question has been discussed in recent work on nearly hyper-uniform networks, in particular, by quantifying the inverse height (H–1) of the first sharp diffraction peak in the structure factor at around 2 Å–1. This quantity is taken as a measure for the degree of structural ordering.
We compare our structure with the current state of the art, viz., a-Si systems containing 100 000 atoms, in Figure 4b. Although the latter were generated with an improved WWW algorithm, not by slow quenching, they allow us to place our work in the context of existing ultralarge structural models. Surprisingly, the latter system size alone does not seem to be needed if the structural modeling itself is sufficiently accurate. Indeed, looking at H–1, our GAP approach outperforms the previous simulation results in much larger cells, and leads, again, to almost quantitative agreement with experiment (H–1 = 0.58 with GAP, H–1 = 0.57 in experiment; Figure 4b). By comparison, an a-Si structure of the same size (4,096 atoms) but generated using empirical potentials gave a much larger H–1 = 0.81 (ref (54)). Moreover, our slowest-quenched GAP-based system, even smaller with 512 atoms/cell, yields H–1 = 0.66, remarkably still outperforming the 100 000-atom structure from ref (14) (H–1 = 0.68). Beyond the first sharp diffraction peak alone, Figure 4b also shows that the agreement in the structure factor between the 4096-atom GAP system and experimental data at larger Q is excellent, and significantly better than for the VBSB 100 000-atom system.
In conclusion, we have shown that machine-learning-based interatomic potentials can lead to an unprecedented level of quality in the structural modeling of amorphous materials. We used a Gaussian approximation potential (GAP) to generate high-quality atomistic models of amorphous silicon, quenching from the liquid at a rate of 1011 K/s, hitherto inaccessible to DFT-quality simulations. These structural models agree convincingly with calorimetry, 29Si NMR experiments, and X-ray structure factors, including the height of the first sharp diffraction peak. We note that ML potentials are critically dependent on the quality of the quantum-mechanical input data, and as of today require significant effort to be developed in the first place; in the present case, our GAP has “seen” diverse liquid and amorphous configurations and interpolates between these. These findings will have implications for future research on disordered and amorphous materials, opening the door for quantitatively accurate atomistic modeling with direct links to experiments, for a-Si and beyond.
 
Designing exceptional gas-separation polymer membranes using machine learning
Abstract
The field of polymer membrane design is primarily based on empirical observation, which limits discovery of new materials optimized for separating a given gas pair. Instead of relying on exhaustive experimental investigations, we trained a machine learning (ML) algorithm, using a topological, path-based hash of the polymer repeating unit. We used a limited set of experimental gas permeability data for six different gases in ~700 polymeric constructs that have been measured to date to predict the gas-separation behavior of over 11,000 homopolymers not previously tested for these properties. To test the algorithm’s accuracy, we synthesized two of the most promising polymer membranes predicted by this approach and found that they exceeded the upper bound for CO2/CH4 separation performance. This ML technique, which is trained using a relatively small body of experimental data (and no simulation data), evidently represents an innovative means of exploring the vast phase space available for polymer membrane design.
INTRODUCTION
Polymer membranes are used to effect a variety of gas separations such as the removal of carbon dioxide from natural gas, oxygen from air, hydrogen recovery, and more recently in carbon capture. Separation performance is typically characterized by the membrane’s permeability (Pi), i.e., the throughput of gas type i, and selectivity (α), the purity of the output stream. Pi is defined from Fick’s law of diffusion, ∣∣Ji∣=PiΔpℓ, where Ji is the flux of gas i and ∆p is the pressure drop across a membrane of thickness ℓ. Pi is further decomposed into the product of a thermodynamic solubility constant and a diffusion constant, Pi = Di × Si. The ideal selectivity, α, between two gases is the ratio of their permeabilities: αA/B=PAPB=DADB×SASB, where DADB and SASB are the diffusivity and solubility selectivities, respectively. While there has been an increased emphasis on the use of permeabilities and selectivities when gas mixtures (rather than pure gases) are used, the data on these systems are sparse, and hence, for the purposes of this work, we discuss pure gases.
While an optimal polymer membrane for a given gas pair should have both high permeability and high selectivity, these quantities are typically observed to be negatively correlated. This concept is demonstrated in a “Robeson plot” for a variety of polymers and gas pairs. The Robeson plot for CO2/CH4 separations is shown in Fig. 1; a multitude of other Robeson plots exist for different gas separations. These plots illustrate the empirically determined current best performance for a given separation as defined by the upper bound correlation (lines in Fig. 1  Note that the upper bound evolves with time as scientists invent new materials so that while the slope of this line is apparently unchanged, the intercept increases with time. Thus, we use designations such as the 1991 upper bound or the 2008 upper bound  to designate the temporal evolution of this observed trade-off relation. The challenge, therefore, in synthesizing next-generation polymer membranes is in designing materials that cross the current upper bound. These ideas have motivated the discovery of new classes of polymeric materials, e.g., thermally rearranged (TR) polymers and polymers of intrinsic microporosity with improved performance over conventional polymers.
Synthesizing and testing the vast number of possible polymer constructs and their potential chemical modifications with our currently available chemistry toolbox is an expensive and time-consuming proposition. Instead, several theoretical methods and models have been developed as a means to understand diffusion and solubility in polymeric materials, with the goal of permitting a more rational design of next-generation materials.
On the most basic level, gas permeability can be empirically predicted using group contribution methods, where polymer repeat units are decomposed into subunits and the estimated gas permeability contribution of each of these moieties is added together. This approach is only sensitive to the presence of various atoms/functional groups in a polymer backbone but does not necessarily take their connectivity into account. Further, these methods do not systematically evolve as newer classes of polymers are synthesized and measurement tools improve. Group contribution methods therefore represent a first step for predicting the gas transport properties of these polymeric materials. A more theoretically underpinned concept is that permeability, in the framework of the solution-diffusion model, can be predicted with knowledge of polymer free volume. This follows by relating the diffusion of a gas molecule of a known size with the amount of volume in the polymer that facilitates its motion. This idea has been developed to relate the “slope” of the upper bound line to the relative sizes of the gas molecules involved in a given separation (i.e., for a given Robeson plot) However, this correlation is imperfect, and there is an incomplete understanding of the underpinning free volume concept. These free volume models have also been developed for estimating gas solubility in polymers. This concept is important for glassy polymers, which are known to swell and plasticize in the presence of CO2, thereby markedly altering their gas solubility. While gas solubility in polymers can be elegantly derived from well-understood models such as Sanchez-Lacombe theory extended to the nonequilibrium polymer glass state (such as the non-equilibrium lattice fluid model), these results often contain a number of unknown parameters to describe polymer-gas interactions and the extent of glassiness. This complicates a full, predictive understanding of the underlying phenomena. Other models, such as the dual-mode sorption, have also been found to qualitatively explain trends observed for gas solubility in glassy materials, but they are often limited to specific families of polymers. It is safe to say therefore that while there is good qualitative understanding of gas transport in glassy polymers, there is hardly any scientifically grounded, predictive models in this context. This concept is underpinned experimentally by the enormous scatter in the data shown on a Robeson plot, which represents not only our empirical understanding of gas transport but also the lack of design cues that can guide the synthesis of new materials. A means of rationally designing advanced membrane materials, without resorting to empirical experimentation, thus remains an open challenge.
Here we propose a different approach, which could eventually lead to the understanding of the underlying molecular processes, i.e., machine learning (ML) In its current form, ML represents a class of statistical models that make predictions on properties based on a set of data, but without a detailed understanding of the underlying physics in these situations. These models are greatly dependent on the availability and accuracy of large sets of applicable data. Thus, when ML has been used for polymer property prediction, researchers have primarily focused on large sets of theoretically generated data (“Materials Project”). Other ML methods in the past have typically been applied to experimental datasets with less than 100 data points for any given property, which tends to limit the accuracy of the predictions of this exercise.
Our approach uses all the gas permeation data that we could find in the literature, i.e., typically 500 to 1000 polymers for each gas, to develop an ML model as outlined schematically in Fig. 2. While we have not chosen to curate these datasets to prevent user bias, this larger dataset appears to allow us to develop more reliable models. We train the ML algorithm using a training dataset (which is part of the available dataset) and test its predictions on the remaining polymers for which gas permeation data exist. This validated model can then predict the gas permeation behavior of a large body of polymers that have been synthesized to date (~11,000), but which have not been experimentally characterized in this context. Our ideas have some parallels to the group contribution methods discussed above, but with the advantage that we do not define the chemical building blocks ahead of time. Instead, we explore the polymers whose permeabilities have been measured by using a topological, path-based, fingerprinting method to describe the polymer backbone structure so that materials with previously unexplored chemistries can be easily added to the dataset as synthetic advances are made (23). Once the predictions on the 11,000 polymers have been made, we focus specifically on the polymers that are predicted to lie well above the upper bound, i.e., polymers particularly well suited for that separation but ones that have not been tested to date. We then experimentally validate the predicted values of PCO2 and PCO2/PCH4 for these previously unexplored polymers. Thus, ML appears to be a powerful method to predict (and hence design) materials that are optimal for a given application, particularly with limited sets of experimental data.
RESULTS
We compiled a literature-based database of the diffusivities, solubilities, and permeabilities for six gases—methane (CH4), carbon dioxide (CO2), helium (He), hydrogen (H2), nitrogen (N2), and oxygen (O2)—in a variety of polymers. The number of data points for each gas varied somewhat due to what was available in the literature, with a majority of the datasets having at least 500 polymers for each gas, as shown in Table 1; this represents a sizable portion of the polymers that are typically included in the most up-to-date Robeson plots. We then randomly split this dataset into one of two categories for each gas; one is used for training the ML model, while the other is initially withheld during training. The training datasets were ≈75% of our total database for each gas, which represented at least 250 polymers for each gas. We then apply the trained model to the remaining 25% of the polymers (test set) and use these data as verification of the model’s accuracy. We found that the prediction of the ML model on these test datasets typically had an R2value of 0.8 or larger, although this correlation improves as the training dataset is made larger (see Table 1).
One challenge when creating ML models for evaluating physical properties is choosing appropriate descriptors to describe the materials being studied. Our first approach only included the number of each atom type in a repeat unit. However, this was found to be an ineffective means to properly model the experimental permeation data. Instead, we choose to use a fingerprinting method where the chemical connectivity in a polymer’s repeating unit is represented numerically. Fingerprinting has a distinct advantage over traditional group contribution methods, where all of the possible building blocks must be defined a priori and remain static; fingerprinting methods are an inherently more dynamic representation because they can evolve to include materials as they are synthesized. Further, they take into account the chemical connectivity between the different units. We transformed each polymer into a binary “fingerprint” using the Daylight-like fingerprinting algorithm as implemented in RDKit. This topological-based approach analyzes the various fragments of a molecule containing a certain number of bonds and then hashes each fragment to produce a binary fingerprint that computationally represents the molecule; this is shown schematically in Fig. 2. After a polymer’s repeat unit was read into memory via a molfile, it was broken down into fragments containing between 1 and 7 units (represented for n = 1 to n = 4 in Fig. 2), and the structure was hashed into a fingerprint with 2048 bits of information to encode all of the possible connectivity pathways of the monomer. This process is repeated for each group in the molecule to generate the full fingerprint. Each bit was treated as a single feature in our model, which allows us to study the effects of various functional groups and their linkages on gas transport. Each monomer was connected to at least nine other identical repeat units to properly account for longer paths along the polymer backbone. This fingerprinting technique is the simplest representation of the polymer chemistry and structure that is sufficient to capture trends observed in the experimental data.
After training our model [which uses the Gaussian process regression (GPR) method] on each gas’s permeability dataset (see Materials and Methods), we used both cross-validation in the training set and a hold-out test set to evaluate model performance. While Table 1 includes data from relatively large train set sizes, we have systematically varied the size of this initial training set—we find that the mean squared errors only begin to decrease for train sizes larger than ~400 and that the mean square error of the model (see Supplementary Materials) decreased monotonically as this size is increased. This explains why previous efforts, which typically used 100 polymers in their ML studies, were less insightful. Our choice of large train sizes thus reflects our goal to have a more generally applicable ML-derived model. Despite the varying amount of test data for each gas, each model performed similarly well with mean squared errors on the order of 2 to 4 Barrer (1 Barrer = 1 × 10−10 cm3[STP] cm2/cm3 s cmHg; correlation curves for each gas are provided in the Supplementary Materials). Overall, we were satisfied with the test set performance and retrained the models on the full dataset to be used in predictions on new polymers never before tested. We then downloaded 11,325 molfiles from the National Institute for Materials Science (NIMS) Materials database (which represents a large repository of previously synthesized polymers) and apply the ML model to these polymers to predict their gas transport performance Only a few structures (≈1.5%) in this prediction dataset were also in our full training set, meaning that the vast majority of polymers in the NIMS database that we predict represent new gas transport data, with no known experimental data.
One of the challenges in using ML modeling for property prediction is associating these predictions with physically meaningful quantities. This is the focus of much current research. Our model, which uses a fingerprinting method, makes it difficult to point to a specific set of physical quantities that are important in the prediction of gas permeabilities, such as free volume descriptors of the polymer chain. However, by examining the higher-performing materials—those which are above the upper bound—and their common characteristics, we are able to gain insight into what physical quantities are important for enhancing gas permeability and selectivity. We can also analyze the chemical structure of these high-performance materials to discern design motifs that are expected to give the best performance. Figure 3 (A and B) shows the learned gas transport data of the polymers in the NIMS database for O2/H2 and CO2/CH4, plotted in the Robeson plot format. Representative data used for model training are also shown.
Almost all predicted selectivities/permeabilities remain just below the Robeson 2008 upper bound line for the O2/N2 and CO2/CH4 gas pairs. However, more than 100 polymers are significantly above the 2008 upper bound for the CO2/CH4 gas pair. The polymers that are above this bound have several common characteristics. Of the 11,325 polymers in the dataset, polysulfides accounted for only 7.00%; however, they made up most (53.00%) of the polymers that crossed the CO2/CH4 2008 upper bound. In addition, the percentage of polysulfones (5.30% total, 18.00% above the upper bound) and polyimides (17.65% total, 35.00% above the upper bound) have a larger share in the upper bound–breaking group. Aromatic polyethers consisted of 30.78% of the total prediction dataset, but only 21.00% of the upper bound–breaking group; similarly, polyvinyls consisted of 13.7% of the total dataset but only 1% were above the upper bound. This implies that these functional groups are typically linked to suboptimal membrane performance (additional statistical analysis of the polymer classes in the CO2/CH4 Robeson plot is shown in the Supplementary Materials). The upper bound–breaking polymers were further analyzed by creating a two-dimensional histogram for group pairs. It was found that 18.00% belonged to both the polysulfone and polyimide classes, and 17.00% belonged to both the polysulfone and polyether classes. Thus, it was observed that materials containing a sulfur group, an oxygen along the backbone, and/or nitrogen rings performed the best in this context. Thus, our models for this gas pair seem to point to physically meaningful chemistries that can be used to enhance gas separations and may be further used in the future to identify strategies that have not been experimentally studied.
We focused our attention on two polymers predicted to lie well above the upper bound for CO2/CH4 separations (SDs are from the GPR). These two polymers are identified in the NIMS database as poly[(1,3-dioxoisoindoline-2,5-diyl)sulfonyl(1,3-dioxoisoindoline-5,2-diyl)-1,4-phenyleneoxy-1,4-phenylene] (ID: P432092) and poly[(1,3-dioxoisoindoline-2,5-diyl)sulfonyl(1,3-dioxoisoindoline-5,2-diyl)-1,4-phenylenemethylene-1,4-phenylene] (ID: P432095). Their locations on the CO2/CH4 Robeson plot, as well as the structure of their repeat units, are shown in Fig. 4. Both of these polymers are polyimides containing sulfone groups; in addition, P432092 contains an aromatic ether linkage; each of these groups is highlighted during our analysis of the ML data as being related to high CO2/CH4 selectivities.
Although similar sulfur-containing polyimides have been tested for gas separations in general, CO2/CH4 selectivity has not been tested with these specific polymers. We synthesized both polymers and tested their CO2/CH4 transport performance to experimentally verify the ML data. The synthesized polymers were cast from solution into thin (≈30 μm) films via doctor-blading and tested using the well-known constant volume/variable pressure experimental technique with an upstream experimental pressure of ≈2 atm. The experimental results are plotted in relation to their predicted values in Fig. 4; the polymers exceed the 2008 Robeson upper bound for this gas pair as predicted by the ML model, and both P432092 and P432095 exhibit selectivities ~7 and 5.5 times, respectively, that of the upper bound at the same permeability value. Further, we find that the experimental and predicted data points are in relatively good agreement with each other (within the error of the prediction), indicating that the ML model may be used as a predictive tool in identifying previously unexplored polymers for gas separations.
DISCUSSION
The ML algorithm–based approach derives permeability predictions by using a detailed knowledge of the monomer structure and chemistry. We began with an approach that looked at only atoms, but found it to be insufficient; a description that includes connectivity within a monomer is found to be sufficient in terms of predicting permeability. This approach ignores all higher-order polymer descriptors such as stereoregularity, polarity, and chain length. While we find that these variables are not required to gain reasonably accurate predictions of polymer properties, more sophisticated means of representing the polymer chain that can include these nuances may further increase the accuracy of the ML model and allow us to properly hone in on these more complex design cues. However, with current fingerprinting methods, there is no logical means by which appropriate descriptors can be defined for predicting an arbitrary property. How this choice should be made remains a topic of research. We also observe that other fingerprinting tools with similar complexity may have similar accuracy as the Daylight-like fingerprinting method we used here. An open question in the ML field is choosing the proper descriptor for a set application.
Our ML approach is designed with the specific goal of quickly characterizing gas permeabilities for an extremely large set of polymers and then a posteriori correlating high-performance materials with common functional groups and bond linkages; this allows us to determine which chemistries and structures are worth experimental observation. We emphasize that we do not relate these results to a molecular understanding of a polymer property as viewed through one of the many theoretical models available, e.g., for gas transport. In a similar vein, a number of past experimental work have focused on the effect of various polymer backbone properties on either solubility or diffusion, e.g., the effect of sulfur groups on CO2 solubility and more polymer backbone stiffness on gas diffusion constants. Our approach focused specifically on predicting polymer permeabilities, as the available literature data that decomposes permeabilities into solubility and diffusion are less plentiful. The specific dependence of solubility and diffusivity on polymer structure can be potentially probed using this approach in the future provided a more complete database is available—this might allow us to probe the factors affecting solubility and diffusivity separately.
Our ML algorithm, as currently used, only tests against already synthesized polymers. A superior approach would be to include out-of-the-box polymer architectures in the algorithm and then imposing a “synthesizability” constraint as a means of selecting polymers for further study. However, practical implementation of this approach has not been determined, and it remains a topic of debate.
The approach presented above is easily amenable to an inverse design approach. Namely, we can design polymers with a desired combination of permeability and selectivity for a gas pair by using, e.g., a genetic algorithm to construct the optimal fingerprint vectors. This is ongoing work in our laboratory.
