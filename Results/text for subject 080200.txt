Abstract

Digital Twins are core enablers of smart and autonomous manufacturing systems. Although they strive to represent their physical counterpart as accurately as possible, slight model or data errors will remain. We present an algorithm to compensate for those residual errors through Reinforcement Learning (RL) and data fed back from the manufacturing system. When learning, the Digital Twin acts as teacher and safety policy to ensure minimal performance. We test the algorithm in a sheet metal assembly context, in which locators of the .xture are optimally adjusted for individual assemblies. Our results show a fast adaption and improved performance of the autonomous system. 

INTRODUCTION 

The emergence of Internet of Things, Big Data and cloud computing has given rise to the concept of the Digital Twin in manufacturing. The Digital Twin is seen as a core enabler for smart and autonomous manufacturing systems [1]. In essence, it is an ultra-realistic digital model of the product or system with bidirectional automated data exchange used for simulation, optimization and control. Irregardless of its accuracy, slight model or data errors will remain, due to limitations in modelling and data capturing. These resid-ual errors create an improvement potential. As repeatedly pointed out in the literature on Digital Twins, machine learning and Arti.cial Intelligence (AI) could realize those improvements through learning expressive nonlinear models. However, to the best of our knowledge only few publications address how that could be achieved. Jaensch et al. [2] present a generic concept for incorporating learning methods into Digital Twins. Wang et al. [3] and Sapronov et al. [4] tune parameters of the Digital Twin through machine learning. We are interested in generic algorithms that leverage and adapt Digital Twins in the context of control, though. 
In this context, we study requirements on an AI solution to compensate for remaining model or data errors. We propose an algorithm based on Reinforcement Learning to adapt the Digital Twin¡¯s control policy derived from erroneous models. To that end, we require minimal performance of the learning algorithm to address safety and quality concerns. We show its application to a case of industrial size. Our results indicate a fast realization of the improvement potentials, and overall an increased performance (see Figure 1). The main contributions of this paper are: the discussion and demonstration of using


Fig. 1. The performance of the proposed EDiT Reinforcement Learning algorithm applied to an industrial problem. Two sheet metal parts of varying geometry need to be joined through spot welding. The 12 locators of the .xture can be adjusted for each individual assembly to improve the resulting part geometry. The performance of the Digital Twin¡¯s default control policy is based on optimizing a Finite Element model and used as a baseline (black) in this plot. Our EDiT agent can not perform worse than 0.5% of the default control policy at any time, depicted as the exploration constraint (red). After an initial exploration phase, our EDiT agent improves upon the Digital Twin¡¯s default policy, leading to an enhanced overall performance. 
deep learning to enhance Digital Twins; and a novel deep Reinforcement Learning algorithm for enhancing Digital Twins. 
The paper is structured as follows: Section II elaborates on the concept of Digital Twins and the identi.ed research gap; furthermore, Reinforcement Learning and its reduced formulation of Contextual Bandits are discussed; Section III presents necessary considerations on, and our solution to, enhancing Digital Twins through RL; Section IV gives results of our experimental test case; and Section V concludes the paper. 
II. PRELIMINARIES 

A. Digital Twins 
The term Digital Twin originates from a NASA technology report, in which a Digital Twin is de.ned as: ¡°an integrated multiphysics, mulitscale simulation of a vehicle or system that uses the best available physical models, sensor updates, .eet history, etc. to mirror the life of its corresponding .ying twin [5].¡± As such, the Digital Twin is both a collection of algorithms and a data structure of high .delity. In the manufacturing context, researchers are yet to agree on a common de.nition of the concept [6]. This may be due to most publications being either conceptual work or case studies [6]. Another reason may be the broad application area of Digital Twins to all phases of the product life cycle: design, engineering, manufacturing, operation and service [7], [8]. In manufacturing, the application to production planning and control dominates the available literature [6]. We, too, focus on production planning and control in this paper. 
A number of challenges present themselves in realizing a Digital Twin. One is understanding and modelling all relevant physical phenomena, as well as exploring their potentially massive state space to uncover any undesired behaviors [9]. Another is the automatic and continuous update of models and model parameters needed over the life cycle of the system [1], [10]. A third is to identify and utilize incon-sistencies between model and real system [10]. Lastly, how deep learning methods can be employed and continuously improved in the context of Digital Twins is a key challenge that needs to be addressed [8]. 
For the remainder of this paper, we assume the Digital Twin to be a high-.delity, but ¨C due to above¡¯s challenges 
¨C ultimately inaccurate digital model, used for simulation, control and optimization, with a bidirectional, automated data exchange to the physical counterpart. We hypothesize what types of data are exchanged. If the Digital Twin contains algorithms for optimization, decision making and control, data that in.uence the state of the physical system represent the sole sensible type of data sent from the Digital Twin to the physical system. We refer to this type of data as control actions. In the opposite direction, sent data are either state updates or feedback signals. Since the Digital Twin tracks all information about the physical system, changes in the state of the system must be transmitted to the twin to keep them synchronized. Feedback signals that re.ect the goodness of control actions may be viewed as a sub-type of state updates. 
Due to the characteristics of the data link, any control method would suit the concept of Digital Twins. However, a method, capable of leveraging the comprehensive informa-tion available in the Digital Twin and capable of handling potentially nonlinear behavior of the system, is preferred. Reinforcement Learning lends itself as such method and also as a deep learning method that could address the key challenge of enhancing Digital Twins through learning. 
B. Reinforcement Learning 
Reinforcement Learning is inspired by the way humans learn. The learning agent observes the state xt ¡ÊX of the environment, decides on a control action ut ¡ÊU that alters the state of the environment, possibly receives a reward r according to some reward function R(x,u), and observes the new state xt+1 ¡ÊX of the environment. Over time, it will learn to distinguish good from bad actions. 
More formally, the underlying model is a Markov Decision Process, or MDP in short. An MDP is de.ned through the tuple X,U,R,T,¦Ã, where X is the space of states, U is the space of actions, Ris the function of rewards rt = R(xt,ut), T is the transition probability function T(xt,ut,xt+1)= p(xt+1|xt,ut), and ¦Ã ¡Ê[0,1) is the discount factor. An important property of the state is the Markov Property, meaning the state must contain all information necessary to predict the associated reward and next state. 

C. Contextual Bandits 
For many learning tasks in manufacturing, the full Rein-forcement Learning setting does not apply. When considering tasks per part basis, the problem formulation may be free of dynamics. In other words, if the system state depends on the particular part that is being processed, but, irregardless of the selected action for that part, the next state is already decided -for instance by the production schedule -, the transition probability function T(xt,ut,xt+1) is obsolete. And with that, the discount factor ¦Ã is also no longer required. This setting is referred to as a Contextual Bandit in literature. 
A Contextual Bandit is de.ned by the tuple X,U,R, where X is the space of states, U is the space of actions, R is the function of rewards rt = R(xt,ut). At each round t, the environment prepares a state xt (also called context), the learner selects and action ut and receives a reward rt. The next state xt+1 prepared by the environment is unrelated to xt and ut. The agent aims to learn learn a policy ¦Ð : X

that satis.es the optimal value function: 


In continuous control problems the policy ¦Ð maps states to probability distributions of actions: ¦Ð : X,U
¡úp(U). The value function of a particular action ut in state xt is de.ned by 
Q(xt,ut)= E [R(xt,ut)] . (2) 

D. Function Approximation 
We consider problems of industrial size with high-dimensional continuous state and action spaces. In such cases, the number of states and actions is intractable and too large for a discrete, tabular representation of the value functions V and Q, and the policy ¦Ð. To handle that problem, function approximation has been proven successful [11]. More speci.cally, we consider arti.cial neural networks for our context, due to their expressiveness. 
Neural networks a class of nonlinear function ap-proximators, consisting of M linear combinations of input variables ¦Õ1,...,¦ÕD (e.g. parameters of the state xt) and weights ¦È1,1,...,¦ÈM,D. The linear combinations are com-plemented by a nonlinear, differentiable activation function h(¡¤) 


where j =1,...,M, and ¦Èj,0 is the bias [12]. The outputs y1,...,yM of the network layer can be in turn the inputs to a subsequent layer. The neural network learns via minimizing a loss function L(¦È). The gradients of the loss function are back propagated through the network to tune its weights. 
Digital Twin 

Fig. 2. The architecture of our EDiT algorithm. The Digital Twin observes a state xt and decides on a control action dt based on its default policy ¦Ðd. Our RL algorithm EDiT observes, both, xt and dt. It decides then whether to apply dt or ut = ¦Ða(xt) to the physical system G. The system then generates a feedback signal (reward) rt and a next state xt+1 that is observed by the Digital Twin. rt is used to improve the EDiT policy ¦Ða. 
III. ALGORITHM 
Based on the reduced Reinforcement Learning problem formulation of Contextual Bandits, we propose the deep learning algorithm EDiT for Enhancing Digital Twins. While introducing the algorithm, we elaborate on key re-quirements on an AI solution aiming at compensating for model inaccuracies of the Digital Twin in control and op-timization. For ease of understanding, Figure 2 depicts the schema of the algorithm¡¯s architecture. 
A. Policy Function 
Deep Reinforcement Learning allows for continuous con-trol policies1, needed for our application case. Assuming a normally distributed policy distribution, we propose a neural network to approximate mean ¦Ì and standard deviation ¦Ò of the policy function ¦Ða, such that 

where Nis a multivariate Normal distribution in U. This can be interpreted as the network outputting deterministic values of the action (¦Ì) and adding some normally distributed exploration noise ¦Ò to it. 
For better training stability and sample ef.ciency, we use a trust region policy optimization method [13], [14]. In particular, we apply Proximal Policy Optimization [15] and update ¦Ða by minimizing the loss 
where A.t = Q.(xt,ut) .V.(xt) is the advantage function estimate, E is a small trust region parameter, and ft(¦È) is the fraction 

ft is used as a replacement of the logprobability of the policy that is used in policy gradient methods. Clipping 
1Although we present only the continuous case here, the EDiT algorithm can easily be adapted to the discrete case, in which the policy would be a discrete probability distribution. 

ft at a lower or higher bound has the effect of bounding the loss. In the minimization step, that bound limits the gradients, resulting in smaller policy updates such that the policy remains proximally in the trust region. Intuitively, this method mitigates the risk of destructive policy updates that move ft too far away from 1. For further details on this method, we refer the reader to [15]. 
B. Value Functions 
To estimate the advantage of an action under the current policy that is required for updating the policy, we need to approximate its value function V.(xt). As loss function for the value function network, we choose the mean square error 


where N is the batch size of training data, and c a small regularization constant for the L2-norm of the weights ¦È. Similarly, the loss of the advantage function A.¦È(xt,ut) is 


(8) Since the network V¦ÈV is, both, being updated as well as being used to compute the target values for A¦ÈA ,we 

maintain a copy V¦ÈVÂ¬ of the network V¦ÈV used for the target 
computations. The weights ¦ÈVÂ¬ of the copy are updated using 


a soft target update ¦ÈV¡û¦Ó¦ÈV +(1 .¦Ó)¦ÈV, where ¦Ó is a small value. This has been shown to stabilize the convergence during training [16], [17] -an important property when learning online. 
C. Safe Exploration 
In a manufacturing context, performance of the control policy at any time is paramount. When learning, the agent must explore the action space to improve its potentially sub-optimal policy. This process may lead to catastrophic errors. While this is undesirable in any real-world context, in manufacturing it causes an additional economic cost. We are thus interested in safe exploration strategies that maintain on average with high probability a given level of performance during learning. 
Garcia and Fernandez [18] identi.ed Teacher Advice as common approach to incorporate external knowledge in the exploration process to make the same safer. With the availability of the Digital Twin, have to 
default policy ¦Ðd that can be regarded as such teacher advice. The default policy ¦Ðd is the original control policy of the Digital Twin, before we apply deep learning to compensate for model inaccuracies. This default policy may be sub-optimal, but arguably superior to the agent¡¯s policy ¦Ða in the initial learning period. A problem formulation similar to Wu et al. [19] then suits our manufacturing case. Accordingly, 
de.ne a cumulative performance constraint for safe exploration: For all rounds t, the sum of rewards ru of the agent¡¯s policy ¦Ða can at most be worse by a fraction ¦Áof the sum of rewards rd of the default policy ¦Ðd, formally written as

..
D. Bayesian Neural Networks 
By themselves, neural networks do not compute con.-dence bounds on their predictions. However, Gal and Ghahra-mani [20] have shown that dropout can be used as a Bayesian approximation. Dropout refers to the technique of randomly disabling units of a neural network layer. Originally used as a method to mitigate over-.tting, it can be used to approximate a Gaussian Process. Con.dence bounds can then be derived from the statistics of a Monte Carlo sample. The intuition behind using dropout as a Bayesian approximation is that the variance ¦Ò.2 of the sample will be high in regions of low data density and lower in regions with an abundance of available data. 
We propose to apply this technique to the advantage network A to compute the required values of Q.UCB and Q.LCB. For that purpose, we sample S times from A(xt,ut) 
and compute 
Q.LCB/UCB(xt,ut)= V.(xt) .3.¦ÒS A.(xt,ut), (15) 

where ¦Ò.S is the standard deviation of the drawn samples S. 
E. Sample Ef.ciency 
In the context of manufacturing, each data sample may represent a particular physical part. Given that the num-ber of samples is limited, we wish to be ef.cient with the data and learn quickly. We store all collected data samples xt,ut,rt,¦Ða(ut|xt)in a data structure D and repeatedly retrain a random subset of D to improve sampleef.ciency.Intonhat, we take inspiration from [21] and keep demonstrations of the Digital Twin¡¯s default policy ¦Ðd separate. When sampling a minibatch from D, we ensure that a small percentage of the samples originates from those demonstrations. Through that mechanism, the default policy is constantly kept present in the learner, helping with training stability. 
F. EDiT Algorithm 
Drawing on the previous subsections, the full EDiT al-gorithm presents itself as listed in Algorithm 1. In eachroundt,EDiTobservenows the state xt, and the action dt proposed by the default policy. We then sample K-times from the policy distribution ¦Ð to build up a set of possible actions ut,1,...,ut,K for which we compute the mean action u¡¥t. This results in a sample bias for u¡¥t that allows for exploration. u¡¥t will be applied to the system as long as the lower bound on the expected reward Q.LCB(xt,u¡¥t) is within the exploration budget. Otherwise, dt will be applied. We store the resulting data and update our neural networks on a minibatch sample of the replay buffer D. 

EXPERIMENTAL RESULTS 


computes adjustments of the .xture locators based on a Finite Element Analysis; and the adjustments are applied to the physical system to improve the geometrical quality of the .nal assembly. A pre-study to using Reinforcement Learning for adapting the Digital Twin policy to the real system in this context can be found in [23]. 
Our particular test case consists of two sheet metal parts of a car body shown in Figure 3. The geometry of the parts is given by their point clouds of ¡«2.5k points each and represent our state xt. We evaluate our algorithm on 250 part instances. Each of the .xture¡¯s 12 locators are adjustable along their axis in the range [.2,2] and represent our action space U. As reward function R, we de.ne the relative perfor-mance of the agent compared to the default policy in terms of the root mean square error of the resulting assembly from the nominal assembly: rt ¡«RMSE(xt,ut)/RMSE(xt,dt). The default policy of the Digital Twin is derived from the outcome of a Genetic Algorithm on the nominal model [24]. To emulate the real system, we use a slightly perturbed version of the nominal model. 
A. Implementation Details 
We .rst transform the point cloud of the parts through a Principal Component Analysis, and choose the 128 points as new state representation that explain most of the point cloud¡¯s variance. The size of the neural networks and related parameters are given in Table 1. To enable distributed pro-cessing for Digital Twin, EDiT and system emulation, we use Apache Kafka as asynchronous publish/subscribe messaging bus. 


Fig. 3. The test case of our algorithm. Two sheet metal parts (green and yellow) are clamped into position for a subsequent spot welding operation. The 12 locators of the .xture, shown in red here, can be adjusted along their axis to improve geometry of the welded assembly. Our EDiT agent must .nd adjustments for all 12 locators simultaneously that improve upon the optimal adjustments calculated by the Digital Twin¡¯s default policy ¦Ðd for the nominal model. 

B. Results 
We test the proposed EDiT algorithm over 10k rounds and track the cumulative improvement over the twin¡¯s default policy ¦Ðd in terms of accumulated rewards. Figure 1 depicts the outcome of 10 repetitions of the experiment. Overall, we see an improvement over the default policy. In the best case, this improvement is realized just after a few rounds. In the worst case, EDiT requires up to 2k rounds of exploration until improving upon the default policy. In average, though, we see an improvement after a few hundred rounds. 
The mean reward of EDiT over all 10k rounds and 10 repetitions is about 1.0025 as listed in Table II. This corresponds to an average improvement of 0.25%.Wedo not expect EDiT¡¯s learned policy to be the optimal one, yet. The state-action space is rather large in our case, due to the 
12-dimensional actions. We expect EDiT to take many more rounds in this case, before the optimal policy is fully learned. 
We further notice a number of violations of the safety constraints (see Table II). Most notably, the per part con-straint (Eq. 13) is violated 207 times in average over 10k rounds, meaning the received reward ru was below 95% of the Digital Twin¡¯s expected performance. This is due to two inaccuracies in Q: the estimation error of inter-/extrapolating, and the approximation error of the lower con.dence bound (LCB) through dropout (refer to Subsection III-D). An al-ternative implementation of Bayesian neural networks (e.g. ensembles) might reduce the number of per part violations. The cumulative performance constraint (Eq. 9), on the other hand, is in average only twice violated and often not at all. 
V. CONCLUSIONS 
This paper has shown that Digital Twins, used for con-trol of manufacturing processes, can be adapted through Reinforcement Learning. It has also been demonstrated that Contextual Bandits, a reduced formulation of Reinforcement Learning, are suitable for applications within the manufactur-ing context. Based on that, we have introduced the learning algorithm EDiT for enhancing the control policy of Digital Twins in continuous domains. It utilizes the Digital Twin as safety policy to maintain a constraint imposed on the learners performance. EDiT combines recent safe RL and deep learning methods, such as Bayesian neural networks and Proximal Policy Optimization. 
Future research directions include extensions of the algo-rithm to improve safety and sample-ef.ciency. The behaviour of deep neural network estimates can be unpredictable while learning. Although we employ a performance constraint and Bayesian neural networks to estimate uncertainty, we see further safety guarantees needed for the application of deep Reinforcement Learning in industry. 
Abstract 

Because of their cross-functional nature in the company, enhancing Production Planning and Control (PPC) functions can lead to a global improvement of manufacturing systems. With the advent of the Industry 4.0 (I4.0), copious availability of data, high-computing power and large storage capacity have made of Machine Learning (ML) approaches an appealing solution to tackle manufacturing challenges. As such, this paper presents a state-of-the-art of ML-aided PPC (ML-PPC) done through a systematic literature review analyzing 93 recent research application articles. This study has two main objec-tives: contribute to the definition of a methodology to implement ML-PPC and propose a mapping to classify the scientific literature to identify further research perspectives. To achieve the first objective, ML techniques, tools, activities, and data sources which are required to implement a ML-PPC are reviewed. The second objective is developed through the analysis of the use cases and the addressed characteristics of the I4.0. Results suggest that 75% of the possible research domains in ML-PPC are barely explored or not addressed at all. This lack of research originates from two possible causes: firstly, scientific literature rarely considers customer, environmental, and human-in-the-loop aspects when linking ML to PPC. Secondly, recent applications seldom couple PPC to logistics as well as to design of products and processes. Finally, two key pitfalls are identified in the implementation of ML-PPC models: the complexity of using Internet of Things technologies to collect data and the difficulty of updating the ML model to adapt it to the manufacturing system changes. 
Keywords Machine learning.¡¤ Industry 4.0.¡¤ Smart manufacturing.¡¤ Production planning and control.¡¤ State-of-the-art.¡¤

Introduction 

The current manufacturing environment is characterized by high complexity, dynamic production conditions and volatile markets. Additionally, companies must offer cus-tomized products while engaging low costs and reducing the time-to-market if they want to remain competitive in a globalized world (Schuh et.al. 2017b; Carvajal Soto et.al. 2019). This situation poses tremendous challenges for manufacturers who seek to implement new technologies to meet their objectives while expecting a return on invest-ment. Several countries have developed projects that aim to help companies adapt their industries to new production technologies. For instance, Germany created Industry 4.0 (I4.0), the United States proposed the Smart Manufacturing Leadership Coalition, and China introduced the plan called China Manufacturing 2025 (Wang et.al. 2018a). This has led to significant financial support for manufacturing research; 
iFAKT France SAS, Toulouse, France 
for example in the European Union around €7 billion will be invested by 2020 in Factories of the Future (Kusiak 2017). 
Among the Industry 4.0 groups of technologies (Ruess-mann et.al. 2015), Big Data and Analytics (BDA) allows the constantly growing mass of produced data to be harnessed to generate added value. In fact, data generation in modern manufacturing has undergone explosive growth, reaching around 1000 Exabytes per year (Tao et.al. 2018). However, the potential of this data has been found to be insufficiently exploited by companies (Manns et.al. 2015; Moeuf et.al. 2018). As BDA enables the exploitation of data, the scope of this review will focus on this technology, and more spe-cifically ML applied in Production Planning and Control. 
In the context of I4.0, Production Planning and Control (PPC) can be defined as the function determining the global quantities to be produced (production plan) to satisfy the commercial plan and to meet the profitability, productivity and delivery time objectives. It also encompasses the con-trol of production process, allowing real-time synchroniza-tion of resources as well as product customization (Tony Arnold et.al. 2012; Moeuf et.al. 2018). In this review, I4.0 is considered a synonym of Smart Manufacturing, as they both refer to technological advances that value data to draw improvements in production. For example, Ruessmann et.al. (2015) proposed nine technologies for I4.0 while Kusiak (2019) suggested six, but for Smart Manufacturing. Both proposals tend to refer to similar technologies and variations depend on the authors¡¯ focus. Hence, as the PPC is a core function of manufacturing, this paper regards its improve-ment through I4.0 technologies, namely ML, which concerns BDA. Regarding ML, the definition that will be retained is the one of a computer program capable of learning from experience to improve a performance measure at a given task (Mitchell 1997). 
Classical approaches to performing PPC include analyti-cal methods and precise simulations, providing solutions that may rapidly become unfeasible in the execution phase due to the stochastic nature of the production system and uncertain-ties such as machine breakdowns, scrap rate, delayed deliv-eries, etc. Moreover, Enterprise Resource Planning (ERP) systems perform poorly at the operative level (Gyulai et.al. 2015). To tackle this issue, ML can endow the PPC with the capacity of learning from historical or real-time data to react to predictable and unpredictable events. Even though this may suggest that organizations must invest in data ware-housing to handle the mass amount of collected data, stud-ies have reported that enterprises successfully implementing data-driven solutions have experienced a payback of 10¨C70 times their investment in data warehousing (Rainer 2013). 
Having introduced the synergism between ML and PPC, this study aims to provide an analysis of its state-of-the-art through a systematic literature review. This will contribute to the definition of a methodology to implement a ML-PPC and to the proposal of a map to classify scientific literature. This paper analyzes research produced in the context of the I4.0 and is guided by five research questions: 
1. 
Which are the activities employed to perform a ML-PPC? 

2. 
Which are the techniques and tools used to implement a ML-PPC? 

3. 
Which are the currently harnessed data sources to imple-ment a ML-PPC? 

4. 
Which are the addressed use cases by the recent scien-tific literature in ML-PPC? 

5. 
Which are the characteristics of the I4.0 targeted by the recent scientific literature in ML-PPC? 


The first three questions are related to the first objective of this research. They will contribute to the definition of a methodology to implement a ML-PPC. The last two ques-tions address the second objective, as they will provide the basis to create a classification map. 
The reminder of this paper is organized as follows: sec-tion ¡°Research methodology and contribution¡± will explain the systematic literature review methodology employed to search and choose the sample of scientific articles. Addition-ally, the contribution of this paper with respect to similar studies will be briefly highlighted and a short bibliometric analysis is presented to assess the keywords used as string chains. The ¡°Analytical framework¡± section will explain the four axes encompassed by the analytical framework. After-wards, the ¡°Results¡± section will focus on the results of the systematic literature review and an analysis of it. Finally, the ¡°Conclusion and further research perspectives¡± sec-tion will conclude this study and provide further research perspectives. 


Research methodology and.contribution 

To meet the two objectives of this study, a systematic litera-ture review was carried out following the method proposed by Tranfield et.al. (2003) who extended research methods from the medical sector to the management sciences. This method has been successfully employed by other authors to draw insights from the scientific literature (Garengo et.al. 2005; Moeuf et.al. 2018). This literature review focuses exclusively on applications of ML in PPC in the context of I4.0. 
In another domain, Zhong et.al. (2016), proposed a bibli-ometric analysis of big data applications on different sectors such as healthcare, supply chain, finance, etc. but its focus on manufacturing was limited. (Kusiak 2017; Tao et.al. 2018; Wang et.al. 2018a) provided a literature analysis of data-driven smart manufacturing, citing representative references. 
However, these references were not chosen through a sys-tematic literature review. Finally, (Sharp et.al. 2018) could be considered as a study close to this paper as the authors used a pre-defined methodology to select the articles to ana-lyze. Nevertheless, they employed Natural Language Pro-cessing (NLP) to analyze around 4000 unique articles and provide insights about the scientific literature of ML applied in I4.0. The use of NLP can be useful to identify important trends, but it does not allow the authors to analyze the detail of the reviewed papers, where it is likely to find interesting research gaps and insights. On the other hand, a systematic review allows the authors to both follow a rigorous method-ology and perform a detailed study of each chosen article. 
Even though the PPC is closely related to the domain of supply chain, the latter is not included in the scope of this review as its vastness would increase the risk of stray-ing from the focus on PPC. Therefore, to learn about recent trends on this topic, the authors invite readers to refer to Hosseini et.al. (2019), who performed a comprehensive review of quantitative methods, technologies, definitions, and key drivers of supply chain resilience. In fact, supply chain resilience is a growing research area that examines the ability of a supply chain to respond to disruptive events (Hosseini et.al. 2019). Applications of this topic have been done by Hosseini and Barker (2016), who applied Bayesian networks to perform supplier selection based on primary, green, and resilience criteria; and Hosseini and Ivanov (2019), who proposed a method using Bayesian networks to assess the resilience of suppliers in identifying critical links in a supply network. 
The queries were performed between 10/10/2018 and 24/03/2019 in two scientific databases: ScienceDirect and SCOPUS. The following keywords conducted the research: 
. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction scheduling¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction planning¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction control¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Line balancing¡±) 


To consider the context of I4.0, only papers published since 2011 were considered because this year corresponds to the formal introduction of I4.0 at the Hannover Fair. Addition-ally, only communications labeled as ¡°Research Articles¡± in ScienceDirect and ¡°Conference paper¡± OR ¡°Article¡± in SCOPUS were included to solely capture articles present-ing application models. Subsequently, a review of titles and abstracts allowed for the exclusion of articles not related to ML-PPC. After the removal of duplicates, a full text analysis allowed a final selection that excluded papers that did not fit with research questions. The sample size obtained encom-passes 93 scientific papers. The article selection methodol-ogy with its Restrictions (R) is described in Fig..1. 

A brief focus on.the.query keywords 

The used string chains represent a core strategic choice for review. Therefore, this sub section aims to provide an analy-sis of the employed keywords. 
Concerning the keywords used in the first parenthesis of the string chains, the use of ¡°Deep Learning¡± and ¡°Machine Learning¡± was done for two reasons: firstly, they are rela-tively new terms, which eases the identification of recent trends in the literature; and secondly, they are directly related to one of the two core subjects in this study, which is ML. Other terms such as ¡°Data Mining¡± or ¡°Statistical Learn-ing¡± could have been sensible choices too, as they are often used interchangeably with ¡°Machine Learning¡± and ¡°Deep Learning¡±. Nevertheless, using these two terms might have deviated this study from its core topic. In fact, a recent study suggests that the differences between ML and Data Min-ing are not consistently defined in the literature. Thus, Data Mining is mostly considered to be the process of generat-ing useful knowledge from data (Schuh et.al. 2019). To do so, it draws from other fields such as Artificial Intelligence, Statistics, ML, and Data Analytics. Therefore, Data Mining can be a vast topic, and does not exclusively concern ML, which could potentially affect the focus of this study. As there seems to be no clear boundary between these terms, a short bibliometric analysis was performed to assess the chosen keywords. The analysis was done using VOSviewer, software developed by the University of Leiden to draw insights from scientific literature. Furthermore, using key-words related to specific ML techniques such as ¡°Random Forest¡± or ¡°k-means¡± did not seem appropriate due to the risk of introducing a bias when answering the second research question. In fact, this could have artificially boosted the results of the queried techniques. 
The bibliometric analysis followed a similar methodology to that used to choose the final article sample (cf. Fig..1). The objective was to briefly assess the influence of differ-ent keywords on the queries¡¯ results. For the analysis, three different string chains were considered: ¡°Deep Learning¡± OR ¡°Machine Learning¡±, ¡°Data Mining¡±, and ¡°Statistical Learning¡±. The queries were performed on 06/10/2019 and the detail of the search strategy can be found in ¡°Appendix I¡±. Finally, as the aim is to analyze the available literature when querying with a certain string chain, no Title and abstract review was done as this could introduce a bias into the results due to the authors¡¯ influence. 
The bibliometric analysis focused on the keywords defined by the authors for all of the papers of each of the three samples. To represent the results, the network visualization from VOSviewer was employed. In such a net-work, the nodes represent the keywords or items, their sizes represent the keyword importance determined by the number of occurrences, and the links between the nodes represent their co-occurrence. Furthermore, the relatedness between two terms is represented through their spatial distance in the network: two keywords closely related will be spatially closer. For this review, the obtained networks were displayed under the ¡°overlay visualization,¡± which shows the average publication year for each of the keywords through a color scale. For clarity reasons, a filter was applied on the mini-mum number of occurrences to display; at most, 50 items per graph. Also, the queried keywords were highlighted with a red frame to assist in their identification. The networks are presented on Figs..2, 3, 4. 


Results from the bibliometric study suggest that ¡°Statisti-cal Learning¡± may not be a common keyword to find in ML-PPC research because the size of the obtained article sample (241 articles) is far below the results obtained with the other two queries. In fact, ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± and ¡°Data Mining¡± provided 2862 and 2166 articles, respectively (cf. ¡°Appendix I¡±). This is also stated in all of the networks, in which the item ¡°Statistical Learning¡± does not appear, probably due to the filter excluding keywords with a low number of occurrences. 
Analyzing the relatedness between ¡°Data Mining¡± and ¡°Machine Learning¡± by their spatial distance on the net-works provides an idea of how these concepts are associ-ated: they are spatially closer on the ¡°Data Mining¡± Network (Fig..3) than on the ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± network (Fig..2). This suggests that Data Mining tends to relate more often to ML, rather than ML to Data Mining. Such a relation may support what is said in (Schuh et.al. 2019), in which Data Mining is considered a field drawing from ML, Artificial Intelligence, Statistics, etc. to produce useful insights. 
Findings from the network visualizations show that the item ¡°Machine Learning¡± is always associated with a more recent average publication year than ¡°Data Mining¡±. This supports the idea that ¡°Machine Learning¡± is a relatively new term, which can lead to the identification of recent trends in literature. Furthermore, querying with ¡°Deep 


Fig. 2 Network visualization with the average publication year for ¡°Deep Learning¡± OR ¡°Machine Learning¡± 

Fig. 3 Network visualization with the average publication year for ¡°Data Mining¡± Fig. 4 Network visualization with the average publication year for ¡°Statistical Learning¡± 

Learning¡± OR ¡°Machine Learning¡± provides a more recent average publication year (2017.06) for the item ¡°Machine Learning¡± than with the other two queries: 2016.95 when querying with ¡°Data Mining¡± and 2016.41 when querying with ¡°Statistical Learning¡±. Finally, ¡°Deep Learning¡± OR ¡°Machine Learning¡± was the only query enabling the inclu-sion of the item ¡°Deep Learning¡± with enough occurrences 
(25) to pass the filter, which is a recent research topic with an average publication year of 2018.28. 
From the bibliometric analysis, it could be concluded that using ¡°Deep Learning¡± OR ¡°Machine Learning¡± as part of the query keywords is appropriate enough, as this allows for the identification of a big sample of recent papers, enabling the identification of new trends. It seems that ¡°Statistical Learning¡± does not provide enough recent results to be con-sidered. Finally, even if ¡°Data Mining¡± is closely related to ¡°Machine Learning,¡± it covers a vast domain that can deviate from the focus of this review. 
Regarding the keywords employed in the second paren-thesis of the string chains, the objective was to represent the main functions of the PPC under the definition provided in the introduction section. Consequently, a determination of the global production quantities was represented by ¡°Produc-tion Planning¡± and the aspect of the main objectives (i.e. profitability, productivity, and delivery time) was depicted by ¡°Production Control¡±. Finally, the real time synchroni-zation of resources as well as product customizations were represented by both ¡°Production Scheduling¡± and ¡°Line Balancing,¡± given the fact that companies should be able to perform balanced scheduling even when facing customized client orders. 
As the PPC is a transverse topic tangled with other func-tions such as maintenance, quality control, logistics, etc., the challenge was to decide whether or not these related subjects should be included as explicit keywords for the queries. The final choice was to not include them through keywords, as this would broaden the perimeter of the research too much, losing a focus on PPC. Nevertheless, it was decided to include, in the final article sample, the studies dealing with other functions only if they were related to the PPC. 

METHOD

Analytical framework 
This section presents the four axes that build the analytical framework that will be employed to harness knowledge and insight from the final sample of 93 scientific articles. 
First axis of.the.analytical framework: the.elements of.a.method 
This axis concerns the first and second research questions: the activities, techniques, and tools to implement a ML-PPC model. To link these three elements, the concept of ¡°Man-datory Elements of a Method¡± (MEM) proposed by Zellner (2011) is used. In fact, this concept has been successfully employed by other authors to propose methodologies in research domains such as product development (Lemieux et.al. 2015) and lean in hospitals (Curatolo et.al. 2014). Moreover, (Talhi et.al. 2017) suggested its use to develop a methodology in the context of cloud manufacturing applied to product lifecycle management. Thus, the MEM suits the first objective of this study, which concerns the definition of a methodology to implement a ML-PPC. There are five elements in the MEM: 
1. 
A procedure: order of activities to be followed when the method is employed. 

2. 
Techniques: the means to generate the results. Activities from the procedure are supported by techniques, while the latter is supported by tools. 

3. 
Results: they correspond to the output of an activity. 

4. 
Role: the point of view adopted by the person who per-forms the activity and is responsible for it. 

5. 
Information model: this refers to the relation between the first four mandatory elements. 


In the scope of this study, the first two elements are the con-cern. Firstly, to evaluate the procedure, the activities used to perform a ML-PPC implementation will be recognized and their use will be measured. By activities, this research refers to tasks such as ¡°model comparison and selection¡± or ¡°data cleaning¡±. Secondly, to address the techniques, ML models and tools will be identified, and their use will be measured. ML models point to elements such as Support Vector Machines or Neural Networks, while tools relate to programming languages or software used to implement these ML models. 
To provide further insight concerning the ML techniques, the learning types will also be measured. This will be used to summarize the information regarding the techniques as well as to ease the identification of trends and research perspec-tives. Additionally, the learning types will serve as a bridge between the first and second objectives of this study, as they will be used in the mapping to classify scientific literature. Based on the work of Jordan and Mitchell (2015), three main learning types can be identified: 
1. Supervised Learning (SL), which concerns ML tech-niques approximating a function f (X)= Y by learning the relationship between the inputs X and the outputs 
Y. For instance, learning the mapping between the Red Green, and Blue (RGB) codes (input X) in an image and the objects in it (output Y) to determine if a certain picture contains a misplaced product in a stock rack. 
2. Unsupervised Learning (UL), which encompasses tech-niques allowing data exploration to find patterns and hidden structures in a given dataset X. For instance, finding categories in maintenance reports by using the description of the problem and the duration of the main-tenance intervention. 
3. Reinforcement Learning (RL), which are techniques allowing the learning of actions to be performed by an agent interacting with a certain environment to maxi-mize a reward. For example, teaching an Automated Guided Vehicle (AGV) in a warehouse how to avoid obstacles to maximize the number of delivered pack-ages. 

Second axis of.the.analytical framework: employed data sources 
This axis addresses the third research question: the har-nessed data sources. Identifying which are the data sources used to perform a ML-PPC is capital. In fact, data could be considered as the raw material allowing ML models to develop autonomous computer knowledge gain (Sharp et.al. 2018). Moreover, the quality of the final model will depend to a great extent on the quality and appropriateness of the used data. Therefore, the choice of the data source is an important decision when training a ML model. To address this axis of the analytical framework, the data source types proposed by Tao et.al. (2018) will be used. They mention that there are five main data sources used in the data-driven smart manufacturing: 
1. 
Management data (M): historical data coming from com-pany¡¯s information systems such as the ERP, Manufac-turing Execution System (MES), Customer Relationship Management system (CRM), etc. M data will concern production planning, maintenance, logistics, customer information, etc. 

2. 
Equipment data (E): data coming from Internet of Things (IoT) technologies implemented in the factory. It refers to sensors installed in physical resources such as machines, places such as workstations or human resources such as workers. In the case of workers, data is collected passively, such as by RFID sensors installed on helmets. 

3. 
User data (U): consumer information collected from e-commerce platforms, social media, etc. It also encom-passes feedback given by workers or experts that will be used to train the ML-PPC model. User data coming from workers is collected actively, for example through interviews or questionnaires. 

4. 
Product data (P): data originating from products or ser-vices either during the production process or from the final consumer. 

5. 
Public data (Pb): data available in public databases from universities, governments or from other researchers. 


The analysis of the 93 shortlisted articles suggested that some of them did not fit into the five data sources proposed by Tao et.al. (2018): these communications used artificially generated data through computer simulations. Therefore, a sixth data source is proposed, which corresponds to the first contribution of this paper to the scientific literature: 
6. Artificial data (A): data generated by computers (e.g. simulations) to assess ML-PPC implementations. 

Third axis of.the.analytical framework: the.use cases of.the.ML.PPC in.the.I4.0 
This axis concerns the fourth question: it aims to show which applications can be achieved when applying a ML-PPC. Moreover, identifying the use cases and quantifying their use frequency is important to detect trends as well as further research gaps. By use cases, this study refers to the different possible applications in a certain domain, such as maintenance, quality control, distribution, etc. In fact, as the PPC is entwined with several manufacturing subjects, is difficult to perform a complete review on PPC if these topics are ignored. For example, if there were a predictive maintenance study meant to enable a more robust produc-tion scheduling, such application would be directly related to the PPC through maintenance. To start this analysis, the use cases of I4.0 initially proposed by Tao et.al. (2018) were considered. They identified six of them: 
1. 
Smart Maintenance: harnessing data to perform preven-tive and predictive maintenance. For instance, monitor-ing machine components to estimate the best date to perform a maintenance intervention. 

2. 
Quality Control: applying BDA to supervise the manu-facturing process or products, seeking for possible qual-ity problems and/or allowing the identification of root causes. 

3. 
Process Control and Monitoring: constantly analyzing data coming from the shop floor to perform a smart adjustment of the functioning parameters of physical resources (machines, AGVs, etc.). The objective is to automatically control these physical resources and/or optimize their parameters with respect to the working conditions. 

4. 
Inventory and Distribution Control: stock management, parts and tools tracking, and distribution control with the use real-time and/or historical data. 

5. 
Smart Planning and Scheduling: considering produc-tion uncertainties to perform a production planning and scheduling closer to the current state of the production system. For instance, considering unexpected mainte-nance problems to reschedule a production order and minimize the delay. 


6. 
Smart Design of Products and Processes: using BDA to support new products and processes development. For instance, using NLP to analyze the technical require-ments of a new product and then to propose the poten-tially suitable manufacturing process. 

The analysis of the 93 scientific articles suggests that these six use cases are not enough to fully characterize the recent publications. Additionally, papers not fitting in the initially proposed use cases shared the same application: time esti-mation (cycle time, operation time, etc.). Consequently, a seventh use case is proposed: 

7. 
Time estimation: adaptation of different manufactur-ing related times to current working conditions. For instance, estimating the operation times to the actual work rate of each employee instead of using the data from the Method Time Measurement (MTM) approach. 



Fourth axis of.the.analytical framework: the.characteristics of.I4.0 
The I4.0 aims to transform the collected data during the product¡¯s lifecycle into ¡°intelligence¡± to enhance the manu-facturing process (Tao et.al. 2018). With this transforma-tion, the objective is to reduce costs while improving the quality, productivity, sustainability of the production system (Wang et.al. 2018a). However, what specific benefits could be expected when embracing the I4.0? To answer this ques-tion, the characteristics of I4.0 need to be identified. Tao et.al. (2018) argue that I4.0 enables the following paradigms: 
1. 
Customer-Centric Product Development: production systems in the I4.0 should be able to adjust their param-eters by considering variables coming from customers such as their behavior, their needs, the way they use the products, inter alia. It is the case of manufacturing per-sonalized products, designing processes from the cus-tomer requirements or proposing a target manufacturing cost for each consumer profile. 

2. 
Self-Organization of Resources: I4.0 should endow pro-duction systems with the capacity of considering data coming from the manufacturing process to better engage the available resources. Additionally, this data should also be used to plan capital and operational expendi-tures. For example, updating the scheduling of machines the shop floor after new urgent order is released. 

3. 
Self-Execution of Resources and Processes: in the I4.0, resources should become ¡°smart¡± by providing them a real-time awareness and interaction capacity with the manufacturing environment (Huang et.al. 2019). There-fore, the self-execution of resources concerns their fac-ulty of making decisions depending on the received 


information or measured data. It is the case of machines automatically adapting their functioning parameters to work optimally or trolleys automatically replenishing workstations when these reach a certain level of security stock. 
4. 
Self-Regulation of the Production Process: unexpected events should be effectively handled in the I4.0. Thus, this characteristic concerns the capability to perform the required adjustments to respond to unpredicted prob-lems. For example, relaunching the scheduling process for a certain production line when one of the machines experienced a breakdown. 

5. 
Self-Learning of the Production Process: this charac-teristic follows a similar logic as the self-regulation of processes in terms of adjustability. However, it relates to the capacity of the production system to adapt to pre-dicted events. It is the case of predictive maintenance, which uses BDA to estimate the remaining useful life of machine¡¯s components. Afterwards, the manufacturing system can adapt to the results of this prediction. 

After concluding the analysis of the 93 articles, three char-acteristics seem to be overlooked: the environmental dimen-sion, the knowledge generation, and the inclusion of the human being. To consider these dimensions that seem to not be explicitly raised in the work of Tao et.al. (2018), three new characteristics are proposed: 

6. 
Environment-Centric Processes: estimations suggest that the electronics and home appliances industry scrapped around 100 million goods in China in 2012 (Tian et.al. 2013). As exemplified, the environmental impact of industry is far from being negligible, which is the reason why industrialized countries have started to tighten regu-lations and engage environmentally friendly practices in manufacturing (Tuncel et.al. 2014). Research done in the context of I4.0 must not overlook this aspect. Therefore, this characteristic concerns the use of new technologies to create environment-centric processes. For example, optimizing the disassembly scheduling process to maxi-mize the number of components that can be recycled. 

7. 
Knowledge Discovery and Generation: most of the com-panies have been computerized for a long time, which has eased the collection of data. Despite the access to a plethora of information systems, generating knowledge from raw data still supposes a major industrial and aca-demic challenge. Besides, the generation of knowledge is a mandatory step to improve the adoption of BDA by companies (Grabot 2018). In fact, knowledge could be considered as one of the most valuable assets in manu-facturing (Harding et.al. 2006), the reason why generat-
ing it represents an important gain behind the adoption of BDA. Therefore, as I4.0 is characterized by allowing 


knowledge creation, research efforts must include it to generate value. One example of this is harnessing data from maintenance reports to provide the production of responsible real-time information about the root causes of machine breakdowns. 
8. Smart Human Interaction: even with the advent of multi-ple I4.0 technologies, its adoption would be significantly hindered by not keeping humans in the loop or not con-sidering their interaction with the proposed solutions. For instance, Thomas et.al. (2018a) experienced the case of a company that was not willing to introduce an improved version of a quality control system because it somehow excluded the person from the process. There-fore, this characteristic concerns the consideration and/ or inclusion of a human being when implementing new technologies. Examples of this would be a worker behavior recognition system based on computer vision or software interacting with operators through NLP. 
Figure 5 summarizes this section. It also presents the relationship between the Research Questions (RQ), the analytical framework axes, the research objectives, and the expected outputs of this study. 



RESULTS

First research question: activities employed in.ML.PPC 
To identify the activities, the tasks used to implement a ML-PPC in each of the 93 communications were identified. Afterwards, these tasks were grouped into categories to ease the information analysis. These groups of activities were analyzed by two experts to keep the most meaningful ones. Results suggest eleven standard and recurrent activities: 
1. 
Data Acquisition system design and integration (DA): design and implementation of IoT systems to collect data. This activity also encompasses the data storage and communication protocols. 

2. 
Data Exploration (DE): use of data visualization tech-niques, inferential statistics, and others to derive initial insights and conclusions about the dataset. 

3. 
Data Cleaning and formatting (DC): data preparation from the raw data to make it exploitable by the ML-PPC model. It concerns tasks such as outlier removal or missing values handling. 

4. 
Feature Selection (FS): choice of the most suitable inputs to the ML-PPC model. It can be done through statistical techniques, e.g. stepwise regression or by means of expert insight. 




Fig. 5 Relationship between the building blocks, research objectives and expected outputs of this study 
5. 
Feature Extraction (FE): use of variables from the ini-tial dataset to calculate more meaningful features. 

6. 
Feature Transformation (FT): representation of the initial features into different spaces or scales using techniques such as normalization, standardization or kernel transformations. 

7. 
Hyperparameter Tuning and architecture design (HT): definition of the ML model architecture and adjustment of its hyperparameters to improve the performance. For instance, optimizing the learning rate and defining the activation function in a neural network. 

8. 
Model Training, validation, testing, and assessment (MT): using the data to perform the training, validation and testing process. It can be done through techniques such as k-fold cross-validation. It also encompasses the choice of the training/validation/testing set split and the model¡¯s performance assessment. 

9. 
Model Comparison and selection (MC): several ML techniques can be used to achieve a certain task. This activity concerns the comparison of multiple ML mod-els to choose the one that better suits the needs. 

10. 
Contextualized Analysis or application (CA): going further than just assessing the model¡¯s performance. It concerns the actual implementation of the ML-PPC model or the analysis of its results in the context of the problem that is addressed by the study. 


11. Model Update (MU): data used to train ML models represents the context of the studied environment at a given moment. However, this context is dynamic, hence the ML-PPC model must be adapted. Therefore, this task concerns the model update through new data. 
To address this research question, the percentage of papers using each activity was measured. These results are summa-rized in Fig..6. Findings suggests that four groups of activi-ties can be proposed following their usage: 


These groups show that a considerable amount of research papers only focus on the architecture design, training, and assessment of ML-PPC models (CUAs cluster), while not employing or documenting the use of other activities. Con-sidering OUAs, it is surprising to find that only half of the communications used the CA, which corresponds to an actual implementation of the proposed model in the context of the study. This suggests that half of the studies go no further than just training and evaluating the performance of the model. 
MUAs group encompasses data pre-processing tasks, which are capital to any ML implementation. Even if these activities are frequently employed in practice, their low usage is probably because researchers do not mention them, implying a lack of documentation. Moreover, as one of the characteristics of big data is the variety (in type, nature, for-mat, etc.) (Zhou et.al. 2017), it is crucial to employ data pre-processing activities to ensure the quality of the final models. Consequently, this lack of documentation can repre-sent a pitfall to practitioners willing to apply ML-PPC based on research papers. 
Finally, SUAs cluster highlights the most important research gaps in scientific literature. Three key findings can be inferred from activities in this group: firstly, the low usage of DA highlights the challenge of coupling IoT technolo-gies with ML-PPC. This is a major pitfall to deploy ML-PPC in companies, as they normally need real-time data or statuses from their manufacturing systems. Secondly, the lack of DE utilization could mean that ML-PPC applica-tions tend to jump directly to activities in the CUAs cluster while overlooking descriptive and basic inferential statis-tics techniques. This represents an obstacle to generating knowledge from data, as DE can draw conclusions easily interpretable by non-ML specialists. Finally, the rare use of MU implies that adapting the ML-PPC model to a dynamic manufacturing context is seldom addressed. This unpredict-able change of the statistical properties and relationships between variables over time is known as concept drift (Ham-mami et.al. 2017). Not addressing this issue can be harmful for the model reliability in the long term. 


Second research question: techniques and.tools used in.ML.PPC 
Concerning the techniques, results present the number of times a given ML model is used. In the case of communica-tions comparing several techniques, only the one chosen by the authors because of its better performance was consid-ered. If this best-performing model employs several tech-niques, each of them is counted as used once. 
There are numerous ML techniques in scientific litera-ture. Therefore, to ease the analysis of results, a grouping of techniques in families is proposed is Table.1. These families were determined with the help of a ML expert. It is impor-tant to mention that the column ¡°Concerned techniques¡± in Table 1 is not an exhaustive list, it is limited to techniques found in the systematic literature review. 
Results are presented in Fig..7. They suggest that NN, Q-Learning, and DT are the most used techniques in ML-PPC. The extensive use of NN is probably due to their abil-ity to learn complex non-linear relationships between vari-ables, often delivering good performance when compared to other techniques. Even if Q-Learning remains, by far, the most used RL technique, other RL models such as Sarsa or R-Learning are used, which points an interest in agent-based modeling in ML-PPC. Finally, the attention drawn by DT techniques is probably linked to their excellent trade-off between accuracy and interpretability, allowing knowledge generation. 
The high use of Clustering techniques could be explained by the fact that data in manufacturing systems is normally unlabeled and can contain meaningful unknown patterns. Therefore, clustering can be employed to discover groups as well as hidden structures in datasets. 
The usage evolution of the six most used technique fam-ilies was also measured. Figures representing this can be found in ¡°Appendix II¡±. Due to an imbalance in the amount of articles over the different years, results are presented as relative frequencies. For example, if the NN achieved a usage of 27% in 2018, it means that 27% of all the techniques used in that year corresponded to such a model. Results sug-gest that there is a strong growth in the use of NN since 2015, this is possibly due to the growing computing power, recent findings in terms of architectures such as CNNs or LSTMs, and the development of specialized frameworks like PyTorch, TensorFlow, Keras, etc. which ease the task of implementing such models. Moreover, results show a grow-ing interest on Ensemble learning techniques which evolved from not being used between 2011 and 2013 to accounting for 14% of applications in 2018. This can possibly explain the loss of interest on DT since 2017, as Random forests (a type of Ensemble learning) can achieve better performance by using committees of decision trees. 
Table 1 Technique families with their respective ML models 
Family Concerned techniques 
As NN and Ensemble learning families seem to be recently attracting the research community, a detailed view of their encompassed techniques is presented in ¡°Appen-
dix III¡±. Concerning NN, the most used technique is the Multi-layer perceptron, which is the classic architecture of a NN. However, more specialized architectures belonging to deep learning are starting to appear in PPC research. Such is the case of the CNNs, LSTMs, and Deep Belief Networks. 
These techniques have presented good performance when dealing with specific problems, such as image recognition for CNNs, time series analysis for LSTMs or feature extrac-tion for Deep Belief Networks. In the case of Ensemble learning, the most used technique is, by far, the Random forests. They seem to provide excellent results while ena-bling knowledge generation. In fact, they allow the most meaningful variables to be easily identified in the SL task, which is the reason why researchers tend to use them to both attain accuracy and model interpretability. 
To measure the utilization of the learning types, each paper was analyzed, and the learning types used were iden-tified and counted. As a given model can use several ML techniques, it can refer to several learning types at the same time. Hence, the different synergies between these were also considered. Results are presented in Fig..8. 
Findings show that the most used learning type is SL. This is probably because SL addresses two recurrent needs in applied research: classification and regression. In fact, SL can be used to learn the relationship between an input X and an output Y that can be either discrete in the case of classification or continuous for regression. Furthermore, it was found that RL techniques are extensively used, which confirms the interest behind agent-based models. 
Concerning UL, it seems to be especially used with SL (SL-UL), which suggests a strong synergy between these 
Learning type 

52 Number of uses 
Fig. 8 Number of uses by learning type 
C++ WEKA Java Python RapidMiner R Others MATLAB Not Mentioned 

41 Number of uses 
Fig. 9 Number of uses by tool 
two learning types. The reason behind this could be that UL techniques are normally used to perform data pre-process-ing, as with Principal Component Analysis, or discovery of hidden patterns in datasets, e.g. with Clustering. There are 6 papers using just UL, however, this learning type seems to unlock all of its potential when used in synergies, allowing for the design of more complex models. 
Even if there are some SL-RL synergies, they are not very common. This is probably because SL is normally coupled with RL when there is a need of performing rapid estimations of functions to save computing time. However, it seems that most of the applications do not reach a scale that needs this kind of configuration. Finally, it was found that using UL-RL and SL-UL-RL is rare in the scientific litera-ture. This does not mean that their synergy does not provide advantages, it is just that there may not be a current need for it. Also, it could be that coupling these learning types over-complexifies the model design, which prevents its use. 
Concerning the tools, only programming languages or software used to implement the ML model were considered. Therefore, other tools such as discrete event simulation soft-ware are out of the scope of this research. Results are pre-sented in Fig..9. 
For clarity sake, tools being used only once were grouped in the category denominated as ¡°Others¡±. These tools were: ACE Datamining System, C#, Clementine, GeNIe Modeler, Hugin 8.1, NetLogo, Neural-SIM, Visual C++, and Xelopes Library. Additionally, it is important to mention that most of the researchers do not mention the tool the use to imple-ment the models. 
It could be said that MATLAB is, by far, the most used tool to perform ML-PPC in research. Besides its robust cal-culation capacity, the reason behind this could be that uni-versities often invest in licenses for this software; therefore, they expect their researchers to use this tool. R is the second most used tool, which may be because it is a free software targeting statistical applications, including ML. Finally, the third most used tools are both RapidMiner and Python. The former eases the implementation of ML models thanks to its visual programming logic, while the latter is a multipur-pose programming language recently characterized by its ML libraries and frameworks such as Scikit-learn, PyToch, Keras, etc. 


Third research question: used data sources to.implement a.ML.PPC 
To answer this question, the data sources used by each of the analyzed papers were identified. These results are sum-marized in Table.2. The column ¡°Identification¡± (ID) will assign a number to each communication. This will be used later to establish a mapping of the scientific literature. 
Results show that ¡°Artificial data¡± is the most used data source in recent scientific literature. This probably highlights the difficulty of accessing data coming from companies. Additionally, it is important to remember the extensive use of RL techniques. These models normally require constant access to data concerning the real-time status of the produc-tion system, which can be difficult to find in real factories. Therefore, researchers normally use Artificial or Public data to test their models. This issue could be addressed by creating digital twins, but this still represents a research challenge. 
The extensive use of artificial data suggests that there are data availability issues. This poses two main challenges: firstly, dealing with highly unbalanced datasets when train-ing, for instance, SL algorithms for classification, and sec-ondly, accessing enough data to enable good generalization capacity, especially in deep learning models. 
The first challenge is common when training ML mod-els to identify disruptions. In fact, disruptive events in PPC such as machine breakdowns or quality problems tend to be scarce when compared to the total size of the dataset. Thus, ML techniques struggle to learn these events. To tackle this issue, some authors have proposed solutions such as data augmentation, a common practice in computer vision that consists of artificially creating new training examples by modifying existent observations (Perez and Wang 2017; Miko.ajczyk and Grochowski 2018). Another approach is to use crafted algorithms adapted to class-imbalance. Bi and Zhang (2018) performed a comprehensive comparison of 
Table 2 Data sources used by 



Fourth research question: addressed use cases by.recent scientific literature 
To answer this question, each analyzed article was allo-cated to one of the seven proposed types of use cases. This allows to measure their importance in the scientific literature (Fig..10). 
Results point out that Smart Planning and Scheduling is the most addressed use case in recent scientific literature, with nearly half of the communications discussing it. This result may come from two main reasons: firstly, the string chains used in the methodology are closely related to this use case; secondly, it normally uses structured data relatively easy to get from information systems, which eases the task of implementing a data-driven approach. The strong use of Time Estimation in ML-PPC (14% of the papers) suggests that classical time measurement methods are not compliant with the growing complexity of the manufacturing systems, which may represent a pitfall to perform a reliable planning. Therefore, ML models considering more diverse variables


as inputs are being adopted. Moreover, some researchers have addressed the coupling of Smart Maintenance, Process Control and Monitoring, and Quality Control with the PPC. However, there is still effort to be made, as the share of these use cases was no higher than 10%. 
Finally, two use cases are targeted as critical: The Inven-tory and Distribution Control (6%) and the Smart Design of Products and Processes (4%). These findings suggest two things: first, a lack of integration of the logistic functions into the ML-PPC, and secondly, a difficulty for harnessing insights from data to serve product and process design. This difficulty is probably because data employed in design is highly unstructured (text data, image data, etc.) and greatly depends on people¡¯s experience. 


Fifth research question: the.characteristics of.I4.0 
To quantify their usage, the addressed characteristics in each of the 93 analyzed papers were identified and counted. Results are summarized in Fig..11. In this figure, the sum of all the totals is higher than 93 as one ML-PPC model can satisfy several characteristics. 
Findings show that the Self-Organization of Resources is, by far, the most addressed characteristic (56 uses) in ML-PPC applications. This result was expected, as this charac-teristic can be achieved through production planning and scheduling, two functions directly related to the PPC and found to be extensively employed in the use cases. There-fore, it can be concluded that the ML-PPC based models effectively enable this characteristic. 

Smart Human Interaction 
Knowledge Discovery and Generation Environment-Centric Processes Self-Learning of the Production Process 
Self-Regulation of the Production Process Self-Execution of Resources and Processes Self-Organization of Resources Customer-Centric Product Development 
Number of papers 

papers), as well as the Knowledge Discovery and Genera-tion (26 papers) appear to be moderately boarded. This leads to two main conclusions: first, ML-PPC models effectively endow manufacturing systems with the capacity of adapting to unexpected events and predicting production problems. This is suitable to handle the stochastic nature of production environments. Secondly, ML is suitable to generate knowl-edge from PPC data, which is crucial in I4.0, where data is abundant, and it can provide useful guidelines to improve the company¡¯s know-how. 
Four characteristics were rarely satisfied: The Customer-Centric Product Development (3 papers), the Self-Execution of Resources and Processes (4 papers), the Smart Human Interaction (7 papers), and the Environment-Centric Pro-cesses (8 papers), which points to strong research perspec-tives of ML-PPC applications enabling these features. Con-cerning the Customer-Centric Product Development, it was rare to find papers including customer-related variables into their PPC. This can be due to the difficulty to access data from customers or end users. For instance, as observed in the data sources section, user data was seldom employed. 
The low number of papers dealing with Self-Execution of Resources and Processes suggests that it is unusual to couple the PPC with autonomous physical resources. This can be due to the complexity of such systems as they require important capital investments as well as multi-disciplinary knowledge in production systems, mechatronics, and control theory. 
It was very surprising to find that the Smart Human Interaction (7 papers) and the Environment-Centric Pro-cesses (8 papers) are rarely addressed. Indeed, manufactur-ing systems can be human based in several steps such as during the execution in the shop floor or during the tactical planning definition. Not considering the interaction of the proposed ML-PPC models with humans can be harmful for the deployment of the proposed system, as it may worsen the working conditions. Therefore, thinking about this human-ML interaction is the cornerstone for a successful adoption. Concerning the Environment-Centric Processes, scarce applications tried to minimize the environmental impact of production processes through ML-PPC. In a world where natural resources are becoming rare, this is a non-negligible aspect that must be considered, not only because of the tightening of environmental laws by gov-ernments but also because of the ethical responsibility of companies. 


Cross.axes analysis: mapping the.scientific literature through.use cases, I4.0 characteristics, and.learning types 
To address the second objective of this study, a mapping of the scientific literature in ML-PPC is proposed. This is achieved through a cross-analysis employing the use cases, characteristics of I4.0, and learning types. Results are repre-sented via a cross-matrix having the use cases in the vertical axis and the characteristics of I4.0 in the horizontal axis. This matrix also allows the maturity of a given use case to be assessed. For instance, a mature use case in the scientific literature will tend to satisfy more I4.0 characteristics. From this point of view, the crossing between a characteristic of I4.0 and a use case will be referred as a domain. 
The ID numbers defined on Table.2 are employed to place the analyzed articles in the matrix. Additionally, the learn-ing types employed by each communication are represented using a color code. Figure.12 provides a summarized view of this matrix, allowing for a high-level analysis that will help to identify research gaps and trends in ML-PPC. Figure.13 is 



a detailed view of the matrix indicating the scientific articles with their respective learning types found in each domain. 
Figure.12 shows that among the 56 possible domains, 18 (32%) were not addressed at all. Furthermore, 24 (43%) domains lie in the range of 1 to 3 papers. This means that nearly half of the domains are in an exploration phase. These two remarks lead to conclude that ML-PPC in the I4.0 is still an active research topic with strong perspectives. 
From Fig. 13, it can be said that there is a strong trend of using multiple synergies between learning types across all of the different use cases. However, there are no applications of RL in Time Estimation and in Smart Design of Products and Processes. The reason for this may be that these use cases have strong strategic impacts. Therefore, current ML implementations in such applications aim to support deci-sions rather than automating them such as with agent-based systems driven by RL. 
There are two use cases achieving a high maturity: Smart Planning and Scheduling and Process Control and Moni-toring. They both cover all but one of the characteristics of I4.0. In the case of Smart Planning and Scheduling, it fails to address the Self-Execution of Resources and Pro-cesses, which suggests that there are research perspectives in coupling the production planning and scheduling with autonomous physical resources. For the Process Control and Monitoring, there is a lack of applications satisfying the Customer-Centric Product Development, which would be an automatic optimization of physical resources from the analysis of customer-related variables. 
Knowledge Discovery and Generation is the only char-acteristic addressed by all the use cases, which denotes an intense interest in knowledge creation from data. Further-more, there is a strong presence of SL, UL, and SL-UL in this characteristic. This implies an important affinity between these learning types and the generation of useful information from raw data. Following a similar trend, there seems to be a generalized interest in Environment-Centric Processes, a characteristic that is addressed by almost all of the use cases. However, its low number of papers implies that there are strong research avenues to be explored. 
Communications addressing the Self-Execution of Resources and Processes focused exclusively on Process Control and Monitoring applications. This shows that the dynamic optimization of working parameters of the machines allows data-driven intelligent resources to be created. However, this characteristic has further potential to be explored in PPC research with other use cases, such as in Inventory and Distribution Control with autonomous AGVs to serve logistic needs or in quality, by automating processes. 

CONCLUSIONS

Conclusion and.further research perspectives 
This state-of-the-art analysis studied 93 research articles chosen through the logic of a systematic literature review. These papers were analyzed by means of an analytical framework composed of four axes. First, the elements of a method were reviewed, which enabled an analysis of activi-ties, techniques, and tools to perform a ML-PPC. Secondly, the data sources employed to implement a ML-PPC model were recognized and assessed. Thirdly, an analysis of the use cases enabled the recognition of the applications of data-driven models in the 4.0. Fourthly, the characteristics of I4.0 were identified and assessed through their usage. Additionally, a mapping of the scientific literature was pro-posed by means of the use cases, characteristics of I4.0 and ML learning types. 
Results concerning the activities allowed the recogni-tion of eleven recurrent tasks that are employed to create a ML-PPC model. They were grouped in four clusters follow-ing their use percentage: CUAs (Commonly Used Activi-ties), OUAs (Often Used Activities), MUAs (Medium Use Activities), and SUAs (Seldom Use Activities). From these clusters, it can be concluded that activities belonging to the CUAs and OUAs clusters are well documented in the scien-tific literature. MUAs activities mainly contain data pre-pro-cessing tasks, which are necessary but not commonly docu-mented by researchers. Finally, the SUAs cluster suggests that there are three activities rarely addressed in literature: the design and implementation of data acquisition methods from the manufacturing system, the exploration of data to get insights, and the constant adaptation of the proposed ML-PPC model to the environment dynamics. 
An extensive review of the techniques identified the most used families in scientific literature. These were found to be the NN, Q-Learning, DT, Clustering, Regression, and Ensemble learning. From these results, a temporal evolu-tion analysis of the top 6 most used families was performed. Findings suggested a growing interest in NN and Ensemble learning, which motivated a focused study on the detailed techniques encompassed by these families. Concerning the NN, the Multi-layer perceptron was the most used technique. Nevertheless, more specialized deep learning techniques such as CNNs, LSTMs, and Deep Belief Networks are starting to be employed. With respect to Ensemble learn-ing, the most used technique was Random forests. 
The ML learning types were also reviewed. Findings showed that scientific literature mainly focused on the individual use of SL and RL. However, synergies between learning types are also employed. For instance, the most used synergy was SL-UL, which allows to explore and pre-process the data through UL to improve the SL training. The UL-RL and SL-UL-RL synergies had only one use each, which could be considered as a research gap, advising improvements in its integration. In fact, each learning type has its advantages and limitations. Hence, it is important to explore more synergy possibilities, as they may help over-come individual limits. 
Other than increasing data availability, one option to encourage the utilization of UL-RL and SL-UL-RL is to boost the development of specialized libraries to build complex models coupling several learning types. Examples of this are deep learning frameworks such as TensorFlow, Keras, PyTorch, etc. which have eased the implementation of deep learning applications. This has allowed researchers to spend more time on the addressed problem than on the coding stage. 
Results concerning the tools showed that MATLAB, R, Python, and RapidMiner are the most used tools in develop-ing ML-PPC models in research. However, most authors did not mention the tool used, which is a limit of this study. Fur-thermore, it is important to mention that these results come from a sample of scientific articles, meaning that results are mainly valid in an academic context. If there are practition-ers willing to implement ML-PPC models in companies, other aspects need to be analyzed such as the cost of the software, its scalability, skill availability in the labor market, compatibility with existing information systems, etc. 
The current horizon of data sources used is dominated by Artificial and Management data. The former points to a difficulty in collecting all of the data required to implement ML-PPC models, while the latter suggest that companies are interested in valuing their data stored in information sys-tems. Data coming from IoT sources such as Equipment and Product data was moderately used, nevertheless showing an interest in these technologies to collect data. Finally, ML-PPC models failed to integrate User data, probably because it is complex to collect and it engages an important respon-sibility concerning data privacy. 
The most addressed use cases were Smart Planning and Scheduling and Time Estimation, probably because they are directly concerned by the PPC, which may lead to its high utilization. The fact that there are research articles in all of the use cases suggests that the PPC is a transversal func-tion that benefits from several applications. Therefore, when designing a ML-PPC system for a company, the impact on all of the use cases must be assessed. Finally, it was found that Inventory and Distribution Control, as well as Smart Design of Products and Processes, are seldom addressed. This suggests that there is still a lot of progress to be made when coupling the PPC to logistics as well as product and process design through ML. 
Concerning the characteristics of I4.0, results suggest that scientific literature in ML-PPC is extremely focused on satisfying the Self-Organization of Resources, which was expected, as one of the main goals of the PPC is resource management to satisfy the commercial plan. At a second level, the Self-Regulation of the Production Process, the Self-Learning of the Production Process, and the Knowl-edge Discovery and Generation seem to be more frequently addressed. However, Fig..13 showed that they are mainly employed for Smart Planning and Scheduling, implying a lack of research in the other applications. Finally, there are three characteristics that are partially overlooked by researchers: Environment-Centric Processes, Smart Human Interaction, and Customer-Centric Product Development. The first two are essential characteristics of building more responsible production systems as they aim to include human beings and reduce the environmental impact of manufactur-ing processes. The latter relates to the alignment of the PPC to the customer¡¯s needs. Hence, it appears that recent ML-PPC research ignores the influence of the customer in the manufacturing process. 
As illustrated in the proposed cross-matrix, 75% of the possible research domains are barely addressed or were not explored at all. This means that the ML-PPC is still a key topic for the enablement of I4.0, which presents strong research avenues. The main future research perspectives could be summarized in the following three key items: 
1. Reinforce the role of IoT in ML-PPC: this would allow an improvement to the data acquisition system¡¯s design and would provide a means to perform a model update to tackle the concept drift issue. To do so, the ML mindset and workflow should be shifted from a linear to a cir-cular process, considering the need to constantly retrain through new data. This way of thinking would enable the identification, from an early development stage, of the retraining policy and the necessary variables that could be measured again at a sensitive cost. By defining these two aspects, the data acquisition system design will be less complex to conceive, as the needs will be clearer. 
This would avoid investment in sensors and resources and architecture that would not be exploited. Concern-ing the retraining policy, a review in the context of PPC reporting common practices, advantages and pitfalls seems to be missing in the scientific literature. 
2. 
Improve the integration between the PPC, logistics, and design: it was stated that the PPC benefits from different use cases. However, recent literature seems to overlook logistics as well as product and process design applica-tions coupled with the PPC. To tackle this challenge, it is necessary enable data availability, continuity and sharing over the design, logistics, and production depart-ments. This could be achieved through interoperability as well as communication of intra-organizational sys-tems such as the PLM, ERP, and MES. Even if projects that are meant to couple such systems are costly, they are necessary to ensure data availability and quality. One way to achieve this is the use of data lakes, which have been recognized as suitable to handle big data reposito-ries of a structured and unstructured nature (Llave 2018; Lo Giudice et.al. 2019). For instance, Llave (2018) con-cluded, through expert interviews, that one of the key purposes of data lakes is to serve as experimentation platforms for data scientists. 

3. 
Set human interaction and environmental aspect as pri-orities to ensure the development of ethical manufactur-ing in I4.0: exploring the interaction of humans with the proposed ML-PPC models is paramount to build-ing inclusive technologies at the service of society. To achieve this, the short-and long-term impact of ML-PPC systems on employees¡¯ working conditions must be assessed. If the system degrades them, it must be redesigned. Concerning the second aspect, seeking a reduction in the environmental impact of manufactur-ing through ML could provide important developments. This can be addressed from a purely PPC approach by optimizing, for instance, the scheduling of disassembly processes or by improving the prediction of production times to avoid energy waste. Other approaches could be the optimization of the supply chain. Even though the supply chain was not covered in this review, it is an appropriate domain for researchers to implement ML applications. For instance, by considering environmental criteria when choosing suppliers, as in (Hosseini and Barker 2016). 


Some of the research gaps indicated in this review could motivate future work. Future work will be focused on the following aspects: 
1. 
The proposed activities will be reviewed to determine an order between them, creating a procedure: this would help shift from a linear to a circular workflow when implementing ML-PPC models. 

2. 
The most suitable techniques and tools will be linked to each of the activities with sectorial information: linking techniques, tools, and activities is the key to creating good practices that could be helpful to new practitioners, both in research and industry. Furthermore, according to Kusiak (2017, 2019), there are profound differences in the volume of data generation and usage across different industries. Therefore, future work will aim to identify trends categorized by sectorial information. 

3. 
The current state of data availability solutions and work-arounds will be explored: as data availability was found to be a main issue, a review of techniques to tackle the class-imbalance problem and the use of transfer learning in the context of PPC will be performed. Additionally, the utilization of data lakes for ML-PPC will also be explored. 

4. 
Future research avenues will be proposed through an NLP analysis: NLP may enable the discovery of non-trivial trends present in the corpus of the 93 sampled articles. This will complement the results of the system-atic literature review. 

Introduction 

Global recession over the last years changed the overview on the industrial sector, now looking at the real value-added that it creates. Companies that followed the trend to relocate activities by looking for low cost labor, are now committed to recover their competitiveness. 
German manufacturing strategy played a key role on this shift-ing, launching initiatives to maintaining and promoting its impor-tance as a ＆＆forerunner§ in the industrial sector [1]. The buzz word ＆＆Industry 4.0§ has been presented and with it big promises arose to face the latest challenges in manufacturing systems. The impeller Industry 4.0 (I4.0) is enabling and reinforcing this trend using its technologies, changing the way of living, creating new business models and new ways of manufacturing, renewing the industry for the so-called digital transformation. 
In 2011, the German government have brought into the world a new heading called Industrie 4.0 (I4.0), assumed as the fourth indus-trial revolution [2每6]. I4.0 aim is to work with a higher level of automatization achieving a higher level of operational productivity and ef.ciency [3,7],connecting the physical to the virtual world[8每 9].It will bring computerization and inter-connection into the tra-ditional industry [3]. According to several authors [3,5每6], I4.0 can be assumed as Cyber-Physical Systems (CPS) production, based on heterogeneous data and knowledge integration and it can be summed up as an interoperable manufacturing process, integrated, adapted, optimized, service-oriented which is correlated with algo-rithms, Big Data (BD) and high technologies such as the Internet of Things (IoT) and Services (IoS), Industrial Automation, Cybersecu-rity (CS), Cloud Computing (CC) or Intelligent Robotics [3,7,9]. From the production approach, Martin and Sch.ffer [8] de.ne I4.0 as the intelligent.owofthe workpieces machine-by-machineinafactory, ona real-time communication between machines.On this environ-ment, I4.0 will make manufacturing become smart and adaptive using .exible and collaborative systems to solve problems and make the best decisions [7].It bringsa good development for the industrial scenario focusing on creating smart products, smart pro-cesses and smart procedures [5]. Companies expected to increase the level of digitalization, working together in digital ecosystems with customers and suppliers [10]. 
Since I4.0 boom, the research community has experienced differ-entapproachestoI4.0 concept;however,thegeneralsocietymaybe confused basedonthelackof understandingonthis area. Thereisa needfor clari.cationofI4.0 related conceptsand technologies. 
This paper deals with the research of I4.0 in manufacturing environments on a literature review over the enabling technolo-gies, focusing on the state-of-the-art and future trends. The approach of I4.0 for manufacturing systems in this paper is based on the Smart Factory (SF) concept. The SF concept makes use of components such as IoT, IoS, the systems integration and Cyber-Physical Production System (CPPS) that is formed by several linked CPS (CPS may use up until nine key enabling technologies, widely assumed by research community). 
The paper is structured as follows: section 2 presents the Refence Architecture Model Industrie 4.0 (RAMI4.0) as the guid-ance for the I4.0 technologies implementation, section3 presents key enabling technologies of I4.0, section4reviews the Smart Fac-tory (SF) concept of the I4.0 structured with its components, and the .nal remarks are in section5 which introduces the summary and gives future outlooks. 
2. 
Reference Model of I4.0 
Several German associations and institutions cooperated on the creation of the reference model for I4.0. This 3D model in Fig.1 is the development of a shared language and a structured framework [11每12] that describes the fundamental bases of I4.0. It is intended to assist on the I4.0 technologies implementation [13]. 
Unlabelled image
The Reference Architecture Model Industrie 4.0 (RAMI4.0) should enable to identify the existing standards and among it, identify and close the gaps, loopholes and identify the overlaps [14]. 
On the left horizontal axis from the IEC 62,890 standard, facili-ties and product lifecycle with the correspondent value stream are showed [15]. RAMI4.0 clearly describes the difference between instance and type. When the design and prototyping is completed, the type becomes an instance, ready for production [14]. 
The hierarchy levels from the IEC 62264 standard are showed in the right axis, representing the different grouped entities by func-tional properties, de.ned to represent all hierarchical levels of the enterprise, from the ＆＆Product§ (e.g., a workpiece) to the ＆＆Connect World§ level. The ＆＆Connect World§ is the last stage of the I4.0 development enterprise environment using IoT and IoS to connect enterprises, customers and suppliers [13每14]. The hierarchy levels are discussed further insidetheSFin section4throughthe Fig.25. 
The layers on the vertical axis represent a reminder to integrate all aspects on the enterprise digitalization [11]. The functionallay-ers of the organized vertical axis describe: 
. 
＆＆Asset Layer§ represents reality, for instance, physical compo-nents including linear axes, robots, conveyor belts, PLC＊s, metal parts, documents, archives also persons that form a part of con-nection to the virtual world via the ＆＆Integration Layer§ [12,14每 15]. Also, non-physical objects such as software or ideas; 

. 
＆＆Integration Layer§ provides processed information for the dig-itization of the assets. Elements connect to Information Tech-nologies (IT) such as sensors, Radio Frequency IDenti.cation (RFID) readers, integration of Human-Machine Interface (HMI) and computer-aided controls the technical processes [12,14]. Persons via HMI also participate on this layer. In the virtual domain, each signi.cant event is mirrored through the enabler [12]; 

. 
＆＆Communication Layer§ with the function of communication standardization. It makes use of uniform data format and prede-.ned protocols, providing services for the ＆＆Integration Layer§ [12,14每15]; 

. 
＆＆Information Layer§ to process and integrate consistently the different available data into useful information [14]. Also receives and transforms events to match the data which are available for the next layer [15]; 

. 
＆＆Functional Layer§ to enable formal descriptions of functions. It creates an horizontal integration platform of several functions that can be with remote access, resulting of the necessity of data 

. ＆＆Business Layer§ enables mapping of the business model and links between different business models. It ensures, within the value stream, the integrity of the functions [14每15]. 
It＊s possible to map all crucial aspects of I4.0, allowing the clas-si.cation accordingto the model,of objects such as machines. This model allows the step-by-step migration from the actual to the future manufacturing environments [13]. 
The I4.0 essential technological elements are compiled at the .rst time as RAMI4.0 and it is registered in Germany in the DIN SPEC 91345 standard [14]. 
3. The Key Technologies of I4.0 
I4.0 is characterized on manufacturing and services by highly developed automation and digitalization processes, electronics and IT [3]. From the production and service management perspec-tive, I4.0 focus on establish intelligent and communicative systems such as Machine-to-Machine and Human-Machine Interaction, dealing with the data .ow from intelligent and distributed system interaction [16]. Among other features, I4.0 promotes autonomous interoperability, agility, .exibility, decision-making, ef.ciency or cost reductions [17]. 
The I4.0 implementation should be interdisciplinary in a closely between different key areas. Several authors [5,18每19] described nine pillars (also called the building blocks) of the I4.0 framework as follows in the subsections.A fundamental key point to achieve the integration of I4.0 framework is the human contribution that will be improved with the development of professional skills of the stakeholders. 
3.1. The Industrial Internet of Things 
On the IT, the IoT is the connection oftwo words i.e. ＆＆internet§ and ＆＆things§. ＆＆Internet§ as the network of the networks. A global system serving users worldwide with interconnected computer networks using Standard Internet Protocol suit (TCP/IP). As individ-ually distinguishable by the real world, the ＆＆things§ can be any-thing like an object or a person [20]. Today, IoT is widely used for instance, in transportation, healthcare or utilities [21]. Thing-to-Thing, Thing-to-Human and Human-to-Human form a network inside IoT, connected to the internet. Individually identi.able objects exchange information inside this network. [22每23]. 
IoT has been increase with the advancement of mobile devices. IoT can be achieved with connected RFID, Wireless Sensor Net-works (WSN), middleware, CC, IoT application software and Soft-ware De.ned Networking (SDN) as the key enabling technologies [23]. Fig.2 presents the associated technologiesin IoT. 
One simple de.nition of IoT described by Sezer et al. [21] is: ＆＆IoT allows people and things to be connected anytime, anyplace, with 
Unlabelled image
anything and anyone, ideally using any path/network and any ser-vice§. In other words, Bortolini et al. [24] de.ned IoT as an ubiqui-tous presence for a common purpose of various things or objects interacting and cooperating each other, digitalizing all physical systems. For different aims, the digitalized information can be used to adjust production patterns, with the use of a virtual copy of the physical world and using sensor data [7]. The entire production systems such as machinery and related resources can be the ＆＆things§ managed and virtualized by I4.0 [4,7]. In addition, the IoT nature as to be decentralized and heterogeneous [25]. 
Regarding to IoT design architecture, Trappey et al. [26] estab-lished a logical framework by layers to classify IoT technology and used to characterize and identify CPS. According to several authors [25,27每28], IoT architecture most common layering in a typical network, includes four main layers as represented in the Fig.3 as follows: 
1) ＆＆Sensing Layer§ to sense the ＆＆things§ status with a unique identity and to integrate, e.g., actuators, sensors, RFID tags as several types of ＆＆things§; 
2) ＆＆Network Layer§ to support the transferred information through wired or wireless network from the ＆＆Sensing Layer§ to ＆＆Service Layer§, being the support＊s infrastructure. This layer determines and maps ＆＆things§ automatically in the network enabling to connect all ＆＆things§ for sharing and exchange data; 
3) ＆＆Service Layer§ makes use of a middleware technology sup-porting services and applications, required by the users or applications. The interoperability among the heterogeneous devices is ensured by this layer, performing useful services, e.g., information search engines and communication, data storage, exchanging and management of data as well as the ontology database; 
4) ＆＆Interface Layer§ to make the interconnection and manage-ment of the ＆＆things§ easier and to display information allow-ing a clear and comprehensible interaction of the user with the system. 
Differing from IoT based users, regarding to industrialenviron-ments needing real-time data availability and high reliability [29], the Industrial InternetofThings(IIoT)isthe connectionof industrial products such as components and/or machines to the internet. For instance, linkingthe collected sensingdatainafactorywithIoTplat-form, IIoT increases production ef.ciency with the BD analysis [22]. 
AtypicalIIoTis showedinFig.4,withwireand wireless connec-tions, increasing value with additional monitoring, analysis and optimization. 
As a natural evolution of IoT, the IoS can be seen as the connec-tivity and interaction of the things creating valuable services and is one of the fundamental basis of the SF. IoS is discussed further in section 4. 

3.2. Cloud Computing 
Cloud Computing (CC) is an alternative technology for compa-nies who intent to invest in IT outsourcing resources [30]. Assante et al. [31] characterized CC for Small and Medium Enterprises (SMEs) as a resource pooling with rapid elasticity and measured service, on-demand self-service and broad network access. The adoption of CC has several advantages related to cost reduction, e.g., the direct and indirect costs on the removal of IT infrastructure in the organization, the resource rationalization service by the dynamically scalable users consuming only the computing resources they actually use or portability when using any type of device connected to the internet such as mobile phones or tablets accessing from any world location [30].Bythis, the cloud can have 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 3. Generic Service-oriented Architecture (SoA) for IoT [25]. 
Unlabelled image
Unlabelled image
Fig. 4. Typical IIoT network [137]. 
any of the four types of access: public (usually on a data center location, managed by vendors and available for all public [32]), pri-vate (same organization location and offering special bene.ts [32]), hybrid (combination of public and private clouds [32])and com-munity (shared by multi organizations and supported by a speci.c sharing of interests and concerns community [33]). Everything is treated as a service in CC. These services de.ne a layered system or types of service models structured for CC as in Fig. 5 and the management overviewis shownin Fig.6,as follows [31,33每34]: 
. Infrastructure as a Service (IaaS) is where cloud service provi-ders supply users with fundamentalcomputingresources, with virtual infrastructures, e.g., virtual servers, networks or storage and where users into the cloud can deploy and run arbitrary software, which can include, for instance, operating systems applications; 
. 
Platform as a Service (PaaS) is where users develop and run applications using programming languages on the cloud infras-tructures. Therefore, it can be achieved scalability, high speed server and storage. Users can build, run and deploy their own applications with the use of remote IT platforms. On this layer, there is no concern on the resource＊s availability and mainte-nance [35]; 

. 
Software as a Service (SaaS) is where applications reside and runs in a cloud infrastructure [34]. Accessible from various cli-ent devices through an interface such as a web browser and programs. The focus is to eliminate the service applications on local devices of individual user, achieving an high ef.ciency and performance for the users. This category enables software applications such as Computer-Aided-Design (CAD) software and Enterprise Resource Planning (ERP) software, with a lower total cost of ownership [35]. 



V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

All underlying Everything as a Service (XaaS) layers allows direct interactions with the user interface layer at the top. 
On manufacturing environments, Cloud Manufacturing (CMfg) concept was proposed to make use of CC technology, in order to improve the current manufacturing systems [36]. Cloud-related manufacturing has two approaches: 
1) CC in manufacturing industry as a manufacturing version of CC -using cloud applications in manufacturing industry directly, web-based manufacturing applications or computer-aided are examples of possible deployments in the CC system. These applications are implemented at two service levels of CC system, matching SaaS and PaaS levels [37]; 
2) CMfg systems as an entire new type of cloud service, based on Service-orientedArchitecture (SoA) in the cloud environ-ment that provides manufacturing capabilities [36].It re.ects the IaaS level on CC system [37]. 
With the combination of advanced technologies, it arises a new computing and service-oriented manufacturing mode as CMfg [38]. A solution such as CMfg enables users to request services fromall stagesofa product lifecycle rangingfrom design, manufac-turing, management and so on [38每39].Bythis meaning, the main characteristics of CMfg is the service-oriented approach [40] and its trend on shifting manufacturing approach from production-oriented to service-oriented [33,41].Abrief CMfg model is shown in Fig.7, consisting on three categories of stakeholders: providers, operators and consumers, with their cooperation to maintain sus-tainable operation of a CMfg system [42每43]: 
. 
Providers 每 own and provide the abilities and the manufactur-ing resources [43].Within the entire product lifecycle, for shar-ing purposes, providers publish manufacturing resources to the CMfg platform and also receive manufacturing tasks from the cloud platform. Everything is transformed into services, under the exclusive management ofthe operator [42]; 

. 
Operator/s -to operate CMfg platform and to deliver services to providers, consumers and even third parties [43]. In an on-demand manner, consumers from the cloud platform can achieve high-quality and sustainable manufacturing services. Providers have permission to publish their resources and capa-bilities with the use of tools provided by the cloud platform [42]; 

. 
Consumers -to subscribe the manufacturing computing ser-vices availability in a CMfg service platform [43]. Under the exclusive management of the operator, consumers, including enterprises consumers and individual consumers, submit their 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
Unlabelled image
Fig. 7. CMfg model [42]. 
requirement tasks to the CMfg platform, e.g., design, manufac-turing, test or simulation tasks and also receive the execution results of their orders [42]. 
CMfg is a manufacturing paradigm based on Knowledge. In the running process, the knowledge plays as the central role [44], e.g., models, standards, protocols, rules and algorithms as knowledge, indispensable in many process and activities within entire lifecycle services as service generation, service management and service applications [42]. 
The concept of CMfg makes use of CC, BD, IoT, CPS, the net-worked manufacturing, service-oriented manufacturing, virtual manufacturing and virtual enterprise [45每46]. Cooperation can be enabled and supported by CMfg, sharing and management of manufacturing resources such as fabrication capabilities, equip-ment, applications, software tools, know-how, etc., of companies [47每48] and these companies can be included into the cloud, becoming accessible to potential consumers, in a pay-as-you-go manner [39]. CMfg enables the recommendation and execution, intelligent mapping and search of a service [33]. CMfg can provide in a form of service scalable, .exible and cost-effective solutions with lower maintenance costs and supports. Manufacturing tasks can be obtained also as services into the CMfg service platform [41]. Cloud data center owns the computational resources and the different organizations, e.g., manufacturing enterprises, owns the manufacturing resources [45]. There is no need for manufac-turers and users to invest in high-tech computers, computer licenses or worrying about software updates or upgrades [48]. Mai et al. [46] in Fig. 8 discussed a CMfg platform integrating resources and services related to 3D printing, including, e.g., design, 3D printers, assembly, simulation, models, software, etc. It is important to consider model library management and the online-device integration on the construction of the 3D printing service CMfg platform, due to the close relation between 3D print-ing and 3D models. 
Usually with a short budget for the initial investment, lack of experience and related technical support, SMEs are seeking novel technologies such as cloud technologies. According to Wang et al. [49], SMEs needs high level of safety and security regarding their customer＊s requirements, i.e., all data and results as to be maintain within the boundaries of the own company. These facts indicate that public or community cloud services probably are not suitable in this scenario. To ful.ll this need, Wang et al. [49] proposed a CMfg system tailored to meet the requirements of SMEs, consider-ing a hybrid cloud structure. Within this, the sensitive data stays inside a private cloud, with integrated and managed hardware and software. Moreover, the data interoperability presence of the public and private clouds is identi.ed on the multiple levels in the CMfg. 
3.3. Big Data 
Huge amount of generated data from different types, can come from interconnected heterogeneous objects [24]. This huge amount of structured, semi-structured and unstructured data can describe Big Data (BD). In order to obtain the correspondent value, these data would need too much time and money to be store and to be analyzed [50]. Bringing value opportunities to industries in the era of Internet of Everything can be achieving with the connection of more physical devices to the internet and with the use of a gen-eration of novel technologies. 
Data collection or storage characterize BD, but the core charac-teristic of BD is the data analysis and without it, BD has no much value [51]. Systematic guidance can be provided by BD for related production activities within entire product lifecycle [52], achieving cost-ef.cient running of the process and fault-free [53], and help managers on decision-making and/or to solve problems related to operation [52]. The use of BD provides a business advantage through the opportunity of generated of value-added [54]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 8. Various services in CMfg [46]. 
Cemernek et al. [55] presentedBDde.nition of the TechAmer-ica Foundation, as ＆＆a ＆＆term§ describing large volumes of high velocity, complex and variable data requiring advanced techniques and tech-niques to enable the capture, storage, distribution, management and analysis of the information§. BD demands a cost-effective, innova-tive forms of information processing for enhanced insights. Accord-ing to the researched de.nitions of BD, differing from the traditional data processing [21], the .rst suggestion to characterize BD was related in terms of Volume, Variety, and Velocity, also named as the Three V＊s. These was the three dimensions that emerged as a common framework of challenges in data manage-ment [56].To 
process continuously large amounts of unstructured heteroge-neous data collected in formats such as video, audio, text, or others [51], additionally, other dimensions have also been attempted to assign for a better characterization such as: Veracity, Vision, Volatility, Veri.cation, Validation, Variability and Value [56]. According to several authors [21,51,56每57], the description of the dimensions as follows: 
. 
Volume 每 great data volume size consuming large storage or consist of enormous number of collections. BD sizes are men-tioned in multiple terabytes and petabytes; 

. 
Variety 每 various types of data, generated from a large sources and formats variety, and multi-dimensional data .elds contents. It refers to the structural heterogeneity in a dataset; 

. 
Velocity 每 rapid production. Generation, analysis, delivery, and data creation measured by its frequency. It refers to the data generation rate and the speed for analyzing and acting upon; 

. 
Veracity 每 represents the unreliability in some data sources. Some data requires BD analysis to gain reliable prediction; 

. 
Vision 每 only a purposeful process should send data generation. The likelihood of data generation process is addressed in this dimension; 


. 
Volatility 每 a limited useful life can characterize data generated. The data lifecycle concept is addressed by this dimension. It ensures the replenishment ofthe outdated data with new data; 

. 
Veri.cation 每 conformity of the data generated by a speci.ca-tion set. It ensures the conformity of the engineering measurements; 

. 
Validation 每 the vision conformity of the data generated. Behind the process, the transparency of assumptions and connections are ensured; 

. 
Variability 每data.ow rates measuredbyits variation. Variability and Complexity was added as two additional dimensions of BD; 

. 
Value 每 through extraction and transformation, de.nes how far BD generates economicallyworthy insights and bene.ts. Value as a de.ning BD attribute. 


On manufacturing domain and at the BD process comprehen-sion, it is the engineering aspects that give value to the BD analysis using its dimensions [51]. These dimensions are dependent from each other, related with the relativity of BD volumes applied to all dimensions [56]. 
To explore data, advanced data analysis is required. Using CC through the advanced analytics, methods and tools, off-line and real-time data are analyzed and mined, e.g., machine learning, forecasting models, among others. Knowledge is extracted from the huge data number enabling manufacturers on understanding the product lifecycle various stages [50]. Moreover, the advanced analytics of BD can be used as a facilitator, identifying and over-coming bottlenecks created by IoT generated data [58]. 
The mutation opportunity from today＊s manufacturing para-digm to smart manufacturing is offered by BD [59]. Therefore, BD can help manufacturers on more rational, informed and responsive decision-making way. Manufacturing competitiveness in the global market is enhanced by these BD characteristics. Various stages in data lifecycle where manufacturing data is exploited are depicted in Fig.9 consisting on the complete manufacturing data journey. 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 9. Manufacturing data lifecycle [59]. 
According to Mourtzis et al. [58],ina framework structuredby levelsofa manufacturing enterprise,the lower level generatesdata directly from machine tools and operators. For an enterprise, this data is very important, providing precious information when used and analyzed enabling adaptivity and .exibility on the higher levels of the enterprise. 
BD analytics is an essential key to digital manufacturing, play-ing as an enabler for technologies. Moreover, the scope of mass customization focusing on the needs of individualized markets, use BD analytics as foundation [58]. 
As mentioned above, IoT data converges to BD in order to ana-lyze it and take conclusions from collected datasets. In other words, IoT data will be a part of BD [21] and BD cannot be explored further without the IoT [57]. Furthermore, CC and BD are consid-ered as a coin with its two faces: BD is seen as the absorbent appli-cation of CC, while CC provides the IT infrastructure of BD [57]. 
3.4. 
Simulation 
For the successfulimplementation of the digital manufacturing [60], an indispensable and powerful tool, the computer simulation, is becoming a technology to better understand the dynamics of business systems [61]. Manufacturing industry current challenges can be approached by this technology [62], dealing with the com-plexity of the systems, with elements of uncertain problems that cannotbe resolvedwith usual mathematical models [63].Ona cus-tomized product manufacturing environment, the value of simula-tion is remarkable and evident. Simulation allows experiments for the validation of products, processes or systems design and con.g-uration [60]. Simulation modeling helps on cost reduction, decrease development cycles and increase product quality [61]. In order to analyze their operations and support decision-making, manufacturers have been using modeling and simulation [64]. Simulation technologies already proved its effectiveness in the approach of several practical real-world problems in manufac-turing sector [65]. Mourtzis et al. [60] presented on their research, the domain areas of simulation as shown in Fig. 10 with the focus on simulation methods and tools. Simulation is de.ned as an oper-ation imitation, over time, of a system or a real-world process. It uses a system＊s arti.cial history and its observation, drawing infer-ences over the operational features of the representation of the real system. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Simulation modeling is the method that makes use of a real models or imagined system models or imagined process models. It helps ona better estimating and understanding the modeled sys-tems or process through its behavioural analysis [61]. A model is an entity (generally a simpli.ed abstraction) used to represent other entity with a particular de.ned purpose [66]. Simulation modeling allows to gain insights into complex systems by the development ofcomplex and versatile products and make possible to test new concepts or systems, resource policies and new operat-ing before its real implementation, allowing to gather information and knowledge with no interference on the actual running system [60]. The Fig. 11 shows types of simulation models discussed by Mourtzis et al. [60] regarding to the classi.cation, dimensions, and differences. 
Choose and develop the best suitable type of simulation model to represent the real system is a multiparameter decision, e.g., sta-tic models for modelling a structure without activity and dynamic models for investigating the behaviour of a system evolving through time [67]. 
Simulation have been playing a spotlight role in design evalua-tion (referred to as off-line) and operational process performance (referred to as on-line) during a manufacturing system [65,67]. 
Its usual the existence of making long-term decisions on the design process [67] in, e.g., facility layouts, system capacity con.g-urations, material handling systems, .exible manufacturing sys-tems and cellular manufacturing systems [65]. Simulation runtimein off-lineisnot signi.cantonthe simulation process, offer-ing the advantages to study and analyze the what-if scenarios [67]. 
On the operational process of the manufacturing system, e.g., manufacturing operations planning and scheduling, real-time con-trol, operation policies and maintenance operations [65], the decision-making is short-term, making the simulation runtime a very important aspect. On-line simulation relates the number of entities belonging to the production system, the number of its gen-erated events, the activities complexity and simulation time hori-zon. If the IT system is integrated with the on-line simulation, for instance, it＊s possible to own the capacity to estimate the future shop .oor behaviour and to emulate and/or determinate the man-ufacturing system logic control [67]. 
Optimal or near-optimal system design is the goal for decision makers. This optimization is possible due a systematically search on a wide decision space without restrictions or pre-speci.ed requirements. This simulation optimization tool will search for the optimal design within a given system, according to the com-


V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
puter simulation model. On dynamic and uncertain environments, this tool has the potential on optimizing control decisions and on supporting real-time decision-making. This can be possible when the required computational ef.ciency is reached [68]. Compared to conventional simulation, real-time simulation, on-line, can ana-lyze the behaviour of user and system in milliseconds, allowing the user to develop and produce ＆＆virtually§a prototype for the product or service [69]. According to Cede.o et al. [69], a real-time simula-tion is when a computer runs at the same rate as the physical sys-tem, so the simulation model needs to be feed with real-time data that can be reached using IoT. 
Ahigh-.delity simulation of a manufacturing factory is de.ned as Virtual Factory (VF). An industrial collaboration environment focusing on Virtual Reality (VR) representation of a factory [70] or an emulationfacility [71] canbe consideredaVF.TheVF vision con-siders validated real factories simulation models to generate data andtobe workedin formatsofreal conditionsinareal factory [64]. 
The new simulation modeling paradigm is based on the concept of Digital Twin (DT) [61].An ultra-high-.delity simulationis pro-vided by the DT concept and it plays an important role in I4.0. It extends simulation to all product lifecycle phases, combining real-life data with simulation models for better performances in productivity and maintenance based on realistic data [61]. 
Technologies based on simulation are the core role in the digital factory approach, allowing experiments and validation upon differ-ent manufacturing system patterns, processes and products [72]. 
3.5. 
Augmented Reality 
New challenges are coming with Augmented Reality (AR) usage in everyday [73]. Increase human performances is the aim of AR, supplying the needed information toa given speci.c task [74]. This novel technology provides powerful tools, acting as an HMI [75].AR technology can be found on a wide range of sectors, e.g., entertain-ments, marketing, tourism, surgery, logistics, manufacturing, main-tenance, etc. [76].Asa growing evolving technology, recently,AR usage is spreading to different manufacturing .elds [77]. The use of AR on manufacturing processes regarding to simulation, assis-tance and guidance has been proven to be an ef.cient technology helping on problems [78]. AR technology increase reality operator＊s perception by making use of arti.cial information about the environment,wheretherealworldis ful.lledbyitsobjects [79每 80].As long as it interacts with human senses, AR can make use of any kind of hardware [74]. Using AR can help on closing some gaps, e.g., between product development and manufacturing operation, due to the ability to reproduce and reuse digital information and knowledge at the same time that supports assembly operations [78]. Fig. 12 shows the most relevant tasks related to industrial environments and manufacturing .elds where the AR brings value. 
The principle of AR is the combination of two scenarios: 1) dig-itally processed reality with2) digitally added arti.cial objects that could be 2D .at objects, or by other de.nitions that only considers 3D objects within the scene [73]. The authors [79每80] de.ned AR system features as: 1) the ability on combining real and virtual objects on a real environment, 2) the ability on align each other the real and the virtual objects, and 3) the ability on running inter-actively, in 3D, and on real-time. 
Making use of conventional hardware, the use of AR has a big advantage that can be minimal or even zero purchase expense. Some cases, the see-through glasses component can be more expensive [73]. On industrial environment, other key advantage was pointed by Blanco-Novoa et al. [81] about the assets: AR pro-vides dynamic real-time information,soitcan suppress mostofthe paperwork. 
The AR system software might be selected based on environ-ment＊s considerations, which obviosity differ among them, e.g., on the military environment the proper use is zero-connectivity to ensure CS, differing from commercial environment that requires providing remote assistance＊s connectivity [74]. 
The essential parts of an AR system make use of electronic devices to directly or indirectly view a real-world combination with virtual elements. According to Fraga-Lamas et al. [75], these elements can be: 
. 
Image capture element 每 web camera is suf.cient [73]; 

. 
Display 每 for projection of the virtual information on the images acquired by the image capture element. Basically, three device types with optical options can be used [80,82]: 1) hand-held (video and optical), 2) head-worn (video, optical, and retinal), and 3) spatial (projector and hologram); 

. 
Processing unit 每 to generate virtual information to be projected; 

. 
Activating elements 每 to trigger the display of virtual informa-tion, e.g., sensors,QR markers, GPS positions,images, etc. 



In order the user to visualize information, these AR devices use types of optics as follows [82]: 
. Video 每 merged worlds (real and virtual) into the same digital view; 


V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 



. 
Optical 每 real world with virtual objects overlaid directly on the view; 

. 
Retinal 每 direct projection of virtual objects onto the retina with the use of low-power laser light; 

. 
Hologram 每 real world mix with virtual objects using a photo-metric emulsion; 

. 
Projection 每 projection of virtual objects directly on real-world objects with the use of a digital projector. 


Related to the quality of products, Segovia et al. [83] proposed an AR system solution to production monitoring, based on Statisti-cal Process Control (SPC) and Six Sigma methodology. It uses AR in realtime reportsto assist qualitydata reportingby monitoringCpk indexes to support the decision-making process. The AR system was linked to a Computer-Aided-Quality (CAQ) to receive data. The CAQ used was Quality Data Analysis (QDA) software that allows the user to verify quality goals. The used measurement device was wireless connected to QDA software. The QDA software generatedreports and exported them automatically in a .le to the AR application. The mobile device used to run the AR application was a tablet. Fig. 13 shows the AR technology with the inside of the facilities and the displayed Key Performance Indicators (KPI) of each workstation. According to Segovia et al. [83], one of the big-gest bene.ts of this tool is the reduction on audit times. 
Maintenance is one of the most promising .elds of AR. It enhances human performances in technical maintenance tasks execution as also supports on maintenance decision-making [76]. One example of AR in maintenance is shown in Fig. 14 on a step-by-step assembly procedure of a consumer device, using Hand-Held Display (HHD) to carry out maintenance tasks. The AR appli-cation has text description of the task on the bottom, right and left arrows to go forward and backward on the procedure. 
Other example in the use of AR technology is on the diagnostics .eld.Ameaningful example is shown in Fig. 15, also with the use of an HHD. The defects inspection and mapping on the pipe was madewitha3Dimage.The defects positionis indicatedonthepipe anditcanbeseenaclearerimageofthe natureandscaleof defects. At the end, the operator can detect, locate and mark defects using a tablet and a marker [84]. 

3.6. Additive Manufacturing 
Products and services innovations needs hard and long research work and development that I4.0 with the novel technologies such as simulation via virtual reality are enabling it. However, on the next step, there is a manufacturing process with its related costs that can be a barrier to competitiveness. Additionally, at the end, there is a dilation of product or service lead time for markets. 
The Additive Manufacturing (AM) paradigm is being increas-ingly developed and it brings into real industry, high feasible appli-cations [85]. Jian et al. [86] discussed the potential of AM on the replacement of many conventional manufacturing processes. AM is an enabling technology helping on new products, new business 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

models and new supply chains.A set of technologies that enables ＆＆3D printing§ of physical objects form the collective term AM [87]. Products such as one-of-a-kind, can be manufactured without the conventional surpluses, so it is a big advantage. AM technolo-gies can be referred also with other synonyms such as rapid proto-typing, solid freeform manufacturing, layer manufacturing, digital manufacturing or 3D printing [88]. With AM it＊s possible to create prototypes to allow value chain elements independence, and therefore, achieving time reduction on design and manufacturing process. 
As follows in Fig. 16,AMprocesses are classi.ed into seven cat-egories according to the standard of the International Organization for Standardization (ISO)/American Society for Testing and Materi-als (ASTM) 52900:2015 (ASTM standard F2792). 
AM technology is de.ned by Kim [85] as a process of creating a 3D object-based on the deposition of materials on layer-by-layer or drop-by-drop undera computer-controlled system. Some potential bene.ts of AM can be summarized as follows [89]: 
. 
Manufactured parts directly from CAD data .les (.nal or near .nal parts with minimal to no additional processing); 

. 
Greater customization without extra tooling or manufacturing cost; 

. 
Manufacturing of complex geometries (some geometries cannot be achieved on conventional processes, otherwise, it is achieved by splitting it into several parts); 

. 
Manufacturing of hollow parts (achieving less weight) or lattice structures; 

. 
Maximization of the material utilization for the ＆＆zero waste§ approach; 

. 
Smaller operational foot-print towards manufacturing a large variety of parts; 

. 
On-demand manufacturing and excellent scalability. 



According to Shin et al. [90],AMwork.ow includes the geome-try design, computational tools and interfaces development, mate-rial design, process modeling and control tools, and it was also discussed the AM applications .elds such as nano-scale (bio-fabrication), micro-scale (electronics), macro-scale (personal prod-ucts, automotive), and large-scale (architecture and construction, aerospace and defense). 
For the next generation of AM processes, Chang et al. [91] dis-cussed novel processes such as micro/nano scale 3D printing, bio-printing (AM of biomaterials), and 4D printing (combination of AM with smart materials (stimulus-responsive that change their shape or functional properties)) to fabricate within high resolution a complex 3D features, in multi-materials, or multi-functionalities. 
On a near future, AM technology will expand eventually to super-advanced technology areas and substitute current technolo-gies [85]. 
3.7. Horizontal and Vertical Systems Integration 
Engineering, production, marketing, suppliers, and supply chain operations, everything connected must create a collaborative sce-nario of systems integration, according to the information .ow and considering the levels of automation [18].In general, the sys-tems integration of I4.0 has two approaches: horizontal and verti-cal integrations [10,92]. Real-time data sharing is enabled by these two types of integration [16]. 
Horizontal integration is the inter-company integration [92] and is the foundation for a close and high-level collaboration between several companies, using information systems to enrich product lifecycle [16], creating an inter-connected ecosystem within the same value creation network [10,92]. It is necessary an independent platform to achieve interoperability on the devel-opment of these systems, based on industrial standards, enabling exchanging data or information [92]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
Vertical integration is a networked manufacturing system [93], the intra-company integration [92] and is the foundation for exchanging information and collaboration among the different levels of the enterprise＊s hierarchy such as corporate planning, pro-duction scheduling or management [10,93]. Vertical integration ＆＆digitizes§ all the process within entire organization, considering all data from the manufacturing processes, e.g., quality manage-ment, process ef.ciency or operations planning that are available on real-time. By this, in a high level and .exible way, providing the small lot sizes production and customized products, the verti-cal integration enables the transformation to SF [16]. It＊s important to refer that standards must be the bases of the vertical integration [92]. 
According to several authors [16,93每96], the paradigm of I4.0 in manufacturing systems has another dimension between horizontal and vertical integration considering the entire product lifecycle. This kind of integration is based on vertical and horizontal integra-tions[93].Ina visionof holistic digital engineering,asthe natural .ow of a persistent and interactive digital model, the scope of the end-to-end digital integration is on closing gaps between product design and manufacturing and the customer [94], e.g., from the acquisition of raw material for the manufacturing system, product use and its end-of-life. The phase of end-of-life product contains reusing, remanufacturing, recovery and disposal, recycling, and the transport between all phases [95]. Fig. 17 shows the relation-ship between the three types of integration on a manufacturing system, considering vertical integration as the corporation(s), hor-izontal integration between corporations, and end-to-end integra-tion linking design, production and logistics as an example. 
3.8. Autonomous Robots 
Manufacturing paradigm is shifting rapidly production from mass towards customized production, requiring robots, for instance, as a recon.gurable automation technology. The impact on the production systems of the manufacturing companies is that this trend leads to the production adaptation for a wider product variation, focusing ideally on batch size one. Nowadays, to reach the .exibility demanded level, robots are essential on production systems [97]. Towards that, abilities on computing, communica-tion, control, autonomy and sociality are achieved terms when combining microprocessors and Arti.cial Intelligence (AI) with products, services, and machines to make them become smarter. Robots with AI, adaptive and .exible, can facilitate different prod-ucts manufacturing and consequently providing decreasing pro-duction costs [16].In addition,a robot also canbe seen as oneof the forms of AI [98]. 
Processes such as product development, manufacturing and assembling phases, are processes that adaptive robots are very use-ful on manufacturing systems [16]. It is important to refer that fully autonomous robots make their own decisions to perform tacks on a constantly changeable environments without operator＊s interaction [99]. Fig. 18 shows an overview, not strict, on the autonomous robot characterizations, considering industrial and non-industrial environments. 

Dirty or hazardous industrial applications on unstructured envi-ronments can be improved by an Autonomous Industrial Robot (AIR) or multiple in a close collaboration. Hassan et al. [100] pre-sented a multiple autonomous robot＊s collaboration approach in Fig. 19, consisting on robots with different capabilities performing grit-blasting and spray painting. 
According to Hassan et al. [100], with the deployment of multi-ple autonomous industrial robots working as a team, it＊s possible to have a larger range of manufacturing applications. Other approach in multi-robot systems can be seen in Fig. 20 during a 

Fig.19.Autonomous industrial robots performing grit-blasting or spray painting [100]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

sequence of collaborative assembly operations, dealing with robot con.gurations to grasp assembly parts and build complex struc-tures such as a chair [101]. 
Collaborative robots concept also introduces the proximity of robotswith humans [102].Onthe visionofSF, collaborative robots (cobots) and humans will work closely together. Cobots are a cat-egory of robots specially designed to interact directly and physi-cally with humans, in a close cooperation [103每104]. This is possible due to the safety existing limits on speed and forces that automatically restarts the cobot allowing to guide the cobot by hand [103]. By this, for manufacturing companies, human-robot barrier is break down offering bigger affordability and .exibility on solutions [104]. 
3.9. 
Cybersecurity 
Every year, increasingly, devices are connected to the global network: the internet. In a close future, the main source of data will be inanimate objects [105].Bythis, IoT, virtual environments, remote access, stored data on cloud systems, etc., are many open opportunities that represents increasing new vulnerabilities lead-ing to a compromised information for people and enterprises. The risk scenario becomes reality because the enterprise bound-aries are unclear and are vanishing [106]. Kannus and Ilvonen 
[107] de.ned Cybersecurity (CS) as a new term on a high level of information security, and through the word ＆＆cyber§ it spreads to apply also on industrial environments and IoT. CS is a technology laying on protecting, detecting and responding to attacks [108]. 
IoT has to be built based on safety communications on each point of the manufacturing process and safety interoperability has to be assured between facilities as basic elements of the supply chain value. I4.0 technologies must allow the creation of a safety cyber environment, bene.ting on CS. 
Direct attacks from evil persons and/or software can be hard jeopardies to Industrial Control Systems (ICS). These ICS of the industrial sectors are basically control such as Supervisory Control and Data Acquisition (SCADA), process control systems, distributed control systems, CPS or Programmable Logic Controllers (PLC) [109]. The increasing of connected devices means more possibili-ties of cyber-attacks. Benias and Markopoulos [110] discussed why industrial devices get hacked, the main reasons as follows: 
. 
Devices running for too much time (weeks or months) without updating security or anti-virus tools; 

. 
Considerable number of old controllers used in ICS networks, designed when CS was not a concern; 

. 
CS threats can enter bypassing CS measures due to the existence of multiple pathways from several ICS networks; 


. Quick spread of malware due to several ICS networks that still remains implemented as a .at network without physical or vir-tual isolation among other unrelated networks. 

I4.0 creates valuable information that needs to be protected. Information and data security are critical for the industry success. It is important that data is available just for authorized persons. Integrity and information sources must be ascertained. I4.0 has raised two demands for CS in order to secure smart manufacturing systems: Security Architecture and Security by Design. Hence, attacks, threats and malware must be automatically detected with zero-installation by the systems [106]. Manufacturing operations can be shut down by a cyber-attack, therefore, companies have money losses, but the main issue are cyber-attacks targeting sys-tems requiring safety operations and representing a serious risk for the safety of the operators [111]. Elhabashy et al. [112] dis-cussed other approach on manufacturing environments regarding to some potential attacks such as modifying product designs (related to CAD .les, tolerances), modifying manufacturing pro-cesses (Computer-Aided-Manufacturing (CAM) .les, machine parameters, used tools, tool paths) or manipulating process/pro-duct data (inspection results, indicators of machine maintenance). These attacks can delay a product＊s launch, cause the production of modi.ed products, can ruin customer trust or increase warranty costs. 
The cyber-attack could be internal and/or external source. According to Khalid et al. [113],in Fig. 21, a cyber-attack can come from an internal source such as an operator that physically access to a data port or an external source such as an outside communi-cation channel or also a wireless transmission. 
The ICS safety is time-sensitive so an automatic incident response is need it. For a variety of industrial attacks, Software-De.ned Networks (SDN) and Network-Function Virtualization (NFV) can facilitate automatic incident response. The incident response in ICS can be achieved using a private-cloud architecture (cost-effective investment). SDN and NFV makes automatic inci-dent response possible to rapidly detect and temporarily replace the failing systems with virtual implementations of those systems. SDN and NFV are technologies to improve the following aspects:1) network visibility, 2) network capabilities (enables network traf.c .ows with better management), and 3) network functions deploy-ment and control using software, instead of speci.c hardware mid-dleboxes [108]. However, the combination of SDN with NFV shows a capable approach in new defense solutions in depth for ICS [114]. 
The concept of defense-in-depth, as showed in Fig. 22, was dis-cussed by Jasen et al. [115], according to the international standard IEC/ISA-62433 with the incorporation of three measures as techno-logical, organizational, and human-centered, as multilayer approach for security ICS. Security controls at system level, net-work and plant must exist on this concept. 
Updating the implemented security controls continuously is obligatory, keeping the protection up-to-date [115], such as fol-lows on: 
. 
Device level -with the installation of new security patches; 

. 
Network level -with the .rewall signatures of new threats updated; 

. 
Plant/factory level -with the analysis and monitoring of the actual log sources. 



4. The Smart Factory of the I4.0 
According to several authors [2,4每8,116], the framework of the I4.0 is the development of the Smart Factory (SF). In conceptual terms, the SF is the heart of I4.0 [117]. CPS, IoT and IoS were assumed as the main components of I4.0 [1]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 


These components have very closely linked each other, enabling the SF and built on the concept of decentralized production system with a social network connecting persons, machines and resources [1]. Using cloud-based manufacturing in SF, both IoT and CPS tech-nologies converges to IoS to create, publish and share the manufac-turing processes, represented in services that could be supply by virtual enterprises [118]. 
Comparedto humans living in two worlds such as the physical and the cyber world, SF will work on the physical and on the DT, in the cyberspace. The DT will collect generated data from manual inputs and sensor networks, will process data on cyberspace and take the corrective actions on real-time to handle the physical world [29]. 
Based on the manufacturing process digitalization, I4.0 is the development of a new generation of SF＊s [24]. According to several authors [2,4,8,10],in this new generation of SF, the main key tech-nology is CPS. SF is the key feature of I4.0 and the core concept component, where vertical integration occurs, the horizontal inte-gration occurs in the SF value network and across different SF＊s, enabling end-to-end engineering integration across the entire value chain [119]. Fig. 23 identi.es the transformation technolo-giesof the current industrial productioninaSF framework. 

4.1. Cyber-Physical Systems 
Cyber-Physical Systems (CPS) has the potential to change our life with concepts that already emerged, e.g., robotic surgery, autonomous cars, intelligent buildings, smart manufacturing, smart electric grid, and implanted medical devices [120] (e.g., a pace maker in a smaller scale [121]). CPS represents the latest and signi.cative developments of Information and Communication Technologies (ICT) and computer science [120]. 
CPS is the merger of ＆＆cyber§ as electric and electronic systems with ＆＆physical§ things. The ＆＆cyber component§ allows the ＆＆physi-cal component§ (such as mechanical systems) to interact with the physical worldby creatinga virtualcopyofit.This virtualcopywill include the ＆＆physical component§ of the CPS (i.e., a cyber-representation) through the digitalization of data and information. By this, CPS can be assumed as a range of transformative technolo-gies to manage interconnected computational and physical capa-bilities [122]. CPS embraces smart elements or machines who has the augmented intelligence and ability to communicate each other to make part of planning, unique or non-repetitive tasks. These smart elements, for instance, can control the needs of workpieces, alter the manufacturing strategies for the optimal production, choose (if already exists) or .nd a new strategy all by themselves. These elements will build their own network [123].In other words, the CPS core is the embedded system to process information about the physical environment. This embedded system will perform tasks that were processed by dedicated computers. CPS model can be described as a control unit with one or more microcon-trollers, controlling sensors and actuators that interacts with the real world and processes the collected data [124每125].A commu-nication interface will enable this embedded system to exchange data with the cloud or with other embeddedsystems. CPS is asso-ciated with the IoT concept [126]. According to Humayed et al. [127], CPS mainly consists of three components such as: 1) com-munication; 2) computation and control and; 3) handling and monitoring. The CPS communication can be both wired or wireless 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 


and connects CPS to a higher level such as control systems, or lower-levels such as physical world components. The intelligence is embedded on the computation and control component with the exchange of control commands and received measures. CPS is connected to the physical world by the handling and monitoring component, using actuators to handle physical components and using sensors to monitor them [127]. 
Referring a manufacturing system and according to Keil [128], Fig. 24 shows a schematic representation of a CPS, an embedded system integrated in physical systems such as production lots or machines. The sensors collect physical data and the electronic hardware and software will save and analyze it. The interaction between data processing and other physical or digital systems are the CPS bases. it＊s also possible to identify an HMI in this CPS schematics for supervision and exchange information. 
Several CPS linked within digital networks formaCyber-Physical Production System (CPPS) [128],based on sub-systemsand autono-mous and cooperative elements linked across all levels of produc-tion [120]. According to Rojas et al. [129], CPS are the building blocksfortheSF, structuredasCPPS.The collecteddatawillbesent to BD and become accessible via CC. The CPPS interaction with the virtual world enables IoTin manufacturing [13,118].As the system are getting intelligence regarding to the so-called smart objects, the IoT creates the connect environment with smart objects to the glo-bal internet. Several authors [3,6,10,19,58,94,113,121,124,130] dis-cuss the level of cooperation and communication of CPPS in manufacturing. 
The implementation of CPPS in the SF leads to a fundamental design principle as the real-time management in industrial pro-duction scenarios. CPPS will make the automation pyramid approach on a different manner. The traditional automation pyra-mid,as showsthe Fig.25,is partly breakatthe PLC＊s level.The .eld level and control remain including closest PLC＊s of the technical processes to improve critical control loops, and the highest levels of the hierarchy will be decentralized [131]. 
In the CPS-based Automation of the Fig. 25, the squares repre-sent inputs/outputs devices, the lines represent service interac-tions and the blue, yellow, grey and black points represent the 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

corresponding functionalities of the .ve-layer architecture of the traditional automation pyramid [126]. 
Some researchers are developing a .ve C＊s structure for better analyzing I4.0. This .ve C＊s architecture can guide the development of I4.0 and it is dependent of CPS attributes. These .ve levels are: Connection Level (main attribute is self-con.gurable), Conversion Level (main attribute is early-aware), Cyber Level (main attribute is controllable), Cognition Level (main attribute is informational) and Con.guration Level (main attribute is communicable) [116,132每133]. 
4.2. Internet of Services 
Replacing physical things by services, the Internet of Services (IoS) is based on the concept that services are available through the internet so that private users and/or companies can create, combine and offer new kind of value-added services [1]. IoS can enable service vendors to offer their services on the internet. Thus, the manufacturing industry of product-oriented trend is rapidly shifting for service-oriented to enable gaining revenue through all lifecycle of a product service system. By this, high quality on products can be enable by SoA, and side-by-side, gives a strong competitive position for companies through the value-added ser-vices. IoS enables collecting product information, e.g., during its operation, for updates and for the development of new services, increasing the perceived product quality [29]. IoS is consider by Andulkar et al. [29] as the technology to monitor the product lifecycle. 

CONCLUSIONS

5. Conclusions and Outlooks 
As aforementioned, the foundations of the I4.0 are the advanced technologies ofautomation, and the ICT present across this review. Key challenge of I4.0 is to make the production systems more .ex-ible and collaborative. For this purpose, the use of enabling tech-nologies is the strategy that is behind of I4.0 paradigm. On an industrial context, each implemented technology in an individual manner will present a lower impact. On the other hand, when implemented together, it offers new possibilities to embrace the future. For instance, one of the I4.0 impact will be the elimination of monotonous work as well as physically demanding jobs. 
IoT is an in.nite world of possibilities on innovation and opti-mization, due to the combination of many advanced systems and technologies such as BD and analytics, AI, networks, clouds, intel-ligent objects, robotics, middleware, people, among others. 
The development of a CMfg service integration platform is pro-posed by Mai et al. [46] as a promising concept. It is an online tool consistingon builda processwith several sub-taskswitha seriesof modules sequentially connected each sub-task. This concept allows consumers to have customized products or even make products in the cloud. Even more, through CMfg, producers can create smart solutions to save costs and improve pro.ts.Acrucial note is the improvement of the safety and security regarding to online services that was mentioned at all examples. The develop-ment of CS technology deserves maximum efforts from all actors, since individual, professional users, and organizations that need to be safe and secured to face these rapid technological advances. 
The Systems integration of I4.0 has two major characteristics relying on vertical and horizontal integration. The vertical integra-tion of the manufacturing processes, breaks the traditional automation pyramid, focusing on distributed and collaborative architectures. The horizontal integration allows the creation of a new kind of value-added [129]. By this, there is an unavoidable surrounding of customers and suppliers that are involved just from the beginning of the product life cycle. 
A challenging scenario with the deployment of I4.0 will be the extinction of the centralized applications used in common manu-facturing environments, that leads to decentralized systems as one of the main I4.0 goals. By this meaning, distributed computing systems also plays a key role on I4.0 paradigm. It allows to save time on computing runtimes, allows working with more accurate details on smaller systems and for the overall system, and decreases the fail reaction time, e.g., if one computing system fails the others can continuing on computing. 
Providing a guideline for the interdisciplinary I4.0 technologies, the RAMI4.0 was developed, describingthe connection between IT, manufactures/plants and product lifecycle througha3Dspace. The integration of RAMI4.0 and I4.0 component (component as, e.g., a production system, an individual machine or an assembly inside the machine) close the gap between standards and I4.0 technolo-gies at the production level, leading to the emerge of CPPS [130]. 
Interoperability is one of the I4.0 design principles and can be found between BD and simulation as discussed by Shao and Jain [64]; BD on its analytics supports simulation by estimating the unknown input parameters and performing data calibration for simulation and its validation results. The return is the support of simulationfor BD analytics on various roles. Data analytics applica-tion can summarize and report production trends (e.g., product 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

variation cycle time or throughput average). Diagnostic data anal-ysis can respond to what has happened and what is happening, identifying causes. Diagnostic analysis can take advantage using of manufacturing system＊ simulation model that emulates the cur-rent operation. Predictive analytics estimates performance based on planned inputs, e. g., product cycle time and throughput estima-tion for several products based on current policies. It will take advantage from simulation models to execute the what-if scenar-ios. Prescriptive analytics can respond to how can we make it hap-pen and what will be the consequences.It uses simulation models to improve the production performancein future periodsby emu-lating operations under paralleled realities and these plans can be improved with the arrangement of simulation and optimization models. 
In the VF level, simulation can be seen as data generator allow-ing VF to generate for instance, streams of production data and resource utilization, and feed data to analytics applications. Can be seen also as supporting evaluation and validation giving an advantage to the real factory. 
Simulation technology on I4.0, using VR, is an integral process to simulate all industrial processes, from planning, design, manu-facturing, providing services, maintenance, try-outs or even quality controls. All processes can be simulated as modular [132]. It＊s pos-sible to simulate and virtual verifya factory manufacturing process before being realized. After approved, all physicals can be done. For instance, if it is considering the combination within simulation and AM, after product simulation, the production of prototypes allows the time reduction on design and production process, by reducing the value-added dependencies. These time reductions are particu-larly relevant on customized markets. 
Grieco et al. [4] presented an interesting case study in fashion manufacturing where a decision support system as a software is developed under the I4.0 concept, aiming the minimization of: 1) orders delivered later than due date, and 2) resource overload cases. 
Many researchers discuss that the data is the raw material of the XXI century and the real world will be a huge information sys-tem. According to this, Lu [3] discussed one of the major chal-lenges in I4.0 that will be the development of algorithms for dealing with data. 
According to Salkin et al. [16], there is no speci.c I4.0 de.nition, and therefore, there is no de.nitive utilization of the enabling tech-nologies to initiate the I4.0 transformation. 
But the fact that this fourth revolution is been announced before it takes place, opens several opportunities for co-working environments between academic researchers and industrial practi-tioners, shaping on the manufacturing future [134]. 


5.1. Looking Forward 
As mentioned by Rojas et al. [129], I4.0 is on its infancy and to make it a reality, several challenges and gaps must be addressed. By this, the roadmap for the I4.0 ful.llment is still not clear to date in both academia and industry [132]. Considering .ve fundamental manufacturing systems to conceive I4.0, Fig. 26 can represent the research gaps between the current manufacturing and the I4.0 requirements [132]. These .ve manufacturing systems are systems where is hard to achieve intelligent concepts, that are the goal of I4.0 development, neither I4.0 lower or upper levels. The closest to I4.0 is the Recon.gurable Manufacturing System. 
5.2. Executing I4.0 in SMEs 
Looking at European Union, SMEs represents the backbone of the economy and the key to competitivity. Inside this enterprise dimension, special approaches must be developed to introduce and apply I4.0 technologies [129]. The enabling technologies of I4.0 are the foundation for the integration of intelligent machines, humans, physical objects, production lines and processes to form a new kind of value chain across organizational boundaries, featur-ing intelligent, networked, and agile. By this, due to the increase level of complexity, manufacturing SMEs has doubts on the required .nancial effort for the transformation technologies and its impact on their business model [135]. 
The implementation of I4.0 in SMEs can be facilitated, for instance, on a SaaS approach, enabling technology acquisition for digital services with appealing investments. A clear example can be an SME integration on the supply chain of a product, allowing collaborative of project development, collaborative working on product＊s launch and time to market reduction, shared innovation, and consequently, minimizing the related risks. 
Acknowledgments 
Authors would like to acknowledge to the reviewers for their valu-able feedback. Special thanks to Freepik for providing vectors/icons for some .gures, available at www..aticon.com. 
Declaration of con.ict interests. 

The authors declare no potential con.ict of interests at all in this paper. 
Keywords:
Wind energy
Wind turbine monitoring Wind farm monitoring Control chart
SCADA monitoring Statistical inertia

Abstract

A method for monitoring wind turbine generators (WTG) using data provided by the SCADA system is proposed. This method relies mainly upon comparing one WTG with the average of all remaining WTGs on a wind farm. Because environmental conditions on a wind farm are roughly the same over its entirety, the difference between each WTG and the average of the remaining WTGs on the wind farm is constant over time. The statistical inertia of averaged conditions for the entire farm provides a good yardstick for WTG monitoring. The results of monitoring four aspects of a WTG are presented here: these are electrical energy produced; tower vibration; nacelle yaw; and gearbox temperature. Control charts are used to detect abnormal behaviour. With regard to the electrical energy produced, one accidental activation of a curtailment algorithm was found. For tower vibration, we describe an application for the detection of rotor imbalance. For yaw, an example showing detection of nacelle drift is covered. Lastly, for gearbox temperature, the proposed methodology succeeded in detecting an issue two months prior to failure. We have included limitations as to the minimum wind farm size required in order to use the wind farm average. A centralized control chart is also proposed.

1. Introduction

During the life time of a Wind Turbine Generator (WTG), many events can affect its performance. These events can be classi?ed according to the time scale on which they occur. Faults such as blade-angle asymmetry or generator over-speed generally affect the produced electrical energy for hours. Downtime resulting from changing or repairing a principal component such as a main bearing or a gearbox can last for days, even weeks [1]. Other events such as blade erosion build up over months and years. The effect of such events on power output can also be divided in two categories: events that partially reduce the production (e.g.: icing, blade erosion) and others that stop the WTG (e.g.: faults, downtime for repairs). Preventing these events or limiting their duration is an important aspect of wind farm operation and maintenance (O&M). The ageing of WTGs is now a timely topic for the wind industry, since many wind farms in Canada and around the world have been in service for decades. With time, failure of components is more frequent and underperformances can appear. Some authors are reporting a performance reduction rate of approximately 1.5% per year [2]. Also, some operators are even considering the option of repowering, as their farms are getting closer to the end of their planned lifetimes, or as new WTG models, signi?cantly larger than the ones built decades ago, become available [3]. Thus, various monitoring method are used to improve availability of WTGs and to achieve condition based maintenance. The ageing of the wind farms also motivates interest in the great amount of data available for the development of monitoring tools.
The objective of this paper is to propose a data-driven method to monitor wind farm WTGs base on the long term, which is also robust and suitable for the industry. Here, control charts are used for the generation of alarms. Unlike most other monitoring methods, the proposed methodology can be used to monitor a wide range of WTG components or aspects. It is also simple to under- stand and use: no advanced knowledge in data mining or modeling is required. Thus, the proposed methodology is suitable for indus- trial applications. Furthermore, this method allows for the moni- toring of various aspects of a WTG simultaneously, with the help of a centralized control chart. This method is suitable for medium and large wind farms (more than 25 WTGs). Since the number of units per wind farm is constantly rising, this method is can be applicable to most cases.
First, previous work on the monitoring of WTGs will be reviewed. Then the proposed method will be detailed, followed by the results of its application to industrial wind farms. The method used here for monitoring a wind farm is based on comparing a single WTG with the average of the other WTGs on the same wind farm. The effectiveness of this approach will be demonstrated by means of four separate industrial study cases.

2. Literature review

One way of increasing the reliability and availability of wind energy is by monitoring WTGs. With proper monitoring, failures can be avoided and their consequent down-times limited, all of which increases availability. Maintenance can be planned ef?- ciently, and replacement parts can be ordered before failure occurs. There are various ways of performing the monitoring of a WTG. These can be classi?ed as follows: condition monitoring systems (CMS); Supervisory Control and Data Acquisition (SCADA) moni- toring; power curve monitoring; and fault prediction.

2.1. Condition monitoring systems

Condition monitoring systems (CMS) involve the use of addi- tional sensors that evaluate the health of WTG components. They are often based on vibration analyses and use methods such as wavelet analysis or Fourier transformations. They can achieve great precision by predicting the failure of a component months before it happens [4] [5] [6]. However, installing additional sensors can be costly in large wind farms [7]. CMS may also include oil analysis, thermography, shock pulse methods, acoustic emissions and ul- trasonic techniques, as reviewed in Ref. [5].

2.2. SCADA monitoring

SCADA monitoring is the monitoring of a WTG using the data provided by its SCADA system and does not need the use of addi- tional sensors [7]. This monitoring method is limited by the data SCADA provides. Often it is component temperature that is analyzed via SCADA monitoring [8]. In some cases, models are used in order to predict component behaviour [4] [9] and in other cases, the monitoring can be based on the signal itself (mean values, standard deviation, slopes, root mean square, spectrum, etc.) Power curve monitoring can be viewed as a subset of SCADA monitoring. Power curve monitoring is based on the relationship  between wind speed and power output. A change in behaviour of a WTG may be re?ected by its power curve [10] [11] [12] [13]. This method of monitoring can be powerful for the detection of small, progres-
sive underperformances [14].
However, power curve monitoring relies upon measurement of wind speed. According to IEC (International Electrotechnical Commission) 64100-12-1 [15], wind speed shall be provided by a met mast in order to assess the performance of a WTG as a function of freestream speed. But since wind farms have only a few met masts, nacelle wind speed is used instead in power curve moni- toring. In fact, the important point is to obtain a reliable, repeatable and representative wind speed measurement. Therefore, the power curves, using nacelle wind speeds, can be used for monitoring. However, a major ?aw in power curve monitoring is that any change in nacelle anemometry can create a considerable shift in the power curve. Fig. 1 illustrates a change in nacelle power curve following various changes in nacelle anemometry. These power curves were obtained with the bin method described in IEC's 64100-12-1 standard, while using nacelle anemonetry [15].  A noteworthy dif?culty while monitoring a WTG's components using power curve monitoring, is that often, their failure will be seen in the power output after a critical point is reached.

2.3. Fault monitoring

Another type of monitoring is the prediction of faulty behav- iours in a WTG. As de?ned by Ref. [16], a fault occurs when a parameter of a system deviates from standard conditions, such as blade angle asymmetry, component over-temperature or generator over-speed. Operational data are analyzed by means of complex algorithms in order to predict or even avoid the shutdown of the WTG [17] [18] [19] [20] [21] [22]. Faults monitoring methods can be classi?ed into two categories: model-based and signal-based [16]. In the ?rst case, a model is used to predict the value of a parameter and the predicted value is compared to the observed value in order to ?nd abnormal behaviours. For the signal based approach, fea- tures of the signal are studied. These features may be in the time domain (mean, root mean squarre, gradient) or frequency domain (spectrum) [16] [23]. Fault monitoring also includes fault-tolerant control, where a system analyses the severity of the fault  and takes appropriate action (compensation, controller recon?guration, etc) [16].
Fault prediction is especially important for offshore wind farms, where access to the WTG is more dif?cult than onshore wind farms. High frequency data (~1 Hz) must be available for use in predicting faults; often, additional sensors or complex models, or algorithms are used as well. The types of conditions the control system of a WTG uses in order to detect faulty behaviour requires input to remain above a certain threshold for a few seconds or minutes. Some work has also been done on the development of a WTG controller that can be optimized according to the conditions of the WTGs subset in order to avoid faults. Used components or component subsets may act differently from their nominal behav- iour and thus, the optimal control strategy should be revised [22]. For more on fault monitoring and diagnosis, see Refs. [16] and [23]. The monitoring methods detailed above can all be useful in the O&M of a wind farm. Operators might consider using a combina- tion of these methods, as each evaluate the condition of a different
aspect of their WTG, based on different criteria.

3. Data source
The data used to develop and test the proposed method was taken from ?ve industrial wind farms located in Canada. All the WTGs were the same model and were MW class and pitch regu- lated. Each wind farm contained over 50 WTGs. While the data was recorded at a frequency of 1 Hz, ten-minute averages are used. This averaging limits the noise in the signals and is the norm in the wind power industry, as suggested by the IEC standard [15]. The database has been in service since 2009, which is of interest for long-term monitoring. No CMS were installed on the WTGs. Available values included (online, repair, maintenance, curtailment, etc.) Because there was no mea- surement of principal-component vibration, monitoring methods using these values could not be used. Extreme values correspond- ing to obvious instrumentation malfunction have been removed from the database. Fig. 2 illustrates the data available and the data acquisition process. Here the acquisition and archiving system is the PI system from OSISoft. There is a local server in each wind farm, linked with an Internet connection to a main server. This redundancy allows to archive data even in the eventuality of connection losses with the main server and the on-site server. The
Fig. 1. Example of shifts in power curve caused by instrumentation. On the left, the effect of a faulty anemometer and on the right, the effect of the anemometer type.
Fig. 2. Available data and data acquisition process.

4. Method

The proposed method for monitoring the WTGs on a wind farm compares each WTG's behaviour with the mean of the other WTGs' behaviour on the wind farm. This method is data-driven and empirical based. No models are used.
The behaviour of a WTG constantly ?uctuates due to the various productive WTGs. However, this difference should be constant in time, the same assumptions are made in Ref. [24], where a moni- toring method using wind farm power curves is presented. Thus, the difference between a speci?c WTG and the average of the remaining WTGs on the wind farm should be statistically constant in time.
the difference with the mean of the others,, can be
expressed by the following relation in its environment (wind speed, wind direction, air density,

ambient temperature, etc.). However, all WTGs on a wind farm are
relatively affected by the same environmental variations, since weather conditions are roughly the same over the entire wind farm. Some WTG will perform better, depending on the con?guration of the wind farm. Because physical quantities such as the components and structural vibrations and component temperatures are pro- portional to power output, their values will be higher in the most

This calculation is made at 10 min intervals for which the WTG is available. The rest of the ?eet's average, or the average of all other WTGs on the wind farm, is in fact the average of all the other available WTGs (N(t)). Units under maintenance, curtailment, repair or other production limiting state should not be included in calculating the average for the remaining WTGs on the wind farm, as these are not in operation at that time.
In the case of industrial wind farms, since there are many WTGs,
the ?eet average is a robust value. Small, instantaneous variations

of statistical hypotheses, this implies the need to test two hy- potheses. The ?rst test is on the expected value for Dxi,j(t). The null hypothesis (H1) is that the expected value, E  Dxi;j  t   , is a constant
(m?0?). The alternate hypothesis (H1) is that the expected value is not or change in behaviour of any one WTG will not affect the moni- toring of the other WTGs. Fig. 3 illustrates the calculation method
for the Dxi,j(t). The minimal size required by a wind farm will be constant. discussed in Section 6.
A change in Dxi,j(t) implies a change in the behaviour of the physical quantity j of the WTG i. It is easier to detect an abnormal wind  farm's  environment  or  by any seasonal  effect  are removed
while calculating the difference between the WTG and the average of all others WTGs. Due to the stochastic nature of wind, the out-

These two parameters are not functions of time since, in the

There are several ways of testing these two hypotheses. For the hypothesis, tests on the mean of a distribution (such as the Student t-test) may be used, and for the second, a test on the normality of distribution (such as the Kolmogorov-Smirnov test) might be appropriate. Here, however, rather than using statistical absence of abnormal behaviour,

As mentioned

tests to detect a change in Dxi,j(t) at any time, we will rely upon before and illustrated on Fig. 4 for the case of tower vibration, this distribution is normal. and, as mentioned above, this bias is constant in time. The standard devi- ation is a random noise caused by the ergodicity of the signal.
The detection of a change in behaviour of the physical quantity j of a WTG i in fact detects a change in distribution f(mi,j,si,j). In terms

control charts: these are more appropriate for continuous moni- toring of a quantity. The control chart methodology will be described in the following section.
Lebranchu et al. have presented in Ref. [8] a method for the monitoring of a main bearing using the temperature difference between it and the remaining bearings on its wind farm. Here we will present its application to a wider range of components of a WTG: we will also discuss this method's limitations.

Fig. 3. Illustration of the calculation of the difference between measurement of a single WTG and average of all other WTGs on the wind farm.
Fig. 4. Example of the distribution of the Dxi,j(t). Here the case of the Difference in tower vibration between a WTG and the wind farm's remaining WTGs.


4.1. Control charts

In order to decide whether or not the WTGs have sustained changes in their behaviour, an Exponentially Weighted Moving Average (EWMA) control chart was used. Control charts are used in Statistical Process Control (SPC). A time series of the data or a sta- tistic of a sample of this data (such as the mean, standard deviation, minimum, maximum) is plotted together with control limits [25]. Control limits are de?ned by the process's history or requirements, to 0.7 provided satisfactory results. However, l 0.3 seems to be the optimal value, and therefore, all our analyses were made using this value.
 The lower and upper control limits, LCL and UCL, were calculated based on the standard deviation of the process within a reference period. Here the reference period used was the ?rst year of avail- able data,  covering  any  seasonal  variations.  These  were  are expressed as follow expressions: and whenever the data steps outside of these limits, the process is viewed as out of control and thus, statistically different from its reference. There are a few rules that can be used to determine if the process is out of control or not.


In the present case, prior to using the control chart,



e main objective of this step was to reduce noise in the data. Here, data was taken at ten minute intervals, and the objective was to detect events building up on a daily to monthly time-scale. The use of a moving average as a high-frequency ?lter helped reduce the noise and improved detection performance. The period of the moving average depended on the quantity monitored and the time-scale over which problems could occur.
After the moving average was taken, the EWMA control chart was applied [26]. The expression for the EWMA Zi,j(t), for Dxi,j(t) was expressed as follows:
with Zi,j(0) is the target value for the process or its historical mean. The EWMA acted as a high-frequency ?lter with l as a smoothing value. l is from 0 to 1 and the smaller the l, the higher the smoothing. Depending on the level of variation in the variable of interest, the optimal lambda changes. For a large, sudden change, l must be relatively high, near 0.6e0.8 and for a small, progressive variation, l the optimal l falls somewhere between 0.2 and 0.4 [27]. Various values for this parameter have been studied and values up
with s, being the variance of the quantity monitored over a refer- ence period where the process has not experienced any abnormal issue. Usually, k 3 correspond to a three sigma or 99.9% con?- dence level. The EWMA control must be applied on data that is normally distributed. This requirement was satis?ed for each application presented.

5. Results

To illustrate the effectiveness of the proposed methodology, four study cases are presented here. In each of these cases, a change in behaviour of a WTG was observed and the use of a monitoring method could have reduced production losses caused by under- performances or prolonged downtime. The produced electrical energy is not directly impacted but nonetheless present. The study cases are from four different WTGs, each on a different wind farm.

5.1. Electrical energy produced

In this case, the monitored quantity is the electrical energy produced by a WTG. This kind of monitoring could re?ect problems such as underperformances or blade erosion. For each ten minute intervals where the WTG was available, the difference in produc- tion between it and the remaining available WTGs on the wind farm was calculated. This difference was then averaged on a monthly basis. Fig. 5 illustrates the application of a EWMA control chart to the difference in electrical energy production between one WTG and the rest of the ?eet.
 Following the application of the proposed methodology, it was found that between April 2012 and October 2012, on this WTG, a curtailment algorithm was mistakenly activated following major maintenance. If the proposed methodology had been used, the control chart would have detected a change in behaviour almost immediately. Instead, this step-type shift caused production losses of approximately 800 MWh per day over the whole period.

5.2. Tower vibration

Tower vibration can provide useful information on the health of the structure or highlight rotor or yaw problems. The tower vi- bration is measured directly under the drivetrain. In this applica- tion, the difference between this WTG and the ?eet average vibration level was calculated for each ten minute average while the WTG is available. Fig. 6 illustrates a case of a sudden change in vibration level: following the replacement of one of the blades on this WTG, the vibration level suddenly increased. This was caused by a mechanical imbalance of the rotor. A couple of weeks after the change, the rotor was rebalanced, and vibrations returned to their prior level. Rotor imbalance is a serious issue that can damage the drive-train's components if not treated rapidly.
For another WTG, a progressive increase in vibrations following preventive maintenance has been investigated (Fig. 7). During the maintenance of this turbine, the yaw system was over-torqued, causing excessive vibrations each time the WTG changed its orientation.
Analyzing the tower vibration can provide useful indications of damage the WTGs sustained in terms of fatigue analysis. This can

help plan preventive maintenance on wind farms and, over the long term perspective, prolong the lifetimes of WTGs.

5.3. Yaw

 Even though a WTG is designed to face the wind at all times, in some cases, like in imposed directional curtailment, yaw or nacelle azimuth is an important value to monitor. While it is assumed that the WTG is always facing the wind correctly, the value measured is not always accurate. In some cases, for two neighbouring WTGs, the SCADA will measure and archive major differences in yaw values. In the present case, since there is a discontinuity in the yaw (passing from 360¡ð to 0¡ð), the average and the difference must be calculated vectorially. Fig. 8 illustrates this issue, where the layout of one section of a wind farm at a certain time is shown. The position of each wind turbine is shown by a dot. Line color and length is an indicator of the turbine production level (blue is the lower and red is the higher), and line direction represents nacelle azimuth, or yaw, as provided by the SCADA system. As seen in Fig. 8, one of the WTGs appears to be drifting in a direction opposite to its neighbours. This situation is caused by an error in the measurement of the yaw itself: this value is measured using a proximity switch which counts the
number of metal teeth. If the teeth are worn, or if the grease con- tains metal particles, the tooth count can be erroneous, resulting in an apparent drift in yaw. Fig. 9 presents the case of an apparent drift of a WTG's nacelle. Since 2009, the difference between the yaw of this particular WTG and the mean yaw of the wind farm's remaining WTGs has been constantly increasing.
In general, this will not be an issue, because even in cases where measured yaw is incorrect, the WTG is, in reality, still facing the wind. However, when directional curtailment is applied to a WTG, if this curtailment is intended for a certain wind direction, yaw measurement must be reliable. The WTG's performance will be reduced in the wrong direction and the WTG will be unprotected in the proper direction.
As mentioned above, since the yaw is an angle, one must be careful with the mathematical operations. Here, sums and

Fig. 5. Case study of the application of a control chart to the difference between the production of a single WTG and the mean production of the remaining WTGs on the wind farm. This example shows the detection of a badly activated curtailment.

Fig. 6. Case study of the application of a control chart comparing a WTG's tower vibration of a WTG a with the mean vibration of the wind farm's remaining WTGs. This example shows detection of a rotor imbalance.
Fig. 7. Case study of the application of a control chart showing one WTG's the tower vibration as compared to the mean vibration of the wind farm remaining WTGs. This example shows the detection of a progressively rising tower vibration. subtractions must be done vectorially. Since yaw is an input parameter for a WTG, model monitoring cannot be used here and thus, proposed methodology of particular interest for monitoring yaw.

5.4. Gearbox temperature

It has been demonstrated that some failures in gearboxes or bearings can be detected using temperature analysis [8]. In these

cases, friction becomes greater and thus transfers energy by heat. Therefore, monitoring the temperature of gearboxes and bearings can possibly warn the operator of upcoming failures.
 Here, two different case studies will be presented on the application of the differences in gearbox temperature between a single WTG and the average of the wind farm's remaining WTGs. The gearbox temperature of the studied WTG is controlled in order to maintain its temperature around 55 ¡ðC. However, if the gearbox is near failure, its cooling system capacity may prove to be

Fig. 8. Partial layout of a wind farm at a certain time. Line color indicates the wind speed and line direction indicates wind direction. (For interpretation of the references to colour in this ?gure legend, the reader is referred to the web version of this article.)

Fig. 9. Case study of the application of a control chart to the yaw difference of WTG as compared to the mean yaw of wind farm's remaining WTGs. This example shows of slow drift.

insuf?cient. In February 2012, this gearbox failed and was replaced with a new one. In the weeks before the failure, the temperature had risen and the methodology proposed here would have raised an alarm in December 2011, two months before the failure occurred, as reported in Fig. 10. For wind farms in cold climates, as the ones studied here, it is important to avoid major repairs in wintertime.
A second issue occurred during winter 2015. Each winter, since, December 2013, the temperature of the gearbox oil of another WTG was found to be lower than the average temperature of the remaining WTGs on the wind farm, as reported by the control chart in Fig. 11. It was found that the oil temperature control system of the gearbox was faulty. Although, this situation is not as important as a gearbox failure, if the gearbox is not suf?ciently cooled or is overcooled, it will suffer damage. In winters of cold climates such as in Canada, the oil must be heated. If control over



6.1. Required wind farm size

 To use the mean of the wind farm as a reference, a certain minimum number of WTGs in the wind farm is required. The mean of the wind farm must be consistent. In the case of a wind farm that has too few WTGs if an important shift occurs on one of them, the average over the wind farm will be affected signi?cantly by that shift. Thus, the statistical inertia of the mean of a population (all WTGs of the wind farm) in this speci?c case must be evaluated.
 The effect on an important shift in the behaviour of one WTG on the average of a quantity is expressed by the following: overcooled gearbox.
 The temperature control of the gearbox makes the use of physical model-based monitoring dif?cult. A model that can pre- dict the temperature of the gearbox must take into account all parameters that could affect oil temperature. However, the power
   
Let's add a shift of dxi to one of the xi. We de?ne ¦Åx as the maximum acceptable impact on the average. Thus, we can re-write eq. (11) in this inequality:

of the cooling or heating system is not measured. Thus, it is now possible with the proposed methodology to provide a model that will adequately monitor a gearbox, based on available measurements.

which can simplify to:

Fig. 10.   Case study of the application of a control chart to the difference between one WTG's gearbox temperature and the mean temperature of the wind farm's remaining WTGs.   This example shows early detection of a gearbox failure.

Fig. 11. Case study of the application of a control chart to the gearbox temperature difference of one WTG as compared with the mean temperature of  the wind farm's  remaining WTGs. This example shows oil temperature control failure.

(13)
   
We choose to accept a maximum shift in the average of 1% of x, which sets ¦Å to 0.01. As for the maximum shift size to be observed that has no in?uence on the average, we choose 25% of x. This lead

In the present case, since all WTGs are in the same operating conditions and thus, each xi are similar, we can af?rm that x and xi are in the same order (x xi ). Thus, the precedent inequality becomes: to minimal wind farm size of around 25 WTGs. This limit is a guideline and the values for ¦Å and d could be different. However, ¦Å should be small, since the aim of this calculation is to limit the variation of x. As for d, smaller values would conclude to smaller wind farm size. However, a greater d would not be appropriate since we have made the assumption that x is similar to xi.
Here, the wind farms studied have each more than 50 WTGs.

Fig. 12. Centralized control chart for a WTG.

Thus, the proposed methodology can be used for all of them.

6.2. Centralized and normalized control chart

Since this methodology is optimal for use in large wind farms, and because several physical quantities are measured on each WTG, the number of control charts or ?gures needed for analysis can be great. One way to reduce the number of control charts is to group  all  the  physical  quantities  of  a  single  WTG  onto  a singlegure. In order to do this, Dxi,j(t) must be nondimensional. This means  reshaping  distribution  to  distribution  f(0,1). average of the remaining WTGs on its wind farm. Following this comparison, control charts are used in order to determine whether or not the WTG is behaving abnormally. Various cases based on the data available have been presented in order to illustrate the effec- tiveness of this monitoring approach.
 In comparison with model-based monitoring, the use of the wind farm as a reference is a simpler method that can be used by O&M engineers. While model-based monitoring can provide better results, it uses complex algorithm such as arti?cial neural networks, random forests, principal components analyses or other data- mining methods. The method we are presenting here is suitablefor industrial applications as it is simple and robust. It could easily be implemented online to continuously monitor WTGs. Moreover, it can be used for a wide range of turbine aspects and components. However, the proposed method can only be used on large wind farms (minimum size of around 25 WTGs) and if an issue is affecting all the WTGs in a wind farms, this method will not be able to generate an alarm.

One important aspect of a monitoring method is the manage- ment of alarms. A good method must maximize the detection rate and speed of changes in the behaviour of a WTG while minimizing the rate of false alarms. This kind of robustness required can be
are the average and the standard deviation of

provided by Control Charts such as the EWMA control chart.

Fig. 12 shows the centralized control chart for a WTG, with applications described in Section 5 (nacelle yaw; tower vibration; and gearbox temperature). For the case of this control chart, the control limits are set to 3 and 3, corresponding to three standard deviation limits. For the case of this particular WTG, the control charts detect a change in yaw in autumn 2010, while the behaviour of the other physical quantities remains in control.




Keywords:
Wind turbine monitoring Wind farm monitoring SCADA data
Fault detection Condition monitoring Performance evaluation
Abstract

The monitoring of wind turbines using SCADA data has received lately a growing interest from the fault diagnosis community because of the very low cost of these data, which are available in number without the need for any additional sensor. Yet, these data are highly variable due to the turbine constantly changing its operating conditions and to the rapid fluctuations of the environmental conditions (wind speed and direction, air density, turbulence, . . . ). This makes the occurrence of a fault difficult to detect. To address this problem, we propose a multi-level (turbine and farm level) strategy combining a mono- and a multi-turbine approach to create fault indicators insensitive to both operating and environmental conditions. At the turbine level, mono-turbine residuals (i.e. a difference between an actual monitored value and the predicted one) obtained with a normal behavior model expressing the causal relations between variables from the same single turbine and learnt during a normal condition period are calculated for each turbine, so as to get rid of the influence of the operating conditions. At the farm level, the residuals are then compared to a wind farm reference in a multi-turbine approach to obtain fault indicators insensitive to environmental conditions. Indicators for the objective performance evaluation are also proposed to compare wind turbine fault detection methods, which aim at evaluating the cost/benefit of the methods from a production manager＊s point of view. The performance of the proposed combined mono- and multi-turbine method is evaluated and compared to more classical methods proposed in the literature on a large real data set made of SCADA data recorded on a French wind farm during four years
: it is shown than it can improve the fault detection performance when compared to a residual analysis
limited at the turbine level only.

Introduction

Renewable energy has been a growing sector for several years, because of the necessity to reduce CO2 emission in the near fu- ture. The electrical power produced by wind turbines has been multiplied by 10 in the last 10 years. A significant amount of the electricity consumed in the world now relies on the electric power produced by wind farms, which have to be operational all along the year. Failures may cause important production losses, mainly due to the damages they cause and the time it takes to repair, which are no longer acceptable. This calls for a drastic change  in maintenance solutions, which must switch from periodic and corrective to condition-based. One of the motivations, and a per- spective, for the work presented in this paper is to develop a fault detection procedure that can deliver information on a developing fault early enough so that it can be used for condition-based or predictive maintenance decision making: indeed an in-advance detection allows the maintenance decision-maker to better plan the maintenance operations.
From the sensor technology point of view, 3 technologies are possible to monitor a wind turbine, [1]:
Using high rate data from a Condition Monitoring System (CMS) monitoring e.g. vibration: the use of these data  for fault detection usually relies on signal processing methods aiming at identifying the signature of a fault, e.g. in the signal spectrum;
Medium rate 1-second SCADA data: these data can be used for fault detection using for example model-based fault de- tection and isolation (FDI) methods, [2];
Slow rate 10-minutes averaged SCADA data: in this case, the measured quantities are averaged on 10-minutes length windows.
Several condition monitoring systems (CMS) are available on the market [1,3每5]. They are based on vibration analysis for the most part. They require additional sensors to be placed on each nacelle to measure the vibrations of several components of the drive train. The highly sampled acceleration signals they produce must be analyzed by an expert using signal processing methods, so as to detect faults in progress. Indeed, the automatic decision systems developed still generate a high number of false alarms. These make them a costly solution to implement [6,7].
On the opposite, SCADA systems have been integrated in wind farms since the emergence of wind energy. They generate loads of cheap data containing useful information on the turbines state but the data low sampling rate, one average measure every 10 min, is not adapted to an accurate in-depth monitoring of wind turbines. However, their availability for free and the breadth of SCADA data scope have encouraged researchers to propose solutions to create SCADA based fault indicators [1,8]. The work presented in this paper is developed for 10-minutes averaged SCADA, classically recorded for production supervision purposes.
SCADA data monitoring relies on the numerous variables mea- sured mainly for production supervision purposes. Fault moni- toring approaches based on SCADA data differ in the way the data can be merged to synthesize a fault indicator residual, and can be categorized in ＆＆mono-turbine methods＊＊ or ＆＆multi-turbine methods＊＊, as proposed in [9]. Mono-turbine methods combine variables recorded on a unique wind turbine while multi-turbine methods combine variables coming from all the turbines within a wind farm or subset of a wind farm. In this work, it is proposed to use 10-minutes SCADA data in an innovative way, combining a mono-turbine (or turbine-level) and a multi-turbine (or farm- level) information processing.
Wind turbines SCADA variables are highly non-stationary be- cause of the frequent changes in operational conditions and of the variations in external conditions. Following a classical diagnosis approach [2], mono-turbine methods aim at explaining a SCADA variable evolution as a function of other SCADA variables recorded on the same turbine and thus generate residuals (unexplained part) for diagnosis purposes. Variables may be linked by causal relations
- a change in some variables induces a change in other variables-  or by similarity relations two variables evolve in the same way because they are submitted to the same excitations, [9]. The most common causal model in wind energy is the power curve [1], which expresses the link between the wind speed and the active power produced by the turbine. It can be used as a visualization tool by comparing the power curve built with data gathered during the current period with the reference curve provided by the construc- tor [10]. It can also be used to create fault indicators by measuring the difference between the produced power and its value predicted by a model using the wind speed. In order to reduce the dispersion of the power data, additional input variables may be added such as the air density, the wind direction [8,11,12], the rotational speed or the pitch angle [13]. The model may be a simple polynomial approximation, a Gaussian process model or an artificial neural network [14]. The fault indicators may be the difference at each sampling time between the measured power and the expected power or it can be a distance between a reference power curve and an on line curve built with the most current measures [8]. The limitation of this approach comes mainly from (i) the length of the training period, which has to be large in order to cover  all the wind speed and temperature ranges, and (ii) the difficulty to localize the fault once it is detected with the power curve. To address the localization issue, one solution is to split the wind turbine into smaller independent systems, such as the rotor, the gearbox, the generator, the transformer and the convertor and to build models of these reduced systems, [15,16]. One popular variable to be modeled is the temperature of some components, whose variations can be explained by changes in the operational conditions or in the outside temperature. Models explaining the temperature variations use at least the produced power, the na- celle temperature and the train rotation speed as input variables. They are learnt on data measured when the turbine operates in normal conditions. The fault indicator is defined as the difference between the actual measured temperature and the expected tem- perature, named residual. Models may be static, i.e. they use data measured at one sampling time [17,18] or dynamic, such as ARX models [19,20]. They can be simple linear regression models or more complex artificial neural networks [21每25]. Let note at this point that several published works follow a classical model-based fault detection and isolation approach (model-based FDI) to build fault indicators sensitive to faults, but robust to disturbances, for wind turbine monitoring, [26每30]. In these works, the residuals are built using state observers or parity equations, [2]. The proposed methods are usually implemented on wind turbine benchmarks, emulated by differential equations models where different faults can be simulated. They use high frequency SCADA data, recorded every second. Though the results obtained are very interesting, the problem addressed in these works is very different in nature from the one addressed in the present paper where 10-minutes SCADA data are used and the methods are validated on real data.
Another approach is to model the temperature of one com- ponent as a function of the temperature of some other reference components that should evolve in a similar way, such as the tem- peratures of two bearings but also the bearing temperature and the stator temperature [18] or the temperature of the hydraulic break and the bearing temperature [31]. Such models using similarity may be simpler than models using causal relations but they rely on two temperatures evolving in the same way.
Mono-turbine approaches merge variables from the same wind turbine to generate fault indicator residuals that are insensitive to changes in its operational conditions. However, these residuals remain sensitive to the variations in the external environment such as the wind direction, air humidity and so on. On the op- posite, multi-turbines approaches merge variables recorded from different turbines of the same wind farm in order to reduce the influence of the environmental conditions. Indeed, turbines of the same farm are submitted to the same weather conditions so variables should evolve in a similar way, somehow. [32] compares the behavior of different turbines in the same farm using curves displaying the temperature of a drive train bearing as a function of the produced power. The temperatures and powers are measured during a period of time and the curves from all the turbines are plot on the same graph. Faults can be identified visually when one of the curves deviates from the others. [10] measures the difference between the power curve given by the manufacturer and an actual power curve built using data gathered during a current period of time. The difference between the two curves can exhibit a loss in performance. The differences measured for all the turbines in the farm are compared one with each other to detect a turbine with a larger loss of performance, which can be due to a fault. [33] compares the evolution in time of the temperature of a component normalized by the external temperature for turbines from the same farm. The deviation of the temperature of one turbine from the others can be the symptom of a fault. [17] builds residuals from the differences in the normalized temperatures and concludes that changes in operational conditions can create themselves fluctua- tions that are too large to allow for reliable fault detection.
The literature review shows that most authors proposed meth- ods to build fault indicators using mono-turbine approaches and so get rid of the influence of the operating conditions on the fault indicators. Few authors adopt a multi-turbine approach that allows getting rid of the influence of the external conditions, and mostly as a visualization tool. A method to synthesize a fault indicator for each turbine in a farm by comparing the temperature measured on a turbine to a farm reference (average or median of temper- atures measured on all the turbines within the farm) has been proposed in [34] and used in [35] on other types of measured SCADA variables. It has been shown that such an indicator remains sensitive to the operating conditions, which can be different from one turbine to another. Hence, no solution able to get rid of the influence of both the operating conditions and external environ- ment has been proposed thus far. Moreover, as stated by [36],  in their extensive review on wind turbines condition monitoring using SCADA data, there is a lack of published performance metrics to properly evaluate the advantage of one method from the others in terms of false alarm, true failure prediction and normal behavior prediction. To address these issues and fill this gap, we propose in this paper a hybrid multi-level synthesis method to take benefits of both approaches 每 mono- and multi-turbine 每 and to build fault indicators combining the two approaches. At the turbine level, residuals obtained with a mono-turbine model learnt during a normal condition period are first calculated for each turbine. At the farm level, these residuals are compared to a wind farm residual reference, in a multi-turbine approach. The use of mono- turbine residuals enables the influence of the operating conditions to be reduced while the use of a wind farm reference enables the changes in the environmental conditions to be accounted for. The performance of the method proposed is evaluated and compared to methods proposed in the literature on a large data set made of SCADA data recorded on a French wind farm during four years. Objective performance evaluation metrics are also proposed to compare the methods, which aim at evaluating the cost/benefit of the methods from a production manager＊s point of view. Fault indicators should warn of a progressing fault early enough for a maintenance operation to be scheduled. However, they should not generate false alarms with the extra cost of sending a maintenance operator team on a remote site for no reason.
The contribution of the work presented in this paper is then
twofold:
The first and main contribution of the hybrid multi-level approach proposed in this paper lies in the online real-time comparison of the prediction residual computed on one tur- bine (i.e. at the turbine level, using a turbine normal behavior model) to a farm-level reference prediction residual, com- puted using the prediction residuals from the neighbor wind turbines. This approach is said ＆＆hybrid multi-level＊＊ because

Second, the other contributions of the work presented in this paper are (i) the definition of objective performance metrics for the fault detection consistent with the applicative requirements of a wind farm manager and (ii) using these performance metrics, the evaluation of the proposed fault detection approach on a 4 years real data base from wind farm with 6 turbines, which shows that it has better performance than a residual analysis limited at the turbine level only.
The outline of this paper is as follows. Section 2 first presents different possible methods for the generation of fault indicators and focuses on the proposed combined mono- and multi-turbine approach developed in this work. The objective performance met- rics considered for the evaluation of the fault detection perfor- mance are also detailed in this section. The data set used to analyze the performances is described in Section 3 followed by a presenta- tion and a discussion of the results obtained.

2. Method
This section presents the proposed two-levels methodology to synthesize fault indicators by combining both a mono-turbine and a multi-turbine approach. Each of the two levels is described: synthesis of the fleet reference at the farm-level, and elaboration of a relevant turbine-level variable for comparison to the fleet reference. Finally, the performance evaluation metrics, which are used latter in Section 3 to compare the obtained fault detection results with different fault indicators, are introduced.
2.1. Fault indicators synthesis

2.1.1. Farm-level: comparison of the turbine variables to a fleet refer- ence
In wind farms, turbines are part of a fleet: they are of the same make and are subject to the same environmental conditions (wind speed, external temperatures, . . . ). Thus, the SCADA variables monitored on turbines from the same farm should evolve in a similar way when the turbines operate under normal conditions. The rationale for the method proposed in this paper relies on this assumption. SCADA variables from different turbines are compared on line. A deviation between a variable computed from a turbine and a farm reference is an indicator of an abnormal behavior. Its general concept is presented in Fig. 1.
 SCADA variables or model residuals computed from SCADA variables are recorded on line, averaged over a given time period and compared to a fleet reference. The distance between the vari- able monitored on a wind turbine and the fleet reference serves as a fault indicator for the turbine.
Let V be a variable measured on a wind turbine or a model residual synthesized on a wind turbine. V is assumed to carry in- formation on the turbine deterioration, and several ways to choose this variable V are considered in Section 2.1.2. Vj(k) is the value of the variable V from turbine j at time k. Let NT be the number of turbines in the wind farm. Let Win be an analysis window of size W . For each turbine j, with j varying from 1 to NT , a fault indicator Fj(k) is built at time k, as follows:
1. For each turbine j in the farm, an averaged value of the turbine-level variable Vj is calculated over the NWj samples present in the analysis window Win:
k

it combines a mono-turbine (or turbine-level) step with a multi-turbine (or farm-level) step. To the best of the authors＊ knowledge, no other existing and published method proposes to compare online in real-time the prediction residual gener- ated on a given turbine to a farm reference synthesized from the prediction residuals generated on the neighbor turbines.

Fig. 1. Multi-turbine approach: generic principle for the synthesis of fault indicator Fj for wind turbine j. The choice of the variable V as a mono-turbine residual corresponds to the combined mono- and multi-turbine method proposed in this paper.




the number of samples NWj is smaller than NWL, V j(k) is not computed as it would not be representative enough.
2. The fleet reference V fleet (k) is calculated using the averaged values V j(k) at the turbine level, as follows:

The fleet reference is calculated using the median and not the mean, so as to make it insensitive to abnormal values generated by faulty turbines. For the practical implemen- tation, the fleet reference is computed if the number of averaged values V j(k) computed at time k is higher than a limit number NTL (i.e. information is available from enough turbines) and the reference should be representative of the farm normal conditions as long as more than half of the turbines used to calculate the reference operate in normal conditions.
3. A fault indicator Fj(k) is then calculated for each turbine j as the distance between the monitored variable Vj(k) and the

In practice, NWL  is set to  W  + 1, which means that the variable  V j(k) should be available during at least half the analysis window W . NTL is set to NT  1 , which means that V j(k) should be available for more than half the turbines of the farm for the fleet reference   to be computed.
2.1.2. Synthesis of a turbine-level variable for comparison to the fleet reference
At the turbine level, the variable V used to create the fault indicator may be of two kinds: it can be either a residual generated as the difference between a measured value of SCADA variable and its prediction by a normal behavior model, or in a more straight- forward way, it can be directly a measured SCADA variable, Fig. 1.
The variable V may be a residual, computed as the difference between a SCADA variable and its prediction by a normal behavior model. In this paper, in the same logic as using a component temperature as a fault indicator, the difference between the measured temperature of a component and its temperature predicted by a physics-related model is used. In the generator, the active power produced by the turbine, the rotation speed of the drive shaft, and the nacelle temper- ature can account for a change in a component temperature.

So a linear model relating these variables to the component temperature is proposed, Eq. (4)

with Tj(k) the component predicted temperature, Pj(k) the active power produced, ?j(k) the shaft rotation speed, Tnacellej
(k) the nacelle temperature, recorded by the SCADA system
and a, b, c and d constant parameters. To built a normal behavior model, the parameters a, b, c and d of the model are regression coefficients learnt using a classical least squares algorithm [2], on data gathered during periods when the turbine is operating under normal conditions. The variable V is then taken equal to the temperature residual Rs taken as the difference between the actual measured temperature and its predicted value, i.e. for turbine j at time k:

The normal behavior model (4) expresses normal tempera- ture variations due to the turbine producing electric power. Thus, the model is valid only when the turbine is in operat- ing conditions and, for the practical implementation, all the samples gathered when the power produced by the turbine is below a production threshold Thp , i.e. when the turbine is stopped, are removed from the analysis window Win.
This hybrid ＆＆mono-multi-turbine＊＊ approach consisting in generating a fault indicator by comparing the residual gen- erated for each (＆＆mono＊＊) turbine to a (＆＆multi-turbine＊＊) farm reference is the original fault indicator synthesis proposed and defended in this paper for its good performance and lower sensitivity to both internal (operating conditions) and external (environmental variations) influences. The main steps for the implementation of this fault indicator synthesis are sketched in Algorithm 1.
In a more direct way, the variable V may be a SCADA variable, relevant in itself as a fault indicator, such as a component temperature. As seen in the literature -e.g. [18,31]-, compo- nent temperatures are commonly used as fault indicators. An overheating component can be the sign of a mechanical prob- lem or of a cooling system failure. In this case, the variable V is taken as the monitored temperature T of a considered component, i.e. for turbine j at time k:

Reading or sensors errors may affect SCADA measures. Thus, in the practical implementation of this fault indicator, any temperature value outside an acceptable range is removed from the analysis window Win.
This approach based on the direct comparison of SCADA data of each individual turbine to a farm reference has already been proposed and assessed in [34] for temperature mea- surements, and extended to other kinds of measurements in [35]: it has been proved too sensitive to the influence of possibly different operating conditions between each indi- vidual turbine. It is considered in this paper for comparison purposes with the proposed combined mono- and multi- turbine method.


Algorihm 1: General algorithm for the combined mono- and multi-turbine fault indicator generation


Phase 1 - Offline learning phase

Phase 2 - Online implementation phase 

2.1.3. Fault detection based on the generated fault indicators
The occurrence of a fault on wind turbine j leads to a change in the corresponding fault indicator Fj. Ideally, a fault indicator should be sensitive to a fault occurrence, but insensitive to the vari- ations of the farm environment and of the wind turbine operating conditions: building such a fault indicator was the motivation to propose a combined ＆＆mono-multi-turbine＊＊ approach. Under these assumptions, the statistical properties, in particular the expected value, of the fault indicator Fj changes with a fault occurrence, and the fault detection can be performed using statistical hypothesis testing tools (e.g. testing  hypothesis H0	E(Fj)	?0j vs. H1 E(Fj)	?0j), and can be implemented by setting a threshold on Fj. In theory, in this classical setting, when no information is available on the properties of the fault indicator Fj in presence of a fault, the value of this threshold can be determined using the estimated or assumed statistical properties of Fj when no fault is present in order
to guarantee, for example, a false alarm rate. In our approach, we want to avoid any additional hypothesis on the distribution of Fj under the ＆＆no fault ＊＊ assumption, which could be different for each turbine and could possibly lead to a different threshold for each turbine. Consequently, in the following, our approach is to set the detection threshold for the whole wind farm, on the basis of the whole available historical 4-years database for the farm in order to get a given false alarm level.

2.2. Performance evaluation indicators

Fault indicators are to be used in wind farms to assist mainte- nance operators. They should allow the detection of faults occur- ring on a wind turbine without generating too many false alarms. However, in the literature on wind turbine fault indicators, no paper addresses the issue of the performance evaluation of the indicators proposed. Most of them merely report the ability of their indicator to detect a particular fault. In this paper, we chose to evaluate the fault indicators proposed from a wind farm produc- tion managers point of view. Fault indicators should definitely be able to detect faults on wind turbines, to avoid a major degradation and then a costly repair. However, they should be able to do it early enough for a maintenance operation to be scheduled. Another major issue is the cost of false alarms. A false alarm results in a useless maintenance action: a maintenance team has to be sent to the production site to inspect the supposedly faulty turbine. Because wind turbines are usually located in remote sites, the inspection takes time and consequently costs a lot of money. So, in this paper, we propose 3 performance evaluation indicators, able to make a fair and objective assessment of the indicator usefulness: the detection time before failure, the useless maintenance action number and the indicator persistence, which gives to the operator some indications about the relevance and performance of the fault detection process based on the proposed indicator.
Performance evaluation indicators are computed from a data base gathered on a wind farm during a period of length D, where faults occurred. The data base is previously split into normal con- ditions periods and faulty periods. A normal conditions period is a period of time where no fault occurred on a wind turbine. A faulty period is a period of time where a fault was continuously present on a turbine. It ends when the failure occurs, i.e. when the turbine stops functioning.
Let Th be a detection threshold, set on the fault indicators Fj(k). When the indicator exceeds Th, an alarm is raised. Th is set for the whole wind farm, i.e. the same value is set for all the turbines fault indicators, Fj(k) .

2.2.1. Detection time before failure
The detection time before failure measures the time separating the first time the fault indicator exceeded the detection threshold Th during a faulty period from the failure time, i.e. the end of the faulty period. It gives an estimation of the time left to the maintenance team to repair the fault before the failure.

2.2.2. Number of useless maintenance actions
A false alarm occurs when the fault indicator exceeds the de- tection threshold Th during a normal conditions period. The false alarm rate is converted into an equivalent ＆＆number of useless maintenance actions＊＊ so as to better consider its cost. A ＆＆false alarm day＊＊ is a day when at least one false alarm occurred on at least one of the turbines. A ＆＆false alarm period＊＊ is a period made of consecutive false alarm days. The shorter possible length for a false alarm period is one day. During a false alarm period, a maintenance team has to be sent once to the wind farm location. So each false alarm period results in a useless maintenance action. The useless maintenance action number is the number of times a maintenance team has to be sent to the wind farm during the period D; it is equal to the number of false alarm periods.

2.2.3. Indicator persistence
The persistence measures the percentage of time during which the fault indicator remains above the detection threshold Th during a faulty period. A persisting indicator makes the maintenance operator more confident in the occurrence of the fault than an indicator constantly being set on and off.

3. Results and discussion

3.1. Data base

The methods presented in Section 2 are applied on a set of real data, gathered on a French wind farm located in the south of France. The 6 wind turbines forming the farm are identical. They are of the same make, conceived to produce 2 MW, with a horizontal axis. SCADA data were recorded every 10 min during 4 years, from November 2009 till December 2013.
During this 4 years, 6 single faults affecting the generator oc- curred on different turbines at different times, some of them gen- erating a major failure and, consequently, the machine shutdown for several weeks, see Fig. 2:
Fault on two bearings: 2 bearings broke down on 2 genera- tors, because of a lack of lubrification. One of the generators had to be replaced. These 2 episodes are named ＆＆faulty bear- ing WT6 (FB_WT6)＊＊ and ＆＆faulty bearing WT9 (FB_WT9)＊＊, for faulty bearing on wind turbines 6 and 9.
? Faults on two stator windings: two generators were stopped

The residual Rsj(k) is generated using Eq. (5), with Tj the predicted bearing or stator temperature using the normal behavior model in Eq. (4). The temperature variations due to changes in the power production are accounted for by the model. The model is learnt for each turbine, during a normal conditions period of two months at the beginning of the 4 years recording. Whenever a fault occurs on a turbine, the turbine model is learnt just after the necessary repair is made, on a fixed period of 2 months.

This model residual Rsj(k) is then averaged to obtained Rsj(k), using Eqs. (1) with Vj Rsj ;
The average residual Rsj(k) is finally compared to the fleet reference Rsfleet (k) computed using Eq. (2) with the averaged residuals from all the turbines, to obtain the fault indicator RESmultij (k).
3.2.2. Implementation of classical fault detection indicators for com- parison
   For each turbine j, three other fault indicators are considered: two of them are generated using a mono-turbine approach, the third one is generated by a simple multi-turbine comparison of the raw measurements (no residual generation at the turbine level).
The two indicators Tmonoj (directly based on raw measure- ments) and RESmonoj (based on residuals) are implemented following a classical mono-turbine approach, i.e. using SCADA data recorded from the turbine j alone:


an over-heating period which damaged the winding insu- lating material. The generators had to be replaced. These episodes are named ＆＆faulty winding WT9 (FW_WT9)＊＊ and ＆＆faulty winding WT11 (FW_WT11)＊＊.
? Faults on two cooling systems: a fault occurred on the cooling

either the bearing temperature or the stator tempera- ture.
In this case, the SCADA temperature is averaged over
24 h and thresholded, in a way very similar to the traditional SCADA detection system.
loose. These episodes are named ＆＆faulty cooling system WT7 (FCS_WT7)＊＊ and ＆＆faulty cooling system WT10 (FCS_WT10)＊＊.
The SCADA data used to implement and compute the fault indicators are the bearing temperatures, the stator temperatures, the nacelle temperatures, the active power produced and the shaft rotational speed on the 6 turbines.

3.2. Implementation of fault indicators
Four fault indicators are implemented and compared using the performance evaluation indicators presented in Section 2.2 calculated on the 4 years of data: the first considered fault indicator corresponds to the proposed combined mono- and multi-turbine approach, the three others are more classical and are implemented for comparison purposes. Each fault indicator is calculated every 10 min, using a sliding window of size W 144 samples, i.e.
24 h. NWL  is thus set to 72 and NTL  to 4. When relevant, the    limit production threshold Thp, above which the temperature can be predicted, is set to 50 kW, mainly because production levels below 50 kW correspond to starting or shutting down transient behaviors. In the following, depending on the considered fault to monitor, Tj can be either the bearing temperature or the stator temperature.

3.2.1. Implementation of the proposed combined mono- and multi- turbine approach	     	     
The indicator Rsfleet (k) is computed for each turbine j, following the proposed combined mono- and multi- turbine approach, see Algorithm 1:

Tj(k) Tj(k) where Tj can be either the bearing temper- ature or the stator temperature.
In this case, the residual generated using Eq. (5) and the model in Eq. (4) is averaged and thresholded. The temperature variations due to changes in the power production are accounted for by the model. The model is learnt for each turbine, during a normal conditions period of two months at the beginning of the 4 years recording. Whenever a fault occurs on a turbine, the turbine model is learnt just after the necessary repair is made, on a fixed period of 2 months.
An indicator Tmultij (directly based on measurements) is im- plemented using a multi-turbine approach, using a fleet ref- erence

	
每 Tmultij (k)   T j(k)   T fleet (k), using Eqs. (1) and (2) with Vj Tj, where Tj is the bearing temperature or the stator temperature. The component temperature is averaged and compared to the fleet averaged temperature refer- ence.

3.3. Performance results

Fig. 3 shows the evolution of the 4 fault indicators during the four years when the SCADA data were recorded on the 6 turbines of the wind farm. The upper figures display the evolution of the mono-turbine indicators (temperatures on the left, residuals on the right), the lower figures the multi-turbine indicators (temper- atures on the left, residuals on the right). The detection thresholds corresponding to a useless maintenance action number of 10, 20 and 30 (i.e. a total of 10, 20 or 30 useless maintenance actions

Fig. 2. Occurrence of failures and ＆＆failure periods＊＊ on the 6 wind turbines of the considered farm during the 4 years recorded database - Bearing failures in red, cooling system failures in green and windings failures in blue. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Fig. 3. Evolution of the 4 indicators for the 6 wind turbines over the 4-years period. The colored shaded areas correspond to the failure periods shown in Fig. 2. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article. will be generated on the whole farm during 4 years) are also displayed. The faulty periods are delimited by vertical lines. The five faulty periods correspond to the following faults, in chronolog- ical order: FB_WT6, FB_WT9, FCS_WT7, FW_WT11 and FCS_WT10 and FW_WT9 during the same faulty period. One can see that the seasonal variations inducing a change in the external temperatures are visible on the mono-turbine fault indicators. Both the tempera- tures and residuals are higher in summer and lower in winter. The fault indicators monitoring the 6 turbines follow the same pattern. During faulty periods, abnormal increases in temperatures can be seen on the faulty turbines. However, the temperature increases are somehow buried in the variations observed in the temper- atures during normal conditions periods. The faulty indicator to normal indicator contrast ratio (defined in a similar way as a signal to noise ratio: the energy of the indicator during a faulty period on the energy of the indicator during a normal condition period) is rather low, which makes the detection of the fault with a minimum amount of false alarms using Tmonoj quite difficult. The evolution of RESmonoj is rather similar. Since it is a residual, its evolution varies around 0. Its standard deviation is reduced compared to Tmonoj since the variations in temperature due to the change in power production are accounted for by the model and thus removed but its mean remains affected by seasonal temperature variations. Abnormal changes due to faults are more visible on the indicators but the fault detection using a fixed detection threshold seems still difficult to do.
The use of a multi-turbine approach makes it possible to remove the external temperature variations (seasonal effects) affecting the mono turbine indicators since the temperature variations are also present in the fleet reference and  consequently  subtracted  from the residuals. The faulty indicator to normal indicator contrast ratio seems to be increased and it is all the more so when RESmultij  is used, since the residual is rid of variations due to external temper- ature changes and variations due to changes in power production. Whenever a fault occurs on a turbine, it is preceded by an increase in the corresponding turbine residual, which clearly stands out from the other residuals. A fault detection strategy using a fixed detection threshold seems much more feasible.

Fig. 4. Performance characteristics curves: advance detection time as a function of the number of useless maintenance interventions for 6 different faults: FB_WT6, FB_WT9, FCS_WT7, FW_WT11, FCS_WT10, FW_WT9.

To compare the four indicators performances, two performance curves plotting the persistence or the detection time before failure as a function of the useless maintenance action numbers are built for each of the 6 faults. The curves are similar to traditional ROC curves, [37]. For a given value of the detection threshold, the detec- tion time before failure (advance detection time), the persistence and the useless maintenance action number are calculated and form a point in the performance curves.
The two performances curves are displayed in Figs. 4 and 5. The performance curves of multi-turbine indicators based detection are displayed in plain lines and mono-turbine ones in dotted lines. One can see that for the faults FB_WT6, FB_WT9, FW_WT11 and FW_WT9, the detection implemented with multi-turbine indica- tors outperforms the detection based on the mono-turbine ones, as could be expected from Fig. 3. Using these multi-turbine indicators allows detecting the faults earlier with a reduced number of useless maintenance actions. If the detection threshold is set at its highest value, i.e. set to obtain zero useless maintenance actions, with RESmultij  it is still possible to detect all the faults at least 500 min
ahead of time i.e. about 4 days. If the threshold is decreased to allow
3 useless maintenance actions, the faults can be detected as early as 1200 min ahead of time i.e. about 8 days, which is sufficient to plan a maintenance intervention. When mono-turbine indicators are used, FB_WT6 and FW_WT11, two major faults, cannot be detected. The detection time before failure remains close to zero whatever the value of the detection threshold. FB_WT9 and FW_WT9 can be detected two weeks ahead of time, with RESmono or Tmono, but at the very high cost of more than 15 useless maintenance actions. The two faults where the mono-turbine indicators performances equal the multi-turbine indicators are the faults on the cooling systems, FCS_WT7 and FCS_WT10, which occurred suddenly and generated a sudden and large increase in the recorded temperature.
The detection time before failure as a function of the useless maintenance action number curves show that RESmultij performs slightly better than Tmultij . The performances are comparable on

FB_WT6, FW_WT9, FCS_WT11, FCS_WT7 and FCS_WT10 but improved with RESmultij on FB_WT9. However, the persistence as a function of the useless maintenance action numbers curves shows
the superiority of RESmultij . Indeed, even if the detection time be- fore failure is equivalent, the persistence is globally higher when RESmultij is used. This is an important criterion because the higher the persistence is, the higher the confidence the operator may have in the indicator. Based on this criterion, RESmultij clearly outper- forms all the other fault indicators on the considered real data set, especially the mono-turbine ones. This is illustrated in Fig. 6 where a zoom in time is made on the period preceding FB_WT9. Indicators are presented in the same order as in Fig. 3. The colored lines are the detection thresholds which set the number of useless maintenance actions at 10, 20 or 30. Their value is adapted to each indicator. One can see that, though a slight increase in Tmonoj can be visually observed during the period preceding the fault, this increase cannot be detected with a fixed threshold. RESmonoj can detect the fault but only because it exceeds the detection threshold for a very short period of time. This could be figured out from the persistence curve whose value remained close to zero. On the contrary, the multi-turbines indicators show an obvious increase, which can be easily detected. The fault symptom appears earlier in RESmultij and the indicator remains more often above the detection threshold, which results in a higher persistence.

4. Conclusion

In this paper, we propose a new hybrid multi-level strategy to build fault indicators to monitor turbines within a wind farm, following a combined mono- and multi-turbine approach. The approach proposed reduces the effect of changes in the operational and environmental conditions on the fault indicators. Residuals computed from a mono-turbine model are compared to a fleet reference residual to form a multi-turbine fault indicator. The mono-turbine residuals are less sensitive to the internal variations because the SCADA temperatures are first processed using a model explaining the variations of the temperature in function of the operational conditions. Rises in the temperature due to increases in the power produced or in the rotational speed are accounted for by the model and removed from the residuals. The mono-turbine residuals are further post processed in a multi-turbine approach, which enables the impact of the variations in the environmental conditions to be reduced. Indeed, turbines in a same wind farm are part of a fleet. They are of the same make and model, and their variables evolve in a similar way since they are submitted to the same environmental conditions. New performance evaluation criteria were proposed to analyze the indicator performances for fault detection purpose, keeping in mind the cost of any useless maintenance action. The performance of the combined mono- and multi-turbine fault indicators were compared to more tradi- tional mono-turbine indicators on a data base made of SCADA data recorded every ten minutes from a real wind farm on a four year period. As far as we know, this is the first time a paper presents such an extensive analysis of fault indicators performance for fault detection, on real data with a wind farm production manager point of view. The results clearly showed that the effects of changes in both environmental and operational conditions on the combined mono- and multi-turbine residuals were reduced during the fault- free periods. This results in the combined mono- and multi-turbine fault indicators being able to detect all the faults occurring on the farm with a limited number of useless maintenance interventions and with an increased persistence.

Optimal Time Interval Between Periodic Inspections for a Two-Component Cold Standby Multistate System
Abstract¡ªThe establishment of the optimal time interval be- tween inspections for multistate redundant systems considering availability and costs related to maintenance and production losses is a challenging issue. This paper extends previous research for redundant multistate systems where the time-to-repair cannot be neglected. Discrete-time Markov chains are used to define transition probabilities between the different system states and the costs related to each transition. To optimize the time interval between inspections, the total cost is minimized utilizing the Markov chains properties followed by a numerical search technique. Two models are analyzed and numerical examples are presented. System I is a binary system with cold standby redundancy and component repair, while system II is a multistate system with cold standby redundancy and component repair. The main contribution of the method presented in this paper is the establishment of the optimal time interval between inspections for cold standby systems comprised of components that have different levels of degradation and where the component state can be determined only through periodic inspections. Systems with these characteristics are widely applied in industry, but are still not fully modeled in the literature.
I.INTRODUCTION
    Although time-based preventive maintenance is a strategy widely applied in industry, it is known that this maintenance approach leads to repair and replacement of components before the end of its useful life, increasing maintenance costs. This fact contributes to foster the interest of organizations and research in the development of models that allows for the use of preventive maintenance based on condition, where different states of system degradation can be observed and the best state for repair and replacement can be established. These systems are called multistate systems (MSSs).
    MSSs with different degradation states allow for more accurate component aging prediction, being more realistic than approaches that consider binary states (operating or failed) components. MSS can have several degradation levels, from perfect operation to complete failure. Some examples of MSS are hydraulic bombs in petroleum refineries, software systems, power generation systems, and train engine bearings. MSS may consist of a single component or many components. As the number of states and number of components increases, reliability analyses become much more complex [1].
    There are situations where MSSs have one or more components in a cold standby position. In these cases, if continuous monitoring is not possible, periodic inspections are recommended. These inspections verify the state of each component at predetermined times and initiate component repair if a high level of degradation is detected. The time between inspections should be optimized to maximize availability and to minimize costs [2]. Frequent inspections can serve to improve the availability, but involve high costs of preventive maintenance. On the other hand, lengthy times between inspections reduce total costs of inspection, but increase the costs of corrective maintenance and downtime, since there can be a long period when the system is unavailable [3], [4]. The establishment of the optimal time between inspections is essential to ensure the required availability with the lowest cost possible.
    Despite recent growth in the number of papers addressing this topic, redundant MSSs have been studied since the 1970s. In one of the first works about this theme, El-Neveihi et al. [5] developed a basic theory for the study of systems with finite number of states. Even though Karpinski [6] established the optimal time interval between periodic inspections for an MSS of a single component using Laplace transforms, most research about MSS consider continuous monitoring, as can be seen in Zhang et al. [7], Hsieh and Chiu [8], and Sheu and Zhang [9]. Also, these approaches study two main problems: 1) the optimal redundancy allocation for system reliability [10], [11], and 2) the optimal time interval for component replacement [9], [12], [13].
    Some authors analyzed the reliability of systems with different levels of degradation, but do not classify them specifically as multistate. These authors based their studies on Markov chains methods [14], [15]. However, it was only after the study presented by Levitin and Lisnianski [10], [16] that many studies on redundant MSS emerged utilizing the universal generating function (UGF) method.
    At the first moment, these authors developed a method for the joint optimization of redundancies and replacement intervals for MSS using UGF and genetic algorithms [10]. Later, the authors improve the method considering imperfect maintenance [16]. This method has been used along with Markov processes in recent studies concerned with MSS [2], [9], [11], [12], [17], [18].
    In addition to Markov process and UGF, some recent studies are applying recursive methods and Lz-transforms method to analyze the reliability and optimize parameters of MSS [19], [9], [20], [21]. The focus of Sheu and Zhang [9] and Lisnianski et al. [21] is optimization of age replacement time and Sheu et al. [19] focus in optimization of preventive maintenance schedule. Most of the studies on MSS analyze complex systems subject to continuous monitoring assuming that repair starts immediately after the failure occurrence. These papers aim to optimize the acquisition, allocation, and redundancy level along with the establishment of the optimal time interval to replacement. Just a few studies consider MSS subject to periodic inspections where the components and system state are verified only at inspections. However, this is a situation observed in many industrial scenarios, especially when the analysis of some component requires local inspection and the access is difficult. In these cases, the components are only monitored periodically.
    Le and Tan [22] optimized sequential inspections and continuous monitoring for an MSS assuming instantaneous repair. In this system, inspections are performed after a warning given by a continuous monitoring system, and repair or replacements are performed for the whole system and not for individual components. Lu et al. [20] utilized non-homogeneous continuous-time Markov chains, z transform, and particle swarm optimization to optimize the maintenance threshold and the inspection intervals of an MSS with five maintenance effects. Ruiz-Castro [23] analyzed the performance of an MSS with and without preventive maintenance using Markov process and reward processes algorithmically. This author considers a system subject to internal failures and external shocks with random inspections. Using semi-Markov process and Monte Carlo simulation, Koutras et al. [24] established inspection time and maintenance policies that optimize the dependability and/or performance of an MSS with deterioration.
    Considering a binary redundant system with instantaneous repair and constant failure rate, Alebrant Mendes et al. [25] established the optimal time interval between periodic inspections using Markov chain and search technique.
    As shown in the previous paragraphs, the interest for studies that address MSS subject to inspections has increased in the last few years. Nevertheless, none of them address the optimization of the time interval between periodic inspections of a system in which the components are only monitored periodically. Also, the application of discrete-time Markov chains to solve such problems is a challenging novelty.
    The main contribution of the method proposed in this paper is the analysis of redundant MSS comprised of components that have different levels of degradation that can be determined only through periodic inspections. This configuration is common in systems that cannot be continuously monitored and are difficult to access, such as, internal components of heavy equipment where substantial disassembly effort is necessary to access and verify components. The establishment of the optimal time interval among inspections, considering inspection, failure, and unavailability costs is important to ensure the desired reliability with the lowest cost possible.
    The complexity of problems involving redundancies increase when the time-to-repair is considered as a random variable distributed according to some defined parametric distribution. Usually, the authors suppose that repair times are exponentially distributed or fit a phase-type distribution, making the problem solution easier using Markov processes, as seen in Tian et al. [17], Mine and Kawai [26], Liu et al. [11], Lu et al. [20], and Montoro-Cazorla and Perez-Ocon [27].
    The study of systems having different levels of degradation using Markov processes also becomes complex as more components and degradation states are added. Soro et al. [28] and Sheu and Zhang [9] took into consideration only the degradation levels of the whole system, reducing the number of variables in the model and allowing the inclusion of many degradation states. In those studies that focused on the degradation states at the component level, as seen in Mokaddis et al. [15], Montoro-Cazorla and Perez-Ocon [27], Jia et al. [29], and Guilani et al. [30], only a few states and components were considered, since increasing the number of states and components substantially increases the number of equations needed to describe the problem.
    This paper presents a model for systems with two components, in which one of them is in the cold standby position, and having three levels of degradation besides the failure state. The addition of components into the system increases the number of equations and the space required to describe them, but the model can be generalized and utilized to determine the optimal time interval between inspections of redundant MSS with n components.
    The maintenance of a redundant MSS is a stochastic process since it has n components and each component can be in any level of degradation or failure at any time. Each combination of component states represents a likelihood in the state space of a Markov chain process. When periodic inspections are performed, the system state (different levels of degradation or failure) is verified at discrete times (only during the inspections). This scenario justifies the application of discrete-time Markov chains to study such problems.
    This paper is organized as follows. Section II describes the system, lists the assumptions and notation used, and ex- plains the methodology applied for modeling the problem. In Section III, the problem is solved defining the transition prob- abilities and modeling the costs of the system. In Section IV, numerical examples are presented and analyzed. Section V summarizes the paper and includes concluding remarks.
II.METHODOLOGY
    This section describes the system modeled and the assumptions accepted.
A. System Description
    This paper builds upon and extends the method developed by Alebrant Mendes et al. [25] to establish the optimal time interval between periodic inspections. Since the time-to-repair cannot be neglected in many situations, this study first extends the proposed method to incorporate this aspect. Through the inclusion of the time-to-repair in the model, its effect in the system reliability and costs can be analyzed. Next, an additional aspect was modeled. Given that many redundant systems suffer degradation during operation, and that, in many cases, the degradation level can be verified through inspections, the method was extended to accommodate MSS and inspection. The systems analyzed in this study are presented next.
    These systems reflect real systems installed in a petrochemical company that operates in southern Brazil. This is a large company comprising several operational systems, many of them 1) in a difficult-to-inspect position (demanding some disassembling for inspection) and 2) protected by redundancy. The operating condition of some of these difficult-to-inspect systems cannot be properly evaluated and, as far as detection is concerned, they change directly from operational to failed state (as system I described next). The operating condition of other difficult-to- inspect systems, however, may be evaluated and a maintenance decision may be taken based on that evaluation (as system II de- scribed next). Regarding the numerical example (presented in Section IV), in order to protect confidential information, the real numbers concerning costs, mean time between failures (MTBF), and mean time-to-failure (MTTF) were modified, but the aver- age proportion [between MTBF/mean time-to-repair (MTTR) and among costs] observed in the petrochemical company was preserved.
    a) System I ? Binary system with cold standby redundancy and component repair
    In this system, one component is active and the other is in a cold standby position. When the active component fails, the cold standby component is instantaneously activated. During the periodic inspection, the binary state (operational, failed) of each component is verified and component repairs are per- formed when necessary. Time-to-repair is a random variable and the system operates unprotected (without redundancy) until the repair is completed. The repaired components return to the system in the cold standby position. The system fails when both components fail between two consecutive inspections or if one component fails while the other is under repair.
    b) System II ? Multistate system with cold standby redundancy and component repair
    In this system, one component is also active and the other is in a cold standby position. Each component has, besides the failure state, three different operational states related to its degradation level:
    1) excellent (e) ? component is operating in a perfect state;
    2) good (g) ? component is operating in a good state, but it is possible to verify some level of degradation; and
    3) poor (p) ? component is still operating, but in an advanced state of degradation, being recommended performing a repair to avoid failure.
    Components move from excellent to good and from good to poor, and if not repaired, from poor to failure state. Components never fail before passing through all levels of degradation. However, the system can pass through more than one state and fail between two consecutive inspections. When the active component fails, the cold standby component is instantaneously activated and experiences the same degradation levels of the previous component. A component is sent to repair if a poor or failure state is detected during inspection.
C. Assumptions
    This study has the following assumptions for system I:
    1) components have only two states (operating or failed);
    2) perfect and instantaneous switching;
    3) times-to-failure follow an exponential probability distribution;
    4) components have different parameters for the probability distribution of times-to-failure and equal parameters for the probability distribution of times-to-repair;
    5) components in cold standby position do not degrade or fail;
    6) each inspection is perfectly and instantaneously performed; and
    7) perfect repair.
    For System II, the assumptions are:
    1) components degrade incrementally, experiencing all degradation states before fail;
    2) perfect and instantaneous switching;
    3) times-to-transition between degradation states and times-to-repair fit an exponential probability distribution;
    4) components have different parameters for the probability distribution of times-to-transition and equal parameters for the probability distribution of times-to-repair;
    5) components in cold standby position do not degrade or fail;
    6) each inspection is perfectly and instantaneously performed; and
    7) perfect repair.
    This study analyzes systems where failures are neither detected nor repaired until an inspection is performed. If the system failure occurs before the inspection, the downtime results in an increase of costs. These costs increase until the next inspection. When the system is still working and a component failure is detected, the component is repaired (with an associated cost and time) to preserve the reliability level. In system II, if inspection detects a poor condition, the component is also repaired (with an associated cost and time) to prevent system failure.
    Markov chains are an attractive model for this class of problem because complex behavior can be represented efficiently, and an appropriate mathematical model developed and used to optimize design or maintenance. However, it should also be clearly stated that use of Markov chains requires a series of assumptions that are mostly appropriate and reasonably accurate. There may be cases where the assumptions simply are not valid and the models proposed in this paper should not be applied. For example, when a repair is ongoing and an inspection occurs, it is assumed that the random repair time begins again.
    Exponential probability distributions were used to facilitate the use of Markov chains. The exponential distribution is appropriate and convenient to model times-to-failure, times-to-repair, and times-to-state transition for many components. However, if it is not the case, other approaches, such as Monte Carlo simulation, should be utilized.
III. RELIABILITY AND COST ANALYSIS OF REDUNDANT SYSTEMS SUBJECT TO PERIODIC INSPECTIONS
    In this section, the transition probabilities for the two systems are developed using Markov chains. Next, the costs related to system maintenance are determined and a cost function is developed and minimized to find the optimal time interval between inspections.
A. Definition of Transition Probabilities Using Discrete-Time Markov Chains
    A Markov chain is a process consisting of a finite number of states and known transition probabilities pij , where pij is the probability of transitioning from state i to state j. The probabilities pij depend only on states i and j, not depending on the time or previous number of transitions n or the previous states visited. The set of states in a Markov chain process is called state space [31].
    Considering that each different configuration possible to be found during an inspection is a state and that transition times from one to another state are random variables, it is possible to define a state space diagram for both systems being studied, as presented next in this paper.
    Every time a periodic inspection is performed, the system state is verified. Once in a failure state, the system does not change to another state anymore. Each state of system failure is an absorbing state of the Markov chain. The state space diagram and the transition matrix of each system are presented as follows. 
    The model corresponds to two-component systems. In theory, a corresponding Markov chain could be developed for larger systems with more components, but practically, it would become inefficient and cumbersome. Therefore, for larger systems, alternative models might be more practical. Stochastic Petri nets and Monte Carlo simulation have demonstrated much promise for problems of this type
    a) System I ? Binary system with cold standby redundancy and component repair: The state space diagram for system I is presented in Fig. 1. The circles Sn represent each state from the state space. Each letter inside the circles represents the com- ponent state: operating (O), standby (S), repair (R), or failure (F). Each number associated with each letter inside the circles represents one component (1 and 2). For example: the state S0 has the component 1 in operating state O1 and the component 2 in cold standby state S2. The states S6, S7, and S8 represent system failure and are absorbing states for the purpose of this study.
    The matrix P1, in Appendix B, shows the transition prob- abilities for system I. For example, the probability p30 is the probability of transitioning from state 3 to state 0. This prob- ability is the probability of component 2 being repaired and component 1 being operative during the time interval between inspections.
    Since the time-to-failure (TTF) of the active component, the time when the standby component starts to operate, and the time- to-repair are random variables, standard probability theory was used to determine these probabilities. For example, the probability p30 of transitioning from state 3 to state 0 can be determined as the probability of the time-to-repair of component 2 being smaller than the time interval between inspections combined with the probability of the TTF of component 1 being higher than the time interval between inspections: Pr R2 < ¦Ó T1 > ¦Ó  . These probabilities are expressed by 15 equations. Equations (1)?(7) are presented next. The other equations can be checked in Appendix A.
    1) Probability of component 1 remaining operational during the time interval between inspections:
    2) Probability of component 1 having failed and component 2 starting to operate but not failing during the time interval between inspections:
    3) Probability of components 1 and 2 failing during the time interval between inspections:
    4) Probability of component 2 remaining operational during the time interval between inspections:
    5) Probability of component 2 having failed and component 1 starting to operate but not failing during the time interval between inspections:
    6) Probability of component 1 being repaired and component 2 remaining operational during the time interval between inspections:
    7) Probability of component 1 being repaired before component 2 fails, given that the component 2 fails and component 1 starts to operate during the time interval between inspections:
    Considering (1)?(7) presented here and (32)?(39) presented in Appendix A, P1 can be rewritten. The matrix P1 can be checked in Appendix B.
    b) System II ? Multistate system with cold standby redundancy and component repair: The state space diagram for system II is presented in Fig. 2. The circles Sn represent each state from the state space. Each letter inside the circles represents the component state: operating in an excellent state (Oe), operating in a good state (Og), operating in a poor state (Op), cold standby (S), repair (R), or failure (F). Each number associated with each letter inside the circles represents one component (1 and 2). For example, the state S0 has component 1 operating in excellent state O1e and component 2 in cold standby state S2.
    Fig. 2 shows that there are 21 possible states (S0 to S20) in the state space of system II. The state of the components of each system state is presented in Fig. 3.
    The system starts with component 1 operating in an excellent state and component 2 in a cold standby position. The system can stay in the same state or move to another state. Repair of failed components starts just after the next periodic inspection. Components in a poor state of operation are also sent to repair after the periodic inspection, if the other component is available to operate. If the other component is being repaired, no action is taken during the periodic inspection. However, if the other component is in a failure state, the failed component is sent to repair during the periodic inspection. In both cases, the system stays operating with the component in a poor state.
    Times-to-repair of each component vary according to their probability distribution. The system fails when all components fail during the time interval between inspections or if a component fails while the other component is being repaired. After failure, the system is not repaired. Thus, states S18, S19, and S20 are absorbing states.
    Matrix P2 (see Appendix B) presents the transition probabilities of the system. The probabilities in matrix P2 were solved using probability theory. For example, the probability p4,15 is the probability of component 1 being repaired before component 2 having failed and, in the sequence, component 2 failing and component 1 degrading until the poor state of operation, during the time interval between inspections. This probability can be calculated through the combination of three probabilities:
    1) probability of the time-to-repair of component 1 being smaller than the sum of the times-to-degradation and failure of component 2;
    2) probability of the sum of times-to-degradation and failure of component 2, plus the time-to-degradation of component 1 until the good state of operation, being smaller than the time interval between inspections; and
    3) probability of the sum of times-to-degradation and failure of component 2, plus the times to component 1 degrading until the poor state of operation, being higher than the time interval between inspections:
    The calculation of these probabilities follows the same methodology used to solve the probabilities of system I. Some probabilities and their solution in terms of integral equations are presented in the following. The probabilities were solved using the software MATLAB 7.9 R2009b.
    The system has a total of 79 equations that represent the states transition probabilities. Two examples of these probabilities and their solutions are presented next:
    1) Probability of component 2 degrading and failing after component 1 has been repaired and component 1 starting to operate and degrading from the excellent state to the poor state of operation, during the time interval between inspections:
    2) Probability of component 2 degrading from the excellent state to the poor state of operation and component 1 remaining under repair, during the time interval between inspections:
B. Cost Models
    Optimizing the measure of performance per unit of time is equivalent to optimizing the measure of performance over a long period. Thus, the cost models in this paper are based in the establishment of the maintenance total cost per cycle. In this application, a cycle is the period of time elapsed from the beginning of the system operation until its failure:
    Four elements were considered to determine the costs associated with the redundant system subject to periodic inspections:
    1) cost of periodic inspection (Ci);
    2) cost of component repair (Cr);
    3) cost of downtime per time (Cp); and
    4) cost of system repair (Cs).
    The cost of periodic inspections comprises costs of manpower, tools and materials required to perform the inspection, even if there are no components in the failure state. The cost of component repair includes costs of manpower, tools, replacement of parts, and materials utilized to repair failed component. The cost of system repair is related to costs incurred to reactivate the system to its completely operational condition after a system failure. Finally, the cost of downtime refers to the production losses during the time that the system is down. This cost includes the loss of sales opportunity and monetary fees for delivery delay.
    The costs of periodic inspection, component repair, and system repair are assumed to be constants and must be determined for each system individually. On the other hand, the cost of downtime is a function of time, because losses increase along with the length of system unavailability. To establish this cost, it is necessary to determine the expected downtime.
    For a binary system (system I) comprised of two components (1 and 2) in parallel, in which one component is in cold standby position and both components have times-to-failure that follow an exponential probability distribution, the mean time to system failure (MTTFij ), given that the system is in state i and goes to state j that is a failure state, is calculated as:
    For an MSS (system II) comprised of two components (1 and 2), in which one component is in cold standby position and both components have times-to-transition that follow an exponential probability distribution, the mean time to system failure (MTTFij ), given that the system is in state i and goes to state j that is a failure state, is calculated as following. The letters associated with the parameter ¦Ë represent the component operation state:
    Consider a system having only one component with times-to-transition following an exponential distribution. Supposing that this component does fail during the time interval between inspections (¦Ó) and that this interval is relatively small when compared with the expected TTF, the expected downtime would be:
    Consider a system with two components where times-to- transition follow an exponential distribution. Supposing that these components both fail during the time interval between inspections (¦Ó) and that this interval is relatively small when compared with the expected TTF, the approximated expected downtime would be:
    Equations (25) and (26) are approximations that are appropriate when the time interval between inspections is smaller than the expected time to system failure, as usual in industry. Exact relations could be derived based on a truncated conditional distribution. However, this approximation is useful for the purpose of this study, given the complexity of the cost rate model.
    The approximations in (25) and (26) are based on three observations:
    1) all transitions related to downtime are associated with a system failure, so the expected downtime is conditional on the occurrence of a system failure (all components in operation in the beginning of the interval fail),
    2) the occurrence of failures in a cold standby system is modeled as a homogeneous Poisson process with rate ¦Ëi until the last component fails, and
    3) the probability of more than one failure for a Poisson process with rate ¦Ëi is small for an interval shorter than the expected time to system failure.
    Combining 2) and 3) it is possible to approximate the occurrence of exactly n failures during the interval, where n is the number of components working in the beginning of the time interval between inspections. For a Poisson process, the expected occurrence of a particular number of failures is distributed uniformly in a fixed interval. So, for n = 1, the expected TTF is ¦Ó /2 and downtime is estimated as ¦Ó /2 + MTTR. For n = 2, the expected times-to-failure are ¦Ó /3 and 2¦Ó /3, where the second fail represents the system fail and downtime is estimated as ¦Ó /3+ MTTR. When the time intervals between inspections become longer than the expected TTF, observation 3) does not hold and this approximation is not appropriate.
    As the time interval between inspections increases, the expected downtime becomes longer than the system MTTF and the approximations used previously are no longer appropriate. For ¦Ó >MTTFij, the expected downtime can be approximated using (¦Ó MTTFij) + MTTR. As a result, the expected downtime for all cases can be approximated by:
    where n is the number of components that are in an operational condition at the beginning of the interval between inspections.
    When the inspection interval becomes much larger or much smaller than the expected failure time, the approximation in (27) converges to the exact solution, and for other cases it provides a reasonable approximation that is practical and efficient. If a decision maker desires an exact solution, a more rigorous probability model could potentially be developed [31]. However, for most applications, there is unlikely to be a detectable difference in the optimal inspection intervals selected.
    The cost matrix of each system is presented next. The costs showed in the matrices represent the costs incurred for each transition between two states in the respective system state space. For both systems, for each inspection, cost of inspection is computed. Whenever a component fails, cost of repair is added to the model. Whenever the whole system fails, the cost of inspection, cost of system repair, and cost of downtime are summed. The matrices C1 and C2 show the costs for systems I and II.
    After determining the costs involved in each redundant system subject to periodic inspections, it is necessary to combine these costs to establish the total cycle cost. This total cost can be calculated utilizing the discrete-time Markov chain properties. Accordingly to these properties, the expected number of times that the process passes in the transient state j, given that it started in the transient state i, is given by the matrix N [see (28) at the bottom of the page], where Q is the transient part of matrix P [31].
    Since the sum of each line in N reveals the expected number of discrete time steps before absorption, given that the chain began in the ith non-absorbing state, and using the probabilities and costs for each transition, the expected cost in a cycle can be calculated by:
    The length of the cycle depends on how many times the system goes from state 0 to the other states and how long the time between inspections is. Then, it is possible to calculate the expected length of the cycle by:
    Based on (29) and (30), the total cost as a function of ¦Ó can be rewritten as:
    The total cost in (31) can be minimized and the optimal time interval between inspections can be established setting values for the costs of inspection, component repair, downtime, and system repair. Since the time interval between inspections is the only variable in the equations, it is possible to determine the minimum total cost using numerical search techniques.
    Genetic algorithms as other meta-heuristic search algorithms could be utilized to obtain solutions for this problem, but this approach was not necessary. This problem has a relatively complex objective function of costs, but its optimization is performed for only one decision variable: time interval between inspections. Consequently, a simple one-dimensional numerical search is sufficient. As this model is extended, there will be more advanced multivariable problems requiring methods such as genetic algorithms.
NUMERICAL EXAMPLES
    Numerical examples for each system studied are presented and analyzed next. For comparison purposes, identical parameters of costs and time-to-repair were utilized in both systems. Equivalent parameters of MTTF were considered. The MTTF of each component in system I (1¦Ë1 and 1¦Ë2), which is a binary system, is equivalent to the sum of mean times-to-transition between states excellent to good, good to poor, and poor to failure of each component in system II (1¦Ë1e + 1¦Ë1g + 1¦Ë1p and 1¦Ë2e + 1¦Ë2g + 1¦Ë2p), assuming that the system is not inspected and repaired.
    The parameters values were determined to represent real systems scenarios. Despite being similar, components often have failure rates that are different. The cost parameters utilized are selected based on real scenarios where the cost of inspection and component repair is smaller than the cost of system repair and downtime. Figs. 4 and 5 show the total cost per cycle per month and the optimal time interval between inspections in days for systems I and II, respectively. The parameters utilized were:
    1) System I?Different parameters of exponential distribution for times-to-failure for each component: ¦Ë1= 0.005, ¦Ë2=0.006 (these parameters represent MTTFs of 200 and 166.67 days, respectively).
    2) System II ? Different parameters of exponential distribution for times-to-transition between different states of degradation for each component: ¦Ë1e=0.02, ¦Ë1g=0.01, ¦Ë1p=0.021, ¦Ë2e=0.025, ¦Ë2g=0.015, and ¦Ë2p=0.026 (these parameters represent MTTFs of 200 and 166.67 days).
    3) Equal parameters of exponential distribution for times-to- repair: ¦Á1 and ¦Á2=0.05.
    4) Ci=10 000 and Cr=30 000: cost of component repair is higher than cost of inspection.
    5) Cs=100 000 and Cp=100 000/units per day: downtime cost is higher than the other costs. It means that unavailability incurs higher costs than those associated with inspection and repair. See the equation shown at the top of the next page.
    Fig. 4 shows that the time interval between inspections that minimize the total cost for system I is 14 days. In the same way, Fig. 5 shows that the optimal time interval between inspections for system II is 28 days. Even with equivalent MTTFs, the optimal value for the time interval between inspections is quite different between the two systems presented. The binary system has a higher minimum total cost and requires a time interval be- tween inspections smaller than the MSS. This happens because in system II it is possible to identify an advanced level of degradation and perform the repair before the failure has happened. This reduces the costs related to system failure and downtime.
    Moreover, results show that the model developed for the optimization of binary systems is not effective for MSSs. The analysis of MSSs assuming that they have a similar behavior to binary systems can lead to an unnecessary reduction of times interval between inspection and a consequently increase of the total costs.
    A sensitivity analysis was conducted to analyze the effect of different parameters (¦Ë1e, ¦Ë1g, ¦Ë1p, ¦Ë2e, ¦Ë2g, ¦Ë2p, ¦Á1, ¦Á2, Ci, Cr, Cs, and Cp) in the total cost per cycle of system II. Fig. 6 presents six graphs. Fig. 6(a) and (b) shows the effect of ¦Á in system with different parameters of ¦Ë1e, ¦Ë1g, ¦Ë1p, ¦Ë2e, ¦Ë2g, ¦Ë2p. Fig. 6(c), (d), and (f) shows the effect of Ci, Cr, Cs, and Cp, on the total cost and the optimal time interval between inspections.
    Accordingly to Fig. 6(a), higher repair rates (¦Á) increase the minimum CT while the optimal times interval between inspections remain approximately the same. When Fig. 6(b) is com- pared with Fig. 6(a), it shows that smaller rates of degradation and failure reduce the system total cost. Fig. 6(c) shows that higher costs of inspection (Ci) cause higher minimum CTs and times interval between inspections. Fig. 6(d) demonstrates that costs of repair (Cr) have a small effect on CT and that this effect does not change over the range of ¦Ó. Fig. 6(e) demonstrates that costs of system repair (Cs) also have a small effect on CT. Finally, Fig. 6(f) shows the effect of costs of downtime on the minimum CT. Costs of downtime have a higher effect on minimum CT and this effect increases when ¦Ó increases.
    There are two ways to generalize the solution of the problem presented in this paper to consider systems consisting of a larger number of components:
    1) the exact solution can be obtained via Markov chains by implementing in software the same reasoning presented in this paper for the two-component systems. Software support would enable to cope with systems constituted by a larger number of components, since the software could handle the hundreds or thousands of transition states that would be established in cases involving a larger number of components.
    2) An approximate solution could be obtained using Monte Carlo simulation. The Monte Carlo simulation would have an important advantage related to the possibility of assuming any probability distribution for the variables involved. In any case, it is worth mentioning that safety systems usually have a backup (as the problem presented in this paper), but a relatively small number of backups, usually two, three, or four components in parallel. This makes it feasible, though laborious, to develop a software or to structure a Monte Carlo simulation.
    Aiming to solve the generalization problem by simulation, discrete event simulation approach could be used to develop the simulation framework along with Monte Carlo simulation to generate the random numbers that represent the probabilities that are characteristic of stochastic processes. The distributions parameters and the costs variables would be the simulation input parameters. As output, we would have the systems TTF, the numbers of inspections and repairs, and the total cost per cycle. The variables of the simulation would be:
    1) the time variable t (referring to the amount of time that has elapsed until the end of the simulation) would be represented by the system TTF;
    2) the counter variables would be the number of inspections, number of repairs, and time of operation; and
    3) system state variable (that describes the system state at t) would be all the system states possible to be found during a periodic inspection.
    The periodic inspections would be the events when the values of the variables would be changed or updated and relevant out- put would be collected. The random variables generated through Monte Carlo simulation would be used to represent the probabilities of the distributions of times-to-transition and times-to- repair. Having the probability, it would be possible to determine the distribution matching time that represents a random time-to- transition or time-to-repair. Every time, when we run the simulation, we would get as output an independent random variable that would have always the same probability distribution. The average of these values would be then used as an approximator of our variable of interest. In order to have a quality approximator and an acceptable error, it would be necessary to run the simulation many times and collect an appropriate number of random variables. The logic that would be needed to build the simulation is similar to the logic developed to solve the problem using Markov Chains. However, the equations that would be needed to be solved are much simpler, because it would not have integral or differential calculations.
V. CONCLUSION
    This paper presented a model to establish the optimal time interval between periodic inspections for MSS with redundancy in cold standby position utilizing discrete-time Markov chains.
    MSS in which different states of degradation are identified captures in detail the component aging, representing in a better perspective many of real situations than other approaches that consider systems as binary (operating or failure state). When continuous monitoring is not possible, periodic inspections are required. During these inspections, component states are verified and repair is started when a failure state or an advanced state of degradation has been detected. The time interval between inspections must be optimized to minimize costs. Frequent inspections improve the system availability, but require higher costs of preventive maintenance. On the other hand, long periods be- tween inspections reduce the costs associated with inspections, but also increase the costs of corrective maintenance, downtime, and costs related to safety accidents. The establishment of an optimal time interval between inspections is important to ensure satisfactory system availability along with the lowest cost possible.
    The method presented in this paper uses discrete-time Markov chains to define both the transition probabilities between system states and the costs related to each transition. To optimize the time interval between inspections, the total cost of maintenance was modeled and minimized using the Markov chains properties and a numerical search technique. The minimal cost and the optimal time interval between inspections were obtained taking into consideration the costs of unavailability (downtime) and maintenance (inspection, component repair, and system repair).
    Two redundant systems in cold standby position were modeled and numerical examples for systems comprised of two components were presented: 1) System I ? Binary system with cold standby redundancy and component repair; 2) System II?MSS with cold standby redundancy and component repair. Results reveal that the binary system has a minimum total cost higher and requires a time interval between inspections smaller than the MSS. This happens because in MSS it is possible to identify advanced levels of degradation and perform the repair before system failure. This reduces the dominant costs related to downtime. Results also show that the model developed for the optimization of binary systems is not effective for MSS. The analysis of MSS assuming they have the same behavior of binary systems conducts to an unnecessary reduction of time intervals between inspection and a consequently increase of the total costs.
    This model pertained to a system with cold standby redundancy. An analogous model could also be developed for active redundancy, or in an extended research effort, a system using both active and cold standby together.
    For future research directions, one promising option would be the establishment of the optimal number of redundant components for an MSS comprised of identical components, aiming to achieve predetermined reliability requirements. Also, a model with imperfect maintenance or immediate detection of failures would be interesting and practical for some applications. Furthermore, an interesting improvement in the model developed could be the assumption of the cost of repair as a function of time. This would represent the inclusion of another random variable into the model.
    
Reliability optimization of series-parallel systems with K-mixed redundancy strategy
    This paper revisits the redundancy allocation problem, a well-known problem in reliability optimization, and implements a novel redundancy strategy, called K-mixed, to improve system reliability. The K-mixed strategy is a general form of a mixed strategy. The mixed strategy is a combination of active and standby redundancy strategies which was introduced in 2014. Initially, the mathematical formulation for calculating the reliability of the K-mixed strategy is investigated and then its power and efficiency are evaluated against different test problems and a famous benchmark problem. In order to solve the proposed benchmark problem, an efficient genetic algorithm is developed and the results are compared with those reported elsewhere. It is found that the proposed K-mixed strategy results in higher reliability in most situations than would otherwise be achieved by its mixed counterpart. It is concluded that by using the K-mixed strategy in an optimization model, system designers have the opportunity to select the best strategy for each subsystem from among all the different strategies available to design systems with maximum reliability.
1. Introduction
    High technology systems are mostly complex and expensive, but also need to be highly reliable because they are required to have important roles in modern industry. Reliability and mean time to failure (MTTF) are two fundamental measures of such systems. Reliability is de?ned as the probability that a system (or an entity) sufficiently performs its specific functions over a specific period of time under specific environmental conditions [1]. In order to improve the reliability of a specific system, one can use highly reliable components and/or a well- designed configuration to meet functional requirements and performance specifications. Finding the best configuration for a system by adding parallel components is called the redundancy allocation problem (RAP). RAP has been proved to belong to the NP-hard class of optimization problems [1]. In RAP, there are discrete component choices with known characteristics such as reliability, cost, and weight, where the objective is to find the optimal number of redundant components in each subsystem to maximize the overall system reliability subject to some constraints.
    The redundancy strategy is used to determine how the redundant components are used in the system (or its subsystems). Traditionally, there are two redundancy strategies: active and standby. The active redundancy strategy is based on the assumption that all redundant components start operation simultaneously from time zero although only one is required for the whole system to operate. The standby strategy comes in three variants of cold, warm, and hot [2]. In the cold- standby strategy, the redundant components are protected from the operational stress associated with system operation so that no component fails before its activation. Components in the warm-standby strategy are somehow affected by the operational stresses. Finally, in the hot-standby strategy, component failure does not depend on whether the components are idle or in operation. The mathematical formulation for the hot-standby strategy is the same as that for the active one [3]. In all three variants of the standby strategy, redundant components are used sequentially based on a predetermined order at failure times of operating components by using a switching system. Recently, a new redundancy strategy, called mixed strategy, has been introduced which combines the active and standby strategies and is shown to outperform both [2]. In other words, the mixed strategy is the general form while the active and standby ones are its special forms.
    Either of two scenarios may be envisioned for a switching system. In the first scenario (S1), the failure detection system/mechanism continually monitors system performance to detect a failure in which a redundant component is then activated. In addition to component failure, a switching system malfunction may occur at any time. In this scenario, switch reliability is a non-increasing function of time (老(t)). In the second scenario (S2), the switching system is used only at the occurrence of failure of a component. Failure of the switching system, therefore, happens only when it is used, and the probability of switch failure is considered as a constant value (老) [2?4].
    Over the past decade, many optimization methods have been proposed for solving the RAP with active strategy [5?9], standby strategy [4,10?14], and a combination of active and standby strategies [9,15,16]. Underlying all these studies is the assumption that the redundancy strategy for a specific subsystem can only be one of the active or standby strategies. In most cases, the redundancy strategy is considered to be a predetermined decision while it is considered as a decision variable in some recent studies [3,14?16]. Recently, a new redundancy strategy, called the ※mixed redundancy strategy§, has been introduced [2]. It potentially uses both active and standby strategies in a subsystem simultaneously. Therefore, the number of active and standby components in each subsystem is a decision variable and must be determined by a mathematical model. Ardakan et al. [17] developed the mixed strategy for a bi-objective RAP problem and demonstrated its advantages over the active and standby counterparts. More specifically, results have shown that the mixed strategy leads to more than 10 structures with superior reliability to those obtained in all previous cases.
    Feizabadi and Jahromi [18] developed the mixed strategy for a RAP problem with non-homogeneous components. Put more clearly, they considered the case in which standby components could be different from active ones but that all active and standby components had to be identical. In another study, they extended their model to a bi-objective RAP [19]. Recently, Gholinezhad and Hamadani [20] developed a mixed strategy for situations in which all the active and standby components were different. They developed a mathematical model for this non-homogeneous RAP problem and used a genetic algorithm (GA) to solve it. Ardakan et al. [21] investigated the capability of the mixed strategy applied to a reliability-redundancy allocation problem (RRAP) which is more complicated than the RAP. Results demonstrated that the mixed strategy led to a higher reliability in all the benchmark problems than did either active or standby strategies. The above mentioned studies con?rm the ability of the newly introduced mixed strategy to create highly reliable systems. In a recent study, Peiravi et al. [22] proposed a new redundancy strategy called K-mixed strategy which is a general form for the mixed strategy. Because of the complex nature of K-mixed strategy, they implemented the strategy only on one specific subsystem with 4 components.
    In this paper, a famous series-parallel system with 14 subsystems is investigated when all four different redundancy strategies, i.e., active, standby, mixed and K-mixed can be used in any subsystem. For this purpose, a new mathematical model is developed where the redundancy strategy and the number of components in each subsystem are considered as the decision variables. The proposed mathematical model must select the best strategy with the best redundancy level to maximize the total system reliability. The results of implementing the K-mixed strategy to the benchmark problem are compared with those reported elsewhere to evaluate its applicability and efficiency. By solving this new mathematical model, the capabilities and advantages of the K-mixed strategy will be revealed, especially in comparison with the mixed one.
    The rest of the paper is organized as follows. In Section 2, the mixed and K-mixed redundancy strategies are described in detail. Also, the mathematical formulation for calculating the reliability of these strategies are investigated in this section. In Section 3, the modeling of the problem is presented and in Section 4, a GA is developed for solving the proposed model. A well-known benchmark problem is adopted and solved using the new strategy in Section 5 and different problems are created and the results are compared to demonstrate the superiority and advantages of the new K-mixed strategy over the previously developed active, standby, and mixed strategies. Conclusions are finally presented in the end of the paper.
2. Mixed and K-mixed redundancy strategies
2.1. Operating mechanism
    In both mixed and K-mixed strategies, the numbers of active and standby components in each subsystem are considered as decision variables which must be optimally determined. This is a common feature of these strategies. The differences between the K-mixed strategy and the mixed one may be illustrated by considering a subsystem with four components. For this specific subsystem, all the different redundancy strategies (i.e., active, standby, mixed, and K-mixed) as outlined in Table 1 below are implemented.
    In order to disclose the distinctive features of these two strategies, they are applied to the proposed subsystem with four components. As seen in Table 1, the proposed subsystem might have two different structures depending on whether the mixed or the K-mixed strategy is applied. These two structures are: I) Three components in the active mode and one in the standby; II) Two active and two standby components.
    Fig. 1 illustrates these two different structures within the framework of a mixed strategy. Clearly, the subsystem starts its operation with 3 active components in structure I or with 2 in structure II. When the first active component fails, nothing happens for either structure and the subsystem continues working with the rest of its active components. When the second component fails in structure I, the subsystem continues operation with the last active component. In structure II, how- ever, the first standby component is activated by the switching system to replace the failed component because the second failing component is the last active one. From this point onwards, only one component is active in both structures and one standby component is replaced in a predetermined order upon the failure of this only component. This procedure continues as long as there is at least one standby component. When the last standby component is activated, the subsystem fails when this component also fails. It is worth noting that the standby components begin to operate in the mixed strategy when the last active component fails; this procedure (i.e., activating a redundant standby component to replace a failed active one) is the same as that in the standby strategy. Numerical results have shown that, for a switch re- liability equal to 0.99, the mixed strategy yields a higher system re- liability than those achieved under either the active or standby strategy [2,17,21].
    Both the mixed and standby strategies require a switching mechanism/system to substitute the redundant component into the sub- system, and the switching system can fail itself. If the switching system fails at the time of component replacement, the subsystem (and thereby the entire system) fails while there may still exist redundant standby components that have not yet been used. Accordingly, the reliability of the switching system is considered to be a weakness of the standby and mixed strategies, as it has an important and decisive role in their performance.
    The main contribution that the K-mixed strategy intends to make is to introduce a new strategy that is less sensitive to switch reliability. This strategy can then be expected to improve system reliability. In the proposed K-mixed strategy as a general form of the mixed strategy, all the active components start operating at time zero. When the first active component fails, the switching system immediately replaces the failing component with the first standby one available. This is where the mixed and the K-mixed strategies show their main difference. While the mixed strategy in this same situation (i.e., when the first active component fails) activates no standby component and the system continues working using the remaining active components, the K-mixed strategy replaces the failed component with the first standby one available. When the next active component fails, it is replaced with the next standby component available, and so on. The procedure continues until all the redundant components are activated and added to the subsystem [2]. In other words, the K-mixed strategy strives to keep a specific number of components in the active mode (i.e., the initial number of active components) as long as possible. From this point onward, the subsystem works with the remaining active components. Subsequently, the subsystem fails when the last active component fails.
    Fig. 2 illustrates the proposed subsystem with the K-mixed redundancy strategy in both structures I and II (compared to Fig. 1). It is seen that the number of active components is considered in the K-mixed strategy to be the same as that in the mixed one. It should be noted that only one component in this structure is required at any given time for the subsystem to operate.
    In structure II, the system starts its operation with two active and two standby components. When the first active component fails as detected by the switching system, the failed active component is replaced with the first standby component. This results in the subsystem still operating with two active components. When the second failure occurs, the failing component is replaced with the last standby component and the subsystem continues again with two active components. When the third failure arrives, no standby component is available anymore and the subsystem must, therefore, operate with only one active component. The subsystem then fails with the next component failure.
    In redundancy strategies with standby components (i.e., standby, mixed, and K-mixed strategies), a switching mechanism or failure detection system is required to detect the failure of a component and to replace it with a standby one. However, the switching mechanism is itself subject to failure. It is, therefore, more realistic and reasonable to consider the reliability of system regarding the switching failure. Coit [4] described two distinct scenarios for an imperfect switching system. In scenario I, the failure detection is continually monitored to detect a failing component and to replace it with a redundant one. In this scenario, the switch can fail at any time and switch reliability is a non- increasing function of time (老(t)) and switch reliability does not depend on the number of required switches. In scenario II, the switching system can only occur in response to a failure. Failure of the switching system occurs with a constant probability (老) when the switch is required.
    There is a significant difference between the K-mixed strategy and the mixed strategy with respect to the switching system. In the mixed strategy, the switching system is activated when the last active component fails and switching failure, therefore, results in subsystem/ system failure. In the K-mixed strategy, however, the switching system is used to substitute a redundant component when the first active one fails. In all the possible combinations, it is assumed that the switching system follows the second scenario and that it is triggered only in response to component failure. If the switch fails upon triggering, then it does not switch on the standby component even if it is repaired until the next failure triggers the switch back into operation. This procedure has two advantages. First, the subsystem/system is still operating when the switching system fails. Second, there is enough time to repair or replace the switching system. This is while the repair time or the time for replacing the switching system is considered to be short. Regarding the new proposed strategy, there is a decrease in the probability that the system fails due to switching failure. This study is an attempt to minimize the required number of switching operations which may result in direct subsystem failure.
2.2. Reliability model
    As explained in Section 2.1, there are 2 different forms of the K-mixed strategy for a subsystem with four components; they include: I) three components in the active mode and one in the standby mode; and II) two components in the active and two in the standby mode. Regarding the complexity of the K-mixed strategy formulation, this specific subsystem is considered and the formulation is described in more detail in order to provide a better demonstration of the formulation. The K-mixed formulations for the proposed subsystem with imperfect switching in structures I and II are defined by Eqs. (1) and (2), respectively. The times-to-failure of all the components follow an exponential distribution. Detailed procedures of obtaining these equations are presented in Peiravi et al. [22].
    In the K-mixed strategy, the reliability formulation for each subsystem depends entirely on the number of active and standby components. The following formula describes a subsystem of structure I with four components (3 active and 1 standby).
    Formula for a subsystem with 4components
    The reliability of a subsystem of structure II with four (2 active and 2 standby) components is given by Eq. (7):
    where, 老is the failure-detection/switching reliability for scenario 2 which is a constant value, (1 ? 老) is the probability that the failure-detection/switching does not work, and ri(t) is the reliability of the component used in subsystem i at time t. Since the component time-to-failure follows an exponential distribution, ri(t) is calculated as follows:
    pdf for the minimum failure time of active components which is calculated as follow:
    In Eq. (4), f(t)is the probability density function of time-to-failure for the components and F(t)is the cumulative distribution function. In Eqs. (1) and (2), subsystem reliability is the sum of probabilities associated with mutually exclusive events that lead to successful subsystem operation during the mission time t.
    Changes in the number of components or the combination of active and standby components lead to changes in the formulation of the K-mixed strategy. It is clear from the example that when the number of active components is changed from three to two, the reliability formulation changes correspondingly and the number of probabilities increases from six to eight segments. Therefore, there exists no fixed number of terms for the reliability formulation in each subsystem so that the formula must be developed and extended for each subsystem based on its own redundancy level.
    Active components have different starting times in some cases; this is important for calculating f(u) that shows the minimum failure time of the active components. In these conditions, if the component time-to-failure follows an exponential distribution, it can be assumed that all the components start working at the same time because of the memoryless property of this distribution; hence, Eq. (4) is justified. For other distributions, calculation of f(u) is more complicated.
    The formulation of mixed strategy in the second switching scenario is given by Eq. (5) [2]:
    where, f(t) is the pdf for the maximum failure times of number of failures of a component in the subsystem and is calculated as follows:
    RAP belongs to the NP-Hard class of problems, so it is hard to obtain an exact solution [1]. It is even more difficult to solve the problem using the proposed mathematical model for the K-mixed strategy. A GA as an efficient meta-heuristic algorithm is, therefore, developed in this paper for the optimization of the problems.
3. Mathematical model for RAP
    Significant research has been directed toward well-known bench- mark problems, including series system, series-parallel system, parallel- series system, and complex system ones. In this paper, a series-parallel system made up of 14 subsystems is considered from the literature. In such a system, each subsystem, i, can have its own unique number of active and standby redundancies considered as a decision variable. There are multiple choices of components available in each subsystem, but only one type of component with an unlimited supply is allowed to be used. Each type has its own levels of such parameters as reliability, weight, and cost. The objective is to maximize system reliability by determining the best strategy and the number of active and standby components in each subsystem based on weight and cost limitations.
    The assumptions of the mathematical model are as follows:
    Components are non-repairable.
    The components＊ time-to-failure follow an exponential distribution.
    The components and the system are two-state (good or bad) ones. 
    Each component's failure is independent of others and does not cause damage in them.
    All the components in each subsystem are identical. 
    The subsystem has an imperfect switching system.
    The switching system is replaceable or repairable and the switch works as perfectly as a new one after each replacement or repair. 
    The second scenario of switching (detection and switching only at the time of failure) is considered for the switching system.
    The mathematical formulation for the proposed problem runs as follows:
    Eq. (7) specifies the objective function which contains the component type, redundancy level, and the best redundancy strategy for each subsystem to achieve maximum system reliability. Constraints on cost and weight are given by Eqs. (8) and (9), respectively. System reliability is calculated using Eq. (11) in which the second scenario of switching (i.e., switch activation only in response to a failure) is considered.
    where, RMixed and RK?Mixed are equations for calculating the reliability of the subsystems with mixed and K-mixed strategies.
4. Genetic algorithm
    GA belongs to the family of meta-heuristic algorithms successfully employed for the optimization of combinatorial problems due to its simplicity and practicality. In this paper, a powerful GA with an efficient encoding procedure is applied for solving the proposed model when all kinds of strategies with different type and number of components can be selected for individual subsystem. GA has different characteristics such as solution encoding, objective function, crossover and mutation which are described in the following subsections.
4.1. Solution encoding
    A new efficient chromosome is chosen to represent the possible solutions which consist of selected component and redundancy level for each subsystem. This solution encoding is represented in a 2 ℅ s matrix. The first and second rows represent the type and number of selected components, respectively. An example of encoding solution for this problem with s = 14 is shown in Fig. 3.
    The main point about this encoding is that all possible strategies which can be applied to the subsystem by considering the redundancy level are considered. Then, the reliability of subsystems by using different strategies is calculated. The strategy which leads to a higher reliability for the subsystem is selected. For example, in subsystem number five, 4 parallel components are considered. All redundancy strategies include active, standby, mixed and K-mixed can be applied to this subsystem. Therefore, the best strategy with the higher reliability must be determined by the algorithm. In previous studies, the redundancy strategy has been considered in the solution encoding and it makes the solution space more complicated.
4.2. Objective function
    After the solutions are generated, their fitness functions are calculated by considering the penalty for constraint violations. In order to transform an infeasible solution to a feasible one in the next iterations of the algorithm, a dynamic penalty function is implemented. In this method, infeasible solutions are penalized by reducing their fitness value regarding to their violation level. Also this penalty function encourages the population to seek the feasible region and near the border of feasible region. The penalty function used here is based on research in [2].
4.3. Crossover and mutation
    In this section, double-point crossover and a modified version of max?min crossover proposed by [16] is applied. The mutation operator is used to increase diversity and prevent premature convergence into a local optimization solution. In this paper, besides the simple mutation, a max-min mutation operator is performed [2,16].
5. Numerical results
    In order to demonstrate the efficiency of the proposed K-mixed strategy, it is initially applied to three different subsystems and then to a well-known benchmark problem. The three different subsystems are considered to have 2, 3, and 4 components and the reliability values for the components and the switching system are assumed to vary over a wide range. The benchmark problem is a series-parallel system with 14 subsystems originally due to Fyffe et al. [23] which was later modified by Coit [4] and subsequently used by many researchers [2?4,16?20,24].
5.1. Three different subsystems
    The reliability of a specific system depends on different parameters as component reliability, switch reliability, redundancy level, and redundancy strategy. The present section investigates the effects of these parameters on the reliability of a subsystem. In these analyses, three different subsystems with 2, 3, and 4 components are considered and the reliability of different strategies in each subsystem is calculated based on different combinations of component and switch reliabilities. The objective is to derive the relationships among the different para- meters of the system and the best strategy in different situations. The following ranges are considered for component and switch reliabilities: 
    Component reliability: [0.95, 0.93, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.50, 0.40, 0.30]
    Switch reliability: [0.99, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.50, 0.40]
    In previous studies of RAP in which this benchmark problem was considered, the optimal structure always used 2, 3, or 4 components in each subsystem and component reliability ranged over [0.80, 0.95]. In this paper, these three different levels of components are, therefore, considered as three different subsystems and the efficiency of each strategy is evaluated. The results are, finally, compared with those obtained from implementing the K-mixed strategy on the benchmark problem.
    All the combinations of component and switch reliabilities are considered and the reliability of each of the four strategies is calculated for each combination and compared. In each combination, the reliability values of different strategies are calculated and sorted in a descending order. The problem is solved with 11 different values for switch reliability and 12 different values for component reliability. This yields 132 test problems. All the different combinations are presented in three 12 ℅ 11 tables, in which the rows represent switch reliability and columns represent component reliability. The results obtained for the subsystem with 4, 3, and 2 components are presented in Figs. 4, 5, and 6, respectively. Each cell in these tables represents the order of different strategies from the best to the worst one. The abbreviations used in these tables are presented in Table 2.
    For more clarification, Figs. 4?6 are presented in different colors, with green representing the range in which standby strategy leads to better results, blue denoting the combinations in which the mixed strategy leads to better result, yellow representing the ranges in which the new strategy (K-mixed) results work better than others, and finally brown indicating the active strategy as the best one.
    As shown in Figs. 4 and 5, the active and standby strategies exhibit better performances at the corners of the matrixes (i.e., highly reliable components with a weak switch, or a highly reliable switch with weak components). In most of the combinations, however, the proposed K-mixed strategy outperforms others. It is worth mentioning that the mixed and K-mixed strategies can be de?ned for subsystems with more than 2 components while in a subsystem with two components, only the active and standby strategies can be de?ned.
    Figs. 7?9 present the effects of the switching system on the different strategies for all the three proposed subsystems. These figures also demonstrate the reliability values of different strategies for a switch reliability reduced from 0.99 to 0.40 and a component reliability fixed at 0.65. Clearly, the standby strategy is the one most sensitive to switch reliability followed by the mixed strategy (in structure II). The active strategy is not sensitive to switch reliability because it does not use any switching system. It is interesting to note that the K-mixed strategy in its first structure is somehow unaffected by switch reliability and may be considered as a reliable strategy. Therefore, if the switching system is not reliable, then the proposed K-mixed strategy is beneficial for the system.
5.2. Series-parallel system
    In this subsection, the famous series?parallel system in RAP is investigated and the K-mixed strategy is implemented on this benchmark problem. The series?parallel system consists of 14 subsystems in each of which three or four component choices with predetermined cost, weight, and reliability are available. The input data for this benchmark is presented in Table 3. The purpose is to maximize system reliability at a given 100 units of time by considering constraints on system cost (C = 130) and system weight (W = 170). It is assumed that the failure detection and switching hardware activate the redundant component only in response to a failure.
    In order to analyze the efficiency of different strategies, particularly the proposed K-mixed one, considering changes in switch reliability, the benchmark problem is solved for the different switch reliability values of 0.99, 0.95, 0.90, and 0.85. The main goal is to investigate the capability of the new strategy to cope with different situations. A second goal is to evaluate the performance of the different strategies studied by changing the switch reliability value. More specifically, it is interesting to observe how the best strategy for a subsystem might change with changes in switch reliability values.
    In order to make a clear comparison of the proposed and the conventional strategies, for each value of switch reliability, the problem is solved in two situations:
    Situation I: The subsystems can use only the conventional redundancy strategies; i.e., standby, active, and mixed,
    Situation II: In addition to the conventional strategies, the subsystems are allowed to use the K-mixed strategy.
    The proposed GA is used to solve the benchmark problem in each situation. The problem is solved then in five trials and the best solution is taken as the final solution. The best solution with maximum reliability is subsequently chosen for comparisons between the two different situations. Four different values are considered for switch reliability. The results obtained for each value are presented below.
    We begin with a switch reliability of 0.99 that was also used in all previous studies of the benchmark problem in question [2?4,16?20,24]. The problem is solved in the two situations (i.e., with and without the K-mixed strategy) and the results are reported in Table 4.
    Ardakan and Hamadani [2] considered the same benchmark problem and employed a mixed strategy to find an optimal solution with a reliability of 0.992328. They assumed that the component time-to- failure (TTF) followed an Erlang distribution. For the K-mixed strategy, we assume that all the component TTFs follow an exponential distribution. Therefore, in this paper, the mixed strategy is also considered with an exponential TTF so that exhaustive comparisons are possible to make. For other problems, the same procedure is applied and the best solutions are obtained. The benchmark problem is formulated and solved by considering the mixed strategy with an exponential distribution. The structure of the best solution obtained in this paper for the benchmark problem with an exponential TTF is the same as that obtained by Ardakan and Hamadani [2] and the reliability of the best solution with an exponential TTF is equal to 0.98194689. Thus, the same best structure is obtained for this problem with either an Erlang or an exponential TTF.
    The benchmark problem is then solved with the proposed K-mixed strategy applied to the system. The optimal structure is observed to be almost the same as the previous one and the three conventional strategies, but the K-mixed one, are used in all the subsystems. This result was predictable from Figs. 4?6. However, the GA developed in this paper finds a solution with a better reliability (i.e., 0.982022) compared to the best solution obtained in previous studies [2]. When component TTF in this new solution is changed to an Erlang distribution, the system reliability is equal to 0.992335, which is greater than the one (0.992328) obtained by Ardakan and Hamadani [2]. Table 4 shows the results of the comparisons between the two situations when a switch reliability of 0.99 is adopted.
    According to Table 4, the mixed strategy outperforms the K-mixed one while this strategy is also used in all the subsystems with more than 3 components when a switching system with reliability of 0.99 is employed. Moreover, the proposed GA is found to be superior to that presented in [2] as it was able to find a better solution. Table 5 reports the results obtained for the proposed benchmark with and without the K-mixed strategy when the switch reliability is equal to 0.95. Because none of the previous studies considered this value for switch reliability, the results of both situations are presented here only with an exponential distribution.
    As seen in Table 5, the final solution uses four subsystems with the K-mixed strategy when it is applied to the benchmark problem. This solution is distinctly different from the best structure with a switch reliability of 0.99. Comparison of the results for Situations I and II reveals that the K-mixed strategy improved the overall reliability of the system. These results are in complete agreement with those shown in Figs. 4?6. For example, the second subsystem uses two components with the active strategy while it is shown in Table 6 that the best strategy for this value of switch reliability (0.95) and two components is the active one.
    These results are also in complete agreement with our findings reported in Figs. 4?6 where it is seen that the best redundancy strategy for each subsystem depends on component reliability when the switch reliability is fixed. For example, in the first subsystem, three components of type 3 are chosen with a component reliability equal to 0.91. It is clear from Fig. 5 that the best strategy in this situation is KM1 (i.e., K-Mixed redundancy strategy in structure I). For other subsystems, the results can be compared with those in Figs. 4?6. Table 6 reveals the differences between the solutions obtained for the two Situations I and II. 
    Tables 7 and 9 report the results obtained for the benchmark problem with switch reliabilities of 0.90 and 0.85, respectively. Comparisons of the two Situations (i.e., using conventional or the K-mixed strategies) are reported in Tables 8 and 10 for each value of switch reliability. The interesting point in these two final test problems is that, as also shown in the previous section, different strategies exhibit different degrees of sensitivity to switch reliability. It was shown that the K-mixed strategy with the structure I had the minimum sensitivity to switch reliability. As a result of this fact, the final solutions in Tables 7 and 9 employ more subsystems with the K-mixed strategy in Structure I (i.e., KM1).
    Fig. 10 presents the reliability values for the different subsystems in the two situations I and II. Fig. 11 presents a complete comparison of Situations I and II for different values of switch reliability. Clearly, the proposed K-mixed strategy helps the system remain highly reliable despite the reduced reliability of the switching system. When switch reliability is lower, the overall system reliability in both Situations I and II get closer because the system prefers to use an active strategy which is not sensitive to the switching system. Fig. 12 presents the structure of the series-parallel system considering different switch reliability values. It shows how each subsystem uses the best redundancy strategy to gain the highest reliability.
6. Conclusion
    In this paper, a recently introduced redundancy strategy called the K-mixed strategy was investigated as a general form of the mixed strategy. This strategy can be used in all systems with redundant components. The mathematical formulation of this strategy was initially presented and a well-known series-parallel system was considered and a novel mathematical model was developed to evaluate the efficiency of the proposed strategy. The problem was formulated as a non-linear integer programming model subject to a number of constraints. The reliability of the system was then calculated in two different situations: with and without the K-mixed strategy used in the subsystems. Moreover, different values of switch reliability were considered and the problem was solved. Finally, the efficiency of the proposed strategy was investigated against each of these values. Numerical results revealed that the proposed K-Mixed strategy outperformed the previously used mixed, standby, and active strategies for most combinations of components and switch reliability. For future studies, the proposed strategy is suggested to be implemented in other reliability problems such as RRAPs.
    
The evolution of system reliability optimization
    System reliability optimization is a living problem, with solutions methodologies that have evolved with the advancements of mathematics, development of new engineering technology, and changes in management perspectives. In this paper, we consider the different types of system reliability optimization problems, including as examples, the redundancy allocation problem (RAP), the reliability allocation problem and the reliability-redundancy allocation problem (RRAP), and provide a flow of discussion and analysis on the evolution of the approaches for their solutions. We consider the development and advancement in the fields of operations research and optimization theory, which have allowed the formalization and continuous improvement of the methods and techniques to address reliability design problems of even very complex systems in different technological domains. Technological advances have naturally brought changes of perspectives in response to the needs, interests and priorities of the practical engineering world. The flow is organized in a structure of successive ¡°Eras of Evolution,¡± namely the Era of Mathematical Programming, the Era of Pragmatism, the Era of Active Reliability Improvement. Insights, challenges and opportunities are highlighted.
1. Introduction
    ¡°Success is walking from failure to failure with no loss of enthusiasm¡± ? Winston Churchill
    Reliability engineering is a formal engineering discipline, founded on mathematical principles, particularly of probability theory and statistics, for systematically and rigorously analyzing functional problems in components and systems with the aim to produce a reliable design. As an engineering discipline, reliability aims at analyzing and evaluating the ability of products and services to perform the functions that they are intended to provide by design.
    While technology improves and advances, the complexity of modern engineered systems also increases. At the same time, consumers¡¯ expectations for high functionality, high performance and high reliability increase, leading to challenges and opportunities. Then, although system reliability optimization problems have been studied, analyzed, dissected and reanalyzed, the continuous rise of new challenging problems demonstrates that this general research area will never be devoid of interesting problems to be solved.
    On one side, the development and advancement in the fields of operations research and optimization theory have allowed the formalization and continuous improvement of the methods and techniques to address reliability design problems of even very complex systems in different technological domains. On another side, the evolution of technology, the advancement of research ideas and theories, have naturally brought changes of perspectives, in response to the needs, interests and priorities of the developing practical engineering world. So, the development and application of formal optimization methods to the practical goal of achieving maximum reliability under various physical and economic constraints, has remained an ongoing topic of scientific development.
    In formal terms, the task of optimization involves formally conceptualizing the decision variables, the constraints and the single or multiple objective functions that describe the performance of the engineering design problem, and searching for the combination of values of the decision variables that achieve the desired goals with respect to the objective functions. Whether expressed explicitly in mathematical terms or not, every engineering design problem has design objectives which should be maximized or minimized or designed to achieve some acceptable requirement. When there is a single predominant objective that can be expressed with a series of explicit mathematical equations, then the problem can potentially be solved using mathematical programming methods or useful heuristics. Researchers working within the broader mathematical programming community are continually developing new methods and algorithms to solve broader classes of problems, larger and more difficult problems, and to do so more efficiently than before.
1.1 Eras of research evolution
    The research in complex system reliability optimization has evolved as a continuum of ideas and techniques. This evolution can be loosely and chronologically classified into the following three eras:
    Era of Mathematical Programming
    Era of Pragmatism
    Era of Active Reliability Improvement
    The era of mathematical programming is preceded by the original development of innovative groundbreaking methods, such as dynamic programming and the simplex algorithm (or linear programming), for which the reliability optimization problem has served as a very interesting and practical vehicle to demonstrate the methods and apply them. Yet, practicing reliability analysts recognized the limitations of these methods in practical applications, mostly because only problems that could be formulated to strictly meet the assumptions required by the methods could be solved, and this is rarely the case in practice. Furthermore, only small and/or unrealistic problems could be solved, because of computation limitations at the time.
    Driven by the desire to apply reliability optimization in practice, the era of pragmatism evolved and became increasingly important. New problems were solved and new methods developed in response to the pressing needs to consider and integrate into the problem some critical issues that could not be readily accommodated by the rigorous mathematical methods. For example, actual engineering systems problems could fall outside the assumptions required by the methods. Analysts were interested in complex forms of redundancy perhaps mixing functionally equivalent, but different components, whose failure behavior may not be simply described as a transition from one functioning state to a failure state, but rather as a process of transition across multiple states. To address a broader range of problems, compromises could be accepted, thus expanding the practical usefulness and applicability of the optimization methods.
    In these previous eras, reliability optimization had been mostly considered as singular, static analyses to be conducted and implemented to yield the desired design reliability of the system. Under this view, common assumptions were made on the existence of populations of homogeneous components and systems, sharing the same failure behavior but with failure occurrences being independent from one another. Furthermore, the implied assumption was that the conditions defined or considered when conducting the analysis are static and remain unchanged throughout the horizon of the analysis (the often-called mission time).
    When changes occur during the system lifetime, then the results of the analysis are simply no longer valid and applicable. We are currently experiencing another industrial revolution, particularly driven by the increase in information sharing, data availability and computational capabilities. In particular, with the proliferation of sensors, environ- mental stresses, usage stresses, failure data, etc., can be collected and processed at regular time intervals, and the advancements in information knowledge that these can bring on the states of the systems, offer new opportunities of development for the analysis and assessment of reliability. The era of active reliability improvement is, then, ongoing and it is visionary in recognizing that components and system conditions change throughout their lives, and system reliability optimization methods, to be practically useful, need to dynamically respond to these changes.
    Within each of the three macro-eras discussed above, further sub- classification can be introduced, in a specific and unique way. In the era of mathematical programming, further sub-classification can be done based on the specific mathematical algorithms developed to solve the reliability design problem. For the era of pragmatism, sub-classification can refer to which practical consideration in the design problem, which previously could not be analyzed, was now readily being addressed by the available optimization models. Within the era of active reliability improvement, sub-classification can be based with reference to the available data and new models for real reliability improvement.
2. Reliability optimization problems
    System reliability design problems have multiple, and often competing objectives. However, there are some universal ones, including reliability (to be maximized) and cost (to be minimized). Often, the approach taken is to follow a prioritization of the objectives by the decision-makers, select the most important objective as the objective function and constrain the other objectives within acceptable limits.
    For each formulation to be studied or solved, system reliability optimization problems must have three elements: decision variables, constraints and an objective function or functions. The decision variables are those variables that can be changed or decisions that can be made to improve performance, with respect to the objective function or functions. Examples of decision variables include component type (with its intrinsic characteristic of failure behavior and reliability), redundancy configuration in the system and others. Constraints are mathematical expressions of practical limitations, such as monetary budget or acceptable reliability, which limit the choice of decision variables in relation to their feasibility of respecting the constraints.  The objective function measures the performance of the system for given values of the decision variables, and thus, enables the decision on the optimal combination of variables values for the optimal solution. The objective function can often be the system reliability to be maximized, or the system cost to be minimized.
    Different forms exist of the system reliability optimization problem. Three typical ones are the redundancy allocation problem (RAP), the reliability allocation problem and the reliability-redundancy allocation problem (RRAP). The solution methods for each problem are obviously different, because of the assumptions and structure of the problem. A thorough review of system reliability optimization is given by Kuo et al. [1?3].
2.1. Redundancy allocation problem
    The most widely studied reliability optimization problem is the RAP. For many systems composed of discrete component types, with fixed cost, reliability and weight, in mathematical terms system design becomes a combinatorial optimization problem. For providing the demanded system functions, there may often be alternative component types available, at different cost, reliability, weight and other proper- ties. The practical problem is to select the optimal combination of components types (decision variables) to collectively meet reliability, weight, etc. (constraints) at a minimum cost (objective function), or alternatively, to maximize reliability (objective function) while achieving given minimum (or maximum) values of other system properties (constraints).
    For the mathematical formulation of this problem, we can consider that there are mi discrete component type choices available for each subsystem (i = 1, ÿ, s), which the system is formed of. Fig. 1 shows a typical example of a system with a number s of k-out-of-n logic sub- systems. If k is equal to 1 for each subsystem, then this is a simple series-parallel system. For each subsystem, ki components must be selected from the mi available choices (e.g., assuming an unlimited amount available for each of the components). The optimal decision is driven by the possibility of placing additional components in parallel in any of the subsystems and/or adding levels of redundancy (> ki) of lower reliability components as an alternative to using more reliable (and expensive) ones. In other words, there is a large number of possible solutions to test, even for relatively small problems (small number of subsystems, small number of components, small number of components types, etc.).
    The RAP for a series-parallel system, as the one shown in Fig. 1, can be formulated as to maximize reliability or minimize cost, under various constraints. Often the RAP is solved for series-parallel systems, but other system structures have been considered, as well as more advanced forms of series-parallel systems, including those with several failure modes, phase mission types, systems with uncovered failures and systems with imperfect fault coverage.
    RAP has proven to be quite difficult to solve. Chern [4] showed that the problem is NP-hard and many different optimization approaches have been used to determine optimal or ¡°very good¡± (i.e., near-optimal) solutions, including (1) dynamic programming, (2) integer programming, (3) mixed integer and nonlinear programming, or (4) evolutionary algorithms.
2.2. Reliability allocation problem
    The reliability allocation problem and RAP are fundamentally different problems. For the reliability allocation problem, the system structure is fixed and the component reliability values are continuous decision variables. For this problem, there is no general restriction on the system structure. An example of a general system is depicted in Fig. 2. Component cost and other parameters are defined as mathematical functions of the component reliability. Increasing the component reliability (and thus, the system reliability) increases the cost, weight and other factors, which may be included as part of the constraints or the objective function. Here, also, the goal of the optimization is typically to maximize system reliability or minimize system cost, and since the decision variables are continuous, different forms of non- linear programming can be used to determine the optimal solutions. To assure that the constraints are satisfied, Lagrangian multipliers are often introduced as part of the objective function.
2.3. Reliability-redundancy allocation problem (RRAP)
    The reliability-redundancy allocation problem is the most general problem formulation. The system is composed of one or more ¡°sub- systems,¡± i.e., collections of logically connected sets of components. Each subsystem has xi components with reliability of ri as decision variables. The problem is then to optimally allocate redundancy and reliability to the components of each subsystem with an objective to maximize the overall system reliability. The system and subsystems can be in any arrangement, as depicted in Fig. 2. Again, typically the objective of the optimization is to maximize system reliability or minimize system cost.
    RAP is often considered for series-parallel systems, but other system structures can be considered as well. The reliability allocation problem and RRAP has been applied to many different system structures, including common structures (series, parallel, etc.), but also consecutively connected systems, sliding window systems, flow transmission systems, common bus systems and others.
2.4. Component assignment and sequencing problems
    Two other related system reliability problems are assignment or sequencing of components within a system. For assignment problems, there is typically a defined system structure, but the available components are assigned to specific locations with the system. Sequencing is particularly interesting and important for systems with standby redundancy, where the components are activated as needed in accordance with a defined sequence. Sequencing problems are solved for optimization for systems with consecutively connected systems and/or standby components with cold or warm redundancy required a defined activation sequence.
    The problem of assignment of components to positions within a system to maximize reliability was originally presented by Derman et al. [5,6]. An important early research effort [7] defined optimal assignments for different system structures. The related problem involves sequencing of redundant components. More recently, sequencing of standby redundant components has been considered by researchers to maximize system reliability [8,9].
2.5. Other optimization problems
    There have been many other related system reliability optimization problems as well. There have been extensions of the original problems, as well as related problems including spares provisioning, optimization of series-parallel topology, optimal load distribution, optimal mission abort policy, test plan allocation, preventive maintenance optimization and others [2].
3. Era of mathematical programming
    Methods initially developed to solve complex system reliability optimization problems can be referred to as belonging to the era of mathematical programming. The emphasis was on applying advanced mathematics to obtain mathematically provable optimal solutions. The priority was on solving problems to optimality, but in doing so, often the structure and size of the problems were limited to be able to apply the rigorous mathematical model. The problems being solved in the end were rarely realistic or indicative of actual design problems. Assumptions were often introduced for mathematical convenience, and problems that did not meet those conditions were avoided. However, these original methods are very important and influential, and they have served as the foundation for much of the research and development work in quantitative reliability engineering that followed.
    Solution of complex system optimization problems was possible because of advancements in operations research theory and the development of new algorithms. A key mathematical challenge was to find an efficient way to at least approximate solutions to problems otherwise unsolvable with classic analytical methods. Numerical and mathematical approaches were introduced to successfully solve such problems, with the turning point having been the realization of the first computer, which provided the possibility to perform sets of operations for hand- ling large numbers of data in a time much shorter than previously possible.
    The newborn field of computer science sparked the mind of several mathematicians that tried to formulate methods to use computers to help solving practical problems involving high computational efforts. The field of mathematical optimization was then born. Several pioneers from the first half of the 20th century contributed to develop formulations and algorithms to be implemented in computing machines to solve difficult optimization problems, including from the fathers of  linear programming, Leonid Kantorovich and George Dantzig, Richard Bellman, originator of dynamic programming, to the founders of evolutionary algorithms, such as Lawrence Fogel, John Holland, Ingo Rechenberg and Hans-Paul Schwefel. Their ideas were the milestones from which other variants of the methods developed until today.
    When mathematical programming methods associated with the field of operations research were being developed and formalized, but still in their infancy, researchers were searching for interesting applications. Maximization of system reliability was considered an attractive application. Indeed, it is a difficult and challenging problem, yet practical and useful to demonstrate the utility of new mathematical programming algorithms. Typical formulations of the problem are challenging, with a highly nonlinear objective function and often integer decision variables.
    Dynamic programming was applied to the system reliability problem as one of the very first applications explored. Considering the initial formulations of RAP, dynamic programming could almost be directly applied to efficiently obtain optimal solutions.  The problem was that it was difficult to extend it to solve more realistic or actual design problems. Linear programming, or the simplex algorithm, is a very powerful advancement, which allowed for very large linear problems to be solved. However, system reliability is a highly nonlinear objective function, so researchers had to be creative to exploit the power of the simplex algorithm to solve reliability problems. RRAP is nonlinear with both continuous and integer decision variables creating another very challenging problem, that was often solved using some variation of nonlinear programming.
3.1. Dynamic programming
    Dynamic programming was originated in 1954 by Richard Bellman [10], and system reliability optimization was among the first problems studied. The aim was to treat mathematical problems arising from the study of multi-stage decision processes. The key advancement, as compared to previous methods, relies on the fact that when analyzing these problems, not all the possible sequences of the present and following stages are needed, i.e., it is possible, instead, to state general conditions to determine for each stage the most suitable decision ac- cording to the current state only, whereas classical approaches gathered information about all the sequences, making the calculation cumber- some and unpractical [10].
    A problem solvable by dynamic programming can be performed as a system, described by a set of quantities, the state parameters, that undergo a state variation caused at a certain time t by a decision made by the user. The solution aims at taking an initial decision for guiding the future ones so that it is possible to maximize a given objective function of the state parameters. In many cases the number of parameters considered to make the decision is very large, especially when considering stochastic processes in which decisions determine a distribution of outcome states. In these cases, the approach allows reducing the dimension of the problem by focusing on the current time. To perform the dynamic programming optimization, the state parameters and the sequence of decisions to analyze, i.e., a policy, are needed. The optimal policy is, then, the one that determines the decision required at each time with respect to the current state of the system.
    Bellman [11] and Bellman and Dreyfuss [12,13] demonstrated that an optimal solution to the RAP could be found using dynamic programming. In their problem, there was only one component choice for each subsystem, and the objective was to maximize reliability with a single cost constraint. For each subsystem, the problem was to identify the optimal levels of redundancy. A well-known disadvantage of dynamic programming formulations is the difficulty of efficiently solving problems with multiple constraints.
    Fyffe et al. [14] used a dynamic programming approach to solve a more difficult design problem. Their problem involved a system with 14 subsystems and cost and weight constraints. For each subsystem in the Fyffe formulation, there are three or four different component choices each with different reliability, cost and weight. However, several of these component choices are dominated by other competing choices. Similar to Bellman, only 1-out-of-n redundancy was considered. To accommodate multiple constraints within a dynamic programming formulation, they used a Lagrangian multiplier for the weight constraint within the objective function.
    Instead of using Lagrangian multipliers, Nakagawa and Miyazaki [15] used a surrogate constraint combining the cost and weight constraints into one. They then solved a series of problem iterations with different surrogate multipliers, with a heuristic to successively update the surrogate multipliers. Stopping criteria was provided to identify cases when their algorithm would not lead to an optimal solution. The algorithm was demonstrated by solving 33 variations of the Fyffe problem with different weight constraints. Of the 33 problems, they found optimal solutions to 30 of the problems. Otherwise, the final solution was not feasible (although there are feasible solutions to the problem).
3.2. Linear programming/integer programming
    Linear programming (LP) and Integer Programming (IP) are powerful methods to find the maximum or minimum of a linear function describing the performance under assessment, which is called the objective function. A standard mathematical definition is the following: max{cx; Ax b,0}or min{cx; Ax	b, x	0}
    For this formulation y = cx is the objective function to be maximized or minimized, x is the vector of non-negative variables to be found, b and c are vectors of known coefficients, and A is a matrix of known coefficients that when multiplied by x have to satisfy the constraints expressed by the vector of coefficients b. The problem is defined within a convex polyhedron-shaped feasible region, intersection of finitely half-spaces represented by linear equalities/inequalities.
    This optimization framework was initially used by the Soviet economist Leonid Kantorovich who was trying to organize the actions of soldiers to decrease expedition costs and increase enemy losses. At the same time another economist, T.C. Koopmans was working on the applicability of linear programming to solve classical problems. Their work was recognized by the Nobel prize in economics in 1975. Following that, mathematician George Dantzig developed an LP methodology to solve optimization problems, providing a formal proof of the solution [16]. One of the most important achievements was the reduction of the possible solutions, and therefore, the advantage of the method in terms of computing power needed.
    If the objective is to maximize reliability or minimize cost given multiple nonlinear but separable constraints, many variations of the problem can be transformed into an equivalent integer programming problem using 0?1 decision variables. This was originally demonstrated by Ghare and Taylor [17] who used a branch-and-bound approach to solve many randomly generated RAPs with 30 subsystems with 15 constraints, and 99 subsystems with 10 constraints. Ghare and Taylor assumed that there was only one component choice for each subsystem and redundancy was active always 1-out-of-n redundancy.
    Bulfin and Liu [18] also used an IP approach to solve the RAP. They developed one heuristic and two exact algorithms to be applied de- pending on the problem structure. They formulated the problem as a knapsack problem and used a surrogate constraints approach, similar to Nakagawa and Miyazaki [15]. The surrogate multipliers were approximated as the optimal Lagrangian multipliers as found by sub- gradient optimization. Bulfin and Liu formulated the Fyffe problem and its variations as integer programs and solved the 33 problems previously investigated by Nakagawa and Miyazaki, and other examples as well. They also considered only subsystems with 1-out-of-n redundancy. Coit and Liu [19] extended their approach to k-out-of-n redundancy subsystems if no mixing of component types is allowed within the subsystem parallel structure.
    Other examples of IP solutions to the redundancy allocation problem were presented by Misra and Sharma [20], Gen et al. [21], and Gen et al. [22]. Misra and Sharma presented a very fast and useful algorithm to solve integer programming problems formulated like those of Ghare and Taylor [17]. Gen, Ida, Tsujimura and Kim and Gen, Ida and Lee formulated the problem as a multi-objective decision-making problem with distinct goals for reliability, cost and weight.
    For other system reliability applications, LP has been proved useful in the context of structural system reliability by Corotis and Nafday [23]. They used LP to identify the most critical failure mode for a structural system. More recent papers [24] demonstrate that LP is particularly useful in structural system reliability analysis. The LP bounds can be applied for any type of system and for different component probabilities. These bounds are the narrowest possible bounds that one can be obtained for a system, for any specified information for the component failure probabilities.
    The main drawback of using LP or IP is that the size of the problem to be solved, and its computational cost, increases exponentially with the number of components, questioning its efficiency when it comes to realistic, complex systems. An approach has been proposed to overcome this issue and extend the applicability of LP. Decomposing the entire system into subsystems based on failure modes can be applied to identify component state probabilities and joint probabilities of the states of a small number of components. It can also provide bounds for the failure probability of large systems. This is particularly useful when other methods are not applicable. This approach has been presented by Chang and Mori [25]. The idea is the development of a relaxed LP (RLP) bounds method to reduce the number of design variables using the universal generating function (UGF) [26].
    RLP bounds method can be applied to a single series or parallel system, but it is not applicable to a general system that consists of both series subsystems and parallel subsystems. For this reason, an additional assumption can be made to obtain the Strategic Relaxed Linear Programming (SRLP). After decomposing the system according to different failure modes, each critical failure mode is also considered as a system (or subsystem) itself. The bounds on the system failure prob- ability can be computed by the RLP bounds method if it is a series or parallel system, and the bounds on its joint failure probability can also be computed by the RLP bounds method. The bounds estimated by the RLP bounds method are, then, used as constraints in solving the LP problem for estimating the failure probability of the entire system.
3.3. Nonlinear programming
    Nonlinear programming (NLP) refers to a collection of optimization methods defined by the same main principles of linear programming, with the difference that the objective function and/or its constraints, and therefore the feasible region of the problem, are defined with at least one nonlinear equation. The addition of nonlinear equations makes the optimization problem much more difficult to be solved, for example:
    In a nonlinear function it is hard to assess whether a maximum is local or global, and unlike linear functions where a max/min location is restricted to the borders of the feasible region, for a nonlinear function it can be in the interior of the feasible region.
    If the objective or any constraints are non-convex, the problem may have multiple disconnected feasible regions and multiple locally optimal points within such regions.
    The numerical method chosen to get to the solution may cause two different starting points to lead to two different solutions.
    It is difficult to ensure that the constraints applied to the problem meet the requirements of the feasible region.
    A tolerance region for the solution has to be considered with a proper uncertainty.
    NLP solvers generally attempt to solve the problem by computing gradient values at various trial solutions, and moving in the direction of the negative gradient (when minimizing, positive gradient when maximizing). They usually also exploit second derivative information to follow the curvature as well as the direction of the problem functions. To solve constrained problems, NLP solvers must take into account feasibility and the direction and curvature of the constraints as well as the objective. A review of nonlinear programming optimization methods is provided by Floudas [27].
    Mixed integer and nonlinear programming have been effectively used to solve the redundancy allocation problem. Considering reliability optimization, important research contributions were provided by Tillman et al. [28,29]. In these problems, component reliability is a continuous decision variable and component cost is expressed as a function of reliability and other parameters.
3.4. Evolutionary algorithms
    Evolutionary algorithms (EA) are a group of optimization methods that perform their task with a built-in ability to evolve. EA have the three following common features:
    1. Population-based, i.e., they handle a group of solutions, the population, manipulated in different ways to optimize the problem;
    2. Fitness-oriented, meaning that EAs favor individuals (a solution belonging to a population) which are fitter than others according to established criteria. Each individual has a gene representation which is its code together with a performance evaluation, i.e., its fitness value. Choosing fitter individuals drives the optimization and the convergence of the algorithm;
    3.Variation-driven: In order to simulate evolution steps, individuals are subject to random variations, necessary to explore the problem's solution space.
    The basic intent of EAs is to implement the Darwinian concept of survival of the fittest, applying it to functions to optimize. Through each generation, the solutions considered weak in terms of the specific criteria adopted for the optimization face extinction, whereas the best ones combine to produce new individuals that potentially can improve the convergence to an optimal solution.
    The first attempts to mimic evolution by simulating genetic processes date back to Fraser [30] and Bremermann [31]. The main contributor, however, is John Holland, who in 1975 published ¡°Adaptation in Natural and Artificial Systems [32]¡± in which he introduced the main fundamental concepts of genetic algorithms (GA). In GA, each in- dividual of the population has two characteristics: a chromosome and a fitness value representing its quality. The chromosome is composed of genes; in the original formulation each gene was considered as a bit, therefore either 1 or 0, and the chromosome was a string of zeros and ones. In the following years, several researchers developed new forms of GAs.
    A chromosome can be viewed as a sorted string or vector. The evolutionary process starts when all fitness values of the initial population have been assigned. Afterwards, the selection process begins, in which some individuals are selected in order to be included in the mating pool. The fittest individuals are more likely to be selected and spread their properties to the offsprings: individuals in the mating pool are combined to produce new hybrids whose finesses are evaluated to decide whether or not to pass onto the next population, replacing other individuals. It is common practice to keep a constant number of individuals inside a population at each stage.
    GA have not been applied practically for system reliability problems until the 1990s, when researchers such as Coit and Smith [33,34] implemented it in a combinatorial reliability design problem. The evolutionary optimization proved very efficient in terms of cost-effectiveness of the selection of the parts and allocation redundancies for system reliability. Several authors then tackled optimization problems by GA. For example, Painton and Campbell [35] presented a model based on such methods, highlighting again their robustness and capability of finding the optimum over a high dimensional nonlinear space in a considerably shorter time than the required one for enumeration. In order to improve the reliability of a personal computer, they identified the main components and their failure modes in order to determine some possible improvement levels.
    With regards to applications, reliability allocation to minimize total operating costs, subject to an overall plant safety goal, was presented by Yang et al. [36]. System optimization was used to enhance the design, operation and safety of new and/or existing nuclear power plants. They determined the reliability characteristics of reactor systems, sub- systems, major components and plant procedures in accordance with a set of top-level performance goals. The cost for improving and/or de- grading the reliability of the system was also included in the reliability allocation process as a multi-objective problem formulation. GA was demonstrated to determine effective solutions for a typical pressurized water reactor.
    Konak et al. [37] presented general guidelines regarding the implementation of GA for multi-objective reliability optimization, pro- posing a list of techniques and highlighting the advantages and difficulties of each of them. The reliable network design problem has been studied using multi-objective GA. Kumar et al. [38] presented a multi- objective GA to optimize telecommunication networks while simultaneously improving network performance and design costs given a system reliability constraint. Kim and Gen [39] studied bicriteria spanning tree networks considering the objectives of cost and reliability, while Marseguerra et al. [40] determined optimal surveillance test intervals using a multi-objective GA to improve reliability and availability.
    Problems studied by Martorell et al. [41,42] involved the selection of technical specifications and maintenance activities at nuclear power plants to increase reliability, availability and maintainability for safety- related equipment. They also considered the optimal allocation of more reliable equipment, testing and maintenance activities to assure high reliability, availability and maintainability levels for safety-related systems. Additional limited resources (e.g., budget and workforce) were required, to form another a multi-objective problem. Solutions were obtained by using both single-objective GA and multi-objective GA, to solve the problem of testing and maintenance optimization with the objective functions of unavailability and cost.
    Various other meta-heuristics have been used for reliability optimization. For example, Ant Colony Optimization (ACO) is a population- based, general search technique for the solution of difficult combinatorial problems [43]. The method is inspired by the pheromone trail laying behavior of real ant colonies. In ACO, artificial ants probabilistically build solutions by taking into account pheromone trails, which change dynamically at run-time, to reflect the agents acquired search experience and heuristic information on the problem instance. ACO algorithms have been applied for the reliability optimization of series- parallel systems [44], also including quantity discounts on the redundant components [45], and network optimization by embedding a Cellular Automata approach combined with Monte Carlo simulation for network availability assessment [46], within a multi-objective ACO search engine [47]. ACO has also been applied in hybrid form with Simulated Annealing (SA), called ACO SA, for the design of communication networks [48], where the design problem is to find the optimal network topology for which the total cost is a minimum and the all- terminal reliability is not less than a given level of reliability.
    SA is another type of meta-heuristics introduced by Kirkpatrick et al. [49] and Cerny [50] as a general probabilistic method for solving combinatorial optimization problems. SA searches the global optimal solution avoiding entrapment in poor local optima by allowing a (probabilistically) occasional uphill move to worse solutions. A SA algorithm for communication network reliability optimization has been proposed [51], which selects the optimal set of links that maximizes the overall reliability of the network subject to a cost constraint, given the allowable node-link incidences, the link costs and the link reliabilities. The algorithm employs a variation of the SA approach coupled with a hierarchical strategy to achieve the global optimum. SA has also been applied to search the optimal solution of system reliability-redundancy allocation problems [52] also considering nonlinear resource constraints [53]. Different SA strategies have been applied to solve multi- objective system reliability optimization problems [54].
    Particle Swarm Optimization (PSO) is another algorithm conceptually based on the social behavior of biological organisms that move in groups, such as birds and fishes [55]. The basic element of PSO is a particle, which can fly throughout the search space toward an optimum by using its own information and that provided by other particles within its neighborhood. As in GA, the performance of a particle is determined by its fitness that is assessed by calculating the objective functions of the problem to be solved. Then, PSO has certainly some similarities to evolutionary algorithms such as GAs, but it also in- corporates a cooperative approach. Indeed, all individuals (particles) which are allowed to survive change their positions over time and one particle's successful adaptation is shared and reflected in the performance of its neighbors. Originally developed for the optimization of continuous unconstrained functions, PSO did not attract much attention from the reliability community, because most reliability optimization problems are of discrete nature and have constraints. However, it has been shown that properly adapted PSO can be an effective tool for solving some discrete constrained reliability optimization problems [56]. PSO has, then, been applied to solve reliability optimization and RAP of complex systems [57].
    Several optimization meta-heuristics have been designed for various optimization applications in reliability engineering, with varying degrees of success. As no meta-heuristic is so versatile to always outperform the other meta-heuristics in all kinds of reliability optimization problems, developing new, good optimization approaches can be very helpful in some specific applications and benefit practitioners providing more options. Overall, some preferences in practice is given to the use of GAs, as they are able to solve both integer reliability problems and mixed-integer reliability problems. Furthermore, their applicability is not limited to series-parallel systems. In many reliability optimization problems, the optimal solutions found by GAs have turned out to be superior to those of the other meta-heuristic methods for both integer reliability problems (in which component reliabilities are given and redundancy allocation is to be decided) and mixed integer reliability problems (in which both the component reliabilities and redundancy allocation are to be decided simultaneously). Therefore, GAs are very competitive and attractive meta-heuristic methods, especially appropriate for design of nonstandard series-parallel systems. In addition, the multiple solutions found by the GA sometimes vary significantly in the
    component reliabilities and/or redundancy allocation for systems. This offers the design engineer a variety of options from which to choose with only small differences in the system reliability.
4. Era of pragmatism
    After exhausting much of the inventory of reliability optimization problems that could be solved to optimality by mathematically rigorous methods, researchers entered into an era of pragmatism. The driver for this was the need to expand the types of problems to treat, considering more complex systems and more realistic reliability behaviors of the components, without necessarily being able to mathematically prove the optimality of the solutions (although this remains highly desirable). 
    Original problem formulations that were solved to optimality often adhered to some common assumptions, although not always, including (i) active redundancy, (ii) perfect switching of redundant components, (iii) limitations on mixing functionally equivalent components within a parallel structure, (iv) binary behavior of components and systems, and others. These assumptions simplified the problems and optimal solutions could be found, but artificially constraining the problem spaces far from real conditions. Therefore, the usefulness of these methods was limited, and there was a need to analyze systems with more realistic behaviors, including multi-state systems, uncertain systems, realistic forms of redundancy, etc.
    For the more realistic and complex problems, the assumptions or model forms required for mathematical programming algorithms could generally not be satisfied. At the same time, more advanced computers and computer processing provided for exhaustive forms of heuristic search. GA and other forms of meta-heuristics were now used pre- dominantly.
4.1. Multi-state systems
    For components and systems used in practice, often a binary state description (functioning or failed) may not be a proper representation of the reliability behavior, because the component and system reliability performance has a range of different levels (Barlow and Wu [58], Hudson and Kapur [59]). However, evaluation of multi-state system (MSS) reliability is more difficult, and potentially mathematically cumbersome.
    Levitin et al. [60], and Levitin and Lisnianski [61] presented pioneering and influential research models to optimize system design for multi-state systems. Levitin et al. [60] determined an optimal system structure, and Levitin and Lisnianski [61] optimized imperfect preventive maintenance intervals. They used a universal generating function (UGF) approach to evaluate multi-state reliability and a GA to search the solution space to determine the best solution, although not guaranteed to be the optimal solution. UGF is a convenient function based on a z-transform that is useful to systematically and efficiently compute multi-state reliability.
    Levitin and his colleagues continued to extend their innovative work to analyze additional applications of multi-state systems. The first formulation of joint structure and maintenance optimization problem for multi-state systems was presented in Levitin and Lisnianski [62], and the optimization approach was extended to systems with common cause failures by Levitin [63]. Later, Levitin and Xing [64] analyzed systems with propagating failures. Each of these research efforts re- presented fundamental advancements. System reliability optimization could, then, be applied to an entirely new class of systems design problems.
    In recent years multi-state models for system reliability assessment have become increasingly popular. In particular, significant research efforts have been devoted to the solution of RAPs for series-parallel multi-state systems (MSSPS) [3,65?67], which was first introduced in [68]. Series-parallel structures are typically considered because they are quite common in practice. Due to the difficulty of the problem, meta-heuristics are often used to solve MSSPS RAP, even though they can become time-consuming, especially on large systems.
    On the other hand, theoretical analysis of meta-heuristics for MSSPS RAP has been generally lacking. Exact/approximated algorithms or guidance for meta-heuristics design have not yet been proposed in the MSSPS RAP literature, while it is important because the application of RAP to multi-state models often requires exhaustive computational resources. Indeed, the difficulty of solving MSSPS RAP is not only due to the well-known problems of MSS reliability evaluation, but also to the discrete, probabilistic and nonlinear nature of RAP problems.
    Another form of a system where the components exhibit multiple states is when component failure time distributions or state prob- abilities and replaced with a stochastic degradation process. This problem can be particularly challenging when the individual component degradation processes are dependent or have interactions. Song et al. [69] determined optimal replacement intervals and inspection intervals for systems with dependent failure processes. Bian and Gebraeel [70] developed a prognostics model for a multi-component system with degradation interactions.
4.2. Uncertainty
    The optimization of system reliability relies on a model to provide a representation of the system failure behavior. The model is built on a number of hypotheses on the types of distributions which the stochastic failure processes of the components obey. The values of the parameters of these distributions need to be estimated, and there is always some level of estimation uncertainty. There is intrinsic uncertainty and in- complete knowledge of the system behavior. Uncertainty can be model or structural uncertainty, which exists on the hypotheses of the model, or parameter uncertainty, which exists on the values of its parameters. 
    In the literature, a number of aspects, factors and causes of un- certainty have been identified, as summarized by Armacost and PetEdwards [71], Zimmermann [72]:
    Lack of information or knowledge: Lack of information, knowledge and/or data is the main source of uncertainty. This type of uncertainty can be reduced by collecting more information and data.
    Approximation: Any model involves some degree of approximation, which is necessary when there is insufficient information to describe exhaustively the phenomenon of interest or when it is desirable to simplify the analysis due to computational constraints or other reasons.
    Abundance of information or knowledge: People are incapable of assimilating many pieces of data and information simultaneously. The analyst usually focuses on those parameters and those pieces of data and information that are considered to be more important, while neglecting the others. This type of uncertainty is related to biases in subjective probability assignments (see Kahneman and Tversky [73] and Aven [74]).
    Conflicting nature of pieces of information/data: When there is conflicting data, increasing the amount of available information and data would not decrease the uncertainty. More data may just increase the conflict among different pieces of information and data. Some information are affected by errors creating the conflict, although the analyst cannot identify them, or otherwise, the model used by the analyst is poor.
    Measurement errors: The measurement of a physical quantity, such as temperature, weight, length, is always affected by the precision of the measurement capability.
    Linguistic ambiguity: An expert may express that something is big, but the meaning of ¡°big¡± is ambiguous, and can be interpreted in different ways.
    Subjectivity of analyst judgments: There can be different interpretations of the same information and data, depending on cultural background and competence of the analyst.
    Uncertainty analysis involves identifying and studying the sources of uncertainty and propagating the effects onto the output of the model. Uncertainty analysis determines the uncertainty in the model output that results from uncertainty in the model inputs (Helton et al. [75]). In the practice of reliability engineering and quantitative risk analysis, it is common to distinguish between aleatory and epistemic uncertainty (Apostolakis [76], Helton and Oberkampf [77]). Aleatory uncertainty refers to phenomena occurring randomly, so probabilistic modeling is appropriate to describe such occurrences. Epistemic uncertainty involves quantifying the degree of belief of the analysts on how well it represents the actual system. It is typically expressed as subjective probabilities of the parameters of the probability models. It can be reduced by gathering information and data to improve the knowledge on the system behavior.
    For system reliability optimization, uncertainty must be properly accounted for. It is often important to consider the uncertainty in the system reliability estimation so that risky solutions with unsatisfactorily high reliability estimation uncertainty can be avoided. System de- signers and users are generally risk-averse. Decision makers would generally prefer the design of a system whose reliability is estimated with large confidence, as assured by the low uncertainty of its estimation. Thus, maximization of the system reliability and minimization of its estimation uncertainty is an important formulation, that should be emphasized.
    System reliability optimization research originally did not consider the uncertainty in the reliability estimation, although Rubinstein et al. [78] is an early example of a model to maximize the expectation of a series-parallel system reliability estimate with component uncertainty. However, maximization of the expectation of the reliability estimate may not be adequate, if it is important to avoid system designs with unacceptably high uncertainty. It is therefore desirable to use a multiple-objective optimization algorithm, which explicitly considers the component uncertainty.
    In Marseguerra et al. [79], a multi-objective GA is developed to select optimal network designs that balance the dual objectives of high system reliability and low uncertainty in its estimation. Monte Carlo simulation is used to evaluate the objective function and Pareto optimality is introduced to handle the multiple preference criteria. The decision variables are the numbers of components, xij, of a given type j to be allocated in the various sections (node pairs & links) i of a network system, i = 1, 2,ÿ, s, and j = 1, 2, ÿ, mi. The network is designed to maximize the expectation of the network reliability and minimize the variance of the estimate (by maximizing the negative variance). Introducing cost and weight constraints, the multi-objective optimization problem may be formulated as follows:
    This is an appropriate formulation for a risk-averse decision maker, as opposed to most optimization algorithms that require or assume risk- neutrality. Many decision makers may prefer a risk-averse solution, with a marginally lower expected value of reliability compared to a solution with a higher expected value, but with unacceptable un- certainty. Epistemic uncertainty has also been accounted for using interval and fuzzy multi-state models [67,80?82].
4.3. Different types of redundancy
    The original formulations of the system reliability optimization problems assumed that all redundancy was active redundancy. This is a convenient assumption because the failure time of a parallel subsystem of components is the maximum of individual component failure times, and the reliability, or probability of survival for some mission time, can be expressed using standard probability principles that are independent of any failure time distribution assumptions. However, many actual subsystem design problems, use a variety of active, cold, warm or hot standby, often within the same design, and therefore the original formulations and solution methods were not practical or applicable for many actual problems.
    System designs with active redundancy have fully activated components that can continue to provide needed design functions in the advent of failure of a primary component, until all redundant components have failed as well. Cold standby redundancy involves the use of non-activated components that can be switched-on in response to failure of a primary component. Often systems are designed with both types of redundancy within different parts of the system, and there are examples where the redundancy type is also a design variable. Cold standby redundancy requires switches to detect failure and activate redundant units. However, the switches can also fail and must be considered in the optimization model. It is assumed that components in cold-standby do not fail, while components in warm standby can fail, but at a lower rate those comparable active or hot standby components. Components in hot standby still require a switching mechanism, but fail at the same rate as active components.
    A solution methodology was developed to determine optimal design configurations for nonrepairable series-parallel systems with cold- standby redundancy by Coit [83], who considered a component with non-constant component hazard functions and imperfect switching. There were multiple component choices available for each subsystem and component failure times are distributed according to an Erlang distribution. Optimal solutions are determined using IP with 0?1 decision variables.
    There are other engineering system design projects the choice of redundancy type becomes an additional design variable. System design optimization was demonstrated in Coit [84] to maximize reliability when either active or cold-standby redundancy can be selectively chosen for individual subsystems. Formulation of the problem allowing a choice of redundancy strategies is more realistic and practical. Optimal solutions to the problem are found using IP considering imperfect switching of standby redundant components [84]. The optimal system design is distinctly different from the corresponding design obtained with only active or only cold standby redundancy. The same problem was later solved using a GA [85]. Most recent research on systems with imperfect switches has been done by Kim [8] and Levitin et al. [86].
    The problem with a choice of redundancy strategies has been ex- tended in several original ways including a mixed strategy [87] combining both active and cold-standby components within the same sub- system. Other recent meaningful system reliability optimization research considering mixed component redundancy have provided important models for more varied and practical applications [88?90]. There have also been multiple objective formulations to the problem with different redundancy types [91,92].
    More recently, other different redundancy strategies or types of problems have been considering including the standby element sequencing problem and the combined sequencing and inspection/ checkpointing/allocation policy optimization for different types of standby (hot, cold, warm, mixed). Some research efforts combining system structure optimization with optimal standby component sequencing are included in [8,9,93,94].
5. Era of active reliability improvement
    There are currently some very promising on-going research activities that can be considered collectively as an era of active reliability improvement. System reliability optimization is not a static model, but it is being conducted continuously in response to new data being collected on failures, component and system degradation, environmental stresses and usage histories. As an integral part of the optimization process, system performance can be optimized and improved dynamically as new data is collected and analyzed to provide a better under- standing of usage conditions and failure behavior, or to compensate for changing conditions. Modern sensor and communications technologies facilitate the collection and transmission of data so that the optimal system design and maintenance plans can be continually enhanced.
    Standard assumptions for most system reliability optimization models have been that component failure times form a homogeneous population, and that the failure time distributions, or reliability for a fixed time, are static or stationary. In practice, both assumptions are at best approximations of actual conditions. Although a population of components may form a homogeneous population, their corresponding failure times are influenced by specific environmental stresses and user requirements/stresses that can vary appreciably for specific sub-populations of users or applications. Also, there can be systemic trends in stresses, that can result in shifting failure time distributions over time. In these cases, there is not actually a homogeneous population of identically distributed failure times, and therefore, most optimization models cannot accommodate these realities
    The era of mathematical programming resulted in entirely new in- sights on optimizing system design, and demonstrated how advanced mathematics can be used to solve this problem. The era of pragmatism extended the more theoretical models or developed new ones to address actual conditions of fielded systems. However, in both of these eras, the optimization results were a final result. The solution to these difficult optimization problems was intended to be performed once, perhaps with associated sensitivity analyses. Of course, as new data was collected, the analysis could be repeated, but the optimization process did not directly integrate changing conditions. In the era of active reliability improvement, the changing conditions and data analyses are an integral part of the model.
5.1. Dynamic system reliability models responding to new data
    Dynamic optimization of system reliability has the potential to achieve responsive system designs, which are highly reliable with changing or diverse conditions. To achieve the highest level of reliability and minimum cost, engineering designs and maintenance plans must address changing conditions or new data that provides better estimates of model coefficients or parameters. To accomplish this, the optimization must be dynamic. The model is solved over-and-over or continually as part of the optimization in response to new data/conditions/etc. as the system is operated.
    Yildirim et al. [95,96] present two comprehensive models that involve integrating sensor-based degradation modeling and remaining life distributions with classical mathematical programming, specifically mixed integer programs. The resulting model optimizes predictive maintenance decisions for a complex system to minimize cost. The application being solved is the unit-commitment problem, a well-studied optimization problem, pertaining to power generation and trans- mission. Yildirim et al. [97] considers opportunistic maintenance scheduling again within an integrated framework that combines mixed integer programming and sensor-based degradation models.
    Hao et al. [98] addresses dynamic optimization of workload assignment to actively control the degradation and failure time for multiple-units system. Components are degrading and failing, but the rate of degradation is a function of workload assignment. Multiple units are arranged in parallel, and several identical machines may need to operate together to simultaneously produce products to meet the high production demand. This parallel configuration is designed with redundancy to compensate for unexpected events. As data is collected, there is Bayesian updating of degradation model parameters and re- optimization of workloads.
    Recent developments by Li et al. [99] have proposed providing industrial assets with a degree of agency, in order to enable real time prognostics and optimization of the asset's operation conditions. They consider the feasibility of improving system-level performance in industrial systems by integrating social networks into the IoT (Internet of Things) concept.
    Bascifti et al. [100] considers a complex system of buses, generators and transmission lines. The model considers the scenarios where un- expected failures happen based on the updated remaining life distributions. The modeling framework in this case is a stochastic optimization model with chance constraints that leverages sensor-based remaining life predictions.
5.2. System reliability optimization customized for specific subsets of users
    Data analytics can also be exploited such that the optimal system design can reflect differences within a population. There can be regional differences or fundamental differences within the user population, and by observing and quantifying specific usage conditions and failure patterns, an optimal design can simultaneously correspond to a collection of diverse users or conditions. A failure time distribution can be considered as a function of usage and environmental stresses, and specific reliability values can then naturally vary to reflect these differences.
    Ramanan et al. [101] studies an advanced distributed optimization problem. There are several interesting aspects and challenges to this problem. The first relates to the computational challenges associated with large scale decentralized optimization and the second relates to the underlying high-performance computing architecture that is would be suitable for such decentralized systems. Advanced data processing of large data sets within subnetworks (local utility companies) is required. 
    Bei et al. [102] presents a model to fully investigate the integrated redundancy allocation and maintenance planning problem with the presence of uncertain future usage stresses. Component failure time distributions are expressed as a function of environmental and usage stresses. A component system design, with component choices and redundancy levels is selected by the optimization model, but specific preventive maintenance intervals are selected for different usage and environmental stress vectors. The problem is formulated as a two-stage stochastic optimization problem with discrete scenarios defined for different usage and environmental conditions. Zhu et al. [103] extends this model by considering uncertain aperiodic changing stresses.
6. Future challenges in system reliability optimization
    The safe and productive performance of industrial systems depends on optimal designs that use equipment reliably, and on testing and maintenance activities that assure the required high level of reliability, availability and maintainability of the equipment. This is done through the efficient assignment of resources that are usually limited. A number of challenges arise in relation to the modern complex systems reliability optimization:
    Integration and response to continual streams of data proving new and updated information.
    Accounting for both aleatory and epistemic uncertainties within the decision-making framework of system reliability optimization 
    Cooperative optimization of multi-agent systems, with individual objectives to be optimized within an overall system optimization 
    Integrated optimization of reliability design, maintenance, spare parts inventory and logistics management
    Dynamic optimization of evolving systems under changing conditions
7. Conclusions
    In this paper, we have provided an organized discussion and review on the evolution of the subject of complex system reliability optimization, which is at the heart of reliability engineering. We have presented how the development of solutions to such problem, and their application, have evolved as a continuum of ideas and techniques, which we have chronologically organized into the three eras of Mathematical Programming, Pragmatism, and Active Reliability Improvement.
    In this flow of development and advancement, we have highlighted the joint pull force coming from the fields of operations research and optimization theory, and from the evolution of technology. Their combination has led to the advancement of research ideas and theories, brought new perspectives from the engineering world, and resulted in the development and continuous improvement of methods and techniques to address reliability optimization problems of increasingly complex systems.
    The underlying message that emerges from this is that system re- liability optimization is an ongoing topic of scientific development and will always be so. The research is actively pulled by the advancements in mathematics, operations research and optimization theory, and in response, researchers will continually develop new methods and algorithms to solve, more efficiently than before, broader classes of problems, and larger and more difficult problems. At the same time, the research is pushed by the changes in technology, and in the engineering and social worlds, practitioners will continually demand for new developments to cope with the practical challenges encountered on the field.
    In conclusion, today we are treating problems that involve more complex systems and more realistic reliability behaviors of the components, including multi-state, uncertain behaviors, etc. We are beginning to address them dynamically, as new data is collected and analyzed to provide a better understanding of usage conditions and failure behavior, or to compensate for changing conditions, so that the optimal system design and maintenance plans can be continually enhanced. This is possible due to the collection and transmission of data by modern sensor and communications technologies. Yet, new opportunities and challenges are always arising, and it will always be necessary to find efficient ways to solve new problems or problems previously unsolvable.
    
    

Keywords:
Fault detection Wind turbine SCADA data
Non-singleton fuzzy inference system Expanded linguistic terms and rules

Abstract

Wind power generation e?ciency has been negatively a?ected by wind turbine (WT) faults, which makes fault detection a very important task in WT maintenance. In fault detection studies, fuzzy inference is a commonly- used method. However, it can hardly detect early faults or measure fault severities due to the singleton input and the limited linguistic terms and rules. To solve this problem, this paper proposes a WT fault detection method based on expanded linguistic terms and rules using non-singleton fuzzy logic. Firstly, a generation method of non-singleton fuzzy input is proposed. Using the generated fuzzy inputs, non-singleton fuzzy inference system (FIS) can be applied in WT fault detection. Secondly, a mechanism of expanding linguistic terms and rules is presented, so that the expanded terms and rules can provide more fault information and help to detect early faults. Thirdly, the consequent of FIS is designed by the expanded consequent terms. The defuzzi?ed result, which is de?ned as the fault factor, can measure fault severities. Finally, four groups of experiments were conducted using the real WT data collected from a wind farm in northern China. Experiment results show that the proposed method is e?ective in detecting WT faults.


1. Introduction

The rapid development of wind energy has boosted the installations of wind turbines (WTs) [1]. Meanwhile, an increasing demand for higher power generation e?ciency has put more pressure on the op- eration and maintenance (O&M) of WTs [2]. One of the essential tasks of O&M is to deal with WT faults, which have negatively a?ected power generation e?ciency and caused a heavy economic loss [3]. If WT faults are detected in time, it would greatly reduce O&M costs [4]. Therefore, due to its signi?cant role in the O&M of WTs, there have been increasing studies on WT fault detection, such as the detection methods based on signal processing [5], image processing [6], machine learning [7], deep learning [8], etc.


Fault detection based on condition monitoring (CM) is one of  the most commonly-used methods. The main principle of the method is to examine whether the collected on-line data are within  the  normal range. If the data are out of the normal range, there might be  an anomaly or a fault. Furthermore, WT faults can be detected and diag- nosed according to di?erent anomaly data. Fault detection based on CM is e?ective and easy to operate. In [9], an unsupervised anomaly de- tection approach for WT condition monitoring was proposed. In [10], a temperature-based real-time aging monitoring method was  presented for power converter modules. In [11,12], CM-based fault detection methods were put forward to detect the WT faults in gearbox and generator respectively.
In fault detection methods based on CM, many types of data can be used, including Supervisory Control and Data Acquisition (SCADA) data [13], vibration data [14], strain data [15], acoustic data [16], and lu- brication oil data [17]. However, the acquisition of the vibration data and other types of data requires additional sensors, which increases the maintenance cost [18]. Fortunately, modern wind farms (WFs) usually install SCADA system, which collect hundreds types of on-line WT data at a certain interval. SCADA data are cost e?ective, as no additional sensors are needed [19]. A lot of research based on SCADA data has been put forward to realize the fault diagnosis and fault prognosis. In [20], machine learning models based on domain knowledge were proposed to realize fault detection. In [21], deep neural network (DNN)-based framework was put forward to detect WT gearbox faults. In [22], many data-mining approaches for wind power prediction were evaluated, which can be used for fault detection. In [23], a fault de- tection method based on multiclass support vector machine algorithms was proposed. Meantime, fault prognosis has gained a rapid develop- ment in recent years [24]. Based on SCADA data, an on-line fault and help to detect early faults. Moreover, the defuzzi?ed  fault  factor can tell fault severities.

The rest of this paper is organized as follows. Section 2 introduces the related work and the problem description. Section 3 describes the proposed method in detail. Experiments and comparisons are listed in Section 4. Section 5 is the conclusion of the present work.

2. Related work and problem description

2.1. Review of the FIS method in WT FAULT detection

FIS method of WT fault detection based on SCADA data generally consists of the following three steps [30,31].
Step 1: Data prediction. First, the original normal SCADA data
Dorg (x) (consisting of n variables) are collected, as shown in Eq. (1).


prognosis method [25] and a prior knowledge-based prognosis method of WT pitch faults were proposed.
Among the CM and fault detection approaches, one of the com- monly-used method is the fuzzy inference system (FIS). FIS evaluates inputs with ¡°if-then¡± rules based on fuzzy logic. There are two parts in  the rule: ¡°if¡± part gives the evaluation of the input and ¡°then¡± part returns a fuzzy output according to the rule [27]. Based on FIS, many  fault detection methods have been proposed. In [28], FIS was estab- lished   with  rare  association   rule  to  predict   the  spatiotemporal   dis-
tribution  of  energy  security  weaknesses  in  transmission  networks.   

In where, Di (x) is the ith variable in Dorg (x). The SCADA data usually consist of various variables [34,35], including wind speed (m/s), active power (kw), generator temperature (¡ãC), and bearing temperature (¡ãC), etc.
Then, fault-related variables of SCADA data are chosen. These variables are predicted by their relevant data (including current data, historical data and current data of other WTs [31]), as indicated in Eq.  (2).


[29], an adaptive neuro-fuzzy inference system and hybrid models were developed. In [30], a system  for WT condition  monitoring using where, Dk is the fault-related variable and Dd (j) is the jth relevant data adaptive neuro-fuzzy interference systems was proposed, and it has achieved good experiment results. In [31], an e?ective generalized model for WT anomaly identi?cation method based on fuzzy synthetic
evaluation was put forward. In [32], a development of a fault diagnosis of Dprd. fpredict is the prediction method, which mainly includes time series method [36], neural network method [37], etc.
Step 2: Anomaly detection by the prediction error. With the collected data Dk (x) and its predicted data Dk (x), the prediction error scheme based on identi?ed fuzzy models was presented. In [33], a  monitoring strategy of short-circuit fault between turns of the stator can be calculated by Eq. (3).

windings and open stator phases by fuzzy logic technique was proposed.

However, several problems remain in the WT fault detection based on FIS: (1) Early faults cannot be detected using conventional FIS methods due to the averaged singleton input. The widely-applied methods usually use daily averaged data as the input to reduce false alarms of WTs. However, the details of the data are omitted. (2) Fault severities cannot be measured by conventional FIS methods due to the limited number of FIS consequent terms (for example, only ¡°normal¡± and ¡°fault¡±).
In order to tackle the above problems, this paper proposes a WT   fault detection method with SCADA data based on expanded linguistic terms and rules using non-singleton fuzzy logic. First, a method of generating non-singleton input is proposed. The non-singleton  inputs can be obtained by transforming probability density functions (PDFs) of the grouped prediction errors, which enables the application of non- single FIS in WT faut detection. Second, more antecedent and con- sequent terms are expanded by the proposed method. The rules of fault detection are also expanded accordingly. Consequently, the  fuzzy system can detect the fault at an early stage and provide more in- formation about fault severities. Third, the FIS consequent  is designed by the expanded consequent terms, and the defuzzi?ed output is de- ?ned as the fault factor, which can tell the severity of the fault.
The contributions of this paper include:

(1) A generation method of non-singleton fuzzy input is proposed, and non-singleton FIS is applied to detecting WT faults. To the authors¡¯ knowledge, it is the ?rst time that non-singleton FIS has been ap- plied to detecting WT faults.
(2) A method of expanding linguistic terms and rules in FIS is proposed. The expanded terms and rules can provide more fault information,

Then, probability distribution function (PDF) of the prediction errors can be obtained. Also, the upper bound Bup and the lower bound Bdn of the PDF are calculated as follows: (1) A con?dence interval is set, and
(2) the value of the left endpoint of the con?dence interval is de?ned as the lower bound and that of the right endpoint as the upper bound. For example, if   is set as 99%, the value corresponding to 0.5% in the PDF    is the lower bound and the value corresponding to 99.5% in the PDF is  the upper bound. Therefore, if the collected on-line prediction error dr   is greater than Bup, it is marked as ¡°high¡±, whereas if dr  is less than Bdn,  it is marked as ¡°low¡± (in some researches, more bounds are introduced, such as ¡°very high¡± and ¡°very low¡± [30]).
Step 3: Fault detection based on FIS. In order to detect a certain fault, prior knowledge is extracted as rules to detect WT faults [38]. According to certain combinations of data anomalies, WT faults can be diagnosed. Fig. 1 shows an example of the WT fault detection with the conventional FIS method. Two variables of WT SCADA data (Data 1 and Data 2) are collected and predicted. Then, PDFs of the two prediction errors are calculated and the lower bounds and the upper bounds are obtained. At a certain moment, Data 1 is marked as ¡°high¡± and Data 2 is marked as ¡°low¡± (as illustrated by red points in Fig. 1). Then, it is de- termined as Fault X according to Rule 001.

2.2. Problem description

There are several problems that prevent the conventional FIS method from detecting the early fault. The ?rst problem is the limita- tion of singleton input in FIS. In order to reduce false alarms, daily averaged data, rather than the 10-min data, are often used by FIS methods, which would result in the lack of data details. Fig. 2 shows an example. In this case, the gearbox oil temperature is monitored at an interval of 10 min. The blue dot region is a time window T, in which the data points are averaged as one value. It can be found that although the averaged value in T is lower than the upper bound, many data points in T are higher than the upper bound. Such case can be regarded as a potential anomaly, which cannot be detected by the conventional sin- gleton-based FIS.
The second problem is the limited number of linguistic terms of a fault. Linguistic terms in WT fault detection are extracted from the prior knowledge and experts¡¯ experience. The number of linguistic terms is small due to the limited prior knowledge and experts¡¯ experience. In practice, the consequent of rules usually include only two terms of ¡°Normal¡± and ¡°Fault¡±. Consequently, it prevents FIS from detecting  early faults and telling fault severities.

3. Method

3.1. Architecture of the proposed method

The architecture of the proposed method is designed based on the process of conventional FIS, as shown in Fig. 3. There are six phases in this architecture (The red dotted parts are the originalities of the pro- posed method).
Phase 1: Data prediction. As discussed in [30], according to the commonly-used FIS methods, the original data are collected and pre- dicted. In this study, the current data and the historical data are se- lected to predict the target data. Many approaches can be used to predict data [39,40]. In this paper, the widely-applied neural network (NN) method is used to realize the prediction. The prediction error can be obtained by Eqs. (1)¨C(3).
   
Phase 2: Non-singleton input transformation. Di?erent from the conventional FIS method, the proposed method introduces non-sin- gleton FIS input in WT fault detection. The prediction errors are divided into groups. The PDF of each group is calculated. Then each PDF is transformed to a non-singleton input. With the non-singleton input, non-singleton FIS can be applied in WT fault detection. Details are discussed in Section 3.2.
Phase 3: Linguistic terms and rules acquisition. Similar to the con- ventional FIS method [41], linguistic terms and rules are extracted from prior knowledge and experts¡¯ experience. First, rules for detecting WT faults, such as ¡°IF A is High AND B is Low THEN Fault 01 (Fault)¡±, are acquired. Second, linguistic variables and terms are extracted from the rules, such as ¡°A¡± is extracted as a linguistic variable, whereas ¡°High¡±, ¡°Normal¡±, ¡°Low¡± are extracted as the linguistic terms of ¡°A¡±. Conse- quently, the original membership functions (MFs) can be calculated by fuzzy statistics method [42]. There are many types of MFs, such as Gaussian MF [43] and interval type-2 MF [44]. In this paper, the tri- angular/trapezoid MFs are adopted. These MFs can be easily obtained by data-driven method, and many similar methods [30,31] have achieved good results using this type of MFs in detecting WT faults.
Phase 4: Linguistic terms and rules expansion. Linguistic terms of antecedent are expanded, with the aim of enabling the FIS  to detect early faults and measuring fault severities. Then according to di?erent combinations of expanded antecedent terms, consequent terms at dif- ferent fault levels are generated. Accordingly, the expanded rules  are also obtained. Details are described in Section 3.3.
Phase 5: Fuzzy inference. In this phase, fuzzy inference engine [45]     is applied to processing fuzzy input sets into fuzzy output sets. First, ?ring levels are calculated by the non-singleton fuzzy inputs and antecedent MFs. Then, output sets are obtained by the calculation of

consequent MFs and levels.Phase 6: Fault factor. Normally, the conventional FIS method only
uses the output of the rule as the ?nal result of the fault detection. In this paper, benefitted from the expanded linguistic terms and rules, the
   
The designed process is shown in Fig. 4. First, each obtained pre- diction error Dk is regarded as a linguistic variable. Then, Dk is divided into groups by Eq. (4). fault detection can go a step further. The fuzzy output sets are de
 to a crisp output (fault factor), which is designed to measure fault severities. Details are described in Section 3.4.

3.2. Non-singleton input TRANSFORMATION

Compared with singleton FIS, fuzzy logic based on non-singleton  input proved to be a more e?ective method in theory [45]. However, in practice, it has not been applied in the fault detection ?eld. In  this  paper,  the fuzzy number  of non-singleton  input is correlated  to the PDF
of prediction  errors, which realizes  the application  of non-singleton  

where, t is the group number and P is the number of data points in each group (in this paper, P is set as 144). Next, each Dk is converted to a  fuzzy number. Di?erent from singleton FIS, which uses a single value as the input, non-singleton FIS uses fuzzy number as the basic input unit.
To qualify as a fuzzy number, a fuzzy set A must possess the following properties [41]: (1) A must be a normal fuzzy set; (2) A¦Á  must  be  a closed interval for every ¦Á ¡Ê (0, 1] and (3) the support of A must be bounded.
Normally,  the  distribution  of   is  considered  as  a  normal  distribution in this method. 

a closed interval for every ¦Á ¡Ê (0, 1]; (3) By setting an appropriate con?dence interval, the support of f k is bounded. This means fk
completely meets the requirements of a fuzzy number. Thus, it can be used in non-singleton FIS.
Therefore,   the   non-singleton   input   transformation   can   be   implemented. First, the mean ¦Ìk  and the standard deviation ¦Òk  of each Dk are calculated by Eq. (5) and Eq. (6) respectively.

Then, the fuzzy number of non-singleton input can be obtained by transforming each f k , as shown in Eq. (7).

where, t is the group number of Dk and P is the number of data points in each group. The fuzzy number of non-singleton input is brie?y marked as ¦Ìk (x) in Eq. (8).

MFs before the expansion. (b) Terms and their MFs after the expansion.

distribution of the input data set. The non-singleton input makes the calculated membership degree more accurately reflect the actual WT

condition, which could help to detect early faults that are undetected by

Therefore, the input data set ¦Ì? (x1, x2¡­xq) of the lth Rule Rl (with q
linguistic terms) can be calculated by Eq. (9).

3.3.  EXPANDING  linguistic  terms  AND  rules

where, ¡ï is the minimum t-norm operator [46].

   
In this section, a method of expanding linguistic terms and rules is proposed.
The limited linguistic terms is a problem in FIS. In order to solve this problem, Jerry M. Mendel ?rstly proposed a basic theory on linguistic

terms expansion [47]. The main idea is to extract new linguistic terms from the original ones according to their features. Inspired by this

where, ¦ÌFl is the antecedent MF. Then, ?ring levels Fll are calculated with ¦Ì? (x1, x2¡­xq) and ¦ÌFl (x1, x2¡­xq) by Eq. (11).

theory, a mechanism of expanding linguistic terms and rules for WT fault detection is designed in this paper.
PART 1: The expansion of antecedent linguistic terms. Theoretically,

the expanded terms should be generated from the existing ones.


Therefore, in this paper, every two existing adjacent terms are used to generate a new term.



where, X is the domain of the input sets.
Fig. 5 shows an illustration of the computing process of non-sin- gleton membership degrees. It can be found that the membership de- gree is no longer determined by a single input value, but by the

statistical method. Then, the crossing points of MFs and their x-values are marked (PSL and PSH in Fig. 6(b)). The crossing point is the fuzziest point (take PSL for example, it is neither ¡°Low¡± nor ¡°Normal¡±). Thus, its membership degree in the new term is de?ned as 1. Finally, the vertical


Fig. 5. Illustration of the computing process of non-singleton membership degrees. (a): The non-singleton fuzzy input and MFs of the antecedent. (b) and (c): The calculated membership degree (the red dot) of the antecedent terms. (d) Calculation results.



line through the crossing points is made and the original critical points are connected. The obtained triangles (pink dotted lines in Fig. 6(b)) are the MFs of the expanded terms.
Operator  is de?ned as the above generation process. Thus, a new
linguistic term can be obtained by Eq. (12).

where, Tmnew is the expanded new term. Tmleft and Tmright are its two adjacent terms.
PART  2:  Expanding  rules  and  consequent  linguistic  terms.  With  the expansion   of   antecedent   linguistic   terms,   rules   are  accordingly   ex- panded. Supposing that the original rule R of a fault has N antecedents, each term in the antecedent is expanded to more new terms according to the expansion method described in PART 1. Thus, each antecedent in R has two types of terms, the original term Tmorg  and the expanded term Tmexp. Through di?erent combinations of Tmorg  and Tmexp, new rules are generated. If the new rule has m original terms (accordingly, there will be  N ? m  expanded  terms),  the  consequent  of  the  rule  is  de?ned  as Error  Level  m.  Consequently,  there  is  only  one  error  result  before  the expansion,  whereas  N + 1  Error  Levels  after  the  expansion,  with  the most serious fault Error Level N and the least serious fault Error Level 0. Next, consequent linguistic terms are expanded. Each Error Level is de?ned  as  a  new  consequent  linguistic  term.  Thus,  there  would  be N + 1 consequent linguistic terms of the fault. The linguistic terms and
rules expansion algorithm is summarized in Algorithm (1).
Algorithm 1. Linguistic Terms and Rules Expansion

Fig. 7. The designed consequent and the process of calculating the fault factor.
(a) The designed consequent and the calculated ?ring levels. (b) The gravity center of the intercepted consequent.

Then, the triangular MF, which has been e?ectively used in con- sequent de?nition, is applied in all consequent terms, as shown in   Fig. 7(a).
To get the output data set of the lth rule, the lth consequent is calculated by the lth ?ring level obtained by Eq. (11), as shown in Eq. (14).

its linguistic variable), 

3.4.  Design  of  the  consequent  AND  FAULT  FACTOR

In the conventional FIS method, the process of WT fault detection ends once the fuzzy consequent (for example, ¡°fault¡± or ¡°normal¡±) is obtained. However, this result could neither help to ?nd early faults nor tell fault severities due to its limited consequent terms. In this section, defuzzi?cation is applied and the fault factor (used to measure fault severities) is designed. Fig. 7 is an illustration of the designed con- sequent and the process of calculating the fault factor.
First, original consequent terms Tmcsq and the expanded consequent terms Tmexp?csq are combined, as shown in Eq. (13).
   
In summary, the technologies of the proposed method are described as follows: NN-based data predictions and non-singleton transformation (Section 3.2) are used to obtain non-singleton inputs. Expanded lin- guistic terms and rules are obtained by prior knowledge and the ex- panding method (3.3). Then, the non-singleton FIS is applied in fault detection. The defuzzi?ed fault factor (3.4) can tell the severity of the fault. Moreover, multiple types of faults can be detected by one FIS of the proposed method with su?cient rules.

4. Experiments and discussion

4.1. Experiment settings

In order to verify the e?ectiveness of the proposed method, four groups of experiments are conducted using real WT SCADA data collected in a wind farm (WF) in northern China. There are altogether 22 WTs in this WF. All the WTs are of the same type. The capacity of each

WT is 2 MW. The SCADA data collected in two years (with a time in- terval  of  10  min)  are  used  in  the  study.  The  commonly-used back
propagation  neural  network  (BPNN)  method  is  adopted  to  make  the prediction. Moreover, similar to the previous studies [30,31,33], the number of data points in a group is set too large, the condition would be monitored at a long time interval. (2) If the number is set too small, the statistical characteristics of the data would be weakened. Therefore, the number of data points in each group is set as 144 in this paper. In the following experiments, the faults used for evaluation are all real faults, and they undergo the development from early faults to serious faults. The earlier these faults are detected, the more conducive it is for WT maintenance.
Experiments are designed as follows: the proposed non-singleton input is veri?ed to detect the early anomaly in Section 4.2. The e?ec- tiveness of the proposed method for detecting early faults is tested in Section 4.3 (with multivariate input) and Section 4.4 (with one-variable input), respectively. Finally, the robustness of the method is veri?ed in experiments in Section 4.5.


4.2. Experiment 1: electiveness of the non-singleton FIS

This experiment is conducted to verify the e?ectiveness of the proposed non-singleton input in WT fault detection.
In June 2015, the gearbox oil temperature of WT 12 started in- creasing due to the aging of the oil. The high temperature of gearbox oil is a sign of a potential fault. In order to detect this anomaly, the data of gearbox oil temperature is used in this experiment. First, BPNN method is used to predict data and prediction errors are obtained. Fig.  8(a)  shows the prediction errors of the historical data and Fig. 8(b) shows    the PDF of the prediction errors. Then, the upper bound (3.71 ¡ãC) and the lower bound (?3.28 ¡ãC) are calculated.
Then, the corresponding MFs are obtained by the fuzzy statistics method [42]. The mathematical description of the MFs are listed as follows:

where, ¦Ìlow (x), ¦Ìnml (x) and ¦Ìhigh (x) are the MFs of ¡°low¡±, ¡°normal¡± and
¡°high¡±, respectively.
 In this experiment, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method. In order to make a better comparison, the setting of the pro- posed method in this experiment is the same as that of the conventional FIS method except for the fuzzy input part. The conventional FIS method uses singleton input, while the proposed method uses the non- singleton input.
Fig. 9 shows the experiment results. Fig. 9(a) and (c) show the averaged prediction errors in one day (APE-D) and in every 10 min (APE-M) respectively. (1) Using the conventional FIS method, APE-D exceeds the upper bound on 2015-06-24, triggering an alert, as shown in Fig. 9(b). However, from 2015-06-19 to 2015-06-23, although APE-D does not exceed the upper bound due to the high data variance, many APE-M exceed the upper bound, which could be considered as a po- tential anomaly. As can be seen, the conventional FIS method cannot capture these early anomalies. (2) Using the proposed non-singleton FIS method, the anomaly detection is improved, as shown in 9(d). It can be noted that the anomaly is detected on 2015-06-19, ?ve days earlier than the conventional FIS method.
Some details of the anomaly detections are shown in Fig. 10. On 2015-06-15, both the data calculated by the conventional FIS method (DTM) (the blue point in Fig. 10), and the data calculated by the pro- posed method (DPM) (the red point in Fig. 10) are below the upper bound, which indicates that the WT is in normal condition. On 2015- 06-19, DTM is below the upper bound while DPM is above the upper bound. It can be found that due to the large data variance, DPM in- creases. As a result, the anomaly can be detected in advance by the proposed method. It is not until 2015-06-24 that DTM  exceeds  the  upper bound, which is ?ve days later. From this experiment, it can be concluded that the proposed non-singleton FIS method can e?ectively detect early WT anomalies.

4.3. Experiment 2: COMPARATIVE experiment with MULTIVARIATE input
In this section, two groups of experiments are carried out to verify the e?ectiveness of the proposed method in WT fault detection with multivariate FIS input. Similar to Experiment 1, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method.


Fig. 8. The prediction errors and their PDF of gearbox oil temperature (WT 12).
(a) Prediction errors of the historical data. (b) PDF of the prediction errors.
4.3.1. 
Experiment 2.1: detecting cooling system FAULT
In early 2015, the cooling systems of WTs were improved to en- hance their heat dissipation capability. However, several WTs were poorly updated. The converter fan was improperly installed, leading to  an increase of converter temperature. As there are no sensors that can directly measure the converter temperature,the converter choke coil temperature (CCCT) and converter controller top  temperature  (CTT)  are monitored instead. The rule of the fault from the prior knowledge is summarised as: IF (CCCT is high) AND (CTT is high)  THEN  (the  con-  verter temperature is high and there are faults in WT¡¯s cooling system), which is listed as the ?rst rule in Table 1.
First, the prediction errors and their PDFs of CCCT and CTT are calculated. Also, the corresponding MFs are obtained by the fuzzy statistics method. Second, the original MFs and rules are expanded. (1)  As has been described in Section 3, the antecedent terms are expanded
Fig. 9. The anomaly of gearbox oil temperature (WT 12) and the detection results. (a) Averaged prediction errors in one day. (b) Anomaly detection result using the conventional FIS method. (c) Averaged prediction errors in every 10 min. (d) Anomaly detection result using the improved non-singleton input.
Fig. 10. The fault detection based on non-sin- gleton fuzzy input. (a), (b), (c) and (d) are four cases in the fault detection. (a): 2015-06-15. (b): 2015-06-19. (c): 2015-06-24. (d): 2015-06-30.
The blue point indicates the data calculated by the conventional FIS method and the red point indicates the data calculated by the proposed non-singleton FIS method.







Fig. 11. The consequent of converter high-temperature fault.

from three (low, normal and high) to ?ve (low, sub-low, normal, sub- high and high). (2) Accordingly, the original rules are expanded from one to four, as shown in Table 1. As can be seen, the expanded MFs and rules enrich the outputs.
 Then, the FIS consequent  is designed according to the descriptions in Section III, as shown in Fig. 11. The designed consequent has four terms: Normal, Warning (2 sub-highs), Minor error (1 sub-high and 1 high) and Major error (2 highs). It can be found that  the  original method has a consequent of two terms whereas the proposed method has four.
 Fig. 12 shows the monitored data during the fault and the detection results using the conventional FIS method and the proposed method. It can be found that during this period, both the prediction errors of CCCT and CTT increases.
In the conventional FIS method, on 2015-05-01, CCCT exceeds its upper bound, but CTT remains normal. On 2015-05-03 CTT also ex- ceeds its upper bound, triggering an alarm (as shown in Fig. 12(a), (b) and (c)). However, it can be found that from late April to early May, there are tendencies of increasing temperatures in both the prediction errors of CCCT and CTT. The conventional FIS method fails to detect such early faults.
Di?erent from the conventional FIS method, the proposed method
uses non-singleton input, expanded terms and rules in WT fault detec- tion. Moreover, the conventional FIS method depends on upper and lower bounds, while the proposed method uses fuzzy areas to determine membership degrees. From the experiment results, it can be found that:
(1) The fault is detected 5 days earlier by the proposed method. (2) On 2015-04-28, the fault factor rises above zero and a warning is triggered. From 2015-04-28 to 2015-05-10, the fault factor keeps increasing, in- dicating that the fault is getting worse.
 From the results of the experiment, it can be concluded that com- pared with the conventional FIS method: (1) the proposed method can detect faults at an early stage, and (2) it can tell the severity of the fault.

4.3.2. Experiment 2.2: detecting BLADE ANGLE sensor FAULT
In August 2014, the output active power of WT 16 decreased. After a shutdown inspection, a fault of blade pitch angle sensor was found. The measured angle is inconsistent with the actual angle, leading to a misjudgment of the control system. Consequently, the output active power is lower than it should be. According to this fault type, a rule can be summarized: ¡°IF (wind speed is normal) AND (output active power is low) AND (pitch angle is normal) THEN (possible  fault:  pitch  angle  sensor fault)¡±. Then, (1) the conventional method (such as the method  in [33]) and (2) the proposed method are used to detect this fault.
Similar to Experiment 2.1, ?rst, the PDFs of the prediction errors of
wind speed, output active power, and pitch angle are estimated. Then, the upper bounds and the lower bounds of these PDFs are obtained, and MFs are established by the fuzzy statistics method. The prediction er- rors of wind speed, active output power and pitch angle are shown in Fig.  13(a),  Fig.  13(b),  and  Fig.  13(c)  respectively.  The  experiment

Fig. 12. The monitored data and detection results of Experiment 2. (a) The averaged prediction errors of CCCT in one day. (b) The averaged prediction errors of CTT  in one day. (c) Detection result of the conventional FIS method. (d) The averaged prediction errors of CCCT in every 10 min. (e) The averaged prediction errors of CTT in every 10 min. (f) Detection result of the proposed method.

Fig. 13. The monitored data and detection results of Experiment 2.2. (a) Prediction errors of wind speed. (b) Prediction errors of active output power. (c) Prediction errors of pitch angle. (d) Detection result of the conventional FIS method. (e) Detection result of the proposed method.
results are shown in Fig. 13(d) and Fig. 13(e).
It can be found that the proposed method detects the fault (on 2014- 08-15) two days earlier than the conventional method (on 2014-08-17). From the experiment results, it can be concluded that the proposed method is e?ective in detecting WT faults with multivariate inputs.

4.4. Experiment 3: COMPARATIVE experiment with single-input

 This experiment is designed to show the e?ectiveness of the pro- posed method in WT fault detection with one-variable FIS input. Similar to Experiment 1, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method.
In July, 2015, due to the aging of the generator front bearing of WT 06, its temperature rose rapidly within a few days. On July 20th, WT 06 had a sudden breakdown. In this experiment, the monitored generator front bearing temperature (GFBT) is used. The same as that in the above experiments, GFBT is predicted and the prediction errors are obtained. The  upper  bound  and  the  lower  bound  of  the  predicted  errors  are

 As can be seen in Fig. 14(a) and (b), the prediction errors of GFBT increases rapidly from 2015-07-17 to 2015-07-19. The fault detection results of the conventional FIS method and the proposed method are shown in Fig. 14(c) and (d). It can  be found that: (1)  The conventional FIS method does not detect the fault until 2015-07-19, only one day before the breakdown. (2) Using the proposed method, the fault is detected on 2015-07-17, three days before the breakdown, giving more time for maintenances. (3) From 2015-07-17 to 2015-07-19, the fault factor is increasing, which indicates that the fault is getting worse.
It can be concluded that: (1) the proposed method is e?ective in
detecting early WT faults with one-variable input. (2)  The  proposed  fault factor can also tell the severity of the fault.
 Furthermore, in order to make a good comparison of the proposed method with more di?erent FIS methods. Another group of experiments is  conducted.  The experiment  setting is  the same  as that in Experiment
3. The following methods are compared with the proposed method: (1) the FIS method with a single prediction model (such as the method in [30]), (2) the FIS method with multiple prediction models (such as the method  in [31]), (3) the proposed  method  without  non-singleton input,
(4) the proposed method without rule expansion.
 Table 2 shows the experiment results. It can be found that con- ventional FIS methods cannot e?ectively detect early faults, either with

a single prediction model or with multi-prediction models. The pro- posed method can detect the fault at an early stage. Therefore, it can be concluded that: (1) The proposed method can e?ectively detect the early fault. (2) Both the proposed non-singleton input and the proposed expansion of terms and rules are e?ective in WT fault detection.



Robustness has always been an important factor for evaluating the methods in industrial applications [48,49]. In this section, experiments are conducted to verify the robustness of the proposed method.
The experiment settings are the same as those in Experiment 3. The normal data  of 300  days  are added  in the experiments. All  the data of 300 days are used to test the false alarm rate of the proposed method. In order  to  further  verify  the  robustness  of  the  proposed  method,  noises are  added  to  the  experiment  data.  First,  through  the  measurement,  it can  be  found  that  the  normal  data  themselves  have  a  signal-to-noise ratio (SNR) of 40 dB. Then, di?erent Gaussian white noises are added to the  data  to  test  the  robustness  of  the  method.  As  a  result,  the  experi- ment data have the ADDITIONAL SNR from 45 dB to 25 dB. Fig. 15 shows some examples of the experiment  data (has a SNR of 40 dB itself)  and the data with the added noise.
Table 3 shows the result of the experiments. It can be found that: (1) When the experiment data of 300 days (before adding noise) are used, there is no false alarm. Therefore, it can be concluded that the proposed method has a low false alarm rate in WT fault detection. (2) Moreover, the proposed method has no false alarm when the noise (no less  than  35 dB) is added. The experiment result shows that the proposed method has certain anti-noise capability. (3) In reality, in most cases, the noise which a?ects the data is not as intense as that in the experiments. Thus, it can be concluded that the method could keep robust and maintain a low false alarm rate in practice. (4) The missing detection rate is zero in all the experiments. Therefore, it can be concluded that the proposed method is robust in detecting WT faults.

4.6. Discussions

 Four groups of experiments have been conducted in this section, and the experiment results show that: (1) The real faults in experiments have been successfully detected at an early stage and their severities are told by the proposed method. (2) No false alarms occur using the normal data of 300 days. It can be concluded that the detection results

Fig. 14. The monitored data and detection results of Experiment 3. (a) The averaged prediction errors of GFBT in one day. (b) The averaged prediction errors of GFBT in every 10 min. (c)¨C(f): Detection results. (c) The conventional FIS method. (d) The proposed method. (e) The proposed method without non-singleton input. (f) The proposed method without terms & rules expansion.


Table 2
Experiment results of di?erent types of FIS.

Table 3
The Results of the Robustness Experiments.

Fig. 15. The experiment data (has a SNR of 40 dB itself) and the noise-added data of generator phase 3 temperature (one of the data used for predicting GFBT). (a) 40 dB. (b) 35 dB. (c) 30 dB. (d) 25 dB. are correct and the proposed method can e?ectively detect early WT faults and provide more information on fault severities.
 Similar to other FIS methods, linguistic variables and terms are used in the proposed method. In fault detection, each linguistic term is in- volved in calculating the fuzzy output according to the rules, and the detection result is obtained by combining all the fuzzy outputs. The fuzzy process makes the fault detection more ?exible.
 Moreover, the proposed method has the advantage of detecting multi-class faults. In the FIS of the proposed method, there could be many rules (for multi-class fault) in the rule base. Therefore, if more  rules are added to the rule base, more fault types could be detected. In the experiments of the paper, four di?erent types of WT faults are de- tected by only one FIS of the proposed method.
In conventional methods, if the upper bound is adjusted as a very low value, or the lower bound is adjusted as a very high value, the early fault can also be detected. However, in such case, many ¡°Normal¡± samples can be misjudged as ¡°Abnormal¡±, which could result in higher false alarm rate. In comparison, using the proposed method, early faults can be e?ectively detected without adjusting the upper bound and the lower bound.

5. Conclusion
This paper presents a WT fault detection method based on expanded linguistic terms and rules using non-singleton FIS. Di?erent from the conventional FIS methods, the proposed method is improved to detect early WT faults and to tell fault severities. There are two main con- tributions of this paper. First, this paper proposes an e?ective WT fault detection method based on FIS. For the ?rst time, a fuzzy number transformation method is introduced to convert the PDFs of the pre- diction errors of the WT SCADA data into fuzzy numbers, so that the non-singleton FIS can be applied to detecting WT faults. Second, this paper presents a method of expanding linguistic terms and rules gen- erated from the original ones. With the expansion, FIS could detect WT faults at an early stage and fault severities could be told with the de- fuzzi?ed fault factor. Four groups of experiments are conducted, using the SCADA data collected in a real wind farm. The experiment results show that the proposed method can e?ectively detect early WT faults and provide more information on fault severities.

