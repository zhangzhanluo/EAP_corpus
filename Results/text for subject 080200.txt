




Electrochemomechanics of lithium dendrite growth




ABSTRACT

		    A comprehensive roadmap describing the current density- and size-dependent dendrite growth
		mechanisms is presented. Based on a thermodynamically consistent theory, the combined effects of
		chemical diffusion, electrodeposition, and elastic and plastic deformation kinetics are analyzed to
		rationalize their contributions to experimentally observable morphologies. A critical current density,
		ˆı* = zFiliml(DGOki), in the ts o t o tSand range, results in plastic flow at the tips, dendrite bifurcation,
		and bent and kinked morphologies. Three dendrite growth mechanisms are observed: (1) electro-
		chemical shielding, where there is practically no electrodeposition/electrodissolution; (2) stress-
		induced electrodissolution and electrodeposition on those interfaces directly facing each other,
		generating a self-sustained overpotential that pushes the dendrites towards the counter electrode;
		and (3) local, lateral plastic extrusion in those side branches experiencing non-hydrostatic stresses.
		Six regimes of lithium electrodeposit growth are identified: (i) thermodynamic suppression regime,
		(ii) incubation regime, (iii) base-controlled regime, (iv) tip-controlled regime, (v) mixed regime, and
		(vi) Sand's regime.

INTRODUCTION

    The growth of elongated, branched structures, commonly referred
to as “dendrites,” is an inherent problem in the development of
advanced rechargeable lithium-ion batteries, has been classically
attributed to long-range diffusion limitations in the electrolyte,1
poses serious safety issues,2 and has been one of the major
bottlenecks in their commercialization. The problem of lithium
dendrite growth remains mostly unsolved due to the contribution
of multiple, apparently competing, growth mechanisms. It has
been widely observed in several independent experimental reports
that lithium grows from the tip at high applied currents densities
and from the base at low current densities.3–5 Arakawa6 first
proposed that lithium ‘‘extrudes’’ from the base at low current
densities, while the tip morphology remains unchanged.
Crowther and West7 demonstrated the transition of ‘‘tip-
controlled growth’’ to ‘‘base-controlled growth’’ after B420 s
of galvanostatic electrodeposition of lithium metal on a copper
substrate at a current density of 2 mA cm2. Recently, Cui et al.8
showed the nucleation of hemispherical electrodeposits for a
range of current densities (see J in Fig. 1), and confirmed Ely
and Garcı ´a’s analytical predictions.9 Cui and coworkers10 also
demonstrated the growth of non-dendritic, columnar Li elec-
trodeposits under a hollow carbon layer (see x in Fig. 1).
Bazant, Li, and coworkers4 reported experimental observations
of transitions between distinguishable growth mechanisms,
from a base-controlled, mossy microstructure, to a tip-controlled,
diffusion-limited dendritic microstructure (see & in Fig. 1).
Highly branched and symmetrical dendritic patterns (see } in
Fig. 1) were experimentally reported by Tatsuma and coworkers11
for Li electrodeposited on a Cu substrate at 3 mA cm2 for one
hour from a gel-based electrolyte.
    Three timescales of lithium dendrite growth are identified
in the literature: (i) a characteristic deposition time, t0 = 2gRT/
j0O2DGf2 B 1 s, required to overcome the initial electrochemical 
energy barrier for electrodeposition, as proposed by Ely,9
required for the onset of diffusion limitations in the electrolyte,
as recently reported by Bazant and coworkers,4 and (iii) a
characteristic plasticity time, (for B5 MPa stress), required for
initial stresses to relax through plastic flow of lithium, as
proposed in previous work.12 Symbols used in this paper are
summarized in Table 1, while material parameter values are
summarized in Table 2. Bazant et al.4,5 further defined a critical
electrodeposited charge, the Sand’s capacity, Q = itSand, to
define the regime boundary of dendritic and non-dendritic
growth and hypothesized a stress driven base-controlled or
''root growth'' mechanism. Recent analytical work12 rationalized
for the first time the contribution of lithium plasticity5,13–19 to
base-controlled growth and further identified the existence of a
mixed mode regime, where contributions from both the tip and
the base induce dendrite growth.12
    Bockris and coworkers proposed the first analytical model
for dendritic growth for electrodeposition of silver20 and zinc.21
They considered ion diffusion limitations at the dendrite tip,
calculated a critical overpotential, and predicted the growth
velocity for low and high exchange current densities. Diggie and
Bockris further extended the model to show transitions
between spongy and dendritic zinc electrodeposits under very
high overpotentials.21 Aogaki and Makino22 first studied
instabilities at the electrochemical interface, showed the existence 
of an ''induction time'' before electrodeposits form, and a
''transition time'' (or Sand’s time) when the ions at the interface
deplete to zero concentration. Chazalviel, Fleury, and Rosso
established dendrite growth models for dilute electrolytes under
large electric fields at the dendrite tip.1,23 Dendritic patterns were
attributed to deviations from the electroneutrality condition at the
electrochemical interface. Electrodeposition models specifically
for rechargeable batteries were put forward by Monroe and
Newman,24 who showed that the kinetics of electrodeposit growth
was governed by the Butler–Volmer equation for sub-limiting
current densities, a phenomenon that has been repeatedly
observed experimentally.4,8 Ferrese and Newman further extended
the model to account for elastic25 and elasto-plastic deformation15
in order to predict: (a) the effect of elastic deformation on reaction
kinetics, and (b) the thickness of lithium that was redistributed to
plastic flow due to a stiff separator. Recent analytical descriptions
identified the thermodynamic and kinetic regimes of lithium
electrodeposition and dissolution for non-stressed9 and stressed
electrodeposits under elastic and plastic deformation.12
    Thermodynamically consistent variational principles to
describe transport and phase transformation kinetics in 
electrochemical systems were pioneered by Bishop and Garcı ´a.26,27
Guyer and coworkers28 proposed a variational formulation for a
multicomponent electrochemical system. A phase field model of
electrodeposition by Okajima, Shibuta, and Suzuki29 integrated
Butler–Volmer kinetics and simulated the growth of an electr-
odeposit for a very short time (B1 s). The morphology of the
electrodeposit was influenced through a chosen anisotropic 
interfacial energy. Recent electrodeposition phase field models by
Chen and coworkers,30 Ely,31 and most recently by Cogswell32
demonstrated the interfacial velocity to be a combination of total
free energy minimization kinetics and Butler–Volmer electrode-
position kinetics. Recently, Jokisaari and coworkers33 proposed
the only phase field description that includes elastic energy
contributions to demonstrate the shape evolution of an elastically
constrained precipitate with anisotropic stiffness coefficients.
    In spite of the current progress, a fundamental understanding 
that explains the different experimentally reported lithium
dendrite morphologies is missing. In this context, a thermody-
namically consistent variational framework is developed
herein and applied to rationalize the effects of concurrently
occurring electrochemistry and large deformation plasticity on
the lithium electrodeposition thermodynamics and kinetics.
Lithium morphologies predicted herein are in excellent agreement
with experimental results, demonstrate the underlying driving
forces, and are summarized in dendrite growth maps as a function
of the relevant dimensionless quantities


METHOD

    Consider a multicomponent electrochemical system comprised
of N charged species, where zi is the valence of each species and
Oi is its molar volume. The concentration set is described
through the notation, {c} = {c1,c2,c3,. . .,cN}, as proposed by
Steinbach34 and later by Cogswell.35 x is a non-conserved order
parameter equal to one in the electrodeposit phase and equal to
zero in the liquid electrolyte phase and defines the location in
the anode where lithium has been reduced into its solid form.
r is the local charge density and f is the local electrostatic
potential. s$ is the local stress tensor and $ee is the local elastic
strain tensor. The total Gibbs free energy of the system is
defined as:
    Eqn (1) is in agreement with earlier descriptions by Guyer,28
Okajima,29 Chen,36 Garcı ´a,27,31 and Cogswell.32 Here, g({c},x,T) =
gs({c},T)p(x) + gl({c},T)(1  p(x)) is the volumetric chemical free
energy density of mixing, where gs({c},T) is the volumetric
chemical free energy density of mixing of the solid electrodeposit 
and gl({c},T) is that of the liquid electrolyte. The spatial
contributions in each phase are specified through the interpolating
 function, p(x) = x3(10  15x + 6x2), where p(0) = 0
and p(1) = 1. The free energy density of mixing is described
through a Redlich–Kister free energy model, 
    Redlich–Kister coefficient37 and embodies the enthalpic
contributions to the free energy of the electrochemically active
material. h(x) = x2(1  x)2 and defines minima at x = 0 and
x = 1.38,39 Wx is the height of the energy barrier. The term, zx2
2 ,modulates the energy penalty for the creation of an ele-
ctrodeposit-electrolyte interface. Similarly, the term,
zij22 , penalizesthe formation of sharp interfaces between
 two chemical species. The last term in the right side 
of eqn (1) corresponds to the energy penalty to satisfy 
r ¼ PNi¼1ziFci=Oi. l(-x) is a Lagrange multiplier
Away from equilibrium, spatial inhomogeneities of the variational
 derivatives of eqn (1) are the driving forces for microstructure 
evolution, i.e., the kinetic equations of motion:
is the diffusivity of the ith species, and Mr/2 = k is the total
electrical conductivity.27 Further,  The term Oi~Girx 
accounts for mass deposition or dissolution at
the interface. -vprx and -vp,irci account for convective plastic
flow of the solid with a velocity, ~vp ¼ PNp,i is the vectorial
plastic velocity of the ith chemical species. In the limit of large
stresses and concentration gradients, the microstructural evolution
of the system is described by the corresponding large deformation
approximation of eqn (2), as shown in the Appendix.
is the diffusivity of the ith species, and Mr/2 = k is the total
electrical conductivity.27 Further, Mr ¼ P The term Oi~Gi
accounts for mass deposition or dissolution at
the interface. -v prx and -vp,irci account for convective plastic
flow of the solid with a velocity, ~vp ¼ Pp,i is the vectorialplastic 
velocity of the ith chemical species. In the limit of large
stresses and concentration gradients, the microstructural evolution
of the system is described by the corresponding large deformation
approximation of eqn (2), as shown in the Appendix.
    The elastic energy density, 1
$e, suppresses the interfacial deposition rate, as proposed
 by Newman.48 Thus, for stressed dendrites,
higher overpotentials are required for electrodeposition.4,5,48
The molar electrodeposition rate is P where
nˆ = rx/|rx|, is the outward normal to the interface with
a curvature, K = rnˆ, as previously reported.12,49 The first
term inside the parenthesis, ziFZ, accounts for the classical
overpotential induced electrodeposition. The second term
1 2 Ois $  $ee, accounts for the shift in critical overpotential due
to elastic energy density localization, as proposed in earlier
work.12 The third term, gOiK, accounts for the Laplace pressure
of a curved electrodeposit. Laplace pressure and elastic
energy density counter the overpotential-induced electro-
deposition.12,25,48,50 In the absence of stresses and for a planar
electrodeposit, eqn (4) reduces to the classic Butler–Volmer
equation.51 In the limit, 1
    The chemical species in a lithium electrodeposition system are
lithium ions, electrons, salts, and solvents in the electrolyte.
Electronic concentration remains uniform in the solid and fully
absent in the electrolyte phase and hence is trivially solved.
    The specific large deformation equations (eqn (5)) for lithium
electrodeposition are shown in the Appendix.
Eqn (1)–(4) were implemented in parallel by applying the
finite element method using open source libraries in Python
and C++, in the large deformation limit (see Appendix).
Electrodeposition calculations were performed by coupling
two complementary, spatially filling meshes. The first mesh
corresponds to the solid electrodeposit, and the second mesh
corresponds to the liquid electrolyte. The interfacial electro-
deposition flux spatially couples both phases and determines
lithium depletion in the electrolyte and an equimolar lithium
deposition on the solid electrodeposit surface.
    At t = 0, a lithium nucleus of radius of 1.33 mm was placed at
the center of the bottom anode surface. The initial radius was
chosen such that it was larger than the critical thermodynamic
radius (see Table 2). Galvanostatic lithium electrodeposition
simulations from a 1 M LiPF6 in 1 : 1 EC : DMC electrolyte on an
inert substrate were performed. Values of material properties
used are summarized in Table 2. Simulations were carried out
on a 2.6 GHz, 28-core, Ubuntu 16.04 workstation with 128 GB
of RAM. The numerical tolerance for convergence and the
tolerance for the Newton solver were both set to 106. One
hour of electrodeposition simulation took B3 days of wall time


RESULT

    Fig. 2 shows the growth of an isolated lithium dendrite at a
fixed current density of 1 mA cm2. In the initial stages, the
electric field is higher in the liquid electrolyte and nearly zero
inside the highly conducting metallic lithium electrodeposit.
The electric field localizes at the electrodeposit tip (see Fig. 2(d–f))
and induces an overpotential-induced tip-controlled growth,
which in turn induces stress accumulation, as a result of the local
molar volume changes as the solid lithium locally displaces the
SEI and the liquid electrolyte in the vicinity of the dendrite tip.
Electrodeposition at the tip increases the local elastic energy
density and suppresses subsequent electrodeposition in the very
center of the stressed region. The local stress inhomogeneities at
the tip cause the primary dendrite to bifurcate into two branches
(see Fig. 2(b)). This process repeats itself as the newly formed
dendrite tips are directly exposed to the applied current density,
thus forming secondary and tertiary branches (see Fig. 2(c)).
    As these branches develop, the electrolyte between the
central branch is electrically shielded (see Fig. 2(f)). This results
the major and minor axes, and the orientation is specified by
the principal directions. Results demonstrate that during
the initial stages of dendrite growth the electrodeposit is
subjected to a laterally compressive state of stress that pushes
the lithium nucleus to grow vertically, as proposed by Wang
and coworkers,55 Li and Bazant,4,5 Crowther and West,7
Tarascon and coworkers,56 Yamaki and Arakawa,6 and most
recently in analytical work12 (see Fig. 3(a)). In addition, high
compressive stresses in the initial stages induce electrodeposition 
suppression, in agreement with Newman48 and electrodeposition
 experiments by Wilkinson and Wainwright.


DISCUSSION

    As the electrodeposit grows and branches develop, stresses
continuously relax at the base but remain localized at the tip.
Growth at this stage is a result of both base-controlled growth at
the bottom and tip-controlled growth at the top and results in
an overall mixed growth mode, in agreement with previous
work.12 Further, highly localized elastic energy density at
the tip suppresses electrodeposition at the tip, but favor
electrodeposition and electrodissolution on the side branches
(compare Fig. 2(h) and 3(b)). Repeated bifurcations cause the
electrodeposit to develop secondary and tertiary branches.
A direct comparison between Fig. 3(c) and 2(f) and (i) shows
three unreported growth mechanisms: (1) closer to the base,
where stresses are relaxed and the surfaces of the electrodeposit
are electrochemically shielded there is practically no electrode-
position or electrodissolution; (2) along the length of the
lower side branches stresses induce electrodissolution and
electrodeposition on those interfaces directly facing each other,
generating a self-sustained overpotential that moves the
branches up; and (3) in those side branches experiencing
non-hydrostatic stresses, local, lateral extrusion is observed.
    In contrast, in the absence of any stress effects, the electrodeposits
 will grow only from the tip, electrochemically shielding
the back, as a result of electrodeposition at the electrolyte–lithium
interface, following classical electrodeposition theories57 and
as recently demonstrated for lithium.31,49 Further, dendrite
branching will solely be a result of diffusion limitations in
the electrolyte and will develop at times t 4 tSand, as reported in
the classical dendrite literature.1,58
    Recent analytical work12 shows that for the applied boundary
conditions, the incubation time is t0 B 1 s and the stress
relaxation time is ts B 300 s. Also, for the stress 
tensor anisotropy, a = s2/s1 B 2, plastic 
flow is maximal as predicted herein
(see Fig. 4(a)). Further, results demonstrate that base-controlled
growth dominates at the initial stages of electrodeposition,
independent of the applied current density. Thus, while electrod-
eposition is taking place, growth from both the tip and the base
concurrently occurs, enabling the development of additional,
simultaneously occurring growth mechanisms.
    The corresponding shear-induced plastic flow is summarized in Fig. 4.
In the initial stages, mass flows towards the top
to relax the stresses, much like toothpaste being squeezed out
of its tube (see Fig. 4(a)).4,6,12 The streamlines highlight the
direction of local plastic flow. After stresses relax through basec-
ontrolled growth, tip electrodeposition further induces stress
accumulation at the tip of the electrodeposit. For current
densities B1 mA cm2, the electrodeposition rate is faster than
the plastic flow-induced stress relaxation rate (see Fig. 4(b)).
The streamlines show that mass internally flows from high to
low stressed regions and induces dendrite bending. Further,
such directional flow stretches the dendrite branches and
makes them susceptible to localized electrodeposition.
     Fig. 5 shows the lithium-ion concentration distribution for a
high current density, iapp = 10 mA cm2. Fig. 5(a) shows the
concentration at t = 300 s. For t 4 tSand = 292 s, the concentration
drops to zero in the vicinity of the dendrite tip, in
those locations where the local electric field is concentrated,
suppressing the local electrodeposition rate (see Fig. 6). Also,
results demonstrate that the width of the depletion layer remains
unchanged as the dendrite grows, as shown experimentally by
Brissot.3 Crevices between dendrite branches are not lithium
depleted due to electrochemical shielding. Stress-driven electrode-
position/electrodissolution in the side branches that induces
lithium depletion in the intervening electrolyte becomes apparent in 
those branches whose diffusion distance to the counterelectrode is the largest.
    Simulation results support the conclusion drawn by
Brissot,59 and most recently by Bazant and Li5 that dendritic
growth under current densities of B10 mA cm2 is a function
of complex, local microstructural heterogeneities rather than
due to long-range diffusion limitations. Analytically, the limiting 
current density is, ilim = zFDlC0/l = 321 mA cm2, and the
corresponding Sand’s time, tSand ¼ pDl zC0
app = 10 mA cm2. This limiting current value is consistent
with the experimentally reported measurements by Brissot
(164 mA cm2 for l = 29 mm and 241 mA cm2 for l = 20 mm)59
and Cui (152 mA cm2, 198 mA cm2).61 For thicker cells, with
l B 300 mm, experimentally reported limiting current densities are
smaller, in the 50–150 mA cm2 range, as reported in several
experimental and theoretical studies.4,62,63 Thus, the macroscopic
applied current density always lies below the limiting current
density but results in dendritic growth, as widely reported in
literature.7,59,63,64
    Fig. 6 shows the heterogeneous electric field distribution
ahead of the dendrite tip, for iapp = 10 mA cm2. Results
demonstrate that the local electric field in front of the dendrite
tip is twice that of the macroscopic electric field for an initial
hemispherical electrodeposit (see Fig. 6(a)). As the electrodeposit 
grows and bifurcates into thinner whiskers, the electric
field at the tip reaches B three times that of the macroscopic
value (see Fig. 6(c and d)), in agreement with numerical
calculations from Srinivasan and coworkers.65 This suggests
that the local current density,
reaches B30 mA cm2 B
0.1ilim in front of the dendrite tips.
    Concurrently occurring local electrodeposition and electrodissolution 
(shown in Fig. 7(c and d)) result in lateral asymmetry of the 
dendrite arms and make the dendrite prone
to bend or develop kinks and thereby change the growth
direction.53 Similarly, once the dendrite branches fully develop,
lithium slowly dissolves from the outer regions of the dendrite
base (see Fig. 7(d)) and increases the local ionic concentration
in the electrolyte (see Fig. 5(d)). Overall, these results demonstrate 
that for very high current densities (iapp = 10 mA cm2),
diffusion limitations influence the local electrodeposition rates
and change the electrodeposit microstructure heterogeneously
as a result of local electrochemical gradients. Further, branching of
the electrodeposit is a result of the competition of concentration
gradients and stresses. High electrodeposition rates induce
compressive stresses at the tip,66 suppress electrodeposition, and
enhance electrodeposition on the side branches.
    Fig. 8 shows the effect of applied current density on the
electrodeposited microstructure, for fixed total amount of
deposited charge. For a low current density (0.1 mA cm2),
the electrodeposit growth is driven by tip-controlled electrodep-
osition, but shaped through plastic flow (see Fig. 8), and
results in planar lithium growth.5,10 For higher current densities 
(B1 mA cm2), electrodeposition rates dominate over the
stress relaxation kinetics and favor the growth of dendrite
arms, due to elastic energy localization.4,6,55 The morphologies
that are produced are a result of the joint spatial distribution of
electrodeposition-controlled regions coupled to the plastic
flow induced dendrite growth, in agreement with analytical
results.12 For very high current densities (B10 mA cm2), stress
relaxation at the tip is slow due to fast mass accretion and thus
remains confined at the lower branches. A highly branched
or dendritic morphology develops as a result of both stress
localization at the tips and concentration gradients ahead of
the tips.4,11 This heterogeneous electrodeposition forms small
surficial perturbations that are preferentially electrodeposited
and results into thin and long, highly branched, lithium
structures


CONCLUSION

    Fig. 9 summarizes the different regimes of lithium electrode-
position as a function of normalized electrodeposit size,
ˆr = Dgr/2g B DgV1/3/2g and normalized applied current
density. The blue curve denotes
the critical thermodynamic size of the electrodeposit. Lithium
dendrites above the blue curve are stable, while those below the
blue curve are unfavorable and will dissolve back into the
electrolyte. Thus, dendritic growth is completely suppressed
by charging the cell with current densities in the thermodynamic 
suppression regime, as proposed earlier,9 and experimentally 
verified by Cui.8 The black line embodies the kinetic
limit for electrodeposits. Between the blue and the black line
is the incubation regime, where stable lithium nuclei grow
through local coarsening kinetics.8,9 To the left of the red line,
i.e., for low current densities and above the kinetic limit is the
base-controlled growth regime, where lithium plasticity at the
base dominates microstructural evolution. A critical current
density, ˆı* = (zFiliml)/(DgOki) = 2.5  105, defined by the orange
line, (for the chosen material properties, see Table 2), denotes
the onset of the diffusion-controlled or Sand’s regime, where
highly branched dendritic structures develop due to diffusion
limitations in the electrolyte, in agreement with Bazant et al.4
At moderate current densities, in the 103 oˆı o 2.5  105 range,
tip-controlled growth occurs through interfacial electrodep-
osition kinetics. Between the green and the dashed gray
lines, the growth rate is suppressed due to elastic energy
localization, and reaches a minimum at the elastic limit of
lithium, denoted by the dashed gray line.12 For long deposition
times, highly localized stresses result in plastic flow at the tips
that distort and modify the shape of the dendrites, as noted in
recent experiments by Steiger and coworkers.53 Tip branching
for ˆı o 2.5  105 is driven by elastic energy density localization,
while for ˆı 4 2.5  105 is driven by concentration gradients, as
observed in classical dendritic growth. Typical current densities
lie in the 0.1 o ˆı o 100 range, with contributions from
electrodeposition and elastic-plastic deformation, referred
herein as the mixed regime.12 Base-controlled growth induces
initial columnar growth in the mixed regime, while at later
stages, tip branching is induced by elastic energy density
localization and enhanced by concentration gradients.
    A direct comparison with experimental microstructures (see
Fig. 1 and corresponding markers in Fig. 9) show excellent
agreement with the numerical simulations: low current densities 
result in columnar structures and high current densities
result in branched dendritic structures. Further, Fig. 9 shows
that most lithium electrodeposition experiments lie in a narrow
strip in the mixed regime. In order to completely suppress
dendritic growth, the electrodeposits should be limited to
smaller sizes, as depicted by the thermodynamic suppression
regime. Thus, dendrite-free lithium anodes are possible by
designing time-dependent charging patterns that make incursi-
ons of length t o min(ts,tSand) into the high current density
regimes, without enabling the electrodeposit to grow beyond
the kinetic limit (black line) and avoid either base-controlled,
tip-controlled, or mixed mode of growth. The amplitude of the
current density incursions will be a function of the state of
charge, for larger dendrites will require a shorter characteristic
time to develop a morphological instability.
    Overall, the analysis performed herein demonstrates that
experiments can be readily tailored towards establishing correlations 
between the lithium dendrite morphology, such as
dendrite growth direction (angle), dendrite arm spacing, and
developing in situ or residual stresses as well as externally
imposed electrical and chemical stimuli by combining Fourier
analyses, classical characterization techniques, and emergent
machine learning algorithms. The combination defines a stepping 
stone to improve battery life and reliability. In addition,
dendrite suppression analyses through applied external pressure, 
as reported experimentally44,67,68 can be readily modeled
by combining the framework developed herein and previous
analytical work,12 and considering the shift of the free energy
of formation per unit volume, Dg ! Dg  P2=2E ¼ Dh  TDs
and through the Clausius–Clapeyron equation, dP=dT ¼
Ds=DV ¼ Dh=ðTDVÞ, to account for the corresponding
temperature-dependent hydrostatic stress-induced phase transformation effects
In summary, a comprehensive theory has been developed
to describe the microstructural evolution of lithium electrodeposits. 
This theory provides a roadmap to explore and
identify the microstructural mechanisms controlling dendrite
growth. Specifically, stresses on an electrodeposit arise from
the local molar volume changes, as induced by the SEI layer,
from adjoining electrodeposits, from the separator layer
through the relevant mechanical properties, or those induced
by the cell casing, and externally imposing appropriate
boundary conditions that reflect the physical situation at hand.
Results suggest that lithium dendrite growth originates from a
set of coupled, electrochemomechanical driving forces that
result in six regimes of behavior: (i) thermodynamic suppression 
regime, (ii) incubation regime, (iii) base-controlled growth
regime, (iv) tip-controlled growth regime, (v) mixed regime,
and (vi) Sand’s regime, each referring to a distinct growth (or
suppression) mechanism




Lithium dendrite growth mechanisms in liquid electrolytes




ABSTRACT

		    A unified theoretical framework of dendrite growth kinetics has been developed to account for the coupled
		effects of electrodeposition, surface tension, and elastic and plastic deformation. The contribution of each
		driving force is assessed to identify five regimes of lithium growth: thermodynamic suppression regime, incubation
		regime, tip-controlled growth regime, base-controlled growth regime, and mixed growth regime, in agreement with the
		experimental scientific literature. Tip-controlled growth shows a linear time-dependence, while base-controlled
		growth shows an exponential time-dependence. A minimum in the growth rate, as a result of the reaction energy
		barrier increase imposed on the interface by the local elastic energy, is identified in the mixed growth regime.
		Further, two characteristic times are identified: the characteristic deposition time, t∘, which defines the critical
		time scale necessary to overcome the electrochemical energy barrier for nucleation, and the characteristic
		plasticity time, tσ, which corresponds to the time scale necessary for plastic flow to occur, given a local shear
		stress. Examples of experimentally reported transitions between tip-controlled growth and base-controlled growth
		are readily captured through the proposed framework. While one or more mechanisms may dominate the growth
		of the electrodeposit, the proposed formulation defines a road map to design dendrite-free, lithium-based anodes
		as a stepping stone to identify alternate dendrite-free chemistries


INTRODUCTION

    The growth of metallic lithium deposits, broadly referred to as
"dendrites," is a major roadblock in the commercialization of high
power and energy lithium-ion batteries [1,2], as well as the emergence
of lithium anode-based and lithium-air batteries [3,4]. For currently
used commercial graphitic anodes in lithium-ion batteries, dendrites
pose a serious safety concern and have resulted in catastrophic failures,
particularly at high current densities [5]. In all cases, the safety and
performance of existing and emergent rechargeable batteries will determine
their success tied to the survival of nascent markets of electricity-
based technologies, such as of electric vehicles [6], which would
greatly benefit from lithium metal anodes, and the possibility to reduce
the charging time from the present 4 and 36 h to 10 min [7,8]. While
there has been significant progress in the fast-charging infrastructure,
the dendrite growth problem remains the primary bottleneck to implement 
high energy, fast charging batteries. In this context, thin film
(3 µm) lithium anode batteries have been developed; however, to make
them dendrite-free, alternate approaches are required to bring this
technology to the main stream [9–11].
    Fundamentally, the problem of dendrite growth remains largely
unsolved due to the fact that, unlike copper or zinc electrodeposition,
lithium grows at current densities one hundred times lower than the
limiting current density, and shifts between, what appears to be, multiple 
growth mechanisms reported over a wide variety of time and
length scales [12]. The inherent electrochemical instability of lithium
metal in organic liquid electrolytes, combined with the underlying
multiphysical and microstructural complexity of the local environment
that each dendrite is subjected to, contributes a great deal of complexities
that demand a basic science identification of the individual
mechanisms that control its stability and kinetics. These mechanisms
result in different lithium morphologies that determine whether the
sandwiched separator will be able to arrest the dendrite, or if the
dendrite will dissolve back into the electrolyte.
    Fig. 1 shows a catalog of electrodeposition experiments where the
dendrite was reported to grow either as tip-controlled or base-controlled.
For example, Brissot and Chazalviel reported two regimes of lithium
growth, (see in Fig. 1): needle-like at low current densities
(0.05 mA/cm2) after 38 h [13], and “arborescent” for high current
densities (0.7 mA/cm2) after 2300 s [14]. Dollé et al. (see and in
Fig. 1) reported that a single charge cycle at 0.22 mA/cm2 produced
mossy deposits, while a single charge cycle at 0.5 mA/cm2 resulted in
elongated dendrite-like microstructures [15]. Orsini et al. demonstrated
the growth of lithium microstructures on lithium, copper, and graphite
substrates [16]. It was reported that on both lithium and copper, mossy
growth was observed for low C-rates (C/5, 0.45 mA/cm2), while
dendritic growth was observed at high C-rates (1C, 2.2 mA/cm2).
However, for graphite anodes, high C-rates (2C, 0.1 mA/cm2) only
roughened the anode surface. Arakawa et al. were the first to propose
that lithium “extrudes” from the base [17]. Experimentally, it was
observed that lithium pushes out towards the counter electrode from the
base, where the morphology of the tip remains largely unchanged. The
authors predicted different morphologies as a function of the Laplace
pressure on the electrodeposit and its surface tension. Hollenkamp et al.
used Raman spectroscopy to distinguish growth at the tip and from the
base and demonstrated that the tip morphology remains unchanged
during base-controlled growth [18]. Seminal experiments by Crowther
and West (see ▵ in Fig. 1) showed the transition from tip-controlled to
base-controlled in a single galvanostatic recharge cycle [19], where the
growth rate was shown to be proportional to the Butler-Volmer current
density, consistent with Monroe and Newman [20]. Recent experiments
by Cui et al. [21] (see ◊ in Fig. 1) confirmed the observations as well as
predictions by Ely [22], and most recently by Bazant, Li et al. [12,23]
(see and □ in Fig. 1). Transitions between experimentally reported
growth mechanisms from base-controlled to tip-controlled [12] or from
tip-controlled to base-controlled [19], clearly outline that the mechanisms
for growth are a function of the underlying time-dependent conditions
that dendrites are subjected to. Also, additional driving forces are at
play during dendrite growth due to the lightning rod-like shielding
effects of a highly conductive elongated metallic electrodeposit [24,27].
    The first dendrite growth model was developed by Barton and
Bockris, who proposed that ion transport in front of the dendrite tip is
limited by diffusion. The authors demonstrated the existence of a critical
overpotential and described the growth of silver dendrites in liquid
electrolytes. Diggle and Bockris further extended the model for higher
overpotentials and showed transitions between spongy and dendritic
morphologies as the overpotential increased [28]. Aogaki and Makino
followed a Mullins-Sekerka type description to show dendrite growth
due to the electrochemical instability at the electrode interface for
diffusion-limited electrodeposition [29,30]. Chazalviel, Fleury, and
Rosso demonstrated that dendrites formed due to deviations from
electroneutrality at the electrochemical interface [31–33]. Dendrite
propagation velocity was shown to be equal to the anion velocity and
proportional to the electric field in the electroneutral region of the
electrolyte. Dendritic growth was attributed to depletion of ions near
the anode interface at current densities higher than the limiting current
density. At lower and battery-relevant current densities, Monroe and
Newman predicted the dendrite tip velocity and extended the ButlerVolmer
relation to incorporate elastic deformation to assess the separator 
stiffness necessary to block dendrite penetration [20,34].
Newman also demonstrated the effect of elastic energy on the electrochemical 
interface to hinder dendrite growth. Ferrese and Newman
further incorporated the effects of plastic deformation of lithium and
examined lithium movement on the anode during charge-discharge
cycles [35–37]. Ely and García rationalized the contributions from
these apparently disconnected mechanisms and unified its description
into a generalized electrochemical analytical framework that identified
three regimes of dendrite stability and growth [22]. Srinivasan et al.
recently resolved tensile and compressive stresses and assessed the
propensity of dendrite penetration under elastic deformation [38].
    In spite of the great deal of experimental evidence highlighting the
unequivocal existence of tip- and base-controlled growth of lithium, there
is no fundamental framework that explains clearly the observations. In
this context, a generalized electrochemical and chemomechanical
theory of dendrite growth kinetics is presented herein that quantitatively identifies 
the different regimes of lithium growth. Predicted regimes and time-
dependent growth behavior are in excellent agreement with experiments 
that have reported tip-controlled and base-controlled growth, and the 
conditions that lead an isolated dendrite to switch from one mechanism 
to the other one are explained


METHOD

    Define the total Gibbs free energy of transformation of a hemispherical 
electrodeposit that forms on an electrically charged mechanically stressed 
substrate, with a contact angle of 90° as shown in
the bottom right inset in Fig. 1. Here, the chemical, electrical, and
mechanical contributions to the bulk free energy of transformation are
embodied by the expression:
    Used symbols are summarized in Table 1. The state of stress of such an
electrodeposit is described by a symmetric rank two tensor, which for a
hemispherically symmetric cap is simplified in the diagonalized reference system as,
a defines the stress anisotropy and together with the principal (eigen-
stress) direction(s) uniquely quantifies the mechanical state of the
dendrite. In general, the electrodeposit is subjected to multiple sources
of stress: those induced by the SEI, whose stiffness induces an inhomogene-
ous compressive effect [39], the adjoining electrodeposits,
whose differential molar volume expansion contributions induce shear
stresses [40], the separator, which pushes down the dendrite as a means
to suppress its growth into the counter electrode [37], and the metallic
casing of the battery, which hydrostatically compresses the battery
stack [41,42].
    For σ < 0, and a < 0, Eq. (2) shows that the electrodeposit is pushed
laterally inwards while extruded upwards, and thus promotes the
growth of the electrodeposit. For lithium, σ∘ = ±732 MPa, in agreement with [45].
The electrodeposition dynamics for a stressed dendrite are formulated through
 a modified Butler-Volmer relation, in agreement with
Newman et al. [34,37,46]: upon substituting Eq. (2). Here, j∘ = i z ∘/ F is the molar 
exchange current density and α is the transfer coefficient.
    In the absence of stresses, Eq. (7) reduces to the classical ButlerVolmer 
relation for a planar deposit, Γ j α z η RT = − ∘ (exp[(1 ) / ] F
− − exp[ / ] αz η RT F ), in agreement with the existing scientific literature
[20,22].For a symmetric electrodeposition reaction, i.e., α = 1/2, Eq. (7)
reduces to: The elastic energy term contributes an additional energy barrier for
electrodeposition, in agreement with [46,47]. Thus, the state of stress,
dictated by the stress anisotropy, a, will determine the extent of suppression 
of the Butler-Volmer electrodeposition kinetics. In addition,
the elastic energy induces a shift in the critical overpotential required
for electrodeposition, i.e., for stressed dendrites, higher overpotentials
are required to start electrodeposition, and in the absence of curvature
or overpotential contributions, the stress will induce dendrite electrodissolution.
Plastic deformation of lithium, a well known phenomenon
[23,35,36,40,41,48–50], is described herein by a power law creep
model [51,52], as denoted by ε Aσ ˙s = sn. The von Mises stress is
σs = ⎡ ⎣1 2 (( ) ( ) ( ) ) ( 1) σ σ σ aσ aσ σ a σ − + − + − 2 2 2 ⎤ ⎦1/2 = − . a = 
1 indicates a pure hydrostatic state, and will suppress plastic deformation.
A A Dμ b kT 1−n / , where A∘ is Dorn's constant, D D Q RT = − ∘ exp( / ) is
the lithium diffusion coefficient, and b is the Burgers vector [52]. Dorn's
constant is a phenomenological parameter, and has been reported to
scale with the power law creep exponent, n [52]. The diffusion coefficient, 
D, accounts for the microstructurally averaged contributions from
point defects, dislocations, and grain boundaries to the stress induced
transport of lithium [53]. Thus, the plastic deformation rate is given by
dr dt rAσ / = sn, and corresponds to the non-elastic vertical elongation of
the dendrite as a result of an arbitrarily imposed state of stress.
    For an isolated hemispherical nucleus, the growth rate is denoted as
dr dt ΓΩ / = , thus the combined vertical growth rate due to electrode-
position and plastic deformation is defined as:
The function, sign(x), is defined as sign( ) x = 1 for x ≥ 0, and
sign( ) x = −1 for x < 0. Used material parameters are listed in Table 2.
    For small deviations from equilibrium, i.e.,
Dividing both sides by j∘Ω ΔG R 2 f/ T, and using
sign( ) sign( )sign( ab a b = ), the resultant dimensionless growth rate is:
̂dr dt ^ = − (1 ) Π σ η σ 1 2 2 ̂ ̂ ⎡ ⎣ ̂− − r1̂⎤ ⎦ + − sign( ) aσ̂ ̂ ΠΠ122 rσ̂ n (11)
=t t t ^ / ∘, where t∘ = −2 / γRT j Ω ΔG ∘ 2 f2, denotes the characteristic deposition
time, in agreement with Ely [22]. For dendrites whose size are in the
vicinity of the critical size for thermodynamic stability, coarsening kinetics
 govern the growth of the electrodeposit. High interfacial energies
and low Gibbs free energies of transformation will delay the onset of
dendritic growth. For lithium, t∘ = 1.514 s, in agreement with previous
work [22]. The two resultant dimensionless numbers are defined as
In this context, the tip-controlled growth regime corresponds to the
combination of equilibrium and kinetic parameters where the contribution 
from the growth rate from the applied overpotential, concentrated 
at the dendrite tip, is at least ten times greater than the
growth rate due to plastic flow. Similarly, base-controlled growth regime
is identified herein whenever the time-dependent morphological
changes induced by the dendrite's plastic deformation is at least ten
times higher than contributions from electrodeposition. The mixed
growth regime is thus defined when tip-controlled growth and base-
controlled growth contributions are both active. The growth rate is defined
to reach a minimum as a function of the applied stress when


RESULT

    Fig. 2 summarizes five regimes of growth behavior as a function of
electrodeposit size and local overpotential, for a fixed stress tensor. The
blue curve denotes the limit of thermodynamic stability, as defined in
Eq. (5). Lithium dendrites that are larger than the critical thermodynamic 
size are energetically favorable (above the blue curve). Similarly, 
lithium embryos in the thermodynamic suppression regime (below
the blue curve) are energetically unfavorable, and will dissolve back
into the electrolyte. Thus, dendritic growth will safely be avoided by
choosing overpotentials and sizes that are in the thermodynamic suppression
regime, as reported in earlier work [22], and experimentally
verified by Cui [21]. The incubation regime, between the blue and the
black curves, is where thermodynamically stable electrodeposits grow
through coarsening, following incubation kinetics [21,22]. The black
line embodies the kinetic limit, where thermally stable isolated nuclei
will grow. Three additional growth regimes are identified: the tip-
controlled growth regime, to the right of the green line, where electrode-
posits subjected to large overpotentials grow into whisker-like
structures; the base-controlled growth regime, to the left of the red line,
where hemispherical electrodeposits subjected to small overpotentials
deform into elongated structures, in agreement with the numerical
work by Arakawa et al. [56]; and the mixed growth regime, between the
tip-controlled and the base-controlled growth regimes, where lithium 
kinetics is governed by the contributions of plastic deformation and
surface energy, dendrite growth is possible, both through the base and
the tip, in agreement with Steiger et al. [25,26].
    Results show no appreciable base-controlled growth for
− σ < 0.1 MPa, thus this stress value corresponds to the worst case
scenario to observe tip-controlled growth. Similarly, − σ ∼ 5 MPa was
the stress value that delivered the best match between the experimental
results and the performed calculations. In addition, if the stress on the
electrodeposit increases, e.g., from − σ = 0.1 to 5 MPa (see Figs. 2(a)
and (b)), the extent of the incubation regime shrinks, while the extent of
the base-controlled growth regime expands. For dendrites in this regime,
the stress that accumulates at the base induces an outwardly directed
push of the dendrite towards the counter electrode. A direct comparison
against electrodeposit size as a function of overpotential experimental
data [14–16,19,21,23] (see example micrographs in Fig. 1), shows that
for a stress, − σ = 0.1 MPa, lower than the yield stress, dendrites grow
according to the tip-controlled growth regime (see Fig. 2(a)). Electrodep-
osits at the onset of tip-controlled growth regime lie just above the
black line, in agreement with experiments performed by Cui et al. [21].
    Dendritic or whisker like growth, in the tip-controlled growth regime, at
high current densities are in agreement with experiments by Orsini
et al. (2.2 mA/cm2) [16], Brissot et al. (1.3 mA/cm2) [14], and Dollé
et al. (0.5 mA/cm2) [15]. The present analysis further asserts that early
stages of lithium growth are in agreement with experimental observations
of tip-controlled growth by Crowther and West [19]. Potentiostatic
lithium electrodeposition experiments under high applied voltages
(−4.5 V) as reported by Li et al. [23] are also captured in the tip-controlled 
growth regime. For a stress above the yield point, e.g.,
− σ = 5 MPa, a comparison against experiments [14,15,19,21,23],
shows that, in general, dendrite growth occurs as a combination of
electrodeposition and plasticity-induced growth, i.e., in the mixed
growth regime. Only a subset of experiments by Orsini et al. [16] at a low
current density (0.45 mA/cm2) are found to lie in the predicted baseco-
ntrolled growth regime. Calculations further show that for stresses
much larger than the yield stress, the black line shifts downward. For
example, at − σ = 14 MPa, the kinetic (black) and thermodynamic
(blue) line overlap, thus completely altering the coarsening dynamics.
Thus, the dendrite incubation mechanism changes from electrochemically-
controlled to plasticity-controlled. For − σ > 14 MPa, the
kinetic line shifts below the thermodynamic line; therefore, the 
thermodynamic stability of a lithium nucleus becomes dominated by elastic
energy, and becomes kinetically dominated by plastic flow. For these
conditions, large hydrostatic elastic energy contributions will fully
suppress the dendrite nucleation and growth process, in agreement with
experiments [41,42,57]. 
    However, large deviatoric stresses will enable
the plastic flow along directions specified by the principal directions of
the local stress tensor. Overall, the growth direction of the electrodeposit 
will be determined by the combined contributions of the local
electrochemical gradients (tip-controlled), and the principal directions
(eigenvectors) of the local stress tensor (base-controlled). The growth
direction will be additionally influenced by the formation of kinks in
the electrodeposit [23], or by convective flow in the liquid electrolyte
[19] (See Supplemental information).


DISCUSSION

    Results show that a minimum in the growth rate (gray dashed line in
Fig. 2) exists just below the yield stress, − σ σ < = y 0.56 MPa. Physically, 
as the stress increases in a lithium deposit, the energy barrier for
electrodeposition increases exponentially, thus suppressing growth;
however, as non-hydrostatic stress accumulates on the dendrite, the
nuclei will plastically yield, favoring growth. The contribution of plastic
deformation to base-controlled growth is proportional to the dimensionless 
number, Π2, which in turn is a direct function of the anisotropy of 
the stress tensor, see Fig. 3. Calculations show that for σ < 0,
if a < 0, dendrites will be promoted to grow towards the counter
electrode in a manner analogous to toothpaste being squeezed out of its
tube. This effect is maximal for a = −2, which favors vertical basecontrolled 
growth. Deviations from this maximum value will favor dendrite 
growth at an angle determined by the principal directions of the
stress tensor and qualitatively in agreement with experimental 
observations that detail the change of dendrite growth direction
[15,19,23]. Here, the time evolution of the local anisotropy of the stress
tensor (a a t = < ( ) 0), will lead to the formation of nanostructures that
curl on themselves [15], and eventually slow down or change growth
mechanism [12,14,19], as a result of stress relaxation and dendritedendrite 
electrochemical and chemomechanical interactions. In contrast,
 if a > 0, the dendrite will be pushed downwards, kinetically
canceling base-controlled growth, in the limit of a = 1. Values much
greater than unity will plastically smear the nucleus on the plating
substrate.
    Overall, because the mixed growth regime defines a region where
both, electrodeposition and plasticity, contribute to dendrite growth in
comparable amounts, the development of electrochemical or mechanical 
heterogeneities can locally induce the formation of dendrite
morphologies not accounted for in the current theory. For example,
Steiger et al. [25,26], experimentally observed lithium growth from the
tip, the base, and between kinks, and as a result a whole new set of
morphologies was identified. In general, the strength of each of the
contributions to growth will locally impact some regions, and can easily
leave other regions completely isolated [25,26].
    Fig. 4 shows the time-dependent growth behavior for zero or low
stresses, − σ = 0.1 MPa for the case a = −2, the worst case scenario
(i.e., highest lithium growth rate) for selected values of initial dendrite
sizes and applied overpotentials. Results demonstrate that in the limit
of small overpotentials, the forming nucleus must spend 10 to 105 times
the dendrite characteristic deposition time, t∘ = 1.514 s, in order to
appreciably grow, and is thus dominated by coarsening kinetics [22].
    However, those dendrites whose size is smaller than the kinetic critical
radius, r r < k̂, will shrink and dissolve back into the electrolyte
[22,23,58]. In addition, calculations show that applied overpotentials
that are comparable to or higher than η∘ are necessary to experimentally
observe time-dependent linear growth in time scales that are comparable 
to t∘, in agreement with experiments [19,43,58]. Finally, for large
overpotentials, stress-free dendrites will develop quickly, in agreement
with [6], and will deviate from the spherical approximation after a few
instants, [24,59].
    Fig. 5 shows the time-dependent dendrite size for stress values that
are comparable or larger than the yield stress, e.g., − σ = 5 MPa. For
small overpotentials (see Fig. 5(a)), plasticity dominates the
morphological evolution of the electrodeposit, as highlighted by its
exponential growth for those nuclei that are larger than the critical
kinetic radius. The structure is dominated by base-controlled growth after
a characteristic time, tσ = 323t∘. Note that the larger the initial dendrite
size the sooner plastic deformation will become evident, in agreement
with base-controlled growth experiments as reported by Orsini et al. [16].
    For those dendrites that are comparable in size with the kinetic
radius, if the overpotential is comparable in magnitude to the critical
value (see Fig. 5(b)), the growth rate will be dominated by surface
energy contributions. As dendrites become larger, growth shifts to be
dominated by the base-controlled mechanism after the nucleus has
evolved for tσ ∼ 3000 s. Fig. 5(b) also demonstrates that dendrites will
undergo transitions in growth modes as they shift into and out of the
mixed growth regime, and accelerate its growth, even for cases where
stress and overpotential remain constant, in qualitative agreement with
Crowther and West [19] and Brissot et al. [13]. Fig. 5(c) further demons-
trates that individual dendrites will transition between multiple
growth mechanisms. Thus, dendrites start its growth dominated by the
tip-controlled mechanism, but are slowed down by surface energy forces.
    As they become bigger in size, dendrites grow in accordance to the tipcon-
trolled growth mechanism, but will ultimately grow through the basecontrolled 
mechanism, as the electrodeposit overcomes the characteristic
plasticity time, tσ. Such transitions in growth regimes explain the switch
from tip-controlled to base-controlled, at the same applied current density, 
as observed in the experiments reported by Crowther and West
[19].
    Fig. 6 summarizes the five regimes of growth as a function of local
overpotential and stress and depicts the shift in growth regimes as the
electrodeposit size increases. In agreement with Fig. 2, below the blue
curve you will find the thermodynamic suppression regime where an
hemispherical electrodeposit is thermodynamically unstable due to
surface energy, low overpotentials, or large stresses; between the blue
and the black curve you will find the incubation regime, where growth is
thermodynamically favored, but dominated by local dendrite-dendrite
electrochemical and chemomechanical interactions. Above the black
and the blue curves you will find three growth regimes: (i) to the right
of the red curve you will find the base-controlled growth regime, (ii) to the
left of the green curve you will find the tip-controlled growth regime, and
(iii) between the red and the green curves you will find the mixed growth
regime. Isolated hemispherical nuclei that are comparable in size to the
kinetic critical radius (see Fig. 6(a)), will require overpotentials that are
greater than the critical overpotential to grow through the tip-controlled
mechanism, and on the order of a tenth of the yield stress to grow
through the base-controlled mechanism. For low overpotentials
(10 0.0 −5 < < η̂ 1, i.e., 4.4 10 mV 0.44 mV) × < − < −4 η , high elastic
energies corresponding to large stresses (10 0. −3 < < σ̂ 1, i.e.,
0.73 MPa 73 MP < − < σ a), will fully suppress lithium growth (right of
the black line and below the blue line in Fig. 6(a)). For interacting
populations of lithium nuclei, those dendrites below the black line and
above the blue line will be dominated by electrochemical driving forces
and the Gibbs-Thomson effect that lead to electrochemically-induced
coarsening kinetics [22]; however, calculations demonstrate that for
large stresses, plasticity will dominate the coarsening process, when the
kinetic line shifts below the thermodynamic stability line.
    Dendrites that have reached much larger sizes (e.g., see Fig. 6(b)),
will be dominated by the base-controlled growth mechanism across a
much wider range of overpotentials and applied stresses. Also, for
electrodeposits whose radius of curvature contributions are negligible
compared to the applied overpotential and mechanical stresses (see
Fig. 6(c)), such as those experimentally observed in lithium-only anodes, 
dendrite growth will be dominated by base-controlled growth, thus
suggesting that the growth of flat layers will be morphologically unstable.
In this context, the present analysis suggests that a great deal of
electrochemical and mechanical constraints will be necessary to suppress 
dendrite growth in practical designs (see Supplemental information).
    Fig. 7 shows the time-dependent growth behavior for a = −2 and
very small electrodeposits, as compared to the characteristic electrodeposit 
size, r∘̂= 1 (0.01 μm). For large overpotentials, a hundred times
the critical value (see Fig. 7(a)) and stresses up to yield point, there is a
higher contribution from the tip in the early stages of growth, as depicted
by the linearity of the gray and green curves. At later stages,
growth is either in the mixed growth regime, or entirely in the basecontrolled 
growth regime. For overpotentials close to the critical value
(see Fig. 7(b)), the dendrite grows from the tip for a short amount of
time, and starts to grow from the base after t ∼ tσ. For overpotentials
ten times smaller than the critical value (see Fig. 7(c)), small lithium
electrodeposits (r r ̂ ̂ < k) dissolve back to the electrolyte. However, calculations 
predict that for very large shear stresses, − σ ∼ 50 MPa (red
curve in Fig. 7(c)), lithium electrodeposits will deform and will push the
electrodeposit towards the counter electrode, even in the absence of
overpotentials. Predicted growth behavior is in agreement with early
stage lithium nucleation and growth experiments as performed by Cui
et al. [21].
    Fig. 8 demonstrates the time-dependent growth behavior for an
initial electrodeposit size of ∼ 1 μm, i.e., a hundred times the characteristic 
electrodeposit size. Electrodeposits under large
overpotentials, at about ten times the critical value (see Fig. 8(a)) and
low stresses (− < σ σy) grow from the tip for a very long time. For
electrodeposits under moderate overpotentials on the order of the critical 
value (see Fig. 8(b)), there is a very short time where growth is
either in the tip-controlled or in the mixed growth regime, and switches to
base-controlled growth regime, in agreement with the work from Li et al.
[23], and from Crowther and West [19]. For smaller overpotentials,
e.g., on the order of 1/100th of the critical overpotential (see Fig. 8(c)),
moderately sized electrodeposits will undergo base-controlled growth
under moderate to high stresses (5 to 50 MPa).
    Fig. 9 shows the time-dependent growth behavior for very large
initial electrodeposit sizes, about 104 times the kinetic characteristic
size. For high overpotentials, on the order of a hundred times the critical 
overpotential value (see Fig. 9(a)) and under low stresses
(− ∼ σ 0.1 MPa), growth is tip-controlled for a very long time. For stresses
at the yield point (− ∼ σ 0.56 MPa), growth is base-controlled. For moderate 
overpotentials, on the order of the critical value (see Fig. 9(b)),
and moderate stresses, the tip-controlled contribution is negligible, in
agreement with experiments from Brissot et al. [14]. For very small
overpotentials, on the order of 10−5 times the critical overpotential (see
Fig. 9(c)), and moderate to high stresses (− > σ σy), growth is entirely
base-controlled. However, electrodeposits under smaller stresses
(− ∼ σ 0.1 MPa) will shrink back to the electrolyte (see gray curve).
    Fig. 10 describes the growth regimes as a function of electrodeposit
size and stress, for a fixed overpotential. Below the blue curve you will
find the thermodynamic suppression regime, where dendrites are therm-
odynamically unstable. Above the blue line and below the black line you
will find the incubation regime. Above the blue and the black lines are
three growth regimes: (i) to the left of the green curve and above the
black curve you will find the tip-controlled growth regime, where growth is
dominated by electrodeposition, (ii) above the blue and the black curves
and to the right of the red curve you will find the base-controlled growth
regime, where irreversible mechanical deformation, i.e., plastic deformation 
of lithium, dominates the growth dynamics; and finally (iii),
between the red and the green curves you will find the mixed growth
regime, where both electrodeposition and plastic deformation have
comparable contributions to dendrite growth. To the right of the black
line and below the blue line, very large stresses will mechanically suppress 
dendritic growth. The dashed gray line indicates a minimum in the
total growth rate as a result of the contributions from electrochemical,
chemomechanical and surface energy contributions. Calculations demonstrate 
that for small overpotentials (see Fig. 10(a)), the tip-controlled
mechanism dominates the microstructural evolution of dendrites, only in
a very narrow range of thermodynamic parameters that includes very
small and compressive stresses, and very large sizes. For larger stresses
− σ σ >y, calculations predict that base-controlled growth dominates the
kinetics, in agreement with experiments under low current densities
(0.45 mA/cm2) as reported by Orsini et al. [16].
For overpotentials that are comparable to the critical value (see
    Fig. 10(b)), the edge of the base-controlled growth regime shifts by an
order of magnitude. Thus, dendrites that are subjected to mechanical
stresses smaller than the yield point will grow through the tip-controlled
mechanism and will eventually shift to the mixed growth regime. Experimentally 
reported results [13,15,19,23] are found to lie within the
mixed growth regime, except those by Orsini et al. [16]. Finally, for very
large overpotentials (see Fig. 10(c)), calculations show that the
electrodeposit will grow mostly by adding lithium at the tip, in agreement 
with experiments from Li et al. [23]. Overall, these calculations
demonstrate that as the overpotential increases, dendrites are more
likely to transition from tip-controlled to base-controlled growth regimes,
and to spend a significant window of time in the mixed growth regime.
    Fig. 11(a) highlights the effect of stress on the time-dependent
growth rate, for fixed initial size and applied overpotential. Note that
for small stresses, large dendrites require to overcome a long characteristic 
deposition time, t∘. As the stress increases, the characteristic
deposition time is dominated by power law creep. Dendrites that start
at a size one hundred times smaller, display the same characteristic
deposition time when subjected to the same stresses (see Fig. 11(b)).
Finally, for dendrites that are marginally above the critical kinetic radius, 
the effect of stresses become much more pronounced as they
transition directly from tip-controlled to base-controlled growth, without
displaying appreciable mixed growth regime behavior (see Fig. 11(c)).
    Fig. 12 demonstrates the growth behavior for fixed initial dendrite
sizes and constant overpotentials that are comparable to the critical
value, η̂= 0.5 (−22.1 mV). For large initial dendrite sizes, growth is
either tip-controlled or mixed (see Fig. 12(a)). For dendrites that start
one hundred times smaller, display the same characteristic deposition
time when subjected to large stresses, but display a transition from tipco-
ntrolled to base-controlled growth for small stress values (see
    Fig. 12(b)). Finally, dendrites whose initial size is on the same order of
magnitude as the critical kinetic size will prefer to shrink in the absence
of local electrochemical interactions, unless their growth rate is solely
promoted by the base-controlled growth mechanism (see Fig. 12(c)).
Calculations show that under the application of very large stresses
(− ∼ σ 36 MPa), even small electrodeposits will grow through the basecontrolled 
mechanism, in the absence of any charging overpotential.
    Fig. 13 shows the growth behavior for fixed initial dendrite sizes
and constant applied overpotentials that are fifty times the critical kinetic value. 
For very large initial dendrite sizes, the growth of the
electrodeposit has contributions from both tip and base-controlled growth
mechanisms, thus the system evolves in the mixed growth regime for very
long periods of time (see Fig. 13(a)). For dendrites that are only 2000
times the critical kinetic radius, the effect of electrodeposition becomes
more evident, and the lithium nuclei take a long time to transition from
tip-controlled to base-controlled growth (see Fig. 13(b)). Finally, for
dendrites that are on the order of ten times the kinetic radius, calculations 
show that a very large amount of stress, or a large characteristic
plasticity time is necessary to bring the dendrite into the base-controlled
growth regime


CONCLUSION

    An analytical framework has been developed to rationalize the
major experimentally identified mechanisms for dendrite growth in a
liquid electrolyte. Contributions from electrodeposition, surface energy,
and elastic and plastic deformation are assessed through a thermodynamically 
consistent growth theory in order to understand the
driving forces controlling its dynamics. Specifically, five regimes of
dendrite growth behavior have been identified: (i) thermodynamic suppression 
regime, (ii) incubation regime, (iii) tip-controlled growth regime,
(iv) base-controlled growth regime, and (v) mixed growth regime. For a
single, isolated hemispherical lithium electrodeposit, linear growth
signals tip-controlled growth, and exponential growth signals base-
controlled growth. Further, the necessary electrochemical and chemo-
mechanical conditions to minimize growth have been identified,
namely by maximizing the hydrostatic contribution to stresses, and
minimizing the deviatoric contribution. The predicted regimes of
growth are in excellent agreement with experimental reports of tipcontrolled 
growth and base-controlled growth, and suggest the additional
existence of a mixed growth regime. The theory readily explains the 
timedependent transition between one growth regime to another, as 
reported in the experimental scientific literature [12–15,19,23]. Two
characteristic times have been identified: the characteristic deposition
time, t∘, corresponding to overpotential driven growth, and the characteristic 
plasticity time, tσ, corresponding to plastic flow induced
growth. For very large overpotentials, electrodeposits spend a short
amount of time on coarsening, before tip-controlled growth starts. After
the accumulation of stresses for a long time, ( ) ∼tσ in the mixed growth
regime, growth transitions to the base-controlled mechanism, where
plasticity dominates. Two key dimensionless numbers are identified: Π1,
the ratio of the chemical free energy of transformation to the thermal
energy of an isolated lithium electrodeposit, and Π2 the weighted ratio
of the deposition time to the plasticity time.
    The present theory provides an additional degree of freedom to the
interface dendrite suppression strategy proposed by Zhang et al.
Following Ely et al. [22], and by using the Gibbs-Duhem equation,
SdT Ndμ Adγ qdη ε dσ c · , for an isothermal system with
fixed applied overpotential and stress, γ γ χdμ = − ∘ ∫0μe e, where γ∘ is the
surfactant-free, nucleus-electrolyte interfacial free energy; χ is the
surface concentration; μe c = + + μ z η Ω σ E F / /2 2 , the electrochemic-
omechanical potential; and μc the traditional chemical potential
of the interface. Thus, the addition of a surfactant on a mechanically
stressed interface can be used to suppress the nucleation of lithium
embryos.
    Results suggest that mechanical stresses can both suppress or promote 
dendrite growth. Specifically, the performed analysis suggests
that through the use of existing technology such as pulse charging
[60–63], dendrites can be suppressed by minimizing the possibility of
nucleation and growth, as suggested by Ely [22]; however, for already
existing dendrites, the period of the pulses, τ, should be shorter than t∘
to minimize tip-controlled growth, and simultaneously shorter than tσ to
minimize base-controlled growth. Historically, it is well known that the
application of large compressive hydrostatic stresses has resulted in the
full suppression of dendrites [34,37,38,42,45,46]. This analysis suggests an
additional dendrite suppression design variable through the
application of compressive stresses, σ > σ∘, such that a a t = ≥ ( ) 1 for
t⪢tσ.




Solid Electrolyte Interphase Growth to Rank LiIon Cell Performance




ABSTRACT

    		    A non-destructive technique for physically detecting the growth of the solid
		electrolyte interphase (SEI) during cycling of Li-ion pouch cells is presented.
		Operando cell-stack pressure measurements performed on constrained pouch
		cells reveal a correlation between irreversible volume expansion and capacity
		loss caused by a continually thickening SEI. Several silicon-containing fullcell 
		chemistries—LCO/graphite:Si-alloy, NCA/graphite:SiO, and NCA/Si:C—as
		well as a conventional NMC/graphite cell chemistry were investigated. The effect 
		of FEC consumption on catastrophic failure was also investigated by
		comparing cells containing 10% FEC and 1% FEC. We show that once FEC is
		depleted, passivation failure occurs, resulting in massive, irreversible expansion
		indicating runaway SEI thickening and concomitant cell failure. This work demonstrates 
		that irreversible volume expansion caused by SEI growth can be detected with
 		operando pressure measurements, thus presenting a valuable tool
		for studying the degradation of Li-ion pouch cells and ranking the performance
		of different cell chemistries


INTRODUCTION

    The solid electrolyte interphase (SEI) has been called the most important but least
understood part of lithium ion (Li-ion) cells.1 Formed from the insoluble reaction
products of electrode-electrolyte reactions, the SEI passivates electrodes and
suppresses these parasitic reactions that consume lithium inventory, enabling the
Li-ion cells to operate. However, the SEI-forming parasitic reactions are never
entirely eliminated; therefore, the evolution of the SEI ultimately governs the lifetime
of Li-ion cells.
    In this work, focus will be given to the SEI that forms on the negative electrode. The
negative electrode SEI has been shown to comprise organic and inorganic products
such as Li2CO3, LiF, Li2O, semicarbonates, and polymers.1–3 However, the exact
nature and evolution of this passivating film remains elusive due to the inherent
difficulty of studying it. The negative electrode SEI is initially several nanometers
thin and can be mechanically unstable. Many experiments to investigate the
morphology and composition of the SEI must be performed ex situ, exposing it to
conditions that may damage this highly delicate film.4–6 Additionally, the SEI is sensitive 
to the cell chemistry it is formed in—the choice of electrolyte composition and
electrode chemistry influence its composition.1,2,7 Since the formation and growth of
the negative electrode SEI is the primary source of capacity loss through lithium
inventory loss for well-engineered cells, methods to probe the evolution and stability of 
the SEI are critical for developing Li-ion cell chemistries with longer lifetimes
Increasing the lifetime and energy density of Li-ion cells is required to decrease the
cost of Li-ion batteries and promote the widespread adoption of electric vehicles.8
Silicon negative electrodes offer a compromise between these requirements as they
provide a significant capacity boost compared to conventional graphite negative
electrodes (3,579 mAhr/g and 2,194 Ahr/L for silicon and Li15Si4 versus 372 mAhr/g
and 719 Ahr/L for graphite and LiC69,10), but this comes at the expense of lifetime.
The Li-ion cells that employ silicon-containing electrode materials exhibit relatively
poor cycle life compared to the Li-ion cells with conventional graphite electrodes,
which can last for thousands of cycles with a capacity retention over 80%. This
decreased lifetime of silicon-containing cells is a result of the large 280% volume
change the Si component experiences during lithiation and delithiation.11 This large
volume expansion precipitates the deterioration of silicon-containing electrodes by
causing electrode delamination,12 inducing large mechanical stress that can pulverize 
electrode particles11,13–15 and accelerating the growth and repair of the negative
electrode SEI.2,9,16 Each of these processes contributes to capacity loss by
decreasing the supply of active lithium inventory within a cell.17–19
    Lithium inventory loss caused by the loss of active electrode material via electrode
delamination and electrode pulverization has been mitigated in state-of-the-art
silicon-containing electrode materials by confining the active silicon components
to nano-sized domains,13,20–22 utilizing active-inactive alloy materials,23,24 and by
compositing silicon with other volumetrically benign active materials such as
carbon.21,25–28 Although these efforts have largely helped overcome mechanical
electrode degradation via active electrode material mass loss, lithium inventory
loss due to negative electrode SEI growth remains the primary source of capacity
loss for silicon electrodes.28 The conventional SEI that is formed on silicon-containing
 particles cannot withstand the mechanical stress induced by the large volume
expansion and contraction of silicon, resulting in mechanical breakdown of the SEI
and exposure of active electrode material to the electrolyte. As a result, parasitic
reactions between the electrolyte and the exposed charged electrode material
continually reform and repair the damaged SEI. This occurs with every chargedischarge
 cycle; therefore, lithium inventory is continually consumed by SEI growth,
resulting in capacity loss and a thickened, porous SEI layer.2 This has motivated the
development of mechanically flexible, stable SEI chemistries that can withstand
large mechanical stress and volume expansion.29
    Electrolytes containing fluoroethylene carbonate (FEC) have proven beneficial for
the lifetime of Li-ion cells with many silicon-containing negative electrodes.30–32
As such, FEC has become a common electrolyte component utilized with silicon
electrodes. FEC has a higher reduction potential compared to typical organic
solvents such as ethylene carbonate (EC) and ethylmethyl carbonate (EMC).
Therefore, when FEC is introduced as an additive (<10% wt) in typical organic carbonate-
based electrolyte, it has been shown that FEC preferentially reacts at the
negative electrode surface, suppressing the reduction of other electrolyte components.33–35 
Improved cell performance has been attributed to a less porous, more
inorganic, LiF-rich SEI formed from FEC reduction that is more stable during
cycling.30,36 Consequently, it has been shown that sudden cell failure, a rapid capacity loss 
of >30% over ca. 10 cycles, occurs once FEC in the electrolyte is depleted as
a result of parasitic reactions and electrolyte consumption due to continual SEI
growth and repair caused by the mechanical strain of electrode volume expansion.33,34 Such
sudden failure is catastrophic to the performance of Li-ion cells. In
this present work, we ascribe this catastrophic failure caused by FEC depletion in
cells with silicon-composite negative electrodes to passivation failure and thus
runaway SEI growth as revealed by operando pressure measurements, which, herein
we demonstrate probes SEI evolution by measuring irreversible volume expansion.
    Cell-stack volume expansion has previously been measured with thickness evolution
measurements at a constant load using specialized in-situ force-displacement
cells37,38 and with in-situ pressure measurements using strain gauges mounted on
cylindrical-type cells39 to understand electrode swelling during cycling and electrode degradation. 
Further measurements to probe the reversible expansion of
insertion electrodes used in Li-ion cells as well as the irreversible expansion concomitant with 
prolonged cycling have been performed with in-situ atomic force microscopy,11 
electrochemical dilatometry,16,40–42 3D digital image correlation,43,44 and
mechanical stress measurements.45–47 In this work, we present operando pressure
measurements performed on constrained machine-made pouch cells tested under
commercially relevant conditions as a technique to investigate the degradation of
Li-ion cells with various electrode-electrolyte chemistries, specifically cells with
different silicon composite negative electrodes. We have previously employed
operando pressure measurements to reveal the reversible volume expansion of
Li-ion pouch cells caused by the net electrode volume change during charge and
discharge, and we have identified a correlation between irreversible expansion
and cell performance with prolonged cycling.48 Glazier et al.49 have utilized operando 
pressure measurements to determine Li-ion cell chemistries with low
irreversible volume expansion as a figure of merit for ranking long-term cell performance. 
In this present work, we confirm that operando pressure measurements
reveal irreversible volume expansion caused by SEI growth and thus correlate to
cell performance since lithium inventory loss to negative electrode SEI growth
causes capacity fade. We demonstrate how this technique is useful for ranking
cell chemistries with superior lifetimes by correlating capacity loss with SEI growth
revealed by operando pressure measurements.
    Short-term measurements to characterize the long-term performance of Li-ion cells
are highly desired to facilitate the development of new cell chemistries. Other techniques 
such as ultra-high precision coulometry and isothermal microcalorimetry also
serve to indirectly probe SEI evolution to rank the performance of Li-ion cell chemistries. 
Although highly successful,49–52 these techniques require costly high precision equipment.53,54
 In contrast, the operando pressure method presented in this work can be implemented at a fraction
of the cost and is compatible with any commercial charging system. As such, this technique is readily 
translatable to any battery testing facility, which should be of interest to the Li-ion cell development
community


METHOD

Cell Preparation
    Pouch cells werereceived fromreputable manufacturers sealed without electrolyte. Cells
were opened, dried under vacuum at 100C for 14 hr, and transferred into an argon-filled
glovebox for electrolyte filling and resealing. Before testing, cells were allowed to stand
for 24 hr at 1.5 V to allow for the completion of wetting and then were charged-discharged up
 to 4.2 V and back down to 3.8 V at C/20 at 40C as a formation step. Cells
were then transferred back into an argon-filled glovebox and opened to remove any
gas generated during the formation processes and subsequently resealed.
Pressure Measurements
    Figure S1B shows the apparatus used for performing operando pressure measurements. Pouch 
cells were uniaxially constrained in aluminum enclosures such that
any change in thickness of the cell electrode stack (jelly roll) would result in a change
of force exerted on the enclosure. The pouch cells have a gas bag connected to the
jelly roll that protrudes from the aluminum enclosure (see Figures S1A and S1B),
forcing any gas that was formed during cycling out of the jelly roll into the gas bag.
Therefore, any gas generated does not contribute to the force on the enclosure;
only uniaxial swelling of the cell stack is probed. The force within the enclosure was
measured with subminiature load cells (model LCKD—Omega Engineering) connected to strain 
gauge panel meters (model DP25B-S-A—Omega Engineering) and
was converted to pressure in PSI using the area of a pouch cell of 0.93.2 A force distributing plate 
was fastened between the load cell and the pouch cell. Figure S1C shows a
schematic of this setup. During cycling, a change in pressure on the order of 100 PSI
was observed, which corresponds to a change of thickness of tens of micrometers as
revealed from the measured compressibility curves for each cell type shown in Figure S8. Operando
 measurements were performed using an E-One Moli Energy Canada battery testing system. Pouch 
cells were connected to a Moli channel, and the analog 0–10 V output of the strain gauge panel meter 
was connected to an adjacent Moli slave channel, allowing for simultaneous electrochemical and pressure 
measurements. Cells were cycled at 40C between 3–4.2 V at C/3, with periodic C/20 characterization cycles
for differential voltage analysis.
Differential Voltage Analysis
    To elucidate the mechanisms of capacity loss during cycling, differential voltage
(dV/dQ) analysis was performed. Introduced by Bloom et al.,62 dV/dQ analysis can
be used to deconvolute and quantify the contributions to capacity fade via lithium
inventory loss due to negative electrode SEI growth and loss of active positive or
negative electrode material (e.g., via delamination, pulverization or electrical
disconnection) referred to as electrode mass loss in this work.17–19 This is done by
carefully fitting cycled full-cell voltage curves with a linear combination of half-cell
reference voltage curves for each electrode. The parameters derived from this fit
are the positive and negative electrode active masses and the positive and negative
electrode shifts. The electrode masses act as scalar multipliers to the half-cell
voltage curves, and the electrode shifts are translations such that the half-cell curves
are stretched and shifted relative to each other until a good fit of the full-cell curve
is achieved. As the electrodes lose active material, their voltage curves will correspondingly 
shrink in capacity. Degradation due to mass loss is thus tracked by monitoring the 
capacity of half-cell voltage curves required to fit the full-cell given by the
electrode mass parameters. When lithium inventory is lost to growth of the negative
electrode SEI, capacity is lost because completely delithiating the negative electrode can no 
longer fully lithiate the positive electrode at the discharged state of
the full cell. As lithium inventory is continually lost to SEI growth, the difference in
capacity between the fully delithiated negative electrode and the fully lithiated positive
 electrode becomes larger and larger. Degradation due to SEI growth is thus
tracked by monitoring the translations of the half-cell voltage curves required to
achieve a good fit. Some previous works have called this slippage17,58; here, this
is referred to as shift, and the capacity loss due to growing relative shifts between
the half-cell voltage curves is called shift loss.
    Figure S9 shows examples of how the half-cell and full-cell voltage curves are
affected by capacity loss due to mass loss and negative electrode SEI growth (shift
loss). Since Li-ion cell voltage versus capacity curves generally do not exhibit
pronounced features, it is the differential voltage versus capacity curves that are
used to increase the ease and accuracy of fitting.
    2325-type coin cells were prepared as half-cells for each electrode material as reference
 data for dV/dQ analysis for type A, B, and D cells. The positive and negative
electrodes were retrieved from fresh type A, B, and D pouch cells and punched
into 1.28-cm diameter disks. Two Celgard 2,320 thin polypropylene microporous
films were used as separators. A 150-mm-thick lithium metal foil (Chemetall,
>99.99%) was punched into 1.4-cm diameter disks as the reference electrode. 1M
LiPF6 EC:DEC (1:2 by volume) was used as electrolyte, fully wetting the electrodes
and separators. Coin cells were assembled in an argon-filled glove box. Cells
were cycled at 40C on an E-One Moli Energy Canada battery testing system at a
slow C/20 rate to minimize kinetic effects on cell performance, as required to obtain
quality dV/dQ data for fitting. Fitting was performed on full-cell data during periodic
C/20 characterization cycles, which occurred throughout the life of the cells. This
analysis was performed using software developed by Asher Wright, which performed
 the fits and determined the positive and negative electrode mass and shift
parameters, similar to earlier freeware released by Dahn et al.58
X-Ray Photoelectron Spectroscopy
    To investigate the SEI evolution for type A and B pouch cells, X-ray photoelectron spectroscopy
 (XPS) measurements were performed. Cells after formation (cycle 0) and after
40 cycles were fully discharged and transferred to an Ar-filled glove box for disassembly
and sample retrieval. The negative electrodes were dissected and rinsed several times
with EMC to remove dried LiPF6 and EC. Removal of these species is essential for the underlying SEI 
components to be observed by XPS and for maintaining low pressures in the
XPS system. Rinsingwith EMC isnotexpected to dissolve anySEI components, as EMC is
a major component of the electrolyte in which the SEI was formed. Once the rinsed
samples were dried, they were mounted onto a molybdenum sample holder using double-sided, 
ultra-high vacuum-compatible copper tape. The sample holder was then
transferred into the XPS system, without exposure to air, using a specially designed
air-tight apparatus that could be evacuated to low pressures. Electrodes were left under
ultra-high vacuum overnight to allow for off-gassing of any remaining volatile components. The
 samples were then transferred to the analysis chamber of the XPS, which
has a base pressure of 1 3 1010 mbar and was maintained below 2 3 109 mbar during
the experiments. Analysis was performed with a SPECS spectrometer equipped with a
Phoibos 150 hemispherical analyser, using unmonochromatized Mg Ka radiation with
a beam spot width of 5 mm and a pass energy of 20 eV. Preliminary and final survey scans
were compared to ensure that no photochemical degradation was induced during analysis. Data 
analysis was performed using CasaXPS software (v. 2.3.18). Charge correction
was performed by fitting the adventitious carbon peak, and shifting the x axis such that
this peak fell at 285.0 eV. XPS spectra were fit with a non-linear Shirley-type background.
This background was subtracted from the signal to allow for qualitative comparison of
atomic concentrations between samples using relative peak areas. Peaks were fit with
a mixed Gaussian (70%)/Lorentzian (30%) line shape


RESULT

    402035-size pouch cells (40 mm long 3 20 mm wide 3 3.5 mm thick) with a rolled
electrode design, pictured in Figure S1A, were tested in this work. Table 1 lists
the five different cell types that were investigated. The positive electrodes used in
these cells represent typical choices used by industry in products on the market
today: LiCoO2 is most commonly used in portable electronics, while NMC and
NCA positive electrodes are favored for electric vehicles due to their higher energy
density and decreased reliance on expensive cobalt.55 Four out of five chemistries
tested here employ a silicon composite negative electrode, and one chemistry
uses a conventional graphite negative electrode for comparison. Each cell type
exhibits a different electrode chemistry except for type D and E cells, which use
the same electrode materials but differ in loading; type E cells have thicker electrodes.
 The solvents used for the electrolyte in this work were ethylene carbonate
(EC), ethylmethyl carbonate (EMC), and dimethyl carbonate (DMC) (BASF 99.99%).
The additives used were vinylene carbonate (VC, BASF 99.97%), 1,3,2-dioxathiolane-2,
2-dioxide (ethylene sulfate, DTD, Sigma-Aldrich 98%), and fluoroethylene
carbonate (FEC, BASF 99.94%). All electrolytes contained 1.2 M LiPF6. The electrolyte 
formulations used for each cell chemistry are listed in Table 1. The specific
combinations of solvents and additives were chosen based on previous experience
with what performed well with each electrode chemistry.


DISCUSSION

Reversible Volume Expansion
    Figure 1 shows operando pressure data for type A NMC/graphite (A–C) and type B
LCO/graphite:Si-alloy (D–E) cells for one cycle. Figures 1A and 1D show the voltage
versus capacity of these cells during charge and discharge. Figures 1B and 1E show
the pressure profiles. The reversible pressure evolution is caused by the reversible
net volume expansion of the constituent electrodes. This behavior is thoroughly
explained in our previous work.48 In brief, when the full cell is charged, the negative
electrode is lithiated resulting in a volume expansion (e.g., 10% for graphite, 280%
for silicon), and the positive electrode is delithiated, generally resulting in a
small volume change (e.g., a contraction of 3% and 5% for NMC and NCA and an
expansion of 1% for LCO). The opposite occurs during discharge. The relatively
large negative electrode expansion thus dominates the total volume change, resulting in a 
net increase in volume during charge and a net decrease in volume during
discharge, generating a reversible pressure evolution for one cycle.
    Figures 1C and 1F show the negative electrode voltage (dark teal) and differential
voltage (teal) versus capacity curves fit to the full cell determined by differential
voltage (dV/dQ) analysis. Such analysis allows the full-cell voltage curve to be deconvoluted
 into its constituent positive and negative electrode voltage curves; the latter
is shown in Figures 1C and 1F. This allows the two-phase coexistence regions and
the single-phase regions—which appear as plateaus and peaks, respectively, in
the differential voltage curves of the negative electrode—to be identified to elucidate 
the volume expansion profile. In Figure 1C, the plateau observed at 0.1 V
in the graphite voltage curve corresponds to the 2L/2 and the 2/2L staging transition 
during lithiation (charge of the full cell) and de-lithiation (discharge of the full
cell), respectively.56 Previous studies have shown how graphite does not change in
volume during this phase transition.57 This is in excellent agreement with the operando
 pressure data, wherein, over the 2L/2 and 2/2L transitions highlighted, the
cell does not experience any change in volume. Furthermore, Figure 1B shows that
the type A graphite negative cell exhibits a very symmetric volume expansion.
    In contrast, the operando pressure data for the type B cell with a graphite:Si-alloy
negative cell shown in Figure 1E reveal a highly asymmetric expansion profile.
This is caused by the charge-discharge hysteresis exhibited by silicon electrode
materials.9,48 Figure 1E also displays the graphite 2L42 phase transition plateau;
however, here it is not completely flat due to the expansion (during charge) and
contraction (during discharge) that the silicon component of the electrode
experiences during the graphite plateau resulting in a net slope.
Irreversible Volume Expansion
    Figure 2A compares the capacity retention of type A and B cells. The type A cell with
a graphite negative exhibits no capacity loss, whereas the type B cell with a graphite:
Si-alloy negative experiences a 15% capacity loss after 100 cycles. Figure 2B
shows the operando pressure data versus time. During each cycle, there is a reversible 
pressure evolution caused by electrode expansion and contraction. Over several
cycles, an irreversible pressure evolution caused by an irreversible volume expansion
begins to develop. This can be more clearly tracked with the average discharge
pressure for each cycle as a function of time shown in purple triangles and pink
diamonds for type A and type B cells, respectively. The irreversible pressure growth
mirrors the capacity retention data, demonstrating an inverse correlation between
capacity loss and pressure growth. The average discharge pressure data show
that type A cells exhibit minimal irreversible expansion, whereas type B cells experience 
significant irreversible expansion. It should be noted here that gas evolution
cannot be the cause of the irreversible pressure growth because of how the pouch
cells are constrained during measurement, as shown in Figure S1B. Pouch cells are
uniaxially constrained with a pressure sensor positioned to measure the uniaxial
pressure on the jelly roll (the cell electrode stack) such that only thickening of the jelly
roll affects measured pressure. Any gas evolved is pushed in to the adjoining ‘‘pouch
bag and does not contribute to the pressure signal measured. Therefore, the
irreversible pressure growth is caused by thickening of the jelly roll.
    Figure 2C shows the shift loss as a function of cycle number derived from differential
voltage (dV/dQ) analysis, which elucidates the mechanisms of lithium inventory loss
as discussed in the Experimental Procedures section. This analysis yields parameters
for the electrode masses and electrode shifts determined by fitting full-cell voltage
curves with linear combinations of half-cell reference curves. If the electrode mass
parameters are found to decrease as a function of cycle number, at least some
lithium inventory is lost to electrode degradation. If the half-cell voltage curves
are found to shift relative to each other, at least some lithium inventory is lost is
due to SEI growth17—this mode of capacity loss is referred to as shift loss. The
dV/dQ fits for Figure 2C are shown in Figure S2. The positive and negative electrode
mass and shift parameters as a function of cycle number are shown in Figure S3, and
a table of these parameters is shown in Table S1. Figures S3A and S3B show that
there is negligible electrode mass loss that occurs during cycling for both cell chemistries.
The shift loss, calculated as the difference in positive and negative electrode
shifts, quantifies the capacity lost due to negative electrode SEI growth and
repair.17,58 With no significant mass loss, the increasing shift loss shown in Figure 2C
indicates that the primary source of capacity loss in these cells is negative electrode
SEI growth. Figure 2C shows that type A graphite negative cells show minimal
growth of shift loss, whereas type B graphite:Si-alloy cells exhibit a significant increase
of shift loss and thus exhibit significant negative electrode SEI growth
Type B cells have an initial capacity of 230 mAhr, so the 35 mAhr shift loss accounts
for the entirety of the 15% loss of capacity observed in Figure 2A. The trends
observed in irreversible pressure growth shown in Figure 2B are in good agreement
with the shift loss measurements presented in Figure 2C, indicating that negative
electrode SEI growth is the cause of irreversible volume expansion probed with 
operando pressure measurements.
    To further elucidate the cause of the irreversible pressure growth, X-ray photoelectron 
spectroscopy measurements were performed on the negative electrodes of
type A and B cells before and after cycle testing. Figure 3 shows the C1s XPS spectra
of negative electrodes taken from type A and B cells after formation (cycle 0, A,
and C) and after 40 cycles (B, D). After formation, the XPS spectra for both negative
electrode chemistries exhibited peaks at 284 eV (shaded) corresponding to the
underlying graphite component of the electrodes. This suggests that before cycling,
the SEI layers on both electrode chemistries are thinner than the maximum XPS
penetration depth of ca. 10 nm such that the electrode particles beneath the SEI
surface contribute to the signal. After 40 cycles, this graphite peak remains visible
and is of approximately the same area for the type A graphite electrode sample,
which indicates that the thickness of the SEI has not changed significantly during
cycling for this cell. In contrast, the type B graphite:Si-alloy electrode sample does
not exhibit the graphite peak after 40 cycles, indicating the graphite has become
obstructed after cycling. The SEI on the graphite component of type B cells is not
expected to grow significantly; it should be as stable as in the type A negative
electrode. Therefore, these results suggest that the SEI on the silicon component
has grown significantly such that the thickened silicon SEI obstructs the graphite
component of the electrode from the XPS beam. These results are in agreement
with the hypothesis that the irreversible pressure measurements shown in Figure 2B
are a result of significant SEI growth in type B graphite:Si-alloy negative cells and the
relative stability of the SEI in type A graphite negative cells.
    The proposed mechanism illustrating the inverse correlation between irreversible
volume expansion and capacity loss due to negative electrode SEI growth is
depicted in Figure 4. Figure 4A shows a schematic of a generic negative electrode,
consisting of electrode active material particles coated on a current collector. Each
particle is passivated with an SEI layer. When the negative electrode is lithiated, 
during a full-cell charge, the particles will expand based on the active material used
(e.g., 10% expansion for graphite; 280% expansion for silicon). This expansion is
shown in Figure 4B. Unable to fully accommodate large volume expansion, the
SEI layer will be damaged, leaving areas of the charged particles exposed to electrolyte,
 inducing new parasitic reactions that will form new SEI causing further lithium
inventory loss. This results in a thicker SEI after one cycle, as depicted in Figure 4C,
and this effect will scale with the magnitude of electrode expansion. Another
possible degradation mechanism caused by large volume expansion is fracture of
electrode particles resulting in lithium inventory loss due to electrode mass loss,
as illustrated in Figures 4B and 4C. However, the cells investigated in this work do
not exhibit significant electrode mass loss, as evidenced by Figure S3. Therefore,
the primary mechanism of capacity fade for cells studied in this work is lithium 
inventory loss due to negative electrode SEI growth and repair. After repeating this process, 
A through C, for many cycles, a much thickened SEI evolves,2 as depicted in
Figure 4D. This results in a thickened electrode stack and irreversible volume expansion,
 which is detected with operando pressure measurements. Previous works by
Chae et al.28 and Wetjen et al.59 have reported silicon-composite electrode materials,
 which develop fractures and cracks that increase particle surface area but do
not precipitate loss of active material. As a result, a nano-porous structure develops,
and SEI is formed within this porous structure. This situation is analogous to the
scheme presented in Figure 4D, but SEI is formed within the particle in addition to
on the particle surface, resulting in an observable irreversible volume expansion.28,59
    Figure 5 shows conventional electrochemical cycling results often used to rank the
performance of Li-ion cells along with operando pressure measurements. Figure 5A
shows capacity versus cycle number data for each cell type investigated in this work.
Results for type A cells are shown with purple triangles. Type A cells are the only
chemistry investigated here without a silicon-containing negative electrode, and
these cells clearly exhibit the best performance. Type D and E cells, shown with
blue triangles and orange circles, respectively (different shades of each represent
repeat experiments), exhibit similar short-term performance; however, the type E
cells seem to show better performance after ca. 100 cycles. This is difficult to confirm
with short-term cycling results; long-term cycling performed for these cells shown in
Figure S4 indeed shows that type E cells perform better than type D cells. This may
be explained by type D cells containing a slightly larger proportion of silicon than
type E cells, as evidenced by the half-cell data for both cell types presented in Figure S5. 
A larger silicon content would amplify lithium inventory loss due to SEI
growth and repair as illustrated in Figure 4. Type C cells, shown with green squares,
experience poor short-term capacity retention. However, long-term cycling data
presented in Figure S4 show that type C cells experience better long-term capacity
retention compared to type D and E cells. Type B cells shown with pink diamonds
exhibit the worst capacity retention. From the cycling data presented in Figure 5A
and the long-term cycling data presented in Figure S4, a ranking of the performance
of these cells can be made, from best to worst: type A > type C > type E > type D >
type B. Figure 5B shows the change in average discharge pressure versus cycle number 
Type A cells (purple triangles) show the least irreversible volume expansion. In
order of increasing expansion, type C cells (green squares) exhibit the second smallest 
expansion, then type E cells (orange circles), and type D cells (blue triangles), with
type B cells (pink diamonds) exhibiting the largest irreversible volume expansion.
This ranking of irreversible expansion is in good agreement with the ranking of
capacity retention versus cycle number. Figure 5C shows polarization growth (DV)
versus cycle number The rate of polarization growth for each cell chemistry is consistent
 with the rankings of irreversible expansion and capacity retention. This is more
clearly demonstrated in Figures 5D–5F, which show the rate of capacity growth
versus cycle number (Figure 5D), the rate of pressure growth versus cycle number
(Figure 5E) and the rate of polarization growth versus cycle number (Figure 5F),
calculated for the last 10 cycles of testing. The error bars in Figures 5D–5F correspond to
 the range between repeat tests. The rate of pressure growth at the end
of testing shown in Figure 5E exhibits the clearest ranking of the five cell chemistries
tested here. Ranking performance based on short-term measurements of capacity
retention or DV is not as clear, exemplifying the benefit of operando 
pressure measurements beyond short-term electrochemical measurements.
Effect of FEC Consumption
    To determine the effect of FEC consumption on catastrophic cell failure and irreversible
volume expansion, type B LCO/graphite:Si-alloy and type D NCA/Si:C cells
were tested with electrolytes containing 10% and 1% FEC. It has previously been
shown that FEC in electrolyte used in Si-containing cells is gradually consumed as
a result of continual parasitic reactions and reformation of mechanically damaged
negative electrode SEI and that this consumption eventually causes cell failure
once FEC is entirely depleted.33,34 This is in good agreement with Figure 6A which
shows capacity versus cycle number for type B and D cells containing 10% and 1%
FEC, respectively. Type B cells with 1% FEC, shown with full red diamonds, exhibit
dramatic failure after ca. 50 cycles, whereas type B cells with 10% FEC, shown with
empty pink diamonds, run for over 100 cycles without dramatic failure. Type D cells
with 1% FEC, shown with full teal triangles (repeat experiments shown with different
shades), exhibit dramatic failure after ca. 250 cycles, whereas type D cells with 10%
FEC, shown with empty blue triangles (repeat experiments shown with different
shades), run for over 300 cycles without failure. Operando pressure data were only
available for type B cells with 10% FEC up until 130 cycles; conventional cycling
data for type D cells with 10% FEC cycles are included in Figure 6A (purple triangles,
labeled ‘‘No P’’ in the legend) to demonstrate catastrophic failure does not occur up
to 300 cycles.
    Figure 6B shows the average discharge pressure versus cycle number for these cells.
The cells with 10% FEC content exhibit constant pressure growth, mirroring their capacity
loss. The cells with 1% FEC content show massive inflections in pressure
growth corresponding to where catastrophic failure begins. Figure 6C shows the
change in relative slippage for type B and D cells containing 1% FEC derived from
dV/dQ fitting every 50th cycle. The dV/dQ fits for these cells are shown in Figure S6
The negative and positive electrode mass and slippage parameters derived from
this analysis are shown in Figure S7, and a table of these parameters is shown in
Table S1. Figures S7A and S7B show that the electrode masses remain relatively unchanged 
throughout cycling. Therefore, the significant growth of shift loss shown in
Figure 6C indicates the primary mechanism for capacity loss is attributed to negative
electrode SEI growth. Figure 6C shows the relative slippage exhibits an inflection in
accordance to where type B and D cells with 1% FEC exhibit catastrophic failure and
inflections in pressure growth. We conclude that once FEC is consumed, cell failure
occurs due to passivation failure, resulting in rapidly consumed lithium inventory
causing runaway negative electrode SEI growth and sudden cell death.
    It is also observed that type D cells with 10% FEC exhibit a larger rate of irreversible
pressure growth compared to cells containing 1% FEC. This is consistent with the
capacity retention data that shows cells with 10% FEC initially perform slightly worse
than cells with 1% FEC prior to FEC depletion and catastrophic failure. It may seem
counterintuitive that higher FEC content initially results in worse performance;
however, this behavior is consistent with other literature reports. Jaumann et al.60
have shown results for cells with SiC negative electrodes that contain electrolyte
with 20% FEC exhibit a lower CE than cells with 10% FEC; Jung et al.34 have also
shown that increasing FEC content from 5% to 10% to 20% slightly decreases the
initial capacity retention prior to FEC consumption. Nguyen and Lucht61 have shown
that non-composite silicon electrodes exhibit larger impedance with increasing FEC
content, and that the SEI correspondingly is comprised of increased concentrations
of fluorinated species such as LiF and poly(FEC). Our results suggest that cells with
10% FEC exhibit a larger irreversible expansion than cells with 1% FEC, indicating a
thicker evolved SEI. We speculate that higher FEC content results in a thicker, more
resistive, less compliant, and more brittle SEI, which is less capable of adapting to
the large volume expansion of silicon-containing electrodes resulting in higher rates
of SEI repair and lithium inventory loss. Further work studying the effect of varying
FEC content on the evolution of the SEI and the short-term performance of silicon-
containing Li-ion cells is required to elucidate this behavior more completely.
Correlation between Capacity Loss and Pressure Growth
    To further exemplify the inverse correlation between capacity retention and irreversible
pressure growth, Figure 7 shows the rate of capacity loss versus rate of pressure
growth per cycle calculated for the final 10 cycles of testing. A linear trend is found
between the rate of capacity loss and the rate of pressure growth; cells that exhibit
more rapid capacity loss exhibit larger rates of pressure growth. This inverse correlation 
between capacity retention and irreversible expansion is in good agreement
with previous works, which monitored the swelling of Li-ion cells, e.g., the works by
Wilkinson and Wainwright,38 Cannarella and Arnold,46 and Wetjen et al.


CONCLUSION

    In this work, we utilized operando pressure measurements to reveal the reversible and
irreversible volume expansion of Li-ion pouch cells. Mechanically constrained pouch
cells fit with pressure sensors exhibited a reversible pressure evolution because of
the net expansion and contraction of electrode materials during charge and discharge,
as well as an irreversible pressure growth concomitant with cycling. Irreversible pressure
growth was shown to be inversely correlated to cell performance based on capacity
retention and polarization growth measurements for a variety of different electrodee-
lectrolyte chemistries. We attributed this correlation to negative electrode SEI growth,
which causes the irreversible volume expansion of the electrode stack observed with
operando pressure measurements, and consequently results in capacity fade due to
lithium inventory loss. Cell chemistries with silicon-containing negative electrodes
exhibited worse performance and larger irreversible pressure growth compared to a
cell chemistry with a graphite negative electrode due to a less stable negative electrode
SEI resulting from the larger expansion of silicon materials. This was supported with
X-ray photoelectron spectroscopy measurements and differential voltage analysis,
which showed negative electrode SEI growth was the primary degradation mechanism
for all cell chemistries tested here. Furthermore, we attributed the catastrophic failure of
silicon-containing cell chemistries that exhibit FEC depletion to passivation failure and
runaway negative electrode SEI growth. This work has demonstrated that operando
pressure measurements can be used to probe the evolution of the negative electrode
SEI, and thus the benefit of these measurements as a tool for investigating the degradation
and ranking the performance of different Li-ion cell chemistries. Future work
optimizing electrolyte chemistries with new additive combinations to stabilize the SEI
evolution of Li-ion cells with silicon-containing negative electrodes using operando
pressure measurements is encouraged.




An investigation on the anti-loosening characteristics of threaded fasteners
under vibratory conditions




ABSTRACT

                                    A signiﬁcant advantage of the screw fastener is its capability of being mantled and dismantled
                              using simple tools. However, threaded fasteners have inherent and inevitable limitations that
                they loosen eventually under vibrating environment leading to higher frequency of routine
                                                         maintenance of the components, the absence of which may result in fatal accidents. In the
                                                         present work, an attempt has been made to test the anti-loosening ability of various locking
                                                         screw fasteners, such as nylock nut, aerotight nut, chemical lock, cleveloc nut, ﬂat washer,
                                           nylon washer, serrated washer and spring washer with bolts of different materials, sizes and
                                                         types with different initial clamping forces under the accelerated vibrating conditions obtained
                                                         in an indigenously made testing rig. The loss of clamping force gives an indication of the extent
                                                         of loosening. Their anti-loosening characteristics are compared with respect to initial clamping
                                                         forces. On the basis of the test results, chemical locking has been found to show best anti-
                                                         loosening characteristics followed by nylock and aerotight nut. Loosening is found to be
                                                         considerably less, when the initial clamping force is more than 1.1 ton for metric threaded
                                                         bolts, and the same for BSW bolts is found at a force above 0.8 ton, when the bolts are fastened
                                                         with conventional nuts.

INTRODUCTION

    The history [1] of screw fasteners is believed to have begun in Tigris–Euphrates (Shatt-al-Arab) region almost 3000 years ago.
At the initial stages, the cross section of screw threads was plate-like and they were used for irrigation purposes. According to an
Encyclopedia, in ancient Greece people started applying screw to press olives. If this is true, then they are the ﬁrst people to realize
great potential of screw. The next application that followed was the usage as feeders [1]. Leonardo da Vinci [2] is credited with the
mooting and implementation of this important usage of screw threads. He was the man who started the era of development of this
fastener by making sketches of them showing his ideas concerning application of screws.
    Within half a century after Leonardo died, drastic and revolutionary changes were made. The most drastic change came in its
shape—from square to triangle. This invention was made by gunsmiths somewhere in Europe. Although this assumption has not
been conﬁrmed yet, it is certain that a bolt at the end of a gun barrel in the early 1940s had a triangular shape. The potential of
screw fasteners was slowly realized by people who ﬁnally accepted it by 1779 with the ﬁrst implementation made exclusively
during the construction of “Iron Bridge” in Telford, England. After this, there evolved wide ranges of screw fasteners and they
began to be used in various applications.
    The numerous advantages realized by the use of threaded fasteners are — ability to generate high clamping forces and retention of
the same for very long duration, easy assembly and disassembly without damage of the components, wide range of applications of

threaded coupling, standardization leading to cheap production and easy availability for replacement etc. Such unique advantage
makes possible usage of screw threads in numerous engineering applications like holding the components together—parts of large
machines that must be made in small units for ease in manufacturing, assembling or shipping, for production and transmission of
powers e.g. in lead screws of lathes; screw or press, for adjusting and obtaining accurate movements as in micrometers, pumping,
pulling, sealing, etc.
   However, prolonged and sustained vibration in some typical mode can result in loosening of screw fasteners very easily in
several cases—heralding the major drawback of screw fasteners.


METHOD

    In the year 1945, Goodier and Sweeny [3] tested only a dynamically loaded bolted joint. In spite of their failure to obtain a
complete self-loosening of threaded fasteners, they offered an explanation of partial loosening of threaded fasteners. They pointed
out that for axially loaded joint, pulsating tension of a clamped bolted connection creates radial sliding motions between the
thread ﬂanks of the bolt and nut or the interface of the clamped bearing surfaces. The reasons for this are the contraction of the bolt
according to Poisson's ratio and dilation of the nut walls caused by axial tension.
    In the year 1964, Hongo [4] conducted some experiments on axial loading of nut and bolt assemblies. He varied the axial tensile
force of a bolt and nut fastening having JIS M20 coarse screw threads 100 times in the range of 250 to 3000 kg in a reciprocal
fashion. He examined whether there was any relative rotation between the bolt and nut by observing the oscillation of a beam of
light projected on to a mirror pasted on the bolt. The results showed that the bolt did not continue to rotate in a direction that
would loosen the nut. Goodier and Sweeny [3] in a similar way had reported detecting a relative rotation of 6.28 × 10− 3 radian by
varying the axial tensile force of the bolt 100 times. Hongo [4] could not accept this conclusion of Goodier and Sweeny [3] that “…
the bolt and nut undergo relative rotation in the direction that would loosen the fastening as long as there is variation in the axial
tensile force of the bolt” [3].
    In the year 1966, Paland [5] tested various types of threaded fasteners for axial loading and gave the rule of loosening
arithmetically and by measuring the tangential strain on the surface of the nut. He came to the conclusion that a loaded nut widens
elastically in a radial direction at the area near the bearing surfaces and contracts in upper part.
    This very small amount of radial displacement by expansion of the nut would explain why Paland, in spite of heavy impact
loading in an axial direction of the bolt, still needed a small external off-torque to turn the nut so as to loosen completely.
    In the year 1969, Junker [6] described the mechanism of loosening on the basis of friction between the ﬂank surfaces. According
to him, the theory of mechanism of self-loosening of nut and bolt is based on the well-known law of physics that deﬁnes the effect
of friction on two interacting solid bodies. As soon as the friction force between two solid bodies is overcome by an external force
working in one direction, an additional movement in any other direction can be caused by the action of forces that can be
essentially smaller than the friction force.
    He tested cap screws, spring washers and free spinning locking screws with respect to its anti-loosening characteristics. Since
transversely loaded joints tend more to self-loosening, the test procedure suggested imitates these actual conditions. The ﬁrst
attempt was made with a device consisting of two parts clamped together by the specimen, with load cell and displacement pickup
to record transverse load, preload and displacement. He also reported that maximum values of vibration energy (transverse force x
displacement) were signiﬁcantly different for various locking elements.
    Sase et al. [7,8] tested the effectiveness of screw threads, spring washers, nylon inserted nuts, double nuts and eccentric nuts of
few sizes to resist loosening. Test results showed that the popularly known anti-loosening fasteners did not possess much
resistance to loosening.
    In the year 1998, Sase et al. [8] introduced and evaluated the Step Lock Bolt (SLB) with regard to its anti-loosening performance
using a displacement based loosening device. They found the presence of desirable anti-loosening characteristics of SLB. The
displacement and turning angle of the bolts and the nuts were examined in loosening tests.
    Following the experimental procedure and conclusions drawn by Sase et al. [7], a testing rig was designed and fabricated by a
group lead by Das and co-workers [9,10], where a constant vibrating force of constant frequency and amplitude is applied at the
right angle of the bolt axis. In this set-up, several tests were carried out with BSW and metric bolts of different materials, sizes
with various types of nut and washer arrangements to conclude that nyloc nuts give substantial resistance to loosening compared
to other fasteners.


2.1. Deﬁnition of screw loosening

    Materials fastened using screws are held together by the tensile force generated by the elongation of the bolt shaft (the bolt axis
force) and by the force of compression generated in the objects being tightened (the tightening force) [1]. These two forces remain
in balance as long as no external forces are applied to the objects being fastened by the screws. The general term for the forces
involved in pulling or fastening the two materials together is the pretension force.
    In some situations, such as in the course of using machinery, the pretension force applied at the time that the materials forming
the machinery were originally fastened may decrease for a variety of reasons. This spontaneous decrease in the pretension force is
what is described in general terms as screw loosening.


2.2. Loosening without relative rotation between bolt and nut

   In this case, some residual plastic deformation, such as stretching of bolt, shrinking of an intermediate piece, or smoothening of
the contact surface like the sides of the thread ridges and the bearing surface of the nut, invariably exists. Those problems can be
solved by selection of proper material for bolt and nut [1,3,4].

2.3. Loosening caused by relative motion of the bolt and nut

   There are two types of relative motion which occur in threaded fasteners. One is the relative motion between the nut and the
bolt. And the other is the relative motion between the nut/bolt and clamping surfaces.
   There are three common causes of the relative motion occurring in the threads [1,6]:

1. Bending of parts, which result in forces being induced at the friction surface. If slip occurs, the head and threads will slip which
   can lead to loosening.
2. Differential thermal effects caused as a result of either differences in temperature or differences in clamped materials.
3. Applied forces on the joint can lead to shifting of the joint surfaces leading to bolt loosening.


2.4. Vibration loosening of nut and bolt

    When nut is subjected to vibrating force, a cycle of alternative tensile and compressive forces starts acting on mating surface
between the nut and bolt. But since the mating surface has two angles namely, the lead angle and ﬂank angle, the force gets split
into three mutually perpendicular components. One of these components acts along the axis of the bolt, the other act in a radial
direction and the remaining one act tangentially to mating surface.
    So the force that acts along the axis tries to stretch the bolt and deforms it. The radial force acts to deform the thread proﬁle and
tangential force generates a moment in the reverse direction that favours loosening.
    If relative motion occurs between the threaded surfaces and/or other contact surfaces of the clamped and clamping parts
because of an external force, the direction of which is either tangential or radial, the bolted connection will become free of friction
in the circumferential direction. This means that the preload acting on the thread creates a force in a circumferential direction and
results in the rotational loosening of the bolt or nut.
    For axially loaded joint, pulsating tension of a clamped bolted connection creates radial sliding motions between the thread
ﬂanks of the bolt and nut or the interface of the clamped bearing surfaces. The reasons for this are the contraction of the bolt
according to Poisson ratio and dilation of the nut walls caused by axial tension. Thus axially loaded nut widens elastically in radial
direction at the area near the bearing surfaces and contracts in upper part. These create a relative motion between the nut and bolt
which favours loosening of the fastener.
    For dynamically loaded joints, the relative motion between thread ﬂanks and other contact surfaces of the bearing areas can
occur in magnitudes up to the maximum allowance of the thread. These large effects appear when transverse loading, which has to
be transmitted by grip friction, exceed the friction force between the clamped parts. The resultant transverse slippage between the
clamped parts forces the bolt to assume a pendulum movement, which leads to relative motion in the thread hole and thus in the
thread ﬂanks.
    If the amplitude of such transverse slippage of the bolt is large enough, slippage of nut or bolt head bearing surface will ﬁnally
occur and make the join totally free of friction in a circumferential direction. It can be easily realized too that, contrary to the
conditions to the axial loading, relative motion between the ﬂanks will occur in all parts of the nut threads when the joint slips
under transverse force. Thus, the internal off-torque force becomes sufﬁcient to turn the bolt or nut completely loose as soon as the
friction is eliminated from the bearing area as well as from the thread area. Such transverse slippages are more common in practice
than usually accepted. Experience shows that these joints most frequently fail by self-loosening [1,4,6–8].

2.5. Prevention of loosening by means of design

   From the above discussion it can be declared that loosening caused by relative rotation can be minimized if [1],

➢ The lead angle is reduced:
  Relative slip depends on lead angle. With the increase of lead angle relative slip also increases and vice versa. If helix angle
  becomes greater than friction angle then thread becomes overhaul. If lead angle reduces to zero then thread will not be able to
  transfer torque.
➢ The ﬂank angle is made as small as possible—almost zero:
  A very small ﬂank angle results in increased fastening torque as well as due to vibration when it tries to rotate in the loosening
  direction it needs more torque. So a very small ﬂank angle increases the anti-loosening property of thread.
➢ Reducing relative slip between bearing surface nut and the fastened material by introducing a taper between these two
  surfaces:
  In this case contact area as well as friction force increases. But assembly becomes difﬁcult. Intermediate part must have proper
  counter proﬁle.


2.6. Locking fasteners

   A variety of locking fasteners are used nowadays offered by major companies. For example, conventional spring lock washers
are no longer speciﬁed, because it has been shown that they actually aid self-loosening rather than prevent it. There are a
multitude of thread locking devices available. Through the efforts of the American National Standards Subcommittee B18:20 on
locking fasteners, three basic locking fastener categories have been established. They are [1] free spinning, friction locking, and
chemical locking.
a. Free Spinning Type: The free spinning type is plain bolts with a circumferential row of teeth under the washer head. These are
   ramped, allowing the bolt to rotate in the clamping direction, but lock into the bearing surface when rotated in the loosening
   direction. The ‘Whizlock’ is in this category.
b. Friction Locking: Friction locking categories can be sub-divided into two groupings, metallic and non-metallic. The metallic
   friction locking fastener usually has a distorted thread which provides a prevailing torque; an example of this category is the
   ‘Philidas’ nut. Non-metallic friction locking devices have plastic inserts which provide a thread locking effect; an example being
   the ‘Nyloc’ nut.
c. Chemical Locking: The chemical locking category is adhesives which ﬁll the gaps between the male and female threads and
   bond them together; ‘Loctite’ is an example. Such adhesives are now available in micro-encapsulated form and can be pre-
   applied to the thread.

2.7. Some anti-loosening fasteners

    Many new threaded fasteners have been introduced to claim to have considerable anti-loosening ability. One of such attempts
has provided a wedge ramp which can draw the crest of the bolt tightly. ‘Step lock bolt’ has been reported [1,7,8] to show anti-
loosening property by putting ‘step parts’ having no lead angle at certain positions on to the bolt. Combinations of two nuts with
small eccentricity in the sliding part of the convex top of the lower nut have also been tried for self-locking of the fasteners under
vibratory conditions. In Aero-tight nut, a torque prevailing nut of all metal construction, the nut is slotted in two places which,
after the nut has been tapped, are bent slightly inwards and downwards. When the nut is screwed onto the bolt thread the two
slotted parts are forced back to their original position [1]. In Cleveloc nut, the collar of the nut is elliptical in cross section and it is
this that provides the ﬂexible locking element. The nut is pre-lubricated to reduce the tightening torque [1].


RESULT

3.1. The set up

   From previous works it is seen that the loosening mechanism becomes most pronounced when the direction of vibration is
perpendicular to the longitudinal axis of the bolt.

   The testing rig has thus been designed to obtain the following:

1. A repeated oscillatory motion to achieve vibration of ﬁxed frequency and amplitude.
2. Measurement of clamping force at any instant.
3. Any desired number of oscillations.

   To obtain the above requirements the following machines and/or mechanisms have been made use if:

1. A motor provides the basic rotary motion and the motion is transferred to another shaft by a belt–pulley arrangement.
2. A cam mounted on the rotating shaft incorporates rotating motion to the oscillatory motion onto a rocking plate at a ﬁxed
   frequency.
3. A load cell measures the clamping force between the two plates clamped by the bolt to be tested, at any instant.
4. A proximity switch measures the number of oscillation by measuring the rotation of the shaft.

    The rocking plate transmits this vibration to another plate which is clamped to a ﬁxed structure with the help of the nut and
bolt assembly under test. By this way, the nut and bolt assembles the two plates out of which one is ﬁxed and the vibration is
transferred to the other plate via the rocking plate. The vibration provided is of repeated hammering nature. Due to this vibration,
the fasteners will begin to loosen and the corresponding clamping force will decrease. The schematic diagram of the testing device
is shown in Fig. 1, as also referred in Ref. [10]. Fig. 2 shows the pictorial view of the testing rig.
    This continuous decrement in clamping force with the number of revolutions will be a measure of loosening. The above
proposition is the key behind the designing and fabrication of the test rig.
    The eccentricity of the cam is so designed that at the end of the rocking plate, where the bolts is clamped, the amplitude
becomes 0.175 mm and the plate rocks at a ﬁxed frequency of 3 Hz. The frequency of the vibration can be changed by changing the
pulley step.
    The indigenously made testing machine consists of a motor, a belt–pulley drive, a rotating cam, a rocking plate, a testing area
for fastening and a load cell (compression type, Make–Sushma, Bengaluru, India, Model SLC-302) along with the indicator.



3.2. Experimental conditions

    For carrying out the experiments and to judge the anti-loosening ability of the screwed fasteners the bolts used were Metric
type (M16 and M10, both of High Tension Steel (HTS), having 2 mm and 1.5 mm pitch respectively and 5/8 in. BSW HTS threaded
bolts and Low Carbon Steel (LCS) bolt, having 11 TPI; 3/8 in. HTS, LCS and Stainless Steel (SS) bolt having 16 TPI. The initial
tightening torque was 1.5 ton for M16 HTS bolt, 0.8 ton for M10 HTS bolt and 3/8 in BSW HTS, SS, LCS bolts, 1.3 ton for 5/8 in. BSW
HTS, SS bolts. The number of oscillations up to which the constant decrease in clamping force measured was 10,200. The different
types of nuts and washers used for the clamping of the fasteners were Conventional Nuts, Flat Washer, Spring Washer, Inside
Serrated, Outside Serrated washers, Double nuts, Nylock nut, Nylon washer, Cleveloc Nut, Chemical lock (Locktite) and Aerotight
nut. All the experiments were repeated for three times.



DISCUSSION

   Fig. 3 shows the comparison of loosening for metric (M 16) High Tension Steel Bolt with different nuts. The plot shows that the
use of ﬂat washer can reduce the loss of clamping force to a little extent. The double nut can also effectively reduce loosening.
Chemical locking is found to be a comparatively good anti-loosening fastener under vibration, following nylock nut. On the other
hand, the inside serrated washer shows a better anti-loosening ability than outside serrated washer.
       Loosening tendency of metric (M 10) High Tension Steel Bolts with different nuts is compared in Fig. 4. Here the use of ﬂat
washer and nylon washer can reduce the loosening of conventional nut to some extent. Nylon washer has better anti-loosening

ability than ﬂat washer if properly used between two ﬂat washers. Chemical lock has the better anti-loosening property than
others. Nylock nut also has considerable ability to resist loosening under vibration. Double nut, if properly tightened, can provide
good resistance to loosening. Outside serrated washer and spring washer lies in the middle on the basis of anti-loosening property.
The loosening with conventional nut and ﬂat washer is always faster than all other nuts. If the threaded fasteners are categorized
according to their anti-loosening property, then conventional nut, ﬂat washer shows very little effect on anti-loosening; spring
washer, nylon washer and serrated washers exhibit moderate anti-loosening ability, whereas double nut, nylock nut, and chemical
lock provide the best anti-loosening ability under vibratory conditions.
   Fig. 5 and Fig. 6 show the variation of loosening torque with number of oscillations for a 5/8 in. BSW bolt of high tension steel
and stainless steel bolts respectively. In Fig. 5, it is seen that the nylock nut and chemical locking nut have comparatively good anti-
loosening property. Cleveloc nut cannot prevent loosening effectively. Spring washer and serrated washer (both inside and

outside), can reduce loosening to some extent, whereas Fig. 6 shows that Chemical lock nut has comparatively high resistance to
loosening followed by nylock nut. Inside and outside serrated washer have also enough ability to resist loosening under vibration.
Spring washer has considerable anti-loosening effect compare to ﬂat washer.
    Figs. 7, 8 and 9 provide the variation of loosening torque with number of oscillations for a 3/8 in. BSW bolt of high tension steel,
stainless steel and low carbon steel bolts, respectively. Among the three types of fastening materials, low carbon steel being a
ductile material, can deform easily under high tightening torque resulting in high contact friction between the fastening elements
and hence, tends to resist loosening. However, high tension steel and stainless steel are commonly harder materials than low
carbon steels, and do not deform easily making it suitable for repetitive use. Due to high surface ﬁnish of stainless steels, actual
contact area between the fastening elements becomes large leading to high contact friction reducing the tendency for possible
loosening under vibration. According to Fig. 7, very little improvement has been found using Flat washer over conventional nut,
nylon washer has better resistance to loosening than a ﬂat washer, if properly used between two ﬂat washers. On the other hand,
cleveloc nut does not have enough anti-loosening property like aerotight nut. Double nut has good anti-loosening ability if


properly tightened. Chemical lock shows comparatively better method of reducing loosening under vibration followed by nylock
nut. Fig. 8 shows that chemical locking has better anti-loosening property compared to the others. The nylock nut shows
considerable resistance to loosening followed by outside serrated washer. Spring washer and double nut can also reduce the
tendency to loosening under vibration to some extent. However, double nut does not show (Fig. 9) good results consistently. Flat
washer can reduce the loosening to some extent. Spring washer and outside serrated washer are found to reduce loosening
considerably. Chemical locking is the best method of preventing loosening followed by nylock nut.
    Fig. 10 shows the comparison of loosening for 3/8″ BSW low carbon steel bolt with conventional nut when tested with different
initial clamping forces. The plot shows that for every initial clamping force, the nature of loosening is almost the same, i.e. for ﬁrst
few oscillations, the loosening is higher and after certain number of oscillations the rate of loosening becomes slower. The
percentages of loosening after 5000 oscillations corresponding to the different initial clamping forces are shown in Table 1.




    From the Table 1, it can be said that for higher initial clamping force, total loosening is lower for BSW low carbon steel bolt. But
for extensively higher clamping force, the thread as well as the bolt may be deformed and the loosening is lower when the initial
clamping force is higher than 0.8 ton.
    Fig. 11 shows the comparison of loosening for M16 High Tension Steel Bolt with conventional nut when tested with different
initial clamping force. Here, it is seen that for all initial clamping force, the loosening trends are almost the same. The initial
loosening rate is quite high. The total loss of clamping force for different initial clamping forces is shown in Table 2.


    In the case of M16 high tension steel bolt, the percentage of loosening was calculated after 10,200 oscillations and it was found
(Table 2) that the loosening is minimum when the initial clamping force is more than 1.1 ton. The reason behind this may be that,
for higher initial clamping force, the fastener may undergo higher deformation at the contact points leading to more frictional
resistance to loosening. However, very high force may damage the fastener. Hence, appropriate initial tightening torque is to be
applied on a threaded fastener that can restrict loosening.

CONCLUSION

    From the results obtained of repetitive experiments with different combinations of fastening elements, the following
conclusions may be drawn.
    (i) Not only the bolt material, but other fastening elements such as nuts or washers also play a key role behind the anti-
        loosening property of the fasteners; the initial tightening torque also plays a signiﬁcant role behind the self-locking
        property of fasteners.
   (ii) Possibly because of high surface ﬁnish, the stainless steel bolts show better resistance to loosening than that of high tension
        steel and low carbon steel bolts.
 (iii) Out of several anti-loosening fasteners tested, chemical locking provides the best anti-loosening ability followed by nylock
        nut and aerotight nut. Aerotight and nylock nuts have less possibility to loose under hostile vibrating conditions, but the
        effect decreases considerably with repetitive use.
  (iv) Spring washer, inside serrated and outside serrated washers also provide considerable anti-loosening property, but gets
        damaged after single use.
   (v) It is found that ﬂat washers and nylon washers do not prevent loosening, but spring washers and double nuts show
        considerable resistance against loosening.
  (vi) For metric as well as BSW threaded bolts, with the increase in clamping force, the percentage of loosening shows an overall
        decrease because of possible deformation and/or larger contact area between fastening elements.








 Effect of Pitch Difference between the Bolt-Nut Connections upon the  



               Anti-Loosening Performance and Fatigue Life  




                                              


Abstract                               



  In this paper, the effect of a slight pitch difference between a bolt and nut is studied. Firstly,  




by varying the pitch difference, the prevailing torque required for the nut rotation, before the nut  


                                          

touches  the  clamped  body,  is  measured  experimentally.  Secondly,  the  tightening  torque  is  



determined as a function of the axial force of the bolt after the nut touches the clamped body.  



The results show that a large value of pitch difference may provide large prevailing torque that  





causes an anti-loosening effect although a very large pitch difference  may deteriorate the bolt  




axial force under a certain tightening torque. Thirdly, a suitable pitch difference  is  determined  




taking into account the anti-loosening and clamping abilities. Fourthly, fatigue experiments  are  




conducted  using  three  different  values  of  pitch  difference  for  various  stress  amplitudes.  It  is  


found  that  the  fatigue  life  could  be  extended  when  a  suitable  pitch  difference  is  considered  



Furthermore, the chamfered corners at nut ends  are considered,  and it is  found that  the finite  



element  analysis  with  considering  the  chamfered  nut  threads  has  a  good  agreement  with  the  



experimental  observation.  Finally,  the  most  desirable  pitch  difference  required  for  improving  



both anti-loosening and fatigue life is proposed.    



  


1 Introduction  



  The  bolt-nut  connections  are  important  joining  elements  and  are  widely  used  to  



connect  and  disconnect  members  conveniently  at  a  low  cost.  Reference  [1]  fully  


  

reviewed  the  history  as  well  as  the  evolution  of  the  screw  fasteners.  To  ensure  that  


structures are safety joined, good anti-loosening performance and high fatigue strength  


are  required.  Most  previous  studies  have  been  mainly  focusing  on  anti-loosening  



performance  [2-7],  and  few  studies  have  contributed  to  improvements  in  the  fatigue  


strength [8-17]. This is because a high stress concentration factor, e.g. K =3-5, appears  




at the No.1 bolt thread and it is not easy to reduce it. Moreover, usually for special bolt- 




nut  connections  the  anti-loosening  ability  affects  the  fatigue  strength  and  the  cost  





significantly.   In   other   words,   anti-loosening   bolt-nut   connections   have   not   been  

     

                                   

developed until now without a reduction in fatigue strength and a raising in the cost.    




  This paper, therefore, focuses on the effect of pitch difference in a connection on the  


anti-loosing performance and fatigue life. As shown in Fig. 1, if the nut pitch is larger  



than the bolt pitch, the thread No. 1 at the left-hand side is in contact before the loading  



and becomes in no-contact  status after the loading as shown in Fig. 1  (a). However, if  





the nut pitch is smaller than the bolt pitch, the thread No.1 at the  right-hand  side is in  


contact before the loading and remains in contact after loading,  also  the contact force  



becomes  larger  after  the  loading  as  shown  in  Fig.  1  (b).  Therefore,  the  largest  stress  



concentration at thread No.1 can be reduced only by a larger nut pitch.    



  The concept of differential pitch was first suggested by Stromeyer [ 18] in 1918. He  



suggested that the load distribution in a threaded connection thread could be  optimized  



by  varying  the  relative  pitches.  Then,  the  theoretical  load  distribution  in  bolt-nut  has  



been  developed  by  Sopwith  [19],  who  also  used  his  formula  to  discuss  the  load  



distribution improvement along the bolt threads by varing pitch. He found that a smaller  





pitch in the bolt than in the nut would improve the load distribution. Sparling [20] found  



that the fatigue strength of the bolt can be improved by increasing the clearance between  



the  first  few  engaged  threads  at  the  load  bearing  face  of  the  nut  by  tapering  the  nut  



thread,   which   produces   an   effective   difference   in   pitch.   This   modification   was  




investigated  by  Kenny  and  Patterson  [2 1,  22]  by  applying  the  frozen  stress  three- 


dimensional photoelasticity. Maruyama [23] analyzed the influence of pitch error and  



the  loaded  flank  angle  error  of  the  bolt  thread  upon  the  stress  at  the  root  of  the  bolt  

thread   by   copper-electroplating   method   with   the   finite   element   method.   It   was  


considered that the pitch adjustment has a larger effect than the flank angle adjustment  


for improving the fatigue strength of the bolt thread.  


  However,  the  previous  studies  on  pitch  difference  were  limited  to  fatigue  strength  


                                  

improvement, and the effect of pitch difference on the anti-loosening performance has  



not been investigated  yet. There is no systematic experimental data are available, e.g.  
 

the S-N curves for specimens of different pitch differences have not been obtained.  


  Table 1 shows a comparison of some special bolt-nut connections. Most of the special  



bolt-nuts have either more  components  or very special geometry, leading to a complex  


manufacture process and a high cost which is usually more than 3 times of the normal  



bolt-nut.  The suggested nut in this study can be manufactured as the same way as the  





normal nut, and the cost is predicted to be about 1.5 times of the normal nut considering  



the modification of thread tap as well as the checking procedure on the pitch difference.  



  Our  previous  experimental  work  clarified  that  the  fatigue  life  is  improved  by  



introducing a suitable pitch difference under a certain level of stress amplitude [24, 25].  



In this study,  at first,  the effect of pitch difference on the anti-loosening performance  



will be studied experimentally, and the most desirable pitch difference will be proposed  



taking into account the effect on clamping ability. Furthermore, the fatigue experiments  

will be carried out to investigate the effect of pitch difference on the improvement of  



fatigue life. The finite element analysis will also be applied to discuss the stress status at  



bolt  thread.  Taking  the  anti-loosening  performance  and  the  fatigue  life  improvement  



into account, the most desirable pitch difference will be proposed.  


2 Effect of the Pitch Difference on the Nut Rotation    



2.1 Bolt-Nut Specimens                            



  Japanese Industrial Standard (JIS) M16 bolt-nut connections were employed to study  



the  effect  of  a  slight  pitch  difference.  Figure  2  shows  the  dimensions  of  bolt-nut  


specimen  used  in  this  study.  Figure  3  shows  a  schematic  illustration  of  bolt-nut  



connection  having  pitch  differences.  Usually,  standard  M16  bolts  and  nuts  have  the  

 
                                        

same pitch of 2  mm, but herein the nut pitch is slightly larger than the bolt pitch.  The  



clearance  between  the  bolt-nut  threads  is  equal  to  125  μm.  The  bolt  material  was  


chromium-molybdenum steel SCM435 (JIS), and the nut material was medium carbon  


steel S45C (JIS) quenched and tempered, whose properties are indicated in Table 2, and  


whose stress-strain curves are shown in Fig. 4, respectively.    


  Figure  3  (a)  also  shows  a  contact  status  between  bolt  and  nut  threads  during  the  





tightening  process.  As  the  nut  is  screwed  onto  the  bolt,  the  pitch  difference  is  




accumulated.  Finally,  the  first  and  sixth  nut  threads  become  in  contact  with  the  bolt  



threads as shown in Fig.  3  (a). The distance,  δ , where the contact takes place, can be  




obtained geometrically using Equations (1) and (2).          


  



and C  are the horizontal and vertical clearances between bolt and nut as shown in Fig. 3  



(b). The specimens in this study had five different levels of pitch difference  α, namely  



α=0  (for  standard  connection),  αsmall ,  αmiddle,  αlarge  and  αverylarge,  Herein,  it  should  be  



noted that the nut has 8 threads and therefore Equation (1) is valid when  n  is less than  




8. Table 3  shows the distance, δ , and nut thread number  in contact, n , obtained from  

Equations (1) and (2). The distance,  δ , can be predicted for α , α   and  α    ,  


although no thread contact may be expected for α  , because n  is larger than the total  



number of threads number 8 for the nut.        



2.2 Prevailing Torque                   

                                     

  After the nut threads become in contact over distance δ  as shown in Fig. 3 (a), the so- 



                                       

called  prevailing  torque  is  required  for  the  nut  rotation  even  though  the  nut  does  not  





touch  the  clamped  body  yet.  Table  3  also  lists  the  prevailing  torque  T   measured  by  




using an electric torque wrench.    


  For  α=α    ,  the  value  of  n   is  larger  than  8,  and  therefore  all  threads  are  in  non- 


contact  status  and  the  prevailing  torque  was  zero  experimentally.  For  α=αmiddle,  since  





value  n   is smaller than 8, the threads are in contact and prevailing torque was  T =25  





N∙m.  For  α=α   ,  prevailing  torque  was  T =50  N∙m,  and  for  α=α   the  threads  

deformed  largely  and  the  nut  was  locked  before  touching  the  clamped  body  since  it  



cannot be rotated anymore.  



2.3 Prevailing Torque vs Clamping Force    



  Since the bolt and nut are used for connecting components or structures, the clamping  



ability to produce enough bolt-axial force is essential. Therefore, after the nut touches  



the clamped body, the relationship between the tightening torque and the clamping force  



was investigated. Note that tightening  torque  T  is different from prevailing torque  T ,  


  



which  is  defined  only  before  the  nut  touches  the  clamped  body.  To  obtain  the  



relationship between torque and clamping force,  the torque was controlled by using an  



electric torque wrench,  and the clamping force was measured by using the strain gauge  



attached to the clamped body surface as shown in Fig. 5 (a).  The uniaxial strain gauge  





with a length of 2 mm KFG-2 (Kyowa Electronic  Instruments Co.,  Ltd.)  was used in  


this   measurement.   Before   the   experiments,   calibration   tests   were   performed   by  



compressing  the  clamped  body  to  obtain  the  relationship  between  the  clamping  force  

and  surface strain as shown in Fig. 5 (a).  Similar tests  were performed  to calibrate  the  



torque wrench  as shown in Fig. 5 (b).  In order to compare anti-loosening performance  

for  different  pitch  differences,  the  same  tightening  torque  was  applied.  When  the  

tightening torque of 70 N·m was applied to the standard bolt-nut (α=0), the bolt-axial  


                                  

force  became  24  kN.  The  bolt  axial  force  24  kN  is  rather  smaller  compared  to  the  


normal bolt-axial force as the standard bolt-axial force 59.3 kN recommended in [26].  


However,  if  a  larger  bolt-axial  force  is  used,  the  effect  of  α  on  the  anti-loosening  



performance cannot be clearly demonstrated because the bolt-nut seizure occurs. In fact,  



when a torque of 150 N·m was applied in our preliminary experiments, bolt-nut seizure  


was  sometimes  observed  even  for  α ＝0  and  α ＝αsmall .  This  is  because  in  this  study,  




turning was used for manufacturing nuts, which leads to the bolt-nut seizure occurring  


more  easily  than  tapping,  which  is  usually  used  for  manufacturing  nuts.  The  tapping  



was not used in this study because of the high cost. However, in the further research, the  



tapping  nut  can  be  used  to  prevent  the  bolt-nut  seizure.  In  this  study,  therefore,  the  



smaller  tightening  torque  of  70  N·m  is  used  to  compare  the  anti-loosening  ability  



conveniently.  



  Figure 6 shows the tightening torque vs. clamping force as experimentally obtained.  



When  α=αsmall , the torque-clamping force relationship was same  with  the one of  α=0. 
  



When α=αmiddle, the prevailing torque of 25 N∙m was required before the nut touches the  



clamped  plate.  Under  the  same  tightening  torque  of  70  N∙m,  the  clamping  force  was  



reduced to 20 kN. When  α=αlarge, under a torque of 70 N∙m the axial force decreased  



significantly to 8 kN, which was only 1/3 of the axial force of α=0.  

3 Loosening Experiment                           


3.1 Device                                 



  Based  on  the  torque-axial  force  relationship  obtained  above,  the  loosening  



experiments  were  performed  to  investigate  the  effect  of  pitch  difference  on  the  anti- 



loosening  performance.  For  each  pitch  difference  α,  two  specimens  were  tested.  As  




shown in Fig. 7, the experimental device was an impact-vibration testing machine based  



                                  

on NAS3350 (National Aerospace Standard), whose vibration frequency was  about 30  


Hz, and vibration acceleration is 20 g. The maximum vibration cycle of NAS3350 is 30  


000,  therefore,  if  the  number  of  vibration  cycles  was  over  30  000,  the  anti-loosening  


performance  may  be  considered  to  be  good  enough.  A  counter  connected  with  the  

experimental device shows the number of cycles of vibrations. As states in Section 2.3,  

the   bolt-axial   force   24   kN   was   considered   for   the   standard   bolt-nut,   and   the  


corresponding  tightening  torque  was  70  N∙m.  In  order  to  compare  the  anti-loosening  



performance under the same condition, in this paper, the nuts were tightened to the same  



torque of 70 N∙m for all the specimens.  



3.2 Results    



  Table 4 lists the number of cycles for the start loosening and the nut dropping. Table  



4  also  lists  the  prevailing  torque  measured  in  the  loosening  experiments  and  the  bolt  



axial forces estimated from Fig. 6. For α=0 and α=αsmall , the nuts dropped at about 1,000  


cycles. For  α=αmiddle,  the nuts did not drop until 30  000 cycles, but the loosening was  



observed for one specimen. For α=αlarge, no loosening was observed until 30 000 cycles  



although the axial force was estimated to be only 8 kN. It may be concluded that if α is  



too  small,  the  anti-loosening  cannot  be  expected  and  if  α  is  too  large,  the  clamping  



ability  is  not  good  enough.  By  considering  both  the  anti-loosening  and  clamping  


abilities,  α=αmiddle  can  be  selected  as  the  most  suitable  pitch  difference.  It  should  be  



noted that the most desirable pitch difference of α=αmiddle was obtained with a clearance  



of C =125 μm.                             


4 Finite Element Analysis           



  The previous discussion shows that  α=αlarge  has a good  anti-loosening performance  


                                   

but  insufficient  clamping  ability.  This  is  due  to  the  large  deformation  of  the  threads  




during the tightening process. To confirm this, an axisymmetric model of the bolt-nut  



connection  was  constructed  by  using  the  FEM  code  MSC.Marc/Mentat  2012.  The  



material of the bolt was SCM435 and the material of the nut was S45C to match the  



experimental conditions. These stress-strain curves are indicated in Fig. 4. Herein, bolt,  




nut and clamped body are modeled as three bodies in contact. In the tightening process,  



the  accumulated  pitch  difference  causes  the  axial  force  between  the  bolt  threads  



engaged with the nut thread. In this modelling, the bolt head is fixed in the horizontal  



direction,  and  the  tightening  process  is  expressed  by  shifting  the  nut  threads  position  



discontinuously, one by one, at the pitch interval. As the nut is moving towards the bolt  



head, the accumulation of the pitch difference leads to a slight overlap between the bolt  



threads and the nut threads. The direct constraints method is invoked in the detection of  



contact in MSC. Marc  [27], then, the nut is compressed while the bolt is stretched in the  



simulation. In this way, the axial force between the bolt threads can be investigated step  



by  step  as  the  nut  is  shifted  onto  the  bolt.  It  should  be  noted  that  this  axisymmetric  




simulation may include some numerical errors but the real axial force between the bolt  



threads  is  difficult  to  be  measured  experimentally  because  the  nut  is  engaged  at  this  



position.  The  multifrontal  sparse  solver  was  used.  The  isotropic  hardening  law  was  



assumed  with  von  Mises  yield  criterion.  Friction  coefficient  of  0.3  was  assumed  and  




Coulomb  friction  was  used.  In  the  next  sub-section,  the  results  for  α=αmiddle  and  


α=αverylarge will be compared.                  



4.1 Bolt Axial Force                       



  Since  the  nut  pitch  is  larger  than  the  bolt  pitch,  a  bolt  axial  force,  F  ,  in  tension  


appears between the bolt  threads. F   corresponds to prevailing torque  T . It should be  




noted that Fα is different from the bolt axial force  (clamping force) obtained  in Fig.  6.  


                                     

Here,  the  axial  force  Fα  between  bolt  threads  arising  from  the  accumulation  of  pitch  





difference in the tightening process. Figure 8 (a) indicates F   for α=α  before the nut  




touches  the  clamped  body  from  Position  A  to  Position  G.  Position  A  is  where  the  


prevailing  torque  appears,  and  Position  B  is  where  the  nut  thread  shifted  at  the  pitch  


interval from Position A and so on. Finally, Position G is where the nut starts contacting  

the clamped body.  From Position A to Positions B, C, the  whole nut is  being shifted  

onto the bolt, and therefore the accumulated pitch difference affects the results. From  

Position  C  to  Positions  D,  E,  F,  G,  the  pitch  difference  is  not  accumulated  since  the  



whole nut is already in contact with the bolt.    




where the prevailing torque appears, and Position H is where the nut starts contacting  



the clamped body. In contrast to the case of α=αmiddle, as the nut is being shifted onto the  



bolt, the bolt axial forces corresponding to nut threads No.1 and No.8 become smaller  



than that in the middle part. This result is due to nut threads No.2 and No.7, which are  


also in contact as well as threads No.1 and No.8. Under α=αmiddle only nut threads No.1  



and No.8 are in contact with bolt threads.  



4.2 Plastic Deformation  



  Figure 9 (a)  shows the equivalent plastic strain of threads for α=αmiddle  at Position G.  


Similarly,  Fig.  9  (b)  shows  the  equivalent  plastic  strain  of  threads  for  α=αverylarge  at  


Position H. It may be concluded that too large pitch difference α=αverylarge may cause the  


large  deformation  at  nut  threads  resulting  in  deterioration  of  bolt  clamping  ability.  A  




suitable pitch difference may cause the reasonable deformation and may not reduce the  


clamping force.           



5 Effect of the Pitch Difference on the Fatigue Strength  

5.1 Results and Discussion    D  


  Our previous experiments clarified that the fatigue life was improved by introducing  

a pitch difference  α=αsmall under a certain level of stress amplitude [24-25]. According  


to the loosening experiments, it was found that  α=αmiddle  was the most desirable pitch  


difference to realize the anti-loosening performance. To improve the fatigue life as well  


as the anti-loosening performance, fatigue experiments were conducted  systematically  



for three types of specimens, i.e. α=0, α=αsmall and α=αmiddle with various levels of stress  



amplitude.  



  The 392 kN Servo Fatigue Testing Machine with a frequency of 8 Hz was used in this  



study. The pulsating tension fatigue experiments with a stress ratio of R=0.14-0.56 were  



conducted under a fixed average stress of σ  = 213  MPa. Figure 10  shows the obtained  



S-N curves. Independent of α, it was found that the fatigue limit at N=2×10  cycles was  



60 MPa.    




  



  The  fractured  specimens  were  first  investigated.  As  an  example,  Fig.  1 1  shows  



longitudinal sections for  α=0,  α=α and  α=α , when the stress amplitude σ =100  




MPa. For α=0, the initial crack may occur at thread No.2, and final fracture happened at  



thread No.1. For  α=αsmall  and α=αmiddle, long cracks were observed at threads No.5 and  




No.6, and therefore initial crack may occur at threads No.5 or No.6 extending towards  


thread  No.1.  Moreover,  when  the  stress  amplitude  was  σ =60  MPa,  the  fractured  



specimens of α=αsmall  and α=αmiddle  also showed more than 1 mm long cracks initiating  


from the thread surface  although no long crack  was observed  for  α=0.  Therefore, the  

actual  fatigue  limit  of  the  bolt  specimen  may  be  lower  than  60  MPa  for  α=αsmall  and  



α=αmiddle .                        



  Figure  12  shows  the  crack  initiation  and  extension  mechanism  for  α=αsmall  and  


α=αmiddle .  As  shown  in  Fig.12  (a),  crack  initiated  at  thread  No.6.  After  the  crack  


extended at No.6, the distributed load F  became smaller and F became larger as shown  



in Fig. 12 (b),        . Then, a new crack initiated at thread No.5 as show in Fig.12 (c).  



By extending new cracks from No.6 toward No.1, the finial fracture happened nearby  

No.1.  In this way,  since many cracks initiated and propagated one by one,  the fatigue  




life of α=αsmall and α=αmiddle can be extended compared with the one of α=0.  



  The experimental observation in Fig.11 shows that the crack initiated around the root  



of bolt thread ψ=-60 ～60 , instead of the nut thread contact region. Therefore, in this  



study, the contact fatigue concept was not considered.  



  When the stress amplitude was larger than 80 MPa,  as shown in Fig. 10, the fatigue  



life for  α=αsmall  was about 1.5 times larger than that of  α=0. Also, the fatigue life for  



α=αmiddle was about 1.2 times larger than that of  α=0. The results showed that the most  



desirable pitch difference  α=αsmall  for fatigue performance was different from the most  



desirable pitch difference of α=αmiddle for anti-loosening performance.  



  In Fig.10, there are different fatigue data between α=αsmall and α=0 because the stress  



status at bolt thread changed when α=αsmall was introduced. On the other hand, as shown  



in  Fig.6,  since  there  was  no  prevailing  torque  appears  in  the  tightening  process  for  



α=αsmall , it has the  same torque-axial force relationship with the normal specimen  α=0.  



The effect of pitch difference on the fatigue life is different from the effect on tightening  

process.  This  is  because  that  the  fatigue  damage  is  mainly  controlled  by  the  stress  




amplitude produced by the axial loading at the bolt threads.  


5.2 Strength Analysis                 

  To  clarify  the  effect  of  the  pitch  difference  on  the  stress  at  the  bolt  threads,  the  



elastic-plastic FE analyses were performed for  α=0 and  α=αsmall under load F=30±14.1  


kN. The  axisymmetric finite element model of bolt-nut connection is  shown in Fig. 13.  


A  cylindrical  clamped  plate  was  modeled  with  an  inner  diameter  of  17.5  mm,  outer  

diameter of 50 mm and thickness of 35 mm. The material of the bolt and clamped body  



was  SCM435  and  the  material  of  the  nut  was  S45C  to  match  the  experimental  



conditions. These stress-strain curves are indicated in Fig. 4. The bolt, nut and clamped  




body were modeled as three contact bodies. A fine mesh was  created at the root of bolt  

thread  with  the  size  of  0.015mm ×0.01mm,  and  4-noded,  axisymmetric  solid,  full  




integration element was used. The isotropic hardening law was assumed with von Mises  



yield  criterion.  Friction  coefficient  of  0.3  with  Coulomb  friction  was  used  for  the  



analysis.  The  clamped  body  was  fixed  in  the  horizontal  direction,  and  cyclic  load  



F=30±14.1 kN was applied on the bolt head as shown in Fig.  13. Then, the stress status  



under  the  maximum  load  F=30+ 14.1  kN  and  the  minimum  load  F=30-14.1  kN  was  



considered to obtain the endurance limit diagrams. Figure 14 defines the angle  ψ  at the  



bolt thread. In the FE analysis σψmax was the stress σψ at each thread under the maximum  



  



load,  and  σ   was  the  stress  σ    at  each  thread  under  the  minimum  load.  The  stress  




amplitude and mean  stress were  investigated at the same angle  ψ where the maximum  



stress amplitude appears,  since the stress amplitude is the most important parameter for  



fatigue analysis. The mean stress σ   and stress amplitude σ at each thread are defined as  


follows:                                            



Fig.  15  and  compared  with  Soderberg  line  representing  the  endurance  limit  for  plain  



specimen.  Figure  15  indicates  that  the  stress  amplitude  at  thread  No.1  for  α=αsmall  is  



much smaller than that of  α=0 although the stress amplitudes of threads No.4 to No.8  



are much larger than those of  α=0. Therefore, the cracks may appear faster at No.4 to  




No.8  for  α=αsmall ,  but  the  fatigue  life  time  is  extended  as  shown  in  Fig.  10  since  the  



crack propagation from threads No.8 to No.1 needs longer time.    


6 Effect of incomplete nut thread  




  In the above discussion, the complete thread model of 8-thread-nuts were  considered  

by FE analyses, but usually as shown in Fig. 16 (a) both ends of nuts have chamfered  



corners,  which  are  required  to  make  bolt  inserted  smoothly.  This  types  of  nuts  were  



used in the fatigue experiments. Therefore, the chamfered corner was modeled  first by  



an incomplete thread model A as shown in Fig. 16 (b). Figure 17  shows FE mesh for  



model A and  the  endurance limit diagram, when  α=α   and  σ =  100 MPa. From Fig.  

                                              small   a 



17 (b), it can be seen that the stress in thread No.8 decreases and the stress in thread  



No.6 increases. However, the stress in thread No.6 is not the most dangerous because  



thread No.8 is still in contact with a nut thread.    






  Therefore,  thread  model  B  as  shown  in  Fig.   16  (c)  is  considered,  where  the  



incomplete nut thread does not contact bolt thread anymore due to the chamfered nut- 



ends.  Figure  18  (a)  shows  the  FE  mesh  for  model  B.  Figure  18  (b)  (c)  show  the  



maximum and minimum stresses in each thread when the maximum and minimum load  



F=30±14.1 kN are applied. When α=0, the maximum stress amplitude appears at thread  


No.2. Therefore, the analytical result coincides with the experimental result in Fig. 1 1  





(a).  When  α=αsmall ,  the  maximum  stress  amplitude  appears  at  thread  No.6,  which  is  



close to the crack location in Fig. 1 1 (b).        



  Figure   18  (d)  (e)  show  the  endurance  limit  diagrams  for  α=0  and  α=αsmall .  By  


changing  8-thread-model  to  6-thread-model  B,  the  most  dangerous  thread  for  α=0  is  


changed from thread No.1 to thread No.2. For  α=αsmall , thread No.6 becomes the most  

                                  

dangerous, corresponding to Fig. 1 1 (b). It is seen that the 6-thread-model B is useful to  


consider  the  chamfered  nut  threads  at  both  ends  in  order  to  explain  the  experimental  


results.            


  One may think that replicating the actual geometry of chamfered threads in Fig. 16  



(a)  should be used  in the  FE model. However, the chamfered angle is not always the  

same.  And  the  difference  between  the  results  for  model  B  and  the  chamfered  model  



with actual geometry  is not very large for  α ＝αsmall because threads No.1 and No.8 are  





not in contact. Only the largest difference appears at thread No. 1 for  α ＝0 because for  



model B there are no threads in contact  at No.1 thread.  In this study, therefore, simple  



incomplete  thread  model  B  has  been  used  because  our  main  target  is  to  analyze  the  



model having pitch difference  α. The results of the chamfered model for standard bolt- 



nut α ＝0 are indicated in appendix A.    


7 Suitable Pitch Difference    



  The  main  goal  of  this  study  is  to  find  out  a  suitable  pitch  difference  in  order  to  



improve  both  anti-loosening  effect  and  fatigue  life.  Figure  19  shows  a  schematic  



illustration of the fatigue life improvement and anti-loosening improvement by varying  



the pitch difference when the results of α=0 are regarded as the reference level. On one  



hand, to improve the fatigue life, the most desirable pitch difference may  be close to  


αsmall   as   shown   in   Fig.   10.   On   the   other   hand,   to   improve   the   anti-loosening  


performance, the most desirable pitch difference should be larger than  αmiddle  and close  



to αlarge  as shown in Table 4, although the nut locking phenomenon may happen if α is  





over αverylarge . Therefore, a suitable range for α can be indicated as shown in Fig. 19.  



  In  this  study,  the  bolt  material  SCM435  and  nut  material  S45C  are  assumed.  The  


                                  

stress-strain  curves  are  indicated  in  Fig.  4.  This  design  can  be  applied  to  bolt-nut  





connections made in other materials which have suitable elastic-plastic properties since  



the plastic deformation is required in order to realize the anti-loosening performance.  


8 Conclusions   



  In  this  study,  a  slight  pitch  difference  α  was  considered  for  the  M16  bolt-nut  




connections.  The  loosening  experiments  as  well  as  the  fatigue  experiments  were  



conducted  under  different  pitch  differences.  Finite  element  analysis  was  used  to  



investigate the stress and deformation at the bolt threads and the fatigue strength. The  



conclusions can be summarized as follows:  



(1) Considering both the anti-loosening performance and the clamping ability, α=αmiddle  



is found to be the most desirable pitch difference. This is because the nuts did not drop  



for α=αmiddle without losing clamping ability.  



(2)  The  anti-loosening  experiments  show  that  the  nuts  did  not  drop  for  α=αlarge,  but  




  



clamping  ability  is  deteriorated.  FEA  shows  that  for  α=αverylarge,  the  large  plastic  



deformation happens at threads of nut.    



(3) It is found that  α=αsmall  is the most desirable pitch difference to extend the fatigue  



life  of  the  bolt-nut  connection.  Compared  with  the  standard  bolt-nut  connection,  the  


fatigue life for α=αsmall can be extended to about 1.5 times.  


(4) The 6-thread-model  as shown in Fig. 18  is useful for analyzing 8-thread-nut model  





because nuts always have chamfered threads at both ends. Then, the results are  in good  


agreement with the experimental results.   



(5)  A  suitable  pitch  difference  to  improve  both anti-loosening  and  fatigue  life  can  be  

                                     

illustrated as shown in Fig. 19.   



  The  errors  and  uncertainties  associated  with  the  measurements  or  predictions  are  


                                    

always of concern in a study of this nature. In the loosening experiment, two specimens  



with the same pitch difference were tested together in  order to avoid the uncertainties.  



In the fatigue experiment in Fig. 10, the S-N  curves may have variations but they are  



distinct depending on the pitch difference. In the  axisymmetric FE modelling may have  




some errors but  previously one of the authors have compared the load distributions in  

bolt threads between the axisymmetric modelling and the three-dimensional modelling.  


Then, the relative errors between the two models are found to be less than 12% [28].  




    



Appendix A: The results for chamfered model  



    Figure A1 shows the chamfered model replicating the actual geometry in Fig. 16 (a).  



Figure A2 shows the results of the chamfered model in comparison with the results of  



the complete thread model in Fig. 13 when α=0. It is seen that because of no contact at  



thread  No.8 in the chamfered model,  mean  stress  σ    and stress amplitude  σ   increase  





except at thread No.1. Since the rigidity of nut thread No.1 decreases in the chamfered  




  



model, the stress at bolt thread No.1 does not change very much.  



Acknowledgments  



  The authors wish to express their thanks to the member of their group, Mr. Yu-Ichiro  


Akaishi and Mr. Yang Yu for their assistance in this study. The authors acknowledge  



the international collaboration grant funded by Commissie Wetenschappelijk Onderzoek  



(CWO),  Faculty  of  Engineering  and  Architecture,  Ghent  University.  The  research  for  


this  paper  was  financially  supported  by  the  Japanese  Ministry  of  Education  research  





expenses [grant number 23560164]; and Kitakyushu Foundation for the Advancement  



of Industry Science and Technology.  






Abstract

Digital Twins are core enablers of smart and autonomous manufacturing systems. Although they strive to represent their physical counterpart as accurately as possible, slight model or data errors will remain. We present an algorithm to compensate for those residual errors through Reinforcement Learning (RL) and data fed back from the manufacturing system. When learning, the Digital Twin acts as teacher and safety policy to ensure minimal performance. We test the algorithm in a sheet metal assembly context, in which locators of the .xture are optimally adjusted for individual assemblies. Our results show a fast adaption and improved performance of the autonomous system. 

INTRODUCTION 

The emergence of Internet of Things, Big Data and cloud computing has given rise to the concept of the Digital Twin in manufacturing. The Digital Twin is seen as a core enabler for smart and autonomous manufacturing systems [1]. In essence, it is an ultra-realistic digital model of the product or system with bidirectional automated data exchange used for simulation, optimization and control. Irregardless of its accuracy, slight model or data errors will remain, due to limitations in modelling and data capturing. These resid-ual errors create an improvement potential. As repeatedly pointed out in the literature on Digital Twins, machine learning and Arti.cial Intelligence (AI) could realize those improvements through learning expressive nonlinear models. However, to the best of our knowledge only few publications address how that could be achieved. Jaensch et al. [2] present a generic concept for incorporating learning methods into Digital Twins. Wang et al. [3] and Sapronov et al. [4] tune parameters of the Digital Twin through machine learning. We are interested in generic algorithms that leverage and adapt Digital Twins in the context of control, though. 
In this context, we study requirements on an AI solution to compensate for remaining model or data errors. We propose an algorithm based on Reinforcement Learning to adapt the Digital Twin¡¯s control policy derived from erroneous models. To that end, we require minimal performance of the learning algorithm to address safety and quality concerns. We show its application to a case of industrial size. Our results indicate a fast realization of the improvement potentials, and overall an increased performance (see Figure 1). The main contributions of this paper are: the discussion and demonstration of using


Fig. 1. The performance of the proposed EDiT Reinforcement Learning algorithm applied to an industrial problem. Two sheet metal parts of varying geometry need to be joined through spot welding. The 12 locators of the .xture can be adjusted for each individual assembly to improve the resulting part geometry. The performance of the Digital Twin¡¯s default control policy is based on optimizing a Finite Element model and used as a baseline (black) in this plot. Our EDiT agent can not perform worse than 0.5% of the default control policy at any time, depicted as the exploration constraint (red). After an initial exploration phase, our EDiT agent improves upon the Digital Twin¡¯s default policy, leading to an enhanced overall performance. 
deep learning to enhance Digital Twins; and a novel deep Reinforcement Learning algorithm for enhancing Digital Twins. 
The paper is structured as follows: Section II elaborates on the concept of Digital Twins and the identi.ed research gap; furthermore, Reinforcement Learning and its reduced formulation of Contextual Bandits are discussed; Section III presents necessary considerations on, and our solution to, enhancing Digital Twins through RL; Section IV gives results of our experimental test case; and Section V concludes the paper. 
II. PRELIMINARIES 

A. Digital Twins 
The term Digital Twin originates from a NASA technology report, in which a Digital Twin is de.ned as: ¡°an integrated multiphysics, mulitscale simulation of a vehicle or system that uses the best available physical models, sensor updates, .eet history, etc. to mirror the life of its corresponding .ying twin [5].¡± As such, the Digital Twin is both a collection of algorithms and a data structure of high .delity. In the manufacturing context, researchers are yet to agree on a common de.nition of the concept [6]. This may be due to most publications being either conceptual work or case studies [6]. Another reason may be the broad application area of Digital Twins to all phases of the product life cycle: design, engineering, manufacturing, operation and service [7], [8]. In manufacturing, the application to production planning and control dominates the available literature [6]. We, too, focus on production planning and control in this paper. 
A number of challenges present themselves in realizing a Digital Twin. One is understanding and modelling all relevant physical phenomena, as well as exploring their potentially massive state space to uncover any undesired behaviors [9]. Another is the automatic and continuous update of models and model parameters needed over the life cycle of the system [1], [10]. A third is to identify and utilize incon-sistencies between model and real system [10]. Lastly, how deep learning methods can be employed and continuously improved in the context of Digital Twins is a key challenge that needs to be addressed [8]. 
For the remainder of this paper, we assume the Digital Twin to be a high-.delity, but ¨C due to above¡¯s challenges 
¨C ultimately inaccurate digital model, used for simulation, control and optimization, with a bidirectional, automated data exchange to the physical counterpart. We hypothesize what types of data are exchanged. If the Digital Twin contains algorithms for optimization, decision making and control, data that in.uence the state of the physical system represent the sole sensible type of data sent from the Digital Twin to the physical system. We refer to this type of data as control actions. In the opposite direction, sent data are either state updates or feedback signals. Since the Digital Twin tracks all information about the physical system, changes in the state of the system must be transmitted to the twin to keep them synchronized. Feedback signals that re.ect the goodness of control actions may be viewed as a sub-type of state updates. 
Due to the characteristics of the data link, any control method would suit the concept of Digital Twins. However, a method, capable of leveraging the comprehensive informa-tion available in the Digital Twin and capable of handling potentially nonlinear behavior of the system, is preferred. Reinforcement Learning lends itself as such method and also as a deep learning method that could address the key challenge of enhancing Digital Twins through learning. 
B. Reinforcement Learning 
Reinforcement Learning is inspired by the way humans learn. The learning agent observes the state xt ¡ÊX of the environment, decides on a control action ut ¡ÊU that alters the state of the environment, possibly receives a reward r according to some reward function R(x,u), and observes the new state xt+1 ¡ÊX of the environment. Over time, it will learn to distinguish good from bad actions. 
More formally, the underlying model is a Markov Decision Process, or MDP in short. An MDP is de.ned through the tuple X,U,R,T,¦Ã, where X is the space of states, U is the space of actions, Ris the function of rewards rt = R(xt,ut), T is the transition probability function T(xt,ut,xt+1)= p(xt+1|xt,ut), and ¦Ã ¡Ê[0,1) is the discount factor. An important property of the state is the Markov Property, meaning the state must contain all information necessary to predict the associated reward and next state. 

C. Contextual Bandits 
For many learning tasks in manufacturing, the full Rein-forcement Learning setting does not apply. When considering tasks per part basis, the problem formulation may be free of dynamics. In other words, if the system state depends on the particular part that is being processed, but, irregardless of the selected action for that part, the next state is already decided -for instance by the production schedule -, the transition probability function T(xt,ut,xt+1) is obsolete. And with that, the discount factor ¦Ã is also no longer required. This setting is referred to as a Contextual Bandit in literature. 
A Contextual Bandit is de.ned by the tuple X,U,R, where X is the space of states, U is the space of actions, R is the function of rewards rt = R(xt,ut). At each round t, the environment prepares a state xt (also called context), the learner selects and action ut and receives a reward rt. The next state xt+1 prepared by the environment is unrelated to xt and ut. The agent aims to learn learn a policy ¦Ð : X

that satis.es the optimal value function: 


In continuous control problems the policy ¦Ð maps states to probability distributions of actions: ¦Ð : X,U
¡úp(U). The value function of a particular action ut in state xt is de.ned by 
Q(xt,ut)= E [R(xt,ut)] . (2) 

D. Function Approximation 
We consider problems of industrial size with high-dimensional continuous state and action spaces. In such cases, the number of states and actions is intractable and too large for a discrete, tabular representation of the value functions V and Q, and the policy ¦Ð. To handle that problem, function approximation has been proven successful [11]. More speci.cally, we consider arti.cial neural networks for our context, due to their expressiveness. 
Neural networks a class of nonlinear function ap-proximators, consisting of M linear combinations of input variables ¦Õ1,...,¦ÕD (e.g. parameters of the state xt) and weights ¦È1,1,...,¦ÈM,D. The linear combinations are com-plemented by a nonlinear, differentiable activation function h(¡¤) 


where j =1,...,M, and ¦Èj,0 is the bias [12]. The outputs y1,...,yM of the network layer can be in turn the inputs to a subsequent layer. The neural network learns via minimizing a loss function L(¦È). The gradients of the loss function are back propagated through the network to tune its weights. 
Digital Twin 

Fig. 2. The architecture of our EDiT algorithm. The Digital Twin observes a state xt and decides on a control action dt based on its default policy ¦Ðd. Our RL algorithm EDiT observes, both, xt and dt. It decides then whether to apply dt or ut = ¦Ða(xt) to the physical system G. The system then generates a feedback signal (reward) rt and a next state xt+1 that is observed by the Digital Twin. rt is used to improve the EDiT policy ¦Ða. 
III. ALGORITHM 
Based on the reduced Reinforcement Learning problem formulation of Contextual Bandits, we propose the deep learning algorithm EDiT for Enhancing Digital Twins. While introducing the algorithm, we elaborate on key re-quirements on an AI solution aiming at compensating for model inaccuracies of the Digital Twin in control and op-timization. For ease of understanding, Figure 2 depicts the schema of the algorithm¡¯s architecture. 
A. Policy Function 
Deep Reinforcement Learning allows for continuous con-trol policies1, needed for our application case. Assuming a normally distributed policy distribution, we propose a neural network to approximate mean ¦Ì and standard deviation ¦Ò of the policy function ¦Ða, such that 

where Nis a multivariate Normal distribution in U. This can be interpreted as the network outputting deterministic values of the action (¦Ì) and adding some normally distributed exploration noise ¦Ò to it. 
For better training stability and sample ef.ciency, we use a trust region policy optimization method [13], [14]. In particular, we apply Proximal Policy Optimization [15] and update ¦Ða by minimizing the loss 
where A.t = Q.(xt,ut) .V.(xt) is the advantage function estimate, E is a small trust region parameter, and ft(¦È) is the fraction 

ft is used as a replacement of the logprobability of the policy that is used in policy gradient methods. Clipping 
1Although we present only the continuous case here, the EDiT algorithm can easily be adapted to the discrete case, in which the policy would be a discrete probability distribution. 

ft at a lower or higher bound has the effect of bounding the loss. In the minimization step, that bound limits the gradients, resulting in smaller policy updates such that the policy remains proximally in the trust region. Intuitively, this method mitigates the risk of destructive policy updates that move ft too far away from 1. For further details on this method, we refer the reader to [15]. 
B. Value Functions 
To estimate the advantage of an action under the current policy that is required for updating the policy, we need to approximate its value function V.(xt). As loss function for the value function network, we choose the mean square error 


where N is the batch size of training data, and c a small regularization constant for the L2-norm of the weights ¦È. Similarly, the loss of the advantage function A.¦È(xt,ut) is 


(8) Since the network V¦ÈV is, both, being updated as well as being used to compute the target values for A¦ÈA ,we 

maintain a copy V¦ÈVÂ¬ of the network V¦ÈV used for the target 
computations. The weights ¦ÈVÂ¬ of the copy are updated using 


a soft target update ¦ÈV¡û¦Ó¦ÈV +(1 .¦Ó)¦ÈV, where ¦Ó is a small value. This has been shown to stabilize the convergence during training [16], [17] -an important property when learning online. 
C. Safe Exploration 
In a manufacturing context, performance of the control policy at any time is paramount. When learning, the agent must explore the action space to improve its potentially sub-optimal policy. This process may lead to catastrophic errors. While this is undesirable in any real-world context, in manufacturing it causes an additional economic cost. We are thus interested in safe exploration strategies that maintain on average with high probability a given level of performance during learning. 
Garcia and Fernandez [18] identi.ed Teacher Advice as common approach to incorporate external knowledge in the exploration process to make the same safer. With the availability of the Digital Twin, have to 
default policy ¦Ðd that can be regarded as such teacher advice. The default policy ¦Ðd is the original control policy of the Digital Twin, before we apply deep learning to compensate for model inaccuracies. This default policy may be sub-optimal, but arguably superior to the agent¡¯s policy ¦Ða in the initial learning period. A problem formulation similar to Wu et al. [19] then suits our manufacturing case. Accordingly, 
de.ne a cumulative performance constraint for safe exploration: For all rounds t, the sum of rewards ru of the agent¡¯s policy ¦Ða can at most be worse by a fraction ¦Áof the sum of rewards rd of the default policy ¦Ðd, formally written as

..
D. Bayesian Neural Networks 
By themselves, neural networks do not compute con.-dence bounds on their predictions. However, Gal and Ghahra-mani [20] have shown that dropout can be used as a Bayesian approximation. Dropout refers to the technique of randomly disabling units of a neural network layer. Originally used as a method to mitigate over-.tting, it can be used to approximate a Gaussian Process. Con.dence bounds can then be derived from the statistics of a Monte Carlo sample. The intuition behind using dropout as a Bayesian approximation is that the variance ¦Ò.2 of the sample will be high in regions of low data density and lower in regions with an abundance of available data. 
We propose to apply this technique to the advantage network A to compute the required values of Q.UCB and Q.LCB. For that purpose, we sample S times from A(xt,ut) 
and compute 
Q.LCB/UCB(xt,ut)= V.(xt) .3.¦ÒS A.(xt,ut), (15) 

where ¦Ò.S is the standard deviation of the drawn samples S. 
E. Sample Ef.ciency 
In the context of manufacturing, each data sample may represent a particular physical part. Given that the num-ber of samples is limited, we wish to be ef.cient with the data and learn quickly. We store all collected data samples xt,ut,rt,¦Ða(ut|xt)in a data structure D and repeatedly retrain a random subset of D to improve sampleef.ciency.Intonhat, we take inspiration from [21] and keep demonstrations of the Digital Twin¡¯s default policy ¦Ðd separate. When sampling a minibatch from D, we ensure that a small percentage of the samples originates from those demonstrations. Through that mechanism, the default policy is constantly kept present in the learner, helping with training stability. 
F. EDiT Algorithm 
Drawing on the previous subsections, the full EDiT al-gorithm presents itself as listed in Algorithm 1. In eachroundt,EDiTobservenows the state xt, and the action dt proposed by the default policy. We then sample K-times from the policy distribution ¦Ð to build up a set of possible actions ut,1,...,ut,K for which we compute the mean action u¡¥t. This results in a sample bias for u¡¥t that allows for exploration. u¡¥t will be applied to the system as long as the lower bound on the expected reward Q.LCB(xt,u¡¥t) is within the exploration budget. Otherwise, dt will be applied. We store the resulting data and update our neural networks on a minibatch sample of the replay buffer D. 

EXPERIMENTAL RESULTS 


computes adjustments of the .xture locators based on a Finite Element Analysis; and the adjustments are applied to the physical system to improve the geometrical quality of the .nal assembly. A pre-study to using Reinforcement Learning for adapting the Digital Twin policy to the real system in this context can be found in [23]. 
Our particular test case consists of two sheet metal parts of a car body shown in Figure 3. The geometry of the parts is given by their point clouds of ¡«2.5k points each and represent our state xt. We evaluate our algorithm on 250 part instances. Each of the .xture¡¯s 12 locators are adjustable along their axis in the range [.2,2] and represent our action space U. As reward function R, we de.ne the relative perfor-mance of the agent compared to the default policy in terms of the root mean square error of the resulting assembly from the nominal assembly: rt ¡«RMSE(xt,ut)/RMSE(xt,dt). The default policy of the Digital Twin is derived from the outcome of a Genetic Algorithm on the nominal model [24]. To emulate the real system, we use a slightly perturbed version of the nominal model. 
A. Implementation Details 
We .rst transform the point cloud of the parts through a Principal Component Analysis, and choose the 128 points as new state representation that explain most of the point cloud¡¯s variance. The size of the neural networks and related parameters are given in Table 1. To enable distributed pro-cessing for Digital Twin, EDiT and system emulation, we use Apache Kafka as asynchronous publish/subscribe messaging bus. 


Fig. 3. The test case of our algorithm. Two sheet metal parts (green and yellow) are clamped into position for a subsequent spot welding operation. The 12 locators of the .xture, shown in red here, can be adjusted along their axis to improve geometry of the welded assembly. Our EDiT agent must .nd adjustments for all 12 locators simultaneously that improve upon the optimal adjustments calculated by the Digital Twin¡¯s default policy ¦Ðd for the nominal model. 

B. Results 
We test the proposed EDiT algorithm over 10k rounds and track the cumulative improvement over the twin¡¯s default policy ¦Ðd in terms of accumulated rewards. Figure 1 depicts the outcome of 10 repetitions of the experiment. Overall, we see an improvement over the default policy. In the best case, this improvement is realized just after a few rounds. In the worst case, EDiT requires up to 2k rounds of exploration until improving upon the default policy. In average, though, we see an improvement after a few hundred rounds. 
The mean reward of EDiT over all 10k rounds and 10 repetitions is about 1.0025 as listed in Table II. This corresponds to an average improvement of 0.25%.Wedo not expect EDiT¡¯s learned policy to be the optimal one, yet. The state-action space is rather large in our case, due to the 
12-dimensional actions. We expect EDiT to take many more rounds in this case, before the optimal policy is fully learned. 
We further notice a number of violations of the safety constraints (see Table II). Most notably, the per part con-straint (Eq. 13) is violated 207 times in average over 10k rounds, meaning the received reward ru was below 95% of the Digital Twin¡¯s expected performance. This is due to two inaccuracies in Q: the estimation error of inter-/extrapolating, and the approximation error of the lower con.dence bound (LCB) through dropout (refer to Subsection III-D). An al-ternative implementation of Bayesian neural networks (e.g. ensembles) might reduce the number of per part violations. The cumulative performance constraint (Eq. 9), on the other hand, is in average only twice violated and often not at all. 
V. CONCLUSIONS 
This paper has shown that Digital Twins, used for con-trol of manufacturing processes, can be adapted through Reinforcement Learning. It has also been demonstrated that Contextual Bandits, a reduced formulation of Reinforcement Learning, are suitable for applications within the manufactur-ing context. Based on that, we have introduced the learning algorithm EDiT for enhancing the control policy of Digital Twins in continuous domains. It utilizes the Digital Twin as safety policy to maintain a constraint imposed on the learners performance. EDiT combines recent safe RL and deep learning methods, such as Bayesian neural networks and Proximal Policy Optimization. 
Future research directions include extensions of the algo-rithm to improve safety and sample-ef.ciency. The behaviour of deep neural network estimates can be unpredictable while learning. Although we employ a performance constraint and Bayesian neural networks to estimate uncertainty, we see further safety guarantees needed for the application of deep Reinforcement Learning in industry. Imbalanced fault diagnosis of rotating machinery via multi-domain feature extraction and cost-sensitive learning 
Qifa Xu1,2 ¡¤ Shixiang Lu1 ¡¤ Weiyin Jia3 ¡¤ Cuixia Jiang1 

Received: 2 August 2019 / Accepted: 7 December 2019 / Published online: 14 December 2019 . Springer Science+Business Media, LLC, part of Springer Nature 2019 
Abstract 

Fault diagnosis plays an essential role in rotating machinery manufacturing systems to reduce their maintenance costs. How to improve diagnosis accuracy remains an open issue. To this end, we develop a novel framework through combined use of multi-domain vibration feature extraction, feature selection and cost-sensitive learning method. First, we extract time-domain, frequency-domain, and time-frequency-domain features to make full use of vibration signals. Second, a feature selection technique is employed to obtain a feature subset with good generalization properties, by simultaneously measuring the relevance and redundancy of features. Third, a cost-sensitive learning method is designed for a classi.er to effectively learn the discriminating boundaries, with an extremely imbalanced distribution of fault instances. For illustration, a real-world dataset of rotating machinery collected from an oil re.nery in China is utilized. The extensive experiments have demonstrated that our multi-domain feature extraction and feature selection can signi.cantly improve the diagnosis accuracy. Meanwhile, our cost-sensitive learning method consistently outperforms the traditional classi.ers such as support vector machine (SVM), gradient boosting decision tree (GBDT), etc., and even better than the classi.cation method calibrated by six popular imbalanced data resampling algorithms, such as the Synthetic Minority Over-sampling Technique (SMOTE) and the Adaptive Synthetic sampling method (ADASYN), in terms of decreasing missed alarms and reducing the average cost. Owing to its high evaluation scores and low average misclassi.cation cost, cost-sensitive GBDT (CS-GBDT) is preferred for imbalanced fault diagnosis in practice. 
Keywords Rotating machinery ¡¤ Fault diagnosis ¡¤ Imbalanced classi.cation ¡¤ Feature extraction ¡¤ Cost-sensitive learning 
B Cuixia Jiang jiangcuixia@hfut.edu.cn 
Qifa Xu 
xuqifa@hfut.edu.cn 
Shixiang Lu lushixiang@mail.hfut.edu.cn 
Weiyin Jia weiyin.jia@ronds.com.cn 
1 School of Management, Hefei University of Technology, Hefei 230009, Anhui, People¡¯s Republic of China 
2 Key Laboratory of Process Optimization and Intelligent Decision-making, Ministry of Education, Hefei 230009, Anhui, People¡¯s Republic of China 
3 Anhui Ronds Science & Technology Incorporated Company, Hefei 230088, People¡¯s Republic of China 



Introduction 
The breakthrough development of AI technology and mil-lions of machineries equipped with smart sensors are accel-erating the transformation from traditional manufacturing industry towards smart manufacturing (Dou et al. 2018;Liu et al. 2018b). Rotating machinery fault diagnosis plays an indispensable role in smart manufacturing, and the demand for effective diagnosis of its operation condition is increas-ing rapidly. With a large number of high quality and reliable real-time equipment operation data collected, it enables to construct an automatic monitoring, intelligent diagnosis and prognosis system of rotating machinery, which can reduce the maintenance cost signi.cantly (Tao et al. 2018;Wuetal. 2019a; S¨¢nchez et al. 2018). 
There are numerous useful data acquisition techniques adopted in the fault diagnosis of rotating machinery, includ-ingvibrationanalysis,oilanalysis,acousticemissionmethod, temperature monitoring, and microwave .aw detection. In 
123 

practice, vibration analysis is popular for its solid theoreti-cal foundation and mature measurement tool (Ben Ali et al. 2018;ZhaoandLin 2018;Ganetal. 2018).However,thehigh frequency dynamic signal of rotating machinery is usually a superposition of multiple components of different ampli-tudes, and presents non-stationarity and non-linearity (Zhao et al. 2019; Amrhein et al. 2016). Thus, it calls for more effective and robust methods to extract features from vibra-tion signals. 
Apart from feature extraction, designing an irreplaceable fault diagnosis method is another crucial step in rotating machinery diagnosis. Fault diagnosis methods can be gener-ally categorized into two types: mechanism-based methods and data-driven methods (Ren et al. 2018; Han et al. 2019a). Mechanism-based methods are employed only when accu-rate mathematical models of the failure can be built (Wu et al. 2019b). In fact, they have been greatly limited in the diagnosis of rotating machinery, owing to the complexity of their internal structure and the diversity of the external oper-ating environment. On the other hand, data-driven methods are especially powerful for the complex industrial processes, since they directly use condition monitoring data to infer mechanical failure without any assumption on the underly-ing failure mechanism (Kang 2018; Tidriri et al. 2016; Wang et al. 2019). Although data-driven methods are effective in fault diagnosis, most studies are far from the actual operat-ing conditions of rotating machinery, because the balanced datasets they used are too few to get (Santos et al. 2015). 
In practical industrial applications, the machinery works in a normal condition throughout the whole operation cycle, and there are seldom faults happening in its operating phases (Jia et al. 2018), which means that fault instances are seriously insuf.cient (Han et al. 2019a; Liu et al. 2018a; Jiang et al. 2019; Sun et al. 2007). The .rst China Indus-trial Big Data Innovation Competition in 2017 (http://www. industrial-bigdata.com) makes them more speci.c, e.g., the open access data contains 300,000 instances and the ratio of fault/normal is about 1/10, which is a typical imbalanced classi.cation problem. Under the assumption of equal mis-classi.cation cost or balanced class distribution, traditional classi.ers lose their ability to deal with rare classes (Jia et al. 2018; Zhang et al. 2019;Lietal. 2020). In the fault diagno-sis of rotating machinery, missing the detection of a failure condition, a rare class in mechanical operation may cause a catastrophic accident, whereas misclassifying a normal con-dition as a failure can be veri.ed by manual checking easily (Zhangetal. 2018).Thisremindsusthemisclassi.cationcost differs signi.cantly between two different types of errors. Much more attention should be paid to missed alarm for its greater cost of misclassi.cation than false alarm (Kuo et al. 2018; Han et al. 2019b; Zan et al. 2019). 
In this paper, we propose a novel data-driven framework for rotating machinery diagnosis, comprising comprehen-sively extracting and selecting of a series of features from collected vibration signals, and effectively diagnosing the imbalanced operating condition of rotating machinery via cost-sensitive learning. For this purpose, multiple features are extracted through the combination of time-domain, frequency-domain, and time-frequency-domain methods to discover useful information from vibration signals. Then, feature selection reduces the extracted feature set to a more compact one. Additionally, the cost-sensitive learn-ing method takes misclassi.cation costs into consideration to calibrate the classi.cation results, which aims to minimize the average cost. To illustrate the ef.cacy of our method, extensive experiments are conducted on a real-world dataset of rotating machinery collected from an oil re.nery in China. With the same feature selection technique, our multi-domain feature extraction method is compared with several previ-ous feature extraction studies and it has been proved that our method can signi.cantly improve the accuracy of fault diagnosis. Meanwhile, our cost-sensitive learning shows its great advantages in reducing average misclassi.cation cost, compared with the cost-insensitive learning (classi.ers treat each type misclassi.cation cost equally) and the classi.-cation methods calibrated by imbalanced data resampling. Moreover, a sensitivity experiment shows that the classi.ca-tion performance of our cost-sensitive learning method will be affected by different cost matrices. It is noteworthy that CS-GBDT (cost-sensitive gradient boosting decision tree) is preferred in imbalanced fault diagnosis of rotating machin-ery, for its relatively high evaluation scores and low average misclassi.cation cost across different cost matrices. 

The remainder of this paper proceeds as follows. ¡°Related works¡± section brie.y reviews some related works. ¡°The framework¡±sectionpresentstheproposedframeworkinclud-ing multi-domain feature extraction, feature selection and cost-sensitive learning method. In ¡°The real-world applica-tion¡± section, we apply the novel framework to a real-world application and illustrate its superiority through extensive comparisons. ¡°Conclusions¡± section summarizes and con-cludes the paper. 

Related works 
The fault diagnosis procedure can be roughly divided into two steps: extracting robust features and diagnosing the con-dition of the machine (Song et al. 2018). In the step of feature extraction, after the properties of the vibration sig-nals being well-understood, researchers could employ their domain knowledge in signal processing to design suitable features. In the second step, some new classi.cation tech-niques have been developed to effectively diagnose the fault type of rotating machinery with imbalanced data. 
123 

Feature extraction from vibration signals 
Vibration signals are an important information source for feature extraction in fault diagnosis because they contain high frequency and time-varying information. Speci.cally, feature extraction from vibration signals can be character-ized into three domains: time-domain, frequency-domain, and time-frequency-domain. 
First, in the time-domain, different statistical features are extracted to represent how signal amplitude varies with respect to time. However, vibration signals produced by a machine contain many components and are often dif.-cult to be observed in the time-domain. Therefore, it is unlikely that a fault will be detected by a simple visual inspection. Second, frequency-domain analysis is to extract features in a given frequency band, and fast Fourier trans-form (FFT) is considered as one of the most commonly used feature extraction technique (Seera et al. 2014). In addition, amplitude spectrum (Larsson et al. 2002), power spectrum (Jiang et al. 2018) and cepstrum (Hwang et al. 2009) are often used in some frequency-domain scenarios as well. Unfortunately, the transition from time-domain to frequency-domain is based on the hypothesis of stationarity, which is often violated in the stage of mechanical failure (Tao et al. 2018). Third, time-frequency-domain analysis jointly representing the information from both time-domain andfrequency-domain,isgoodatdealingwithnon-stationary signals. The two most powerful and extensively used tech-niques are wavelet transform (WT) (Wang et al. 2017) and empirical mode decomposition (EMD) (Georgoulas et al. 2013).The remarkablemerit of WT is that it has a good local-ization property in both time-domain and frequency-domain. It can automatically adjust the scale of the frequency compo-nents, so as to observe and analyze the arbitrary details of the signals. EDM depicts the oscillation structure and frequency component of each part of the signals by decomposing the signal into a set of orthogonal complete intrinsic mode func-tions (IMFs). However, their common drawback is the lack of a translation-invariant property in the vibration signal pro-cessing, thus generating some false components (Ciabattoni et al. 2018). 
Moreover, considering that different domains have their own advantages and disadvantages, one starts to combine multiple domain features for effective fault diagnosis. For example, Zhang et al. (2018) and Ragab et al. (2019) utilize time-domain and time-frequency-domain features to clas-sify multiple failure modes of rotating machinery. Zhang et al. (2012) extracts time-domain and frequency-domain features to diagnose different failure stages of a wind tur-bine gearbox. However, the combination only involves two domains and is relatively subjective without any convincing explanation (Ben Ali et al. 2018). Seldom literatures extract features from three domains to make full use of the mean-ingful information contained in vibration signals (Wu et al. 2019b). 


Classification for imbalanced data 
Accordingtothedifferentstagesofintroducingdatahandling techniques, the methods for imbalanced data classi.cation can be grouped into three categories: prior training, during training, and after training methods. 
The .rst type of methods deal with imbalanced data though changing the distribution of training sample, such as manually rebalancing training sample by over-sampling minority instances or under-sampling majority instances (Zhang et al. 2019; Xie et al. 2019), and adjusting the train-ing sample by the misclassi.cation cost of the instances. These methods are often used to convert the arbitrary cost-insensitive classi.ers into a cost-sensitive equivalence, without modifying theunderlying learningalgorithm. Unlike the .rst type, the second one directly modi.es the learning procedure to improve the sensitivity of the classi.er toward minority classes (Mathew et al. 2018; Correa Bahnsen et al. 2015). However, this type of methods designing different algorithms according to different classi.ers are neither ef.-cient nor .exible in practice (Lee et al. 2012). The third type concerns only the estimating membership class probabili-ties generated by cost-insensitive classi.ers, assigning each instance to its lowest risk class based on Bayes risk theory, such as MetaCost (Domingos 1999) and Bayes minimum risk (Khan et al. 2018). Although these methods have been applied to many classi.ers achieving the bias towards the minority but costly classes, they are restricted and require the cost-insensitive classi.ers that can produce accurate pos-teriori probability estimations of the training instances (Lee et al. 2012). 
To sum up, owing to their .exibility and not limited by the posterior probability of the classi.er, the .rst type of methods are more suitable for industrial equipment fault diagnosis (Zhang et al. 2018). There are two different strategies for changing the distribution of training sam-ple: (1) converting the original imbalanced training sample into a balanced one. The random under-sampling, ran-dom over-sampling, synthetic minority over-sampling tech-nique (SMOTE) (Chawla et al. 2002), SMOTE-borderline1, SMOTE-borderline2 (Mathew et al. 2018; Han et al. 2005) and the adaptive synthetic sampling method (ADASYN) (Haibo et al. 2008) are classical resampling methods. How-ever, these methods do not take into account the unequal costs imposed by the imbalanced distributions and may have dif.culty in achieving unbiased results (Castro and Braga 2013). (2) Generating new training sample according to the unequal misclassi.cation costs of instances. Zadrozny et al. (2003) designs two different sampling methods: sampling-with-replacement and cost-proportionate rejection sampling. 
123 

However, the proportional sampling-with-replacement pro-duces duplicated instances in the training process, resulting in over-.tting in model building. On the contrary, ¡°rejec-tion sampling¡± is able to avoid duplication. Despite cost-proportionate rejection sampling has shown its strength in product recommendations (Lee et al. 2012), credit card fraud detection (Correa Bahnsen et al. 2015), and biomedical engi-neering (Gardner and Xiong 2009), its application is very rare or absent in the manufacturing domain to the best of our knowledge. 



The framework 
The purpose of this paper is to develop a new framework for fault diagnosis of rotating machinery. As shown in Fig. 1, the whole framework involves three parts: (1) multi-domain feature extraction for maximizing the information value of vibration data,(2) feature selection foreliminating redundant features and identifying effective ones, and (3) cost-sensitive learning for minimizing misclassi.cation costs. All of them play an indispensable role in actual fault diagnosis. 

Feature extraction 
To make full use of the information in the raw vibration signals, we decompose the high-frequency dynamic signals into time-domain, frequency-domain, and time-frequency-domain features, respectively. 
Time-domain feature extraction 
Time-domain analysis characterizes the vibration change of rotating machinery when a fault occurs by directly construct-ing statistics from the vibration waveform. In this paper, we produce .ve widely used statistical features, namely, mean value (MV), root-mean-square value (RMSV), skew-ness coef.cient (SC), kurtosis coef.cient (KC), and shape factor (SF). The detailed calculations are shown in Table 1, where z(t) is the signal at time t and Nz is the sample size. 
Table 1 Time-domain statistical features 
Statistical feature De.nition 
1 Nz
MV MVz = t=1 z(t)
Nz 
 
1 Nz
RMSV RMSVz = Nz t=1 z2(t) Nz
SC SCz = 1 t=1 (z(t) . MVz)3 
RMSV3 
z 1 Nz 4
KC KCz = t=1 (z(t) . MVz)
RMSV4 
z RMSVz
SF SFz = 1 Nz Nz 
t=1 |z(t)| 


Frequency-domain feature extraction 
In frequency-domain analysis, spectrum analysis and enve-lope spectrum analysis are two widely used methods. Spec-trum analysis is to transform the vibration signals of rotating machinery from time-domain to frequency-domain, so that it can detect the corresponding fault characteristic of fre-quency components. FFT technique plays an important role in extracting spectral features, which is de.ned as 
 +¡Þ 

z( f) = z(t)e. j2¦Ð ftdt, (1) 
.¡Þ 

where z(t) denotes time-domain signal at time t, z( f) denotes frequency-domain signal at frequency f. 
Envelope spectrum is a curve formed by connecting the peak points of amplitudes at different frequencies during a time period. As acclaimed in Jiang et al. (2016), when the local damages or defects of the rotating machinery happen, the process of the load will produce mutational decaying shockpulseforceand high frequency natural vibration. Thus, the .nal vibration waveform of rotating machinery is repre-sented as a complex amplitude modulation wave. Envelope demodulation can be realized by Hilbert transform (HT) and then FFT. The HT of a signal is de.ned as 
 +¡Þ
1 z(¦Ó ) 
H[z(t)]= d¦Ó. (2)
¦Ð t . ¦Ó
.¡Þ 

We combine z(t) and H[z(t)] to form a new analytic signal 
g(t) = z(t) + jH[z(t)]. (3) 
The amplitude A(t) of g(t) is then obtained by 
A(t) = z(t)2 + H[z(t)]2 . (4) 

After HT, envelope spectra could be obtained by FFT to extract the feature of defect frequencies. 
Through the above spectrum analysis and envelope spec-trumanalysis,weextract56frequency-domainfeatures,such as root-mean-square frequency (RMSF), root variance fre-quency (RVF), spectral kurtosis (SK). We do not present all of them here to save limited space. 

Time-frequency-domain feature extraction 
In time-frequency-domain, EMD is considered as one of the most powerful techniques to extract features of rotating machinery. It decomposes the original non-stationary signals into a series of stationary signals indicating the natural oscil-latory mode, which is termed as intrinsic mode functions 
123 


Fig. 1 Framework for fault diagnosis of rotating machinery 
(IMFs). The procedure of intrinsic energy feature extraction from IMFs is described below. 
Step 1, identify all local maxima and minima of the signal z(t) separately, and then connect all the local maxima by a cubic spline line to form the upper envelope. Repeat the same procedure for the local minima points to form the lower envelope. 
Step 2, denote the mean of the upper and lower envelope value as ¦Ì1, and calculate the difference between the signal z(t) and ¦Ì1 as 
¦Ç1(t) = z(t) . ¦Ì1. (5) 
Step 3, determine whether ¦Ç1(t) satis.es: (1) the number of extrema and the number of zero crossings are the same or at most one difference; and (2) its upper and lower envelopes are locally symmetric with respect to zero. If the two require-ments satis.ed, ¦Ç1(t) becomes the .rst IMF component of signal z(t) as 
IMF1(t) = ¦Ç1(t). (6) 
Otherwise, ¦Ç1(t) will be regarded as the raw signal z(t), and return to step 1. Step 4, calculate the residual r1(t) with the form 
r1(t) = z(t) . IMF1(t). (7) 
Stop decomposition if r1(t) is a monotone function. Other-wise, r1(t) is treated as the raw signal z(t). 
Step 5, repeat steps 1 to 4 and obtain all IMFs: IMF1(t), IMF2(t),..., IMFM(t), as well as the .nal residual rM(t). In this regard, the raw signal can be expressed as 
M 


z(t) = IMFj + rM(t), (8) j=1 
where Mdenotes the number of IMFs. 
Step 6, de.ne the intrinsic energy features of rotating machinery as 
L
1  2
Ej = IMFj (ti), (9)
L 
i=1 

where Lrepresents the number of instances in each IMF. 
In general, the fault information of rotating machinery is mainly in high frequency band. As a result, the fault charac-teristic information can be represented by the .rst few IMF components. In this paper, we extract the .rst four IMFs¡¯ energy to represent time-frequency-domain features. 


Feature selection 
From apractical fault diagnosis point of view, some extracted features are redundant or non-signi.cant (Ragab et al. 2019). Directlyusingalltheextractedfeaturesinthemodelmaylead to computation inef.ciency, over-.tting, high maintenance workload, and dif.culty of model interpretation. To address these issues, an effective subset that contains informative features should be selected. 
There are quantities of feature selection methods that can reduce the original feature set to a more compact one. Such methods can be generally categorized into three types: .lter methods, wrapper methods, and embedded methods (Han et al. 2019a; Ding and Peng 2005). In this paper, a .lter method called min-redundancy and max-relevance (mRMR) (Peng etal. 2005) is employed.The advantagesofthe mRMR over the other feature selection methods can be illustrated from two aspects. First, as a .ltering method, mRMR has 
123 

the advantage of computational ef.ciency and the ability to generalize different machine learning models. Out of .l-ter nature, the motivation to adopt mRMR is that it can effectively reduce the redundant features while keeping the relevant features in the model, not perturbing or hiding the physicalmeaningofthefeatures.Therefore,itkeepstheinter-pretability of the proposed diagnosis model, which is very important for the decision maker. 
Actually, mRMR is a mutual-information-based feature selection method that simultaneously measures the relevance and redundancy of features, to obtain a feature subset with good generalization properties. The mutual information used to measure the dependency between two discrete random variables X(1) and X(2), can be expressed as 
(1)(2) 
 p(x ,x )
(1)(2) ij
I(X(1),X(2)) = p(x ,x )log , (10) 
ij (1)(2)
p(x )p(x )
i,j ij 
(1)(2)
where p(x ,x ) denotes the joint probability density of 
ij 
(1)(2)
two random variables X(1) and X(2). p(xi ) and p(xj ) denote the marginal probability density of X(1) and X(2), respectively. 
For features variables, mutual information is used to mea-sure the level of ¡°similarity¡± between the extracted features. The principle of minimum redundancy is to select the fea-tures so that they are mutually maximally dissimilar. The minimum redundancy function can be expressed as 
1 
min WI = I gi ,gj , (11)
|S|2 
(gi ,gj ¡ÊS) 
where S denotes the feature subset we are seeking, |S| is the number of features in S, gi is the ith feature in S. In addition, mutual information is also used to measure the discriminant ability of features to target variable y. I (y,gi )quanti.es the relevance of gi for classi.cation tasks. Thus the maximum relevance function can be expressed as 

1 
max VI = I (y,gi ). (12)
|S| 
(gi ¡ÊS) 
In this paper, we adopt the strategy to combine the two functions into a single criterion one, named the mutual information quotient (MIQ), which maximizes the func-tion max (VI /WI ). Based on this criterion, a small subset is selected from the extracted features using the vibration signal. As we will see later, the 21 selected features show better diagnostic performance. 


Imbalanced cost-sensitive classification 
This subsection designs a cost-sensitive learning method for fault diagnosis of rotating machinery. It consists of a cost matrix that de.nes the cost of misclassi.cation, and a cost-sensitive classi.cation construction that calibrates imbalanced classi.cation results. 
Cost matrix determination 
The effectiveness of cost-sensitive learning method depends strongly on the given cost matrix. Improperly initializing costs are not conducive to the learning process (Zhang and Hu 2014). In other words, too low costs are not enough to adjust the classi.cation boundary, while too high costs may lead to poor generalization capacity of the classi.er on other classes. In this paper, a handcrafted cost matrix based on expert knowledge is recommended to the trade-off. 
In practice, the costs caused by misclassi.cation of differ-ent fault conditions of rotating machinery are heterogeneous. For example, the cost of misclassi.cation of fault condition into normal condition is much greater than a false alarm. Moreover, the misclassi.cation faults of different compo-nents are also different. This usually requires access to industryexpertswhohavetheabilitytoassessthemostrealis-tic cost values. The cost of misclassi.cation given by experts may differ under different fault-tolerant criteria. In order to popularize our methodology, we abstract two basic rules for the cost matrix. 
Rule 1: all costs are nonnegative. The entry of cost matrix C is described as 
Cp,q = 0, p = q

C = (13)
Cp,q ¡Ê N+ , p = q 

where Cp,q represents the cost of misclassifying the class p into the class q. In practice, there are four different situations. 
. 
For Cp,q (p = q), it stands for correct classi.cation. In this paper, we de.ne Cp,q = 0 (p = q). 

. 
For Cp,q (p = 0,q = 1,2,...k), it represents a false alarm, which means classifying a normal condition as a fault condition. 

. 
For Cp,q (p = 1,2,...k,q = 0), it represents a missed alarm, which means classifying a fault condition as a normal condition. 

. 
For Cp,q (p = q, p = 0,q = 0), it means classifying one fault condition as another one. 


Rule 2: the cost of misclassi.cation satis.es 

C0,q < Cp,q < Cp,0, (14) 
123 

where p = q = 0, and the subscript 0 stands for a nor-mal condition. This rule presents the idea that the costs of different misclassi.cations are signi.cantly different from each other, i.e., the cost of false alarm or type I error (C0,q ) is small, as only a con.rmation is required by operator. The cost of missed alarm or type II error (Cp,0) is large, as it may lead to serious damage to equipment or even catastrophe. In addition, misclassifying one fault condition as another one can alert the operator with a failure signal, but it is dif.cult to make an accurate diagnosis. Therefore, Cp,q is between C0,q and Cp,0. 


Cost-sensitive learning by cost-proportionate rejection sampling 
Cost-proportionate rejection sampling processes the training sample through proportional sampling and adjusts the out-come class distribution of the sampling instances. Then a cost-insensitive learning algorithm can be directly applied to the sampled instances. Here we¡¯re just going to give a brief overview of cost-proportionate rejection sampling, and the full details about this algorithm are well presented in Zadrozny et al. (2003). 
In cost-sensitive learning phase, the objective is to learn a classi.er s : X ¡ú Y to minimize the expected cost, 
Ex,y,c¡«D[c ¡¤ I(s(x) = y)], (15) 
where (x, y, c) is the form of given training data, D denotes the distribution with domain X ¡Á Y ¡Á C, X denotes the input space, Y denotes the output space, C denotes the misclassi-.cation costs and I(¡¤) is the indicator function. 
Assuming that there exists a constant Nc = Ex,y,c¡«Dc, the goal of minimizing the expected cost can be transformed into minimizing the ratio of errors under D. , 
Ex,y,c¡«D[c ¡¤ I(s(x) = y)]

= D(x, y, c) ¡¤ c ¡¤ I(s(x) = y) 
x,y,c  , (16) = Nc ¡¤ D. (x, y, c) ¡¤ I(s(x) = y) x,y,c 
= Nc ¡¤ Ex,y,c¡«D.[I(s(x) = y)] 
c
where D. (x, y, c) = Nc D(x, y, c). Speci.cally, the transla-tion theorem in Eq. (16) indicates that each instance in the original training sample (S) is drawn independently once, and accepted into the sample S with the probability c/Zc, where Zc is a prede.ned constant. In this paper, the con-stant Zc is chosen as the maximal misclassi.cation cost. To determine whether to keep an instance, the algorithm .rst generates a random number rv from the uniform distribu-tion U(0, 1), and compares it with the acceptance probability ci /Zc of the instance under evaluation. The instance i is accepted when rv doesn¡¯t exceed its acceptance probabil-ity and rejected otherwise. Eventually, we obtain a new set 

S which is generally smaller than (S). 

This method is very suitable for industrial practice owing to its two distinguished advantages: (1) it comes with a theo-retical guarantee. The use of the sampled set S guarantees a cost-minimizing classi.er, assuming the underlying learn-ing algorithm can achieve an approximate minimization of classi.cation errors (Zadrozny et al. 2003). (2) It is general and .exible in practice. Instances are selected from the origi-nal training sample according to their misclassi.cation costs, and then used to construct a classi.er with an adequate clas-si.cation learning technique, which results in cost-sensitive learning (Lee et al. 2012). 
We should note that cost proportionate rejection sampling is designed especially for binary cost-sensitive classi.ca-tion. However, our task is to distinguish multi-class data. The one-against-rest approach is adopted to solve our multi-classi.cationproblem(Beygelzimeretal.2005).Meanwhile, cost-sensitive learning by cost-proportionate rejection sam-pling only concerns adjusting the distribution of training sample by misclassi.cation costs, which implies that it fol-lows the same procedure of training model and .ne-tuning parameters as traditional cost-insensitive learning methods. 
As mentioned above, this cost-sensitive learning is a gen-eral method, which can improve classi.cation performance through combined use with traditional or cost-insensitive classi.ers. In this paper, we consider .ve classical classi-.ers, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), random forest (RF), and gradient boost decision tree (GBDT), to identify the operational status of rotating machinery. Moreover, we combine them with the cost proportional rejection sampling method to construct .ve cost-sensitive learning models: cost-sensitive logistic regres-sion (CS-LR), cost-sensitive naive Bayes (CS-NB), cost-sensitive support vector machine (CS-SVM), cost-sensitive random forest (RF), and cost-sensitive gradient boost deci-sion tree (CS-GBDT). 



The real-world application 
In this section, we apply the proposed framework to the fault diagnosis of rotating machinery by using the acquired vibra-tiondatafromanoilre.ninginChina.Itssuperiorityhasbeen demonstrated from two aspects. First, our multi-domain fea-ture extraction has shown its advantages in vibration signal analysis when compared with some previous works. Second, the cost-sensitive learning method can effectively reduce the average cost of misclassi.cation and performs well in fault diagnosis with imbalanced data. Meanwhile, a sensitivity analysis is conducted under different misclassi.cation costs, 
123 

1474  Journal of Intelligent Manufacturing (2020) 31:1467¨C1481  
Table 2 type  Description of the fault  No.  Fault type  Number of fault instances  Imbalanced ratio (normal/fault)  
1 2 3 4 5 6 7  Bearing cage fault Bearing inner race fault Bearing outer race fault Bearing rolling element fault Coupling fault Component loosening Other working conditions with faults (e.g., insuf.cient lubrication, seal leakage, etc.)  36 35 27 25 24 54 35  8.25 8.49 11.00 11.88 12.38 5.50 8.49  

k 

and CS-GBDT is recommended for its better performance 1 True PositiveiMA-R =
and more robust results. 
,
k + 1 
True Positivei + False Negativei
i=0 
(19) 

Data description 2 
MA-F = , (20) 
1/MA-P + 1/MA-R 
The raw vibration data used here is collected from an oil 
k 
True Positiveire.nery in Zibo, Shandong Province, China. We conduct 
feature extraction from vibration signals using time-domain, MA-G = 

k+1 


, (21)True Positivei + False Negativei
i=0 
frequency-domainandtime-frequency-domainanalyses,and obtain 65 features eventually. In monitoring the pump with vibration signals for up to two years, we have gathered 7 fault types including bearing cage fault, bearing inner race fault, bearing outer race fault, bearing rolling element fault, coupling fault, component loosening and other working con-ditions with faults, as shown in Table 2. The imbalanced ratio (IR) in Table 2 is de.ned as the ratio of the number of nor-mal condition (297) to the number of each fault type. It is obvious that we have to face a multi-classi.cation problem with imbalanced data. 

where k + 1 denotes the total number of classes. MA-A, MA-P, MA-R, MA-F, and MA-G denote macro-averaging accuracy, precision, recall, F1 and G-mean, respectively. 
Although the above .ve metrics are widely used to evalu-atetheclassi.cationperformance,theactualcostofeachtype of misclassi.cation is not taken into account. To address this issue, the average cost (A-cost), joint consideration for the number and cost of misclassi.cation error in different con-ditions, is the highlight of imbalanced classi.cation metric (Zhou and Liu 2006). In general, it is de.ned as 
kk 


Multiclass performance metrics A-cost = 1

Cp,q ¡¤ Np,q , (22)
Ns 
p=0 q=0 
The main purpose of this paper is to pursue the minimum cost of misclassi.cation without reducing the classi.cation performance of minority and majority classes. The widely used evaluation metrics in binary classi.cation settings are accuracy, precision, recall, F1-score, and G-mean. Here, these evaluation metrics should be extended to meet the multi-classi.cation need. Macro-averaging, generalizes the concept of these evaluation metrics to multiple dimensions, by averaging the metric values of each pair of classes (Santos et al. 2015). The speci.c metrics are expanded to: 
k 
=0 True Positive i
i
MA-A = k , (17) =0 ( True Positive i + False Positive i )
i k
1 

where Np,q represents the number of misclassifying the class p intotheclassq, Ns representsthetotalnumberofinstances. 

Comparison with other feature extraction methods 
To illustrate the effectiveness of our multi-domain feature extraction, it is necessary to compare the extracted features withthose usedinpreviousworks.We introducethree feature sets widely used in fault diagnosis of rotating machinery: feature set 1 (FS1), feature set 2 (FS2), and feature set 3 (FS3), and then compare them with our proposed feature set (PFS). 
(1) FS1: Time-domain and time-frequency-domain fea-
True Positivei 
tures.


MA-P = 
, (18)
k + 1 
True Positivei + False Positivei 
(2) FS2: Time-domain and frequency-domain features. 
i=0 
123 

Journal of Intelligent Manufacturing (2020) 31:1467¨C1481  1475  
Table 3 The speci.ed subset of the hyper-parameter space  Model  Hyperparameter  Search area  
LR  L2-norm parameter  [0.1,0.2,0.3,...,0.8,0.9,1.0]  
NB  Smoothing parameter  [0,0.1,0.2,...,1.8,1.9,2.0]  
SVM  Penalty parameter  10.4 ,10.3 ,10.2 ,...,102 ,103 ,104  
Kernel parameter  [0.1,0.2,0.3,...,1.8,1.9,2.0]  
RF  Number of estimators  [10,20,30,...,80,90,100]  
Maximum depth  [None,2,4,...,16,18,20]  
GBDT  Number of estimators  [20,30,40,...,70,80,90]  
Maximum depth  [2,4,6,...,12,14,16]  
Minimum samples in the leaf node  [10,20,30,...,80,90,100]  

(3) 
FS3: Frequency-domain and time-frequency-domain features. 

(4) 
PFS: Time-domain, frequency-domain, and time-frequency-domain features. 


As mentioned in ¡°Feature selection¡± section, directly using each group of all features may lead to over-.tting and affect its classi.cation performance. To overcome the interference of redundancy and irrelevant features, we use mRMR to select important features in each group and feed them into the .ve classical classi.ers introduced in ¡°Cost-sensitive learning by cost-proportionate rejection sampling¡± section. To better show the role of our feature selection, we investigate the impact of different numbers of features on the experimental results. In each group of experiments, the sample data is randomly partitioned into training set and test set by 6:4. Based on the training set, the parameters of each classi.er are tuned by .ve-fold cross-validation (80 % for training and 20% for testing at each CV iteration) indepen-dently. We adopt the grid search approach, an exhaustive searching through a manually speci.ed subset of the hyper-parameter space of a learning algorithm, to select the optimal hyper-parameters. The main hyper-parameter search space for each model is listed in Table 3. 
The diagnosis performance results are shown in Fig. 2, where the x-axis represents the number of important fea-tures selected using mRMR. It is clear that as the number of important features increases, the classi.cation perfor-mance improves signi.cantly in the early stage, and then tends to be stable. In addition, we should note that when too many features are added, the performance of some classi.ers beginstodeclinegradually.Forexample,whenthenumberof selected features in PFS exceeds 31, there is an obvious over-.tting phenomenon in SVM, which veri.es the importance of feature selection. Based on the overall classi.cation per-formance of each classi.er, 21 important features are .nally selected from the extracted features using the vibration sig-nals for later use. 

By comparing four different feature sets, we observe that all the classi.ers have achieved relatively good prediction performance using PFS in terms of higher scores. In other words, PFS extracted from three different domains can bet-ter represent the operational status of rotating machinery. Meanwhile, GBDT has higher scores on all .ve metrics of each feature set and is less likely to be disturbed by noise or irrelevant features, compared with the other four classi-cal classi.ers. It illustrates the accuracy and robustness of GBDT in this practical classi.cation. 

Comparison with traditional data-driven methods 
To verify that our proposed cost-sensitive learning method is superior to the classical cost-insensitive learning and other imbalanced learning methods in fault diagnosis, two com-parative experiments are conducted. The important features selected by mRMR are chosen as input variables, whose effectiveness has been well demonstrated by the experiments in ¡°Comparison with other feature extraction methods¡± sec-tion. 
Comparison with classical cost-insensitive learning methods 
Weinvestigatetheclassi.cationperformanceofcost-sensitive learning based on a given cost matrix. Speci.cally, we use the .ve cost-sensitive models proposed in ¡°Cost-sensitive learning by cost-proportionate rejection sampling¡± section, namely, CS-LR, CS-NB, CS-SVM, CS-RF, and CS-GBDT. Accordingly, to illustrate the calibration effect of cost-sensitive learning based on misclassi.cation costs, the .ve classical cost-insensitive classi.cation models (LR, NB, SVM, RF, and GBDT) are selected as a benchmark. Addi-tionally, based on the determination rules of cost matrix in ¡°Cost matrix determination¡± section, we de.ne a cost matrix with entries 
Ci0 = 10w, C0 j = w, and Cij = 3w for i = j = 0, (23) 
123 


Fig. 2 The comparison of classi.cation performance on different feature sets 
where w denotes the minimum misclassi.cation cost unit. This de.nition indicates that the cost of missed alarm is ten times higher than that of false alarm, while the cost of misclassifying one fault as another one is much smaller. It highlights the dangers and seriousness of missed alarm in fault diagnosis. 
In this experiment, the sample data is randomly divided into training set and test set by 6:4. Based on training set, grid search and .ve-fold cross-validation are used for each model to select the optimal hyper-parameters. The speci.c hyper-parameter spaces of each model are listed in Table 3. The average results with standard deviations in parentheses on test set across 50 independent experiments are listed in Table 4. 
From the results in Table 4, some interesting .ndings emerge. First, cost-sensitive learning method outperforms cost-insensitive learning one, which validates the need for cost-sensitive learning in fault diagnosis. Taking LR for example, cost-sensitive learning method increases the MA-A metric from 0.8326 to 0.8458, the MA-R metric from 0.6736 to 0.7602, the MA-F metric from 0.6959 to 0.7321, and the MA-Gmetricfrom0.8063to0.8618.Meanwhile,theaverage misclassi.cation cost (A-cost) is halved. Second, CS-GBDT is superior to the other cost-sensitive methods in terms of highest gains and lowest loss. As for A-cost, CS-GBDT reduces it to 0.0266, which is only 1/17 to 1/3 of those results from CS-LR, CS-NB, CS-SVM, and CS-RF. Third, we .nd that GBDT without cost-sensitive learning calibration also performs much better than CS-LR, CS-NB, CS-SVM and CS-RF. 

The results show that the fault diagnosis with imbalanced data are not only highly dependent on cost-sensitive learn-ing, but also vary with different classi.ers. Thus, it calls for a framework with .exibility and strong generalization ability, to choose the best cost-sensitive model from different classi-.ersinpracticalapplications.Ourdesignedframeworkmeets the demand quite well, since it is convenient for operators of industrial equipment diagnosis to calibrate the classi.cation results given on a cost matrix, without spending a lot of time and resources in reconstructing the training and .ne-tuning procedures of classi.ers. 
In addition to reporting the average misclassi.cation cost across 50 experiments in Table 4, we further test the signi.-cant improvement in prediction ability of each model under cost-sensitive calibration. To this end, we use the Diebold-Mariano (DM) test (Diebold and Mariano 1995) to compare the average misclassi.cation cost of cost-insensitive learning (model 1) with cost-sensitive learning (model 2). It is note-
123 

Table 4 Experimental results of cost-insensitive learning and cost-sensitive learning 
Metrics  Cost-insensitive  Cost-sensitive  
LR  NB  SVM  RF  GBDT  CS-LR  CS-NB  CS-SVM  CS-RF  CS-GBDT  
MA-A  0.8326  0.9672  0.8541  0.9378  0.9901  0.8458  0.9677  0.8505  0.9695  0.9921  
MA-P  (.0230) 0.7756  (.0138) 0.9536  (.0223) 0.8021  (.0242) 0.9056  (.0087) 0.9965  (.0206) 0.7394  (.0131) 0.9600  (.0220) 0.7578  (.0168) 0.9649  (.0090) 0.9907  
MA-R  (.0502) 0.6736  (.0206) 0.9375  (.0280) 0.7069  (.0366) 0.8840  (.0040) 0.9791  (.0362) 0.7602  (.0120) 0.9412  (.0378) 0.7467  (.0237) 0.9433  (.0108) 0.9837  
MA-F MA-G  (.0420) 0.6959 (.0405) 0.8063  (.0251) 0.9423 (.0231) 0.9655  (.0405) 0.7175 (.0389) 0.8309  (.0459) 0.8869 (.0436) 0.9352  (.0170) 0.9868 (.0109) 0.9881  (.0334) 0.7321 (.0344) 0.8618  (.0243) 0.9437 (.0217) 0.9680  (.0378) 0.7271 (.0380) 0.8546  (.0318) 0.9511 (.0278) 0.9682  (.0172) 0.9860 (.0152) 0.9912  
A-cost  (.0270) 1.0412  (.0140) 0.1508  (.0254) 0.5720  (.0261) 0.2610  (.0098) 0.0932  (.0200) 0.4863  (.0133) 0.0908  (.0230) 0.4566  (.0180) 0.1746  (.0093) 0.0266  
(.2000)  (.0725)  (.1311)  (.1270)  (.0848)  (.0852)  (.0399)  (.0788)  (.1139)  (.0337)  

Boldface indicates the performance of cost-sensitive learning over the corresponding cost-insensitive learning, i.e., LR versus CS-LR, NB versus CS-NB, SVM versus CS-SVM, RF versus CS-RF, GBDT versus CS-GBDT 
Table 5 DM signi.cance test of misclassi.cation cost (A-cost) 
Model comparison  DM test  
(Model 2 vs. model 1)  Statistic  p-value  
CS-LR versus LR  15.6150  2.2e.16***  
CS-NB versus NB  6.4440  4.823e.08***  
CS-SVM versus SVM  5.7616  5.44e.07***  
CS-RF versus RF  3.0786  0.0034***  
CS-GBDT versus GBDT  4.7952  1.556e.05***  

***Statistical signi.cance at the 1% level 
worthy that the argument ¡®alternative¡¯ in R function ¡®dm.test¡¯ issettobe¡®two.sided¡¯.Inotherwords,thealternativehypoth-esis is that the average misclassi.cation cost of model 1 is not equivalent to that of model 2. If p-value is less than 1%, we accept the alternative hypothesis and deem that model 2 is statistically superior to model 1 when the DM statistic is positive, while model 1 is statistically superior to model 2 when the DM statistic is negative. Otherwise, we have to accept the null hypothesis that there is no difference between model 1 and model 2. 
We report in Table 5 the DM test results and .nd that all statistics are positive and signi.cant at the level of 1%. This con.rms thatthe feasibilityofour cost-sensitive method to reducing the misclassi.cation cost in fault diagnosis of rotating machinery. 


Comparison with other imbalanced learning methods 
We have proved the advantages of cost-sensitive learn-ing methods in fault diagnosis over the traditional cost-insensitive classi.cation methods through the above exper-iments. In order to further verify the superiority of our cost-sensitive method in dealing with imbalanced data, we conduct another experiment by comparing our cost-sensitive method with a series of rebalancing methods. To this end, we select six commonly used rebalancing methods, including random under-sampling, random over-sampling, SMOTE, SMOTE-borderline1, SMOTE-borderline2, and ADASYN. They are applied to convert an imbalanced data into a balanced one, which will be used to train the .ve basic clas-si.ers. For fair comparisons, each classi.er takes the same procedure using grid search and .ve-fold cross-validation to select the optimal hyper-parameters, and repeats 50 inde-pendent experiments. The speci.c hyper-parameter spaces of each classi.er are listed in Table 3. 

We use the cost-sensitive learning method as the bench-mark and report in Table 6 the classi.cation results of each classi.er with different rebalancing methods. The results show that the overall performance of each classi.er based on cost-sensitive calibration is better than the six rebalanc-ing methods, except for only a few metrics. For example, SVM with Random over-sampling can obtain the maxi-mum average precision. Taking a closer look at the results of SMOTE-borderline2, we .nd that each classi.er trained using the synthesized data has poor performance, especially forNB. Thepossiblereasonisthat thegenerationofsynthetic instances in SMOTE-borderline2 will be hampered by the skewed distribution of the ¡°danger instance¡±. Meanwhile, the poor performance of random under-sampling may be caused by losing quantities of instances information. 
123 
123 
Table 6 Experimental results of other imbalanced learning methods 
Metrics Models Cost-sensitive Random under-sampling Random over-sampling SMOTE SMOTE-borderline1 SMOTE-borderline2 ADASYN 
MA-A LR 0.8458¡À .0206 0.7911¡À .0419 0.8328¡À .0270 0.8354¡À .0235 0.8342¡À .0250 0.8120¡À .0287 0.8293¡À .0228 NB 0.9677¡À .0131 0.9349¡À .0196 0.9678 ¡À .0140 0.9663¡À .0134 0.9668¡À .0138 0.5783¡À .1769 0.9660¡À .0158 SVM 0.8505¡À .0220 0.7390¡À .0410 0.8293¡À .0237 0.8173¡À .0249 0.8133¡À .0240 0.8165¡À .0250 0.8154¡À .0253 RF 0.9695¡À .0168 0.8412¡À .0525 0.9450¡À .0211 0.9473¡À .0196 0.9393¡À .0226 0.9409¡À .0200 0.9495¡À .0176 GBDT 0.9921¡À .0090 0.9307¡À .0511 0.9919¡À .0074 0.9900¡À .0108 0.9913¡À .0088 0.9803¡À .0167 0.9908¡À .0081 
MA-P LR 0.7394¡À .0362 0.6921¡À .0341 0.7408 ¡À .0368 0.7391¡À .0348 0.7375¡À .0402 0.6912¡À .0451 0.7269¡À .0335 NB 0.9600¡À .0120 0.9021¡À .0312 0.9553¡À .0199 0.9538¡À .0196 0.9547¡À .0200 0.6314¡À .0716 0.9533¡À .0202 SVM 0.7578¡À .0378 0.6473¡À .0443 0.7536¡À .0456 0.7498¡À .0467 0.7447¡À .0430 0.7198¡À .0463 0.7484¡À .0432 RF 0.9649¡À .0237 0.7828¡À .0523 0.9348¡À .0304 0.9285¡À .0276 0.9203¡À .0337 0.9015¡À .0362 0.9374¡À .0268 GBDT 0.9907¡À .0108 0.9231¡À .0464 0.9863¡À .0122 0.9844¡À .0173 0.9863¡À .0138 0.9672¡À .0252 0.9855¡À .0135 

MA-R LR 0.7602¡À .0334 0.7432¡À .0379 0.7907¡À .0398 0.7739¡À .0373 0.7630¡À .0417 0.7503¡À .0454 0.7675¡À .0377 NB 0.9412¡À .0243 0.9127¡À .0321 0.9385¡À .0297 0.9359¡À .0292 0.9368¡À .0298 0.6579¡À .0722 0.9354¡À .0293 SVM 0.7467¡À .0378 0.6836¡À .0438 0.7700 ¡À .0462 0.7386¡À .0478 0.7271¡À .0472 0.7417¡À .0444 0.7370¡À .0491 RF 0.9433¡À .0318 0.8530¡À .0445 0.8987¡À .0394 0.9074¡À .0391 0.8941¡À .0392 0.8912¡À .0385 0.9125¡À .0348 GBDT 0.9837¡À .0172 0.9582¡À .0281 0.9809¡À .0181 0.9773¡À .0261 0.9809¡À .0170 0.9662¡À .0264 0.9789¡À .0196 
MA-F LR 0.7321¡À .0344 0.6903¡À .0330 0.7490 ¡À .0389 0.7410¡À .0356 0.7340¡À .0405 0.7030¡À .0449 0.7329¡À .0344 NB 0.9437¡À .0217 0.8987¡À .0341 0.9427¡À .0275 0.9406¡À .0270 0.9414¡À .0277 0.5570¡À .0960 0.9400¡À .0273 SVM 0.7271¡À .0380 0.6629¡À .0443 0.7386 ¡À .0456 0.7174¡À .0475 0.7054¡À .0457 0.7069¡À .0447 0.7149¡À .0475 RF 0.9511¡À .0278 0.7972¡À .0510 0.9111¡À .0357 0.9122¡À .0342 0.9001¡À .0376 0.8883¡À .0381 0.9199¡À .0313 GBDT 0.9860¡À .0152 0.9287¡À .0448 0.9819¡À .0170 0.9791¡À .0227 0.9820¡À .0172 0.9640¡À .0290 0.9803¡À .0185 
MA-G LR 0.8618¡À .0200 0.8488¡À .0234 0.8781¡À .0238 0.8687¡À .0223 0.8621¡À .0252 0.8542¡À .0275 0.8648¡À .0226 NB 0.9680¡À .0133 0.9507¡À .0178 0.9661¡À .0164 0.9646¡À .0162 0.9651¡À .0165 0.7873¡À .0518 0.9644¡À .0162 SVM 0.8546¡À .0230 0.8112¡À .0276 0.8665¡À .0272 0.8477¡À .0284 0.8408¡À .0283 0.8496¡À .0268 0.8468¡À .0292 RF 0.9682¡À .0180 0.9125¡À .0267 0.9428¡À .0226 0.9480¡À .0222 0.9401¡À .0227 0.9397¡À .0215 0.9505¡À .0197 GBDT 0.9912¡À .0093 0.9741¡À .0171 0.9898¡À .0097 0.9879¡À .0141 0.9897¡À .0092 0.9816¡À .0146 0.9887¡À .0105 
A-cost LR 0.4863¡À .0853 0.5833¡À .1283 0.5081¡À .1071 0.5231¡À .1055 0.5619¡À .1246 0.5257¡À .1050 0.5253¡À .1084 NB 0.0908¡À .0399 0.1875¡À .0556 0.1414¡À .0664 0.1450¡À .0652 0.1436¡À .0653 0.7206¡À .2071 0.1456¡À .0651 SVM 0.4566¡À .0788 0.6378¡À .1215 0.4596¡À .0914 0.5028¡À .0937 0.5322¡À .0994 0.4939¡À .0838 0.4993¡À .1031 RF 0.1746¡À .1139 0.4021¡À .1573 0.3121¡À .1292 0.2511¡À .1270 0.3171¡À .1475 0.2075¡À .0867 0.2749¡À .1169 GBDT 0.0266¡À .0337 0.1284¡À .0918 0.0451¡À .0304 0.0350¡À .0254 0.0465¡À .0322 0.0703¡À .0449 0.0467¡À .0304 
Boldface (1) indicates the best performance on each row. (2) The.gure following ¡°¡À¡± is the standard deviation across 50 experiments 



Sensitivity analysis of cost-sensitive learning with different cost matrices 
In this subsection, we do sensitivity analysis of our cost-sensitive learning method in fault diagnosis. To this end, we consider three groups of cost matrices: 
Group 1: Ci0 = 10w, C0 j = w, and Cij = 3w for i = 0, j = 0, and i = j. 
Group 2: Ci0 = 15w, C0 j = w, and Cij = 2w for i = 0, j = 0, and i = j. 
Group 3: Ci0 = 7w, C0 j = w, and Cij = 4w for i = 0, j = 0, and i = j. 
The details of group 1 have been covered in ¡°Comparison with classical cost-insensitive learning methods¡± section and serves as a benchmark. In group 2, we increase the cost of missed alarm and reduce the cost of misjudging one fault for another one. In contrast, the costof missed alarm in group 3 is reduced, whereas the cost of misjudging one fault as another one is increased. Taking the same procedure as ¡°Compari-sonwithclassical cost-insensitivelearningmethods¡±section, we show the experimental results for three groups in Fig. 3, where the error bars represent the standard deviations. 
From the results in Fig. 3, an intuitive conclusion is that different misclassi.cation costs will lead to different classi.-cation performances of each model. First, by comparing the sensitivity of classi.cation performance in different groups, there are certain .uctuations in CS-GBDT and CS-NB. Con-versely, CS-SVM .uctuates signi.cantly with the change of misclassi.cation costs. Second, by comparing the sensitivity of classi.cation performance in the same group, the stan-dard deviation of CS-RF classi.cation performance in 50 independent experiments is greater than those of CS-GBDT and CS-NB. 

Apart from the above two points, we also notice that the higher evaluation scores are not necessarily accompanied by lower misclassi.cation costs. For example, in terms of eval-uation scores, CS-NB, CS-RF and CS-GBDT are superior to CS-LR and CS-SVM for all cost settings, while the aver-age misclassi.cation cost of CS-RF is greater than that of CS-SVM in group 2. This result validates the necessity of introducing A-cost metric, which is also one of the distinc-tive contributions in this paper. Fortunately, CS-GBDT can achieve better performance and more robust results than the other four models in terms of higher evaluation scores and lower misclassi.cation costs across different cost matrices. Thus, we recommend it as a suitable method for practical use. 



Conclusions 
The purpose of this paper is to develop a novel frame-work to diagnose the imbalanced operation condition of rotating machinery through combined use of multi-domain feature extraction, feature selection and cost-sensitive learn-ing method. We .rst extract 65 features from the perspectives of time-domain, frequency-domain and time-frequency-domain. The multi-domain features can comprehensively re.ect the operational status of a rotating machinery. Second, the mRMR feature selection technique is used to reduce the entire feature set to a more compact one. Third, we design the 
123 

cost-sensitivelearningmethodtoimprovetheperformanceof fault diagnosis with imbalanced data. This is done by impos-ingdifferentmisclassi.cationcostsonfalsealarmandmissed alarm respectively. 
Our framework is evaluated on the acquired vibration data of rotating machinery from an oil re.nery in Zibo, Shandong Province, China. From extensive experimental comparisons, the results illustrate that our multi-domain fea-ture extraction is valuable and the extracted features have the ability to achieve a higher classi.cation performance than previous works. By comparison with traditional cost-insensitive methods as well as other imbalanced learning methods, our cost-sensitive learning method performs better in imbalanced fault classi.cation. Meanwhile, a sensitiv-ity experiment demonstrates that different misclassi.cation costs will lead to different classi.cation performance. How-ever, it is worth mentioning that CS-GBDT is more robust and is preferredforitshighevaluationscoresand low average misclassi.cation cost. 
In the future, this research can be further promoted in two directions. First, we can consider multi-source data fusion in faultdiagnosis.Owingtothelimitationofthedataacquiredin this paper, the features are only extracted from the vibration signals, while the effects of other signals, such as electrical signals and temperature, on the diagnostic results of rotating machinery are ignored. With the new paradigm of Indus-try 4.0, more and more multimode sensors make it possible to fuse multi-source data and conduct more comprehensive fault diagnosis. Second, we could consider the severity of each fault condition. We have roughly set the cost matrix for different fault conditions without considering the severity of each fault condition. It is very useful to divide different levels of severity and specify the cost accordingly. In this regard, the cost-sensitive learning method can play a greater role in fault diagnose and produce more accurate results for scien-ti.c decision-making. We leave this topic for future research. 
Acknowledgements The authors are grateful to the Editor-in-Chief, the Associate Editor, and two anonymous referees for their helpful comments and constructive guidance. This work is supported by the National Natural Science Foundation of China (71671056), the Human-ity andSocial Science Foundationof the Ministry of Education of China (19YJA790035), and the National Statistical Science Research Projects of China (2019LD05). Special thanks to data support from the industrial partner RONDS. 
Compliance with ethical standards 
Con.ict of interest The authors declare that they have no con.ict of interest. 



References 
Amrhein, W., Gruber, W., Bauer, W., & Reisinger, M. (2016). Magnetic levitation systems for cost-sensitive applications-some design 
aspects. IEEE Transactions on Industry Applications, 52(5),3739¨C 3752. 

Ben Ali, J., Saidi, L., Harrath, S., Bechhoefer, E., & Benbouzid, M. (2018). Online automatic diagnosis of wind turbine bearings pro-gressive degradations under real experimental conditions based on unsupervised machine learning. Applied Acoustics, 132, 167¨C181. 
Beygelzimer, A., Dani, V., Hayes, T., Langford, J., & Zadrozny, B. (2005). Error limiting reductions between classi.cation tasks. In Proceedings of the 22nd international conference on machine learning (pp. 49¨C56). 
Castro, C. L., & Braga, A. P. (2013). Novel cost-sensitive approach to improve the multilayer perceptron performance on imbalanced data. IEEE Transactions on Neural Networks and Learning Sys-tems, 24(6), 888¨C899. 
Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: Synthetic minority over-sampling technique. Journal of Arti.cial Intelligence Research, 16, 321¨C357. 
Ciabattoni, L., Ferracuti, F., Freddi, A., & Monteri¨², A. (2018). Sta-tistical spectral analysis for fault diagnosis of rotating machines. IEEE Transactions on Industrial Electronics, 65(5), 4301¨C4310. 
Correa Bahnsen, A., Aouada, D., & Ottersten, B. (2015). Example-dependent cost-sensitive decision trees. Expert Systems with Applications, 42(19), 6609¨C6619. 
Diebold,F.X.,&Mariano,R.S.(1995).Comparingpredictiveaccuracy. Journal of Business and Economic Statistics, 13(3), 253¨C263. 
Ding, C., & Peng, H. (2005). Minmum redundancy feature selection from microarray gene expression data. Journal of Bioinformatics and Computational Biology, 3(2), 185¨C205. 
Domingos, P. (1999). MetaCost: A general method for making clas-si.ers cost-sensitive. In Proceedings of the .fth ACM SIGKDD international conference on knowledge discovery and data mining (pp. 155¨C164). 
Dou, R., He, Z., & Hsu, C. (2018). Foreword: Smart manufacturing, innovative product and service design to empower industry 4.0. Computers & Industrial Engineering, 125, 514¨C516. 
Gan, M., Wang, C., & Zhu, C. (2018). Fault feature enhancement for rotating machinery based on quality factor analysis and manifold learning. Journal of Intelligent Manufacturing, 29(2), 463¨C480. 
Gardner, J., & Xiong, L. (2009). An integrated framework for de-identifying unstructured medical data. Data & Knowledge Engi-neering, 68(12), 1441¨C1451. 
Georgoulas, G., Loutas, T., Stylios, C. D., & Kostopoulos, V. (2013). Bearing fault detection based on hybrid ensemble detector and empirical mode decomposition. Mechanical Systems and Signal Processing, 41(1¨C2), 510¨C525. 
Haibo, H., Yang, B., Garcia, E. A., & Shutao, L. (2008). ADASYN: Adaptive synthetic sampling approach for imbalanced learning. In Proceedings of the .fth ACM SIGKDD international conference on knowledge discovery and data mining (pp. 155¨C164). 
Han,H.,Wang,W.,&Mao,B.(2005).Borderline-SMOTE:Anewover-sampling method in imbalanced data sets learning. In Proceedings of advances in intelligent computing (pp. 878¨C887). 
Han,S.,Choi,H.,Choi,S.,&Oh,J.(2019a).Faultdiagnosisofplanetary gear carrier packs: A class imbalance and multiclass classi.ca-tion problem. International Journal of Precision Engineering and Manufacturing, 20(2), 167¨C179. 
Han, T., Liu, C., Yang, W., & Jiang, D. (2019b). Deep transfer network with joint distribution adaptation: A new intelligent fault diagnosis framework for industry application. ISA Transactions, In press. 
Hwang, Y., Jen, K., & Shen, Y. (2009). Application of cepstrum and neural network to bearing fault detection. Journal of Mechanical Science and Technology, 23(10), 2730¨C2737. 
Jia,F.,Lei,Y.,Lu,N.,&Xing,S.(2018).Deepnormalizedconvolutional neural network for imbalanced fault classi.cation of machinery and its understanding via visualization. Mechanical Systems and Signal Processing, 110, 349¨C367. 
123 

Jiang, G., He, H., Yan, J., & Xie, P. (2019). Multiscale convolutional neural networks for fault diagnosis of wind turbine gearbox. IEEE Transactions on Industrial Electronics, 66(4), 3196¨C3207. 
Jiang, Q., Shen, Y., Li, H., & Xu, F. (2018). New fault recognition method for rotary machinery based on information entropy and a probabilistic neural network. Sensors, 18(2), 337¨C349. 
Jiang, W., Spurgeon, S. K., Twiddle, J. A., Schlindwein, F. S., Feng, Y., & Thanagasundram, S. (2016). A wavelet cluster-based band-pass .ltering and envelope demodulation approach with application to fault diagnosis in a dry vacuum pump. Proceedings of the Insti-tution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 221(11), 1279¨C1286. 
Kang, S. (2018). Joint modeling of classi.cation and regression for improving faultywaferdetectioninsemiconductormanufacturing. Journal of Intelligent Manufacturing,. https://doi.org/10.1007/ s10845-018-1447-2. 
Khan, S. H., Hayat, M., Bennamoun, M., Sohel, F. A., & Togneri, 
R. (2018). Cost-sensitive learning of deep feature representations fromimbalanceddata. IEEE Transactions on Neural Networks and Learning Systems, 29(8), 3573¨C3587. 
Kuo, R. J., Su, P. Y., Zulvia, Ferani E., & Lin, C. C. (2018). Integrat-ing cluster analysis with granular computing for imbalanced data classi.cation problem¡ªa case study on prostate cancer prognosis. Computers & Industrial Engineering, 125, 319¨C332. 
Larsson, E. G., Stoica, P., & Jian, L. (2002). Amplitude spectrum esti-mation for two-dimensional gapped data. IEEE Transactions on Signal Processing, 50(6), 1343¨C1354. 
Lee, Y., Hu, P. J., Cheng, T., & Hsieh, Y. (2012). A cost-sensitive technique for positive-example learning supporting content-based product recommendations in B-to-C e-commerce. Decision Sup-port Systems, 53(1), 245¨C256. 
Li, P., Hu, W., Hu, R., & Chen, Z. (2020). Imbalance fault detection based on the integrated analysis strategy for variable-speed wind turbines. International Journal of Electrical Power & Energy Sys-tems,116, In press. 
Liu, J., An, Y., Dou, R., Ji, H., & Liu, Y. (2018a). Helical fault diagnosis model based on data-driven incremental mergence. Computers & Industrial Engineering, 125, 517¨C532. 
Liu, R., Yang, B., Zio, E., & Chen, X. (2018b). Arti.cial intelligence for fault diagnosis of rotating machinery: A review. Mechanical Systems and Signal Processing, 108, 33¨C47. 
Mathew, J., Pang, C. K., Luo, M., & Leong, W. H. (2018). Classi-.cation of imbalanced data by oversampling in kernel space of support vector machines. IEEE Transactions on Neural Networks and Learning Systems, 29(9), 4065¨C4076. 
Peng, H., Long, F., & Ding, C. (2005). Feature selection based on mutual information: Criteria of max-dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(8), 1226¨C1238. 
Ragab, A., Yacout, S., Ouali, M., & Osman, H. (2019). Prognostics of multiple failure modes in rotating machinery using a pattern-based classi.er and cumulative incidence functions. Journal of Intelligent Manufacturing, 30(1), 255¨C274. 
Ren, L., Sun, Y., Cui, J., & Zhang, L. (2018). Bearing remaining useful life prediction based on deep autoencoder and deep neural net-works. Journal of Manufacturing Systems, 48, 71¨C77. 
S¨¢nchez, R., Lucero, P., V¨¢squez, R. E., Cerrada, M., Macancela, J., & Cabrera, D. (2018). Feature ranking for multi-fault diagnosis of rotating machinery by using random forest and KNN. Journal of Intelligent & Fuzzy Systems, 34(6), 3463¨C3473. 
Santos, P., Maudes, J., & Bustillo, A. (2015). Identifying maximum imbalance in datasets for fault diagnosis of gearboxes. Journal of Intelligent Manufacturing, 29(2), 333¨C351. 
Seera, M., Lim, C. P., & Loo, C. K. (2014). Motor fault detection and diagnosis using a hybrid FMM-CART model with online learning. Journal of Intelligent Manufacturing, 27(6), 1273¨C1285. 

Song, L., Wang, H., & Chen, P. (2018). Vibration-based intelligent fault diagnosis for roller bearings in low-speed rotating machinery. IEEE Transactions on Instrumentation and Measurement, 67(8), 1887¨C1899. 
Sun, Y., Kamel, M. S., Wong, A. K. C., & Wang, Y. (2007). Cost-sensitive boosting for classi.cation of imbalanced data. Pattern Recognition, 40(12), 3358¨C3378. 
Tao, F., Qi, Q., Liu, A., & Kusiak, A. (2018). Data-driven smart manu-facturing. Journal of Manufacturing Systems, 48, 157¨C169. 
Tidriri, K., Chatti, N., Verron, S., & Tiplica, T. (2016). Bridging data-drivenand model-basedapproachesforprocessfault diagnosis and health monitoring: A review of researches and future challenges. Annual Reviews in Control, 42, 63¨C81. 
Wang, P., Ananya, Yan, R., & Gao, R. X. (2017). Virtualization and deep recognition for system fault classi.cation. Journal of Manu-facturing Systems,44, 310¨C316. 
Wang, X., Zhang, X., Li, Z., & Wu, J. (2019). Ensemble extreme learn-ing machines for compound-fault diagnosis of rotating machinery. Knowledge-Based Systems, In press. 
Wu, C., Jiang, P., Ding, C., Feng, F., & Chen, T. (2019a). Intelligent fault diagnosis of rotating machinery based on one-dimensional convolutional neural network. Computers in Industry, 108, 53¨C61. 
Wu, J., Wu, C., Cao, S., Or, S. W., Deng, C., & Shao, X. (2019b). Degradation data-driven time-to-failure prognostics approach for rollingelementbearingsinelectricalmachines. IEEE Transactions on Industrial Electronics, 66(1), 529¨C539. 
Xie, Y., Peng, L., Chen, Z., Yang, B., Zhang, H., & Zhang, H. (2019). Generative learning for imbalanced data using the gaussian mixed model. Applied Soft Computing, 79, 439¨C451. 
Zadrozny, B. Langford, J., & Abe, N. (2003). Cost-sensitive learning by cost-proportionate example weighting. In Proceedings¡ªIEEE international conference on data mining (pp. 435¨C442). 
Zan, T., Liu, Z., Wang, H., Wang, M., & Gao, X. (2019). Control chart pattern recognition using the convolutional neural network. Jour-nal of Intelligent Manufacturing, In press. 
Zhang, X., & Hu, B. (2014). A new strategy of cost-free learning in the class imbalance problem. IEEE Transactions on Knowledge and Data Engineering, 26(12), 2872¨C2885. 
Zhang, Y., Li, X., Gao, L., Wang, L., & Wen, L. (2018). Imbalanced data fault diagnosis ofrotatingmachineryusing syntheticoversampling and feature learning. Journal of Manufacturing Systems, 48, 34¨C 50. 
Zhang, C., Tan, K. C., Li, H., & Hong, G. S. (2019). A cost-sensitive deep belief network for imbalanced classi.cation. IEEE Transac-tions on Neural Networks and Learning Systems, 30(1), 109¨C122. 
Zhang, Z., Verma, A., & Kusiak, A. (2012). Fault analysis and condition monitoring of the wind turbine gearbox. IEEE Transactions on Energy Conversion, 27(2), 526¨C535. 
Zhao, M., Jiao, J., & Lin, J. (2019). A data-driven monitoring scheme for rotating machinery via self-comparison approach. IEEE Trans-actions on Industrial Informatics, 15(4), 2435¨C2445. 
Zhao, M., & Lin, J. (2018). Health assessment of rotating machinery using a rotary encoder. IEEE Transactions on Industrial Electron-ics, 65(3), 2548¨C2556. 
Zhou,Z.,&Liu,X.(2006).Training cost-sensitiveneural networkswith methods addressing the class imbalance problem. IEEE Transac-tions on Knowledge and Data Engineering, 18(1), 63¨C77. 
Publisher¡¯s Note Springer Nature remains neutral with regard to juris-dictional claims in published maps and institutional af.liations. 
123 





Investigation the effect of tightening torque on the fatigue strength of double lap simple bolted and hybrid (bolted–bonded) joints using volumetric method




           a b s t r a c t

                            In this research, the effect of the tightening torque on the fatigue strength of 2024-T3 double lap simple
                              bolted and hybrid (bolted–bonded) joints have been investigated experimentally by conducting fatigue
                        tests and numerically by implementing ﬁnite element analysis. To do so, three sets of specimens were
                                                        prepared and each of them subjected to tightening torque of 1, 2.5 and 5 Nm and then fatigue tests were
                                                         carried out under different cyclic longitudinal load levels. In the numerical method, the effect of the tight-
                                         ening torque on the fatigue strength of the considered joints has been studied by means of volumetric
                                                         method. To obtain stress distribution around the notch (bolt hole) which is required for the volumetric
                                                         method, nonlinear ﬁnite element simulations were carried out. In order to estimate the fatigue life, the
                                      available smooth S–N curve of Al2024-T3 and the fatigue notch factors obtained from the volumetric
                                        method were used. The estimated fatigue life was compared with the available experimental test results.
                                                         The investigation shows that there is a good agreement between the life predicted by the volumetric
                                                         method and the experimental results for different specimens with a various amount of tightening tor-
                                                         ques. The results obtained from the experimental analysis showed that the hybrid joints have a better
                                                         fatigue strength compared to the simple bolted joints. In addition, the volumetric method and experi-
                                                         mental results revealed that the fatigue life of both kinds of the joints were improved by increasing
                                                         the clamping force resulting from the torque tightening due to compressive stresses which appeared
                                                         around the bolt hole.
                                                                                                                            




1. Introduction                                                                               the two kinds of lap joints. First, in a mechanical joint, the overlap-
                                                                                              ping areas are attached to one another at discrete points only, i.e.
    Among the mechanical removable joints, including, riveted,                                by the fasteners. Clearly, severe stress concentrations occur. How-
pined or bolted joints, the bolted joint are the most essential com-                          ever, if the connection is made continuously in the full overlapping
ponents in aerospace structures. However, the existence of geo-                               area by adhesive bonding, these stress concentrations do not occur.
metrical discontinuity in these joints due to essential hole                                  Because, the adhesive bonded joints do not need to the fasteners
drilling process causes stress concentration and thus increases                               and the fastening holes. Therefore, the stress distributions in the
the tendency of early fatigue crack initiation and growth under                               joint are relatively uniform compared to those in the mechanical
cyclic loading [1–3]. Therefore, it is of great importance to reduce                          joint. Secondly, metallic direct contact between the two sheets
the effect of the stress concentration and attain enhanced fatigue                            does not happen in the adhesively bonded joints.
life. According to the results of previous studies, bolted joints have                            In order to overcome the potential weaknesses of the adhe-
higher tensile and fatigue strengths than welded, riveted and pin-                            sive bonding, and the mechanical joints, and therefore, to obtain
ned joints [4–6].                                                                             very effective joints, a combination of the mechanical joints
    An alternative method to mechanical fastening is adhesively                               (riveted, bolted, etc.) and an adhesive, namely hybrid joints,
bonded joint. In order to identify the advantages of adhesive bond-                           are used [7–10]. Hybrid joints are used in many engineering
ing in fatigue, two fundamental differences are important between                             application such as aerospace, automotive, and naval industries
                                                                                              because of their better performance in comparison with the sim-
                                                                                              ple joints such as adhesively bonded, riveted and bolted joints




    Several researchers [13–17] investigated the hybrid joints.                  iment dependent coefﬁcients, which have been obtained for mate-
Pirondi and Moroni [18] compared the hybrid weld-bonded,                         rials used in many years ago. Nowadays, since varieties of
rivet-bonded, clinch-bonded, and simple joints under various con-                materials in industry are ever increasing and new materials and
ditions, by means of experimental analysis. The effect of the mate-              alloys have been produced, using these coefﬁcients is not very
rial, geometrical factors, and environment on static strength and                appropriate.
energy absorption were evaluated through the analysis of variance.                   Differences between these relations are high and the accuracy
A similar investigation conducted by Schvechkov [19] on the                      of these relations is not clear and the effects of the net stress and
effects of adhesive mechanical properties on the fatigue strength                stress gradient in these relations have been neglected. Two com-
of hybrid (riveted-bonded) joints by means of experimental analy-                monly used equations, among the relations that have been pro-
sis. In separate investigations, Kelly [14,20] studied the static and            posed to describe the notch sensitivity factor, are Peterson [30]
fatigue strength of the hybrid (bonded–bolted) single lap joints                 and Neuber [31] equations.
using different modulus adhesives. The results of studies, revealed                  Peterson assumed that the fatigue failure occurs when the
that, the hybrid joints with lower modulus adhesives allowed for                 stress at one point which has a critical distance from the notch root
load sharing between the adhesive and the bolts. In a similar inves-             (ap) is equal to the fatigue strength of the smooth specimen.
tigation conducted by Fu and Mallick [8], they experimentally                    Assuming that the stress near the notch root is decreasing linearly,
showed that the hybrid (bolted–bonded) single lap joints have a                  Peterson proposed the following experimental relation:
higher static and fatigue strength in comparison with only adhe-
                                                                                      
sively bonded joints.                                                            a                                                   
    When a nut and bolt are used to join mechanical members
                                                                                    
together, the nut or bolt can be pre-tensioned by applying tighten-              where r is notch radius and ap is material constant, which depends
ing torque using a torque wrench and then the bolt and nut are                   on the grain size and loading. A widely used empirical expression
pulled toward each other. Pre-tension or clamping force is the                   for Kf for different notch geometries was proposed by Neuber
technical term for the tension caused by tightening the nut that                 [31]. By considering the stress distribution ahead of a notch with
holds the assembled part together [21–25].                                       a given notch root radius, he arrived at an expression for q based
    Previous works revealed that the clamping force can reduce the               on the average stress up to some distance ahead of the notch as
stress concentration at the bolted hole region, and therefore                    follows:
improve the strength of the joint considerably [6,21].
    As mentioned, fatigue fractures usually occur at notches such as                  
                                                                                                                       
holes and grooves. Since virtually all engineering structures                                
                                                                                 
include notches in one form or another, the treatment of this in
fatigue life prediction has received considerable attention over                 where aN is the material parameter with a unit of length and r is the
the years. The geometry of notch and other notch properties affect               notch root radius constant which depends on the grain size. In addi-
the fatigue life. The stress rise will of course be very harmful with            tion, on the basis of a model taking into account how inherent
respect to fatigue strength, but in fatigue tests of the notched                 material ﬂaws like cavities, inclusion, defects alter the stress ﬁeld
geometries, the effect of kt appears to have less inﬂuence than                  at the notch root, Heywood [32] suggested a different empirical
under static loading, which has led to the deﬁnition of a separate               expression of the notch-fatigue. Heywood’s representation of exten-
fatigue notch factor, kf [26–28].                                                sive fatigue is somewhat different from Neuber’s and Peterson’s.
    The fatigue notch factor relates the unnotched fatigue to the                Heywood’s proposed expression for the fatigue notch factor is illus-
notched, nominal fatigue strength in the same numbers of cycles                  trated as follows:
and the same experimental test conditions.
                                                                                       
    There is not a general experimental method with low cost to                                 
determine the kf values, and this is because of the micro-mechan-                
                                                                                         

ical complications and uncertainties of the plastic zone at the
notch root. In general trend, kf is equal to or less than kt. A general          where a0 corresponds to the length of equivalent material ﬂaws. As
deﬁnition of the fatigue notch factor is the ratio of fatigue strength           mentioned previous, the accuracy of these formulas is not speciﬁed
of the smooth specimen to fatigue strength of the notched speci-                 and requires to be checked by numerous, costly and time consum-
men in the same numbers of cycles and the same experimental test                 ing fatigue tests.
conditions [29]:                                                                    Based on a large amount of empirical data, Siebel and Stieler
                                                                                 [33] expressed kf by the relative stress gradient:
       unnotched fatigue limit                                                       
                                                  
        notched fatigue limit                                                    
   The difference between the fatigue notch factor kf and the                    where, for loading in the z direction using cylindrical coordinates
elastic stress concentration factor kt, is expressed by the notch                (z, h, r), the relative stress gradient is deﬁned from the axial stress
sensitivity factor, q, as follows:                                             
     
   There are several relations to express the notch sensitivity                      In this study, the effect of tightening torque on the fatigue
factor in terms of the notch radius. However, the experimental                   strength of 2024-T3 double lap simple bolted and hybrid (bolted/
accuracy of these relations is not very clear, and it is essential to            bonded) joints have been investigated both experimentally and
conduct more experimental tests. Notch sensitivity factor for                    numerically. Therefore, two kinds of joints, i.e. double lap simple
describing the notch effects of the high-cycle fatigue problems                  and hybrid (bolted–bonded) joints were considered. For each kind
have some disadvantages in deﬁnition of the relations between kf                 of the joint, three sets of specimens were prepared and subjected
and kt, as follows:The empirical relation between the kf and kt                  to the tightening torque of 1, 2.5 and 5 Nm and then fatigue tests
are established only for the endurance limit. There are some exper-              were carried out on them under different cyclic longitudinal load
                                                   


levels. In the numerical section, the inﬂuences of tightening torque                       where Xeff is the effective distance, rn is the net stress, ryy is the
on the fatigue life of simple and hybrid joints have been studied                          crack opening stress, and v is the relative stress gradient which is
using the volumetric method. To obtain the stress distribution                             deﬁned as follows:
around the notch (bolt hole) which is required for the volumetric
method, nonlinear ﬁnite element simulations were carried out.                                   
                                                                                    
2. Theoretical aspects of the volumetric method                                                In this method, the fatigue notch factor calculated from Eq. (8)
                                                                                           and the use of reference curve of smooth specimen lead to notched
    From the physical point of view, the volumetric method is an                           curve in fatigue according to Eq. (1). In this method fatigue notch
innovative technique, in order to estimate the fatigue strength of                         factor is calculated by application of the specimen material proper-
notched components. In this method, the fatigue failure needs a                            ties and its geometrical features via ﬁnite element analysis.
physical volume to take place, which is called the fatigue process-                            The volumetric method can also be used in the cases that
ing volume.                                                                                smooth specimen results are not available but another notched
    The fatigue strength of materials is mainly affected by the spec-                      specimen with the same material, specimen geometry and differ-
imen size and the relative stress gradient, which are dimensional                          ent notch features exist.
parameters. This method leads to a two-parameter fatigue initia-
tion criterion, i.e. the effective distance and the effective stress.
The effective distance is the diameter of the fatigue processing vol-                      3. Materials and experimental methods
ume. The shape and size of the volume generally depends on the
specimen geometry, loading mode and material properties. This                                 The specimens employed in this investigation were made from
volume is supposed to be the site where the fatigue damage takes                           2024-T3 aluminium alloy with thickness of 2 mm. Table 1 lists the
place. The effective stress corresponds to an average value over this                      mechanical properties of the aluminium alloy obtained from ten-
distance of the stress distribution weighted by the distance and the                       sion (static) tests, while Table 2 presents the chemical composi-
relative stress gradient [27,29]. It is determined by a nonlinear                          tions of the used aluminium alloy.
stress distribution near the notch tip. Therefore, it is a function                           Two different kinds of joints i.e. double lap simple and hybrid
of the stress ﬁeld and, depends, as the effective distance, on the                         (bolted–bonded) joints were prepared. Test specimens’ conﬁgura-
part geometry, loading and material. So, for a given geometry,                             tions and dimensions for both kinds of the joints are illustrated
material and loading, the effective stress is, in principal, a function                    schematically in Fig. 2.
of net stress. A typical elastic–plastic stress distribution near a                           The hybrid joints were manufactured by means of the structural
notch has been shown in Fig. 1.                                                            two component epoxy adhesive (Loctite 3421) [34], prepared by
    Three different regions can be recognized from Fig. 1. The max-                        mechanical mixing of the resin and hardener in equal amount by
imum stress and its distance can be clariﬁed in zone I. The impor-                         weight. In order to obtain the tensile stress–strain curve of the
tant characteristic of this zone is the existing of the maximum                            adhesive, several dog-bone specimens were prepared according
stress in the elastic–plastic stress distribution, which has been                          to ASTM: D638-10. The adhesives were injected into a mold, as
obtained from the ﬁnite element analysis. The elastic FE analysis                          shown in Fig. 3, and left to be cured at the room temperature for
in the notch root for the crack opening stress reveals that stress                         24 h. Finally, the prepared specimens were tested on a 100 kN
value is decreasing from the notch root toward the outside [29].                           Zwick/Z100 static testing machine with a crosshead speed of
In zone II, the stress value is decreasing and its value reaches to                        5 mm/min. The engineering stress–strain curve of the adhesive is
the effective stress reff. According to the volumetric method, the                         shown in Fig. 4.
fatigue notch factor is given by the following formula:                                       In order to eliminate any possible surface scratches, the surface
            
                                                                                           of the plates (specimens) were polished using different grinding
                                                                         (sand) papers with grits of 400, 600 and 1000 at ﬁrst. To prepare

                                                                the specimens, fastener holes with diameter of 5 mm, were drilled
                                                                                           and reamed in the joint plates. A hex head M5 (class 10.9) steel bolt
                                                                                           was used for the mechanical fastening and suitable types of steel
                                                                                           washers and nuts were used to prepare the joint as illustrated in
                                                                                           Fig. 2. Finally, the nut is tightened by applying torque using a
                                                                                           torque-wrench up to required amounts of torques, i.e. 1 Nm,
                                                                                           2.5 Nm and 5 Nm.






   As mentioned, aluminium alloy 2024-T3 sheet was used as an                       same amounts of torques as the simple bolted joints, i.e. 1 Nm,
adherend to prepare the hybrid joints in this investigation. The                    2.5 Nm and 5 Nm.
preparation of the hybrid joints has been implemented in two
main steps. Firstly, a double lap bonded joint was constructed. In                  3.1. Clamping force measurement
order to obtain high strength joint, the joint plates were cleaned
with acetone and then they were let dry, prior to using the adhe-                      In order to measure the clamping force or bolt pre-tension
sive layer. In order to achieve the constant thickness of adhesive                  resulting from the torque tightening, at different applied torques,
layer, 0.5 mm thick sheets were used between adherends. The pre-
pared bonded joints were left in ambient temperature for 72 h, in
accordance with the adhesive manufacturer suggestion.                                            
   In the second step of preparing the hybrid joints, the double lap
bonded specimens with the adhesive were bolt tightened using the                              
                                                                                       






                                                                                 
sectional area at hand and the axial stress, the axial force in the                                               
                                                                                                                            
bush and therefore the clamp force has been determined. The used             
method and the bush dimensions were shown in Fig. 5.                                                              

   To calibrate the applied torque and clamping force, torques                                                    
were applied in 1 Nm increments from 1 to 7 Nm to the nut using
                                                                                                                           
a torque wrench, and then the axial strains were recorded for each
value of the torques. This test was repeated three times for each                                                         
case to obtain the mean value of the compressive strains (em),                                                            
and determine the corresponding clamping forces using Eq. (10).                                                                 
The relation between the applied tightening torque and the mean                                                                                
value of compressive strains for both types of joints, are given in
Fig. 6. The elastic modulus for the bush material (Ebush) was also                     
                                                                                        compressive strains.
experimentally determined in order to obtain the accurate values
for the mean axial clamping force.
                                


In the above equation, Abush is the area of the bush cross section. The                                             
relation between the calculated clamping forces and the applied
                                                                                                                  
torques for the specimen is shown in Fig. 7. As the ﬁgure shows,
there is a linear relation between the clamping force and the                                                      
applied torque. This conﬁrms that the bush material is still in its                                             
elastic region, even under the maximum applied torque.
                                                                                                                       

                                                                                                                      
3.2. Fatigue tests                                                                                                       
                                                                                                                                               
   In the next step, the prepared specimens for each kind of the
joints were tested in this investigation to study the effect of the                                                         
tightening torque on the fatigue lives of the both kinds of the join-
ing method.
   Fatigue tests have been carried out using constant amplitude                         (fracture) occurred in the main plate. The fatigue test results for
loads in a servo-hydraulic 250 kN Zwick/Roell fatigue testing                           double lap simple bolted specimens have been shown in Fig. 8 in
machine with a frequency of 10 Hz and stress ratio (load ratio) of                      terms of the number of cycles to failure. In addition, in the case
0.1. The fatigue tests have been performed on the double lap sim-                       of double lap hybrid specimens, fatigue tests have been performed
ple bolted specimens with three different amounts of tightening                         with the same amounts of the tightening torques, as simple bolted
torques, i.e. 1 Nm, 2.5 Nm and 5 Nm which created clamping forces                       specimens, i.e. 1 Nm, 2.5 Nm and 5 Nm which created clamping
equal to Fcl = 976, 2440 and 4880 N respectively, according to the                      forces equal to Fcl = 840, 2100 and 4200 N respectively, according
linear equation obtained from Fig. 7. In each case, six fatigue tests                   to the linear equation obtained from Fig. 7. The fatigue test results
were performed with different maximum remote longitudinal                               for the double lap hybrid specimens have been shown in Fig. 9 in
loads to ﬁnal failure. The fatigue tests were run until the failure                     terms of the number of cycles to failure. It is important to mention





                                                     


that, failure in all of the specimens occurred in the main plate near                                                  located at the left end of the FE model were considered to have
the bolt hole edge.                                                                                                    all their degrees of freedom constrained.
                                                                                                                           In order to transfer the pressure between the contacting sur-
                                                                                                                       faces, ﬂexible-to-ﬂexible contact state was used. Each contact pair
4. Numerical analysis                                                                                                  consisted of target element and contact element. TARGET 170 was
                                                                                                                       used as a target element and CONTACT 174 was used as a contact
    In order to obtain the effective distance, the effective stress val-                                               element. The friction effect between the surfaces of the washer
ues and the fatigue notch factors in the volumetric method, the                                                        (bolt head) and Al-alloy plate was included in the FE model using
ﬁnite element analysis has been employed by ANSYS 9.0 general
ﬁnite element code [35]. In the present investigation, to estimate
the fatigue lives of the joint plates for both kinds of the joints with
                                                                                                                                                          
different amounts of the tightening torque, three dimensional
models of the specimens have been simulated using Ansys FE code.                                                                                    
All of the adherends, adhesive layer and bolt have been meshed
                                                                                                                                            



                                                                                                                                                        
with 20-node SOLID95 structural elements. In addition, due to
double symmetry condition, only a quarter of the plates, and bolt                                                                                        
were needed to be modelled. Symmetric displacement boundary
                                                                                                                                                          
conditions were deﬁned for the nodes on the planes of symmetry.
To extract more accurate results, mapped mesh with sufﬁciently                                                                                             
ﬁne mesh around the hole of the plates has been used in FE anal-
                                                                                                                                                           
yses. The bottom face of the bolt shank was used to implement
                                                                                                                                                               
the bolt clamping force. The ﬁnite element mesh of the double
                                                                                                                                                                                          
lap hybrid joint specimens is presented in Fig. 10, together with
its corresponding loading and boundary conditions. The nodes                                                                                                  






Elastic Coulomb model with friction coefﬁcient of l = 0.29 which                       tests and shown in Fig. 11. The elastic modulus and Poisson’s ratio
was obtained from experimental tests based on sliding of the                           were measured to be E = 72 GPa and m = 0.33 respectively. Also, for
washer under its own weight on the sloped surface from Al-alloy                        adhesive layer the multi-linear isotropic material model was used
plate. Also based on the similar experiments, the friction coefﬁ-                      and the Poisson’s ratio was considered equal to 0.35. Furthermore,
cient was found to be l = 0.4 for the contact between the plates.                      for the steel bolt a linear elastic material relation was assumed
   In order to characterize the aluminium alloy 2024-T3 stress–                        with Young’s modulus of 207 GPa and Poisson’s ratio of 0.30 as it
strain behaviour, an elastic–plastic multi-linear kinematic harden-                    was observed that the bolt material remained in elastic when
ing material model with Von Mises criterion was used. This                             it was subjected to maximum applied torque (8 Nm). As the bolt
behaviour of the material was obtained from the simple tensile                         and its washer have approximately the same material properties,




the geometrical model of the washer was added to the bolt head in                     be seen, the most compressive stresses are observed near the edge
order to minimize the contact element use (with ignoring contact                      of the hole for simple bolted specimens, which increased from À20
elements between the bolt head and the washer).                                       to À101 MPa when the tightening torque increased from 1 to
   Numerical analyses were carried out in two main steps includ-                      5 Nm. In addition for the hybrid specimens the compressive stres-
ing the application of the clamping force which was followed by a                     ses increased from À13 to À49 MPa when the tightening torque
longitudinal load to the aluminium alloy. In the ﬁrst step of load-                   increased from 1 to 5 Nm.
ing, axial displacement was applied to the bottom face of the bolt                        In the second load step, in accordance with the experimental
shank to simulate the clamping force. In order to apply clamping                      test process, a tensile remote stress was applied to the FE models
force in the bolt, a displacement boundary condition (using a trial                   to simulate the tensile loading of the specimens. Therefore, a
and error method) was used instead of applying force on the bolt                      remote tensile stress equal to the applied cyclic load range was
shank. This is necessary to have an accurate simulation and con-                      applied on the right end of the main plate while the displacement
sider the bolt pre-tension relaxation during applying tensile load                    of the left end of the connector plates was constrained. After the
to the far end of the plate. As the tensile load is applied to the                    application of clamping force which was followed by a longitudinal
far end of the plate, a part of initial clamping force in the bolt is                 load, the solution of the numerical analysis has been implemented.
reduced due to contraction of the holed plate thickness as a result                   Then the crack opening stress and the relative stress gradient dis-
of Poisson’s ratio. This process was completed for three initial                      tributions versus distance from the hole edge which are important
clamping forces resulting from the different amounts of tightening                    to calculate the fatigue notch factor, can be obtained. To realize and
torques for both kinds of the joints using a trial and error method.                  recognize the volumetric method concept, the elastic–plastic crack
Then the solution was restarted for each state and in the second                      opening stress, and its integration direction should be noticed. In
step, the value of the maximum force in each cyclic loading (as                       this study, the cracks propagate at root of the plate hole and the
in the experimental tests) was applied to the end of the plate in                     crack opening stress is perpendicular to crack face and parallel to
the model as a longitudinal static load.                                              longitudinal applied load at the free end of the plate. The typical
                                                                                      fractured specimens for simple and hybrid joints under fatigue
5. Results and discussion                                                             tests with tightening torque of T = 1 Nm that subjected to maxi-
                                                                                      mum remote longitudinal load equal to 7.2 kN is shown in
    In the current study, the effect of the tightening torque on the                  Fig. 13. Figs. 14 and 15 show the typical crack opening stress and
fatigue strength of 2024-T3 double lap simple bolted and hybrid                       the relative stress gradient distributions versus distance from the
(bolted–bonded) joints subjected to the longitudinal cyclic loading                   hole edge toward the plate edge with different amounts of the
has been studied by means of the volumetric method. As it has                         tightening torque under the applied maximum remote longitudi-
been discussed, the fatigue life estimation based on the volumetric                   nal load equal to 192 MPa for the simple and hybrid specimens,
method needs the stress distribution near the notch roots which                       respectively. As shown in Fig. 14, in the simple bolted specimens
can be obtained by the nonlinear elastic–plastic ﬁnite element                        the maximum stress values occurred at a distance away from the
analysis and the smooth or reference fatigue curve of the material.                   edge of the hole and their values decreased from 402 to 378 MPa
    As it was mentioned previously, three different tightening tor-                   when the tightening torque increased from 1 to 5 Nm. According
que values were selected to be applied. To do so, the corresponding                   to Fig. 15 for hybrid specimens the maximum stress values
clamping force, i.e. Fcl = 976, 2440 and 4880 N for simple bolted                     occurred at the edge of the hole and their values decreased from
joints, and Fcl = 840, 2100 and 4200 N for hybrid joints were to                      278 to 252 MPa when the tightening torque increased from 1 to
be applied on the plates. Therefore, a displacement boundary con-                     5 Nm. This is due to elastic stress distribution in main plate for
dition in the Z direction was applied on the lower face of the bolt                   the hybrid specimens. In addition, as it is clear in the ﬁgures,
shank to achieve the desired clamping forces equal to experimental                    increasing the clamping force of the bolts, decreases the magni-
test results. The magnitude of the required displacement was                          tudes of maximum and effective stresses, and also decreases the
found after a few trial and error processes to achieve the desired                    effective distance. Consequently the increase in the clamping force
reaction forces or the clamping forces resulting from tightening                      decreases the amounts of the fatigue notch factors, results in
torques.                                                                              improved fatigue lives of the joints.
    According to the results of the ﬁrst load step solution of the                        In this section, to investigate the validity and accuracy of the
ﬁnite element analysis, some beneﬁcial compressive stresses were                      volumetric method, the fatigue lives predicted with this method
observed near the hole of the plates. The compressive stress con-                     have been compared with the experimental test results. Fig. 16
tours around the bolt hole of the main plate, created due to 1,                       shows the fatigue strength versus the number of cycles to fracture
2.5 and 5 Nm tightening torques are shown in Fig. 12. As it can                       for different specimens on the volumetric method and experimen-
                                                


tal test results. As it can be seen in this ﬁgure, the volumetric                           The volumetric fatigue life prediction is not similar to the other
method has a good agreement with the experimental test results                           recognized methods such as Neuber and Peterson [30,31]. The vol-
and presents very good and reasonable results. It must be men-                           umetric approach uses the fatigue curve of smooth specimen, and
tioned that, among the technical literature and surveys which are                        the stress distribution near notch tip which obtained by means of
available in the ﬁeld of the fatigue life estimating, a few researches                   the ﬁnite element analysis. Using the ﬁnite element analysis, load-
can be found regarding to the application of the volumetric                              ing mode, relative stress gradient and geometrical effects are con-
approach, in order to obtain the fatigue life of the different types                     sidered and it is clear that this method provides better results. The
of specimens [6,26–28]. According to the Ref. [28], the volumetric                       predicted fatigue life using the volumetric approach, have very
approach has been successfully employed for fatigue life assess-                         good agreement with the experimental results such as different
ment of the multi resistance spot welded joints. As mentioned pre-                       notched specimens, spot welded joints and simple bolted plates
viously, although the essential parameters of the volumetric                             [6,26–28]. The results of this work and other researches [6,26–
approach are ﬁnite element analysis and the values of fatigue                            28] indicates that this approach gives a relative good description
strength reduction factor and fatigue life of smooth specimens,                          of notch effect.
the important prerequisite of this procedure is to recognize the
direction of the fatigue crack path of the notched specimens.

                                                                                             


                                                                                                                          means of the available smooth S–N curve of Al2024-T3 and the
                   
                               
                                                                                                                          fatigue notch factor was used. The estimated fatigue life was com-
                                                                                                                          pared with the available experimental test results. The investiga-
                     
                                                                                                                          tion revealed that the fatigue life estimated by the volumetric
                                                                       method has a good agreement with the experimental results for
                  
                                                                                                                          various specimens under the different amounts of the tightening
                                                       
                                                                                                                          torque. The obtained results showed that the fatigue lives of spec-
                                      imens were improved by increasing the tightening torque due to
                        
                                                                                                                          compressive stresses which appeared around the hole. The
                                                                                      improvement in fatigue life can be attributed to the method that
Fig. 16. Fatigue strength versus the number of cycles to failure for different                                            the joint transmits the applied load. Furthermore, in the hybrid
specimens based on the volumetric method and comparison with experimental                                                 joints, direct metallic contact between the plates does not occur;
results.                                                                                                                  therefore, the possible fretting fatigue is eliminated.

    The advantages of the volumetric method include the possibil-
ity of predicting fatigue life for many loading cases using notched                                                     
geometry, the absence of the empirical and ambiguous coefﬁcients
used in traditional methods and the opportunity to obtain rapid                                                           
and cost-effective results using a ﬁnite element method.                                                                
                                                                                                                         
    Based on this investigation, in the lightly fastened joints, there                                                  
are only very small compressive stresses around the hole due to                                                              
the small clamping force resulting from torque tightening. Conse-                                                       
                                                                                                                              
quently in such cases, bolts and nuts play only a small role in                                                       
delaying the fatigue crack initiation and propagation.                                                                     
    The results obtained from the experimental tests, revealed that                                                        
                                                                                                                         
the fatigue life of the joints were improved by increasing the
                                                                                                                               
tightening torque due to the compressive stresses which                                                                     
appeared around the hole. The improvement in fatigue life can                                                        
                                                                                                                             
be attributed to the method that the joint transmit the applied
                                                                                                                          
load. As the tightening torque is increased, a large portion of                                                                
the load is transmitted by friction. The remaining portion of the                                                          
load is transmitted by the bearing (through the bolt hole surface).                                                           
                                                                                                                               
The improvement in fatigue life can also be related to compres-                                                       
sion around the hole caused by the compression of the plates                                                                  
by the bolt pre-tension. Such negative stresses can reduce the                                                          
                                                                                                                              
total amount of resultant stresses that cause fatigue crack initia-                                                        
tion and propagation in the plate due to the applied tensile exter-                                              
nal loads. This is in complete agreement with available results of                                                 
                                                                                                                          
fatigue life in simple bolted plate and double shear lap bolted                                                             
joints clamped with tightening torques [2,3,6,11].                                                                      
    In addition, in the hybrid joints, the stress concentration around                                                        
                                                                                                                            
the hole is reduced signiﬁcantly. Therefore, the local stress at the
                                                                                                                         
edge of the hole is lower, since some portion of the total load is                                                          
transmitted by adhesive layer.                                                                                          
                                                                                                                               
    Finally, the comparison of the obtained results from the exper-
                                                                                                                         
imental tests, conﬁrms that the hybrid joints have better fatigue                                                            
performance than simple joints for all levels of the tightening tor-                                                          
que. In addition, it should be noted that, in the hybrid joints, direct                                               
                                                                                                                               
metallic contact between the plates does not occur (due to the                                                          
adhesive layer presence); therefore, the possible fretting fatigue                                                         
(that causes an early fatigue crack initiation) is eliminated. This                                               
                                                                                                                             
is in appropriate agreement with some of the previous results                                                                  
reported in the literature, which discussed in introduction such                                                          
as [8,18,20].                                                                                                                  
                                                                                                                         
                                                                                                                               
6. Conclusions                                                                                                        
                                                                                                                              
   In this research, the effects of the torque tightening on the fati-                                                
                                                                                                                        
gue strength of the double lap simple bolted and hybrid joints have                                                        
been investigated via experimental and numerical analysis.                                                                  
                                          

Abstract 

Because of their cross-functional nature in the company, enhancing Production Planning and Control (PPC) functions can lead to a global improvement of manufacturing systems. With the advent of the Industry 4.0 (I4.0), copious availability of data, high-computing power and large storage capacity have made of Machine Learning (ML) approaches an appealing solution to tackle manufacturing challenges. As such, this paper presents a state-of-the-art of ML-aided PPC (ML-PPC) done through a systematic literature review analyzing 93 recent research application articles. This study has two main objec-tives: contribute to the definition of a methodology to implement ML-PPC and propose a mapping to classify the scientific literature to identify further research perspectives. To achieve the first objective, ML techniques, tools, activities, and data sources which are required to implement a ML-PPC are reviewed. The second objective is developed through the analysis of the use cases and the addressed characteristics of the I4.0. Results suggest that 75% of the possible research domains in ML-PPC are barely explored or not addressed at all. This lack of research originates from two possible causes: firstly, scientific literature rarely considers customer, environmental, and human-in-the-loop aspects when linking ML to PPC. Secondly, recent applications seldom couple PPC to logistics as well as to design of products and processes. Finally, two key pitfalls are identified in the implementation of ML-PPC models: the complexity of using Internet of Things technologies to collect data and the difficulty of updating the ML model to adapt it to the manufacturing system changes. 
Keywords Machine learning.¡¤ Industry 4.0.¡¤ Smart manufacturing.¡¤ Production planning and control.¡¤ State-of-the-art.¡¤

Introduction 

The current manufacturing environment is characterized by high complexity, dynamic production conditions and volatile markets. Additionally, companies must offer cus-tomized products while engaging low costs and reducing the time-to-market if they want to remain competitive in a globalized world (Schuh et.al. 2017b; Carvajal Soto et.al. 2019). This situation poses tremendous challenges for manufacturers who seek to implement new technologies to meet their objectives while expecting a return on invest-ment. Several countries have developed projects that aim to help companies adapt their industries to new production technologies. For instance, Germany created Industry 4.0 (I4.0), the United States proposed the Smart Manufacturing Leadership Coalition, and China introduced the plan called China Manufacturing 2025 (Wang et.al. 2018a). This has led to significant financial support for manufacturing research; 
iFAKT France SAS, Toulouse, France 
for example in the European Union around €7 billion will be invested by 2020 in Factories of the Future (Kusiak 2017). 
Among the Industry 4.0 groups of technologies (Ruess-mann et.al. 2015), Big Data and Analytics (BDA) allows the constantly growing mass of produced data to be harnessed to generate added value. In fact, data generation in modern manufacturing has undergone explosive growth, reaching around 1000 Exabytes per year (Tao et.al. 2018). However, the potential of this data has been found to be insufficiently exploited by companies (Manns et.al. 2015; Moeuf et.al. 2018). As BDA enables the exploitation of data, the scope of this review will focus on this technology, and more spe-cifically ML applied in Production Planning and Control. 
In the context of I4.0, Production Planning and Control (PPC) can be defined as the function determining the global quantities to be produced (production plan) to satisfy the commercial plan and to meet the profitability, productivity and delivery time objectives. It also encompasses the con-trol of production process, allowing real-time synchroniza-tion of resources as well as product customization (Tony Arnold et.al. 2012; Moeuf et.al. 2018). In this review, I4.0 is considered a synonym of Smart Manufacturing, as they both refer to technological advances that value data to draw improvements in production. For example, Ruessmann et.al. (2015) proposed nine technologies for I4.0 while Kusiak (2019) suggested six, but for Smart Manufacturing. Both proposals tend to refer to similar technologies and variations depend on the authors¡¯ focus. Hence, as the PPC is a core function of manufacturing, this paper regards its improve-ment through I4.0 technologies, namely ML, which concerns BDA. Regarding ML, the definition that will be retained is the one of a computer program capable of learning from experience to improve a performance measure at a given task (Mitchell 1997). 
Classical approaches to performing PPC include analyti-cal methods and precise simulations, providing solutions that may rapidly become unfeasible in the execution phase due to the stochastic nature of the production system and uncertain-ties such as machine breakdowns, scrap rate, delayed deliv-eries, etc. Moreover, Enterprise Resource Planning (ERP) systems perform poorly at the operative level (Gyulai et.al. 2015). To tackle this issue, ML can endow the PPC with the capacity of learning from historical or real-time data to react to predictable and unpredictable events. Even though this may suggest that organizations must invest in data ware-housing to handle the mass amount of collected data, stud-ies have reported that enterprises successfully implementing data-driven solutions have experienced a payback of 10¨C70 times their investment in data warehousing (Rainer 2013). 
Having introduced the synergism between ML and PPC, this study aims to provide an analysis of its state-of-the-art through a systematic literature review. This will contribute to the definition of a methodology to implement a ML-PPC and to the proposal of a map to classify scientific literature. This paper analyzes research produced in the context of the I4.0 and is guided by five research questions: 
1. 
Which are the activities employed to perform a ML-PPC? 

2. 
Which are the techniques and tools used to implement a ML-PPC? 

3. 
Which are the currently harnessed data sources to imple-ment a ML-PPC? 

4. 
Which are the addressed use cases by the recent scien-tific literature in ML-PPC? 

5. 
Which are the characteristics of the I4.0 targeted by the recent scientific literature in ML-PPC? 


The first three questions are related to the first objective of this research. They will contribute to the definition of a methodology to implement a ML-PPC. The last two ques-tions address the second objective, as they will provide the basis to create a classification map. 
The reminder of this paper is organized as follows: sec-tion ¡°Research methodology and contribution¡± will explain the systematic literature review methodology employed to search and choose the sample of scientific articles. Addition-ally, the contribution of this paper with respect to similar studies will be briefly highlighted and a short bibliometric analysis is presented to assess the keywords used as string chains. The ¡°Analytical framework¡± section will explain the four axes encompassed by the analytical framework. After-wards, the ¡°Results¡± section will focus on the results of the systematic literature review and an analysis of it. Finally, the ¡°Conclusion and further research perspectives¡± sec-tion will conclude this study and provide further research perspectives. 


Research methodology and.contribution 

To meet the two objectives of this study, a systematic litera-ture review was carried out following the method proposed by Tranfield et.al. (2003) who extended research methods from the medical sector to the management sciences. This method has been successfully employed by other authors to draw insights from the scientific literature (Garengo et.al. 2005; Moeuf et.al. 2018). This literature review focuses exclusively on applications of ML in PPC in the context of I4.0. 
In another domain, Zhong et.al. (2016), proposed a bibli-ometric analysis of big data applications on different sectors such as healthcare, supply chain, finance, etc. but its focus on manufacturing was limited. (Kusiak 2017; Tao et.al. 2018; Wang et.al. 2018a) provided a literature analysis of data-driven smart manufacturing, citing representative references. 
However, these references were not chosen through a sys-tematic literature review. Finally, (Sharp et.al. 2018) could be considered as a study close to this paper as the authors used a pre-defined methodology to select the articles to ana-lyze. Nevertheless, they employed Natural Language Pro-cessing (NLP) to analyze around 4000 unique articles and provide insights about the scientific literature of ML applied in I4.0. The use of NLP can be useful to identify important trends, but it does not allow the authors to analyze the detail of the reviewed papers, where it is likely to find interesting research gaps and insights. On the other hand, a systematic review allows the authors to both follow a rigorous method-ology and perform a detailed study of each chosen article. 
Even though the PPC is closely related to the domain of supply chain, the latter is not included in the scope of this review as its vastness would increase the risk of stray-ing from the focus on PPC. Therefore, to learn about recent trends on this topic, the authors invite readers to refer to Hosseini et.al. (2019), who performed a comprehensive review of quantitative methods, technologies, definitions, and key drivers of supply chain resilience. In fact, supply chain resilience is a growing research area that examines the ability of a supply chain to respond to disruptive events (Hosseini et.al. 2019). Applications of this topic have been done by Hosseini and Barker (2016), who applied Bayesian networks to perform supplier selection based on primary, green, and resilience criteria; and Hosseini and Ivanov (2019), who proposed a method using Bayesian networks to assess the resilience of suppliers in identifying critical links in a supply network. 
The queries were performed between 10/10/2018 and 24/03/2019 in two scientific databases: ScienceDirect and SCOPUS. The following keywords conducted the research: 
. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction scheduling¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction planning¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction control¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Line balancing¡±) 


To consider the context of I4.0, only papers published since 2011 were considered because this year corresponds to the formal introduction of I4.0 at the Hannover Fair. Addition-ally, only communications labeled as ¡°Research Articles¡± in ScienceDirect and ¡°Conference paper¡± OR ¡°Article¡± in SCOPUS were included to solely capture articles present-ing application models. Subsequently, a review of titles and abstracts allowed for the exclusion of articles not related to ML-PPC. After the removal of duplicates, a full text analysis allowed a final selection that excluded papers that did not fit with research questions. The sample size obtained encom-passes 93 scientific papers. The article selection methodol-ogy with its Restrictions (R) is described in Fig..1. 

A brief focus on.the.query keywords 

The used string chains represent a core strategic choice for review. Therefore, this sub section aims to provide an analy-sis of the employed keywords. 
Concerning the keywords used in the first parenthesis of the string chains, the use of ¡°Deep Learning¡± and ¡°Machine Learning¡± was done for two reasons: firstly, they are rela-tively new terms, which eases the identification of recent trends in the literature; and secondly, they are directly related to one of the two core subjects in this study, which is ML. Other terms such as ¡°Data Mining¡± or ¡°Statistical Learn-ing¡± could have been sensible choices too, as they are often used interchangeably with ¡°Machine Learning¡± and ¡°Deep Learning¡±. Nevertheless, using these two terms might have deviated this study from its core topic. In fact, a recent study suggests that the differences between ML and Data Min-ing are not consistently defined in the literature. Thus, Data Mining is mostly considered to be the process of generat-ing useful knowledge from data (Schuh et.al. 2019). To do so, it draws from other fields such as Artificial Intelligence, Statistics, ML, and Data Analytics. Therefore, Data Mining can be a vast topic, and does not exclusively concern ML, which could potentially affect the focus of this study. As there seems to be no clear boundary between these terms, a short bibliometric analysis was performed to assess the chosen keywords. The analysis was done using VOSviewer, software developed by the University of Leiden to draw insights from scientific literature. Furthermore, using key-words related to specific ML techniques such as ¡°Random Forest¡± or ¡°k-means¡± did not seem appropriate due to the risk of introducing a bias when answering the second research question. In fact, this could have artificially boosted the results of the queried techniques. 
The bibliometric analysis followed a similar methodology to that used to choose the final article sample (cf. Fig..1). The objective was to briefly assess the influence of differ-ent keywords on the queries¡¯ results. For the analysis, three different string chains were considered: ¡°Deep Learning¡± OR ¡°Machine Learning¡±, ¡°Data Mining¡±, and ¡°Statistical Learning¡±. The queries were performed on 06/10/2019 and the detail of the search strategy can be found in ¡°Appendix I¡±. Finally, as the aim is to analyze the available literature when querying with a certain string chain, no Title and abstract review was done as this could introduce a bias into the results due to the authors¡¯ influence. 
The bibliometric analysis focused on the keywords defined by the authors for all of the papers of each of the three samples. To represent the results, the network visualization from VOSviewer was employed. In such a net-work, the nodes represent the keywords or items, their sizes represent the keyword importance determined by the number of occurrences, and the links between the nodes represent their co-occurrence. Furthermore, the relatedness between two terms is represented through their spatial distance in the network: two keywords closely related will be spatially closer. For this review, the obtained networks were displayed under the ¡°overlay visualization,¡± which shows the average publication year for each of the keywords through a color scale. For clarity reasons, a filter was applied on the mini-mum number of occurrences to display; at most, 50 items per graph. Also, the queried keywords were highlighted with a red frame to assist in their identification. The networks are presented on Figs..2, 3, 4. 


Results from the bibliometric study suggest that ¡°Statisti-cal Learning¡± may not be a common keyword to find in ML-PPC research because the size of the obtained article sample (241 articles) is far below the results obtained with the other two queries. In fact, ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± and ¡°Data Mining¡± provided 2862 and 2166 articles, respectively (cf. ¡°Appendix I¡±). This is also stated in all of the networks, in which the item ¡°Statistical Learning¡± does not appear, probably due to the filter excluding keywords with a low number of occurrences. 
Analyzing the relatedness between ¡°Data Mining¡± and ¡°Machine Learning¡± by their spatial distance on the net-works provides an idea of how these concepts are associ-ated: they are spatially closer on the ¡°Data Mining¡± Network (Fig..3) than on the ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± network (Fig..2). This suggests that Data Mining tends to relate more often to ML, rather than ML to Data Mining. Such a relation may support what is said in (Schuh et.al. 2019), in which Data Mining is considered a field drawing from ML, Artificial Intelligence, Statistics, etc. to produce useful insights. 
Findings from the network visualizations show that the item ¡°Machine Learning¡± is always associated with a more recent average publication year than ¡°Data Mining¡±. This supports the idea that ¡°Machine Learning¡± is a relatively new term, which can lead to the identification of recent trends in literature. Furthermore, querying with ¡°Deep 


Fig. 2 Network visualization with the average publication year for ¡°Deep Learning¡± OR ¡°Machine Learning¡± 

Fig. 3 Network visualization with the average publication year for ¡°Data Mining¡± Fig. 4 Network visualization with the average publication year for ¡°Statistical Learning¡± 

Learning¡± OR ¡°Machine Learning¡± provides a more recent average publication year (2017.06) for the item ¡°Machine Learning¡± than with the other two queries: 2016.95 when querying with ¡°Data Mining¡± and 2016.41 when querying with ¡°Statistical Learning¡±. Finally, ¡°Deep Learning¡± OR ¡°Machine Learning¡± was the only query enabling the inclu-sion of the item ¡°Deep Learning¡± with enough occurrences 
(25) to pass the filter, which is a recent research topic with an average publication year of 2018.28. 
From the bibliometric analysis, it could be concluded that using ¡°Deep Learning¡± OR ¡°Machine Learning¡± as part of the query keywords is appropriate enough, as this allows for the identification of a big sample of recent papers, enabling the identification of new trends. It seems that ¡°Statistical Learning¡± does not provide enough recent results to be con-sidered. Finally, even if ¡°Data Mining¡± is closely related to ¡°Machine Learning,¡± it covers a vast domain that can deviate from the focus of this review. 
Regarding the keywords employed in the second paren-thesis of the string chains, the objective was to represent the main functions of the PPC under the definition provided in the introduction section. Consequently, a determination of the global production quantities was represented by ¡°Produc-tion Planning¡± and the aspect of the main objectives (i.e. profitability, productivity, and delivery time) was depicted by ¡°Production Control¡±. Finally, the real time synchroni-zation of resources as well as product customizations were represented by both ¡°Production Scheduling¡± and ¡°Line Balancing,¡± given the fact that companies should be able to perform balanced scheduling even when facing customized client orders. 
As the PPC is a transverse topic tangled with other func-tions such as maintenance, quality control, logistics, etc., the challenge was to decide whether or not these related subjects should be included as explicit keywords for the queries. The final choice was to not include them through keywords, as this would broaden the perimeter of the research too much, losing a focus on PPC. Nevertheless, it was decided to include, in the final article sample, the studies dealing with other functions only if they were related to the PPC. 

METHOD

Analytical framework 
This section presents the four axes that build the analytical framework that will be employed to harness knowledge and insight from the final sample of 93 scientific articles. 
First axis of.the.analytical framework: the.elements of.a.method 
This axis concerns the first and second research questions: the activities, techniques, and tools to implement a ML-PPC model. To link these three elements, the concept of ¡°Man-datory Elements of a Method¡± (MEM) proposed by Zellner (2011) is used. In fact, this concept has been successfully employed by other authors to propose methodologies in research domains such as product development (Lemieux et.al. 2015) and lean in hospitals (Curatolo et.al. 2014). Moreover, (Talhi et.al. 2017) suggested its use to develop a methodology in the context of cloud manufacturing applied to product lifecycle management. Thus, the MEM suits the first objective of this study, which concerns the definition of a methodology to implement a ML-PPC. There are five elements in the MEM: 
1. 
A procedure: order of activities to be followed when the method is employed. 

2. 
Techniques: the means to generate the results. Activities from the procedure are supported by techniques, while the latter is supported by tools. 

3. 
Results: they correspond to the output of an activity. 

4. 
Role: the point of view adopted by the person who per-forms the activity and is responsible for it. 

5. 
Information model: this refers to the relation between the first four mandatory elements. 


In the scope of this study, the first two elements are the con-cern. Firstly, to evaluate the procedure, the activities used to perform a ML-PPC implementation will be recognized and their use will be measured. By activities, this research refers to tasks such as ¡°model comparison and selection¡± or ¡°data cleaning¡±. Secondly, to address the techniques, ML models and tools will be identified, and their use will be measured. ML models point to elements such as Support Vector Machines or Neural Networks, while tools relate to programming languages or software used to implement these ML models. 
To provide further insight concerning the ML techniques, the learning types will also be measured. This will be used to summarize the information regarding the techniques as well as to ease the identification of trends and research perspec-tives. Additionally, the learning types will serve as a bridge between the first and second objectives of this study, as they will be used in the mapping to classify scientific literature. Based on the work of Jordan and Mitchell (2015), three main learning types can be identified: 
1. Supervised Learning (SL), which concerns ML tech-niques approximating a function f (X)= Y by learning the relationship between the inputs X and the outputs 
Y. For instance, learning the mapping between the Red Green, and Blue (RGB) codes (input X) in an image and the objects in it (output Y) to determine if a certain picture contains a misplaced product in a stock rack. 
2. Unsupervised Learning (UL), which encompasses tech-niques allowing data exploration to find patterns and hidden structures in a given dataset X. For instance, finding categories in maintenance reports by using the description of the problem and the duration of the main-tenance intervention. 
3. Reinforcement Learning (RL), which are techniques allowing the learning of actions to be performed by an agent interacting with a certain environment to maxi-mize a reward. For example, teaching an Automated Guided Vehicle (AGV) in a warehouse how to avoid obstacles to maximize the number of delivered pack-ages. 

Second axis of.the.analytical framework: employed data sources 
This axis addresses the third research question: the har-nessed data sources. Identifying which are the data sources used to perform a ML-PPC is capital. In fact, data could be considered as the raw material allowing ML models to develop autonomous computer knowledge gain (Sharp et.al. 2018). Moreover, the quality of the final model will depend to a great extent on the quality and appropriateness of the used data. Therefore, the choice of the data source is an important decision when training a ML model. To address this axis of the analytical framework, the data source types proposed by Tao et.al. (2018) will be used. They mention that there are five main data sources used in the data-driven smart manufacturing: 
1. 
Management data (M): historical data coming from com-pany¡¯s information systems such as the ERP, Manufac-turing Execution System (MES), Customer Relationship Management system (CRM), etc. M data will concern production planning, maintenance, logistics, customer information, etc. 

2. 
Equipment data (E): data coming from Internet of Things (IoT) technologies implemented in the factory. It refers to sensors installed in physical resources such as machines, places such as workstations or human resources such as workers. In the case of workers, data is collected passively, such as by RFID sensors installed on helmets. 

3. 
User data (U): consumer information collected from e-commerce platforms, social media, etc. It also encom-passes feedback given by workers or experts that will be used to train the ML-PPC model. User data coming from workers is collected actively, for example through interviews or questionnaires. 

4. 
Product data (P): data originating from products or ser-vices either during the production process or from the final consumer. 

5. 
Public data (Pb): data available in public databases from universities, governments or from other researchers. 


The analysis of the 93 shortlisted articles suggested that some of them did not fit into the five data sources proposed by Tao et.al. (2018): these communications used artificially generated data through computer simulations. Therefore, a sixth data source is proposed, which corresponds to the first contribution of this paper to the scientific literature: 
6. Artificial data (A): data generated by computers (e.g. simulations) to assess ML-PPC implementations. 

Third axis of.the.analytical framework: the.use cases of.the.ML.PPC in.the.I4.0 
This axis concerns the fourth question: it aims to show which applications can be achieved when applying a ML-PPC. Moreover, identifying the use cases and quantifying their use frequency is important to detect trends as well as further research gaps. By use cases, this study refers to the different possible applications in a certain domain, such as maintenance, quality control, distribution, etc. In fact, as the PPC is entwined with several manufacturing subjects, is difficult to perform a complete review on PPC if these topics are ignored. For example, if there were a predictive maintenance study meant to enable a more robust produc-tion scheduling, such application would be directly related to the PPC through maintenance. To start this analysis, the use cases of I4.0 initially proposed by Tao et.al. (2018) were considered. They identified six of them: 
1. 
Smart Maintenance: harnessing data to perform preven-tive and predictive maintenance. For instance, monitor-ing machine components to estimate the best date to perform a maintenance intervention. 

2. 
Quality Control: applying BDA to supervise the manu-facturing process or products, seeking for possible qual-ity problems and/or allowing the identification of root causes. 

3. 
Process Control and Monitoring: constantly analyzing data coming from the shop floor to perform a smart adjustment of the functioning parameters of physical resources (machines, AGVs, etc.). The objective is to automatically control these physical resources and/or optimize their parameters with respect to the working conditions. 

4. 
Inventory and Distribution Control: stock management, parts and tools tracking, and distribution control with the use real-time and/or historical data. 

5. 
Smart Planning and Scheduling: considering produc-tion uncertainties to perform a production planning and scheduling closer to the current state of the production system. For instance, considering unexpected mainte-nance problems to reschedule a production order and minimize the delay. 


6. 
Smart Design of Products and Processes: using BDA to support new products and processes development. For instance, using NLP to analyze the technical require-ments of a new product and then to propose the poten-tially suitable manufacturing process. 

The analysis of the 93 scientific articles suggests that these six use cases are not enough to fully characterize the recent publications. Additionally, papers not fitting in the initially proposed use cases shared the same application: time esti-mation (cycle time, operation time, etc.). Consequently, a seventh use case is proposed: 

7. 
Time estimation: adaptation of different manufactur-ing related times to current working conditions. For instance, estimating the operation times to the actual work rate of each employee instead of using the data from the Method Time Measurement (MTM) approach. 



Fourth axis of.the.analytical framework: the.characteristics of.I4.0 
The I4.0 aims to transform the collected data during the product¡¯s lifecycle into ¡°intelligence¡± to enhance the manu-facturing process (Tao et.al. 2018). With this transforma-tion, the objective is to reduce costs while improving the quality, productivity, sustainability of the production system (Wang et.al. 2018a). However, what specific benefits could be expected when embracing the I4.0? To answer this ques-tion, the characteristics of I4.0 need to be identified. Tao et.al. (2018) argue that I4.0 enables the following paradigms: 
1. 
Customer-Centric Product Development: production systems in the I4.0 should be able to adjust their param-eters by considering variables coming from customers such as their behavior, their needs, the way they use the products, inter alia. It is the case of manufacturing per-sonalized products, designing processes from the cus-tomer requirements or proposing a target manufacturing cost for each consumer profile. 

2. 
Self-Organization of Resources: I4.0 should endow pro-duction systems with the capacity of considering data coming from the manufacturing process to better engage the available resources. Additionally, this data should also be used to plan capital and operational expendi-tures. For example, updating the scheduling of machines the shop floor after new urgent order is released. 

3. 
Self-Execution of Resources and Processes: in the I4.0, resources should become ¡°smart¡± by providing them a real-time awareness and interaction capacity with the manufacturing environment (Huang et.al. 2019). There-fore, the self-execution of resources concerns their fac-ulty of making decisions depending on the received 


information or measured data. It is the case of machines automatically adapting their functioning parameters to work optimally or trolleys automatically replenishing workstations when these reach a certain level of security stock. 
4. 
Self-Regulation of the Production Process: unexpected events should be effectively handled in the I4.0. Thus, this characteristic concerns the capability to perform the required adjustments to respond to unpredicted prob-lems. For example, relaunching the scheduling process for a certain production line when one of the machines experienced a breakdown. 

5. 
Self-Learning of the Production Process: this charac-teristic follows a similar logic as the self-regulation of processes in terms of adjustability. However, it relates to the capacity of the production system to adapt to pre-dicted events. It is the case of predictive maintenance, which uses BDA to estimate the remaining useful life of machine¡¯s components. Afterwards, the manufacturing system can adapt to the results of this prediction. 

After concluding the analysis of the 93 articles, three char-acteristics seem to be overlooked: the environmental dimen-sion, the knowledge generation, and the inclusion of the human being. To consider these dimensions that seem to not be explicitly raised in the work of Tao et.al. (2018), three new characteristics are proposed: 

6. 
Environment-Centric Processes: estimations suggest that the electronics and home appliances industry scrapped around 100 million goods in China in 2012 (Tian et.al. 2013). As exemplified, the environmental impact of industry is far from being negligible, which is the reason why industrialized countries have started to tighten regu-lations and engage environmentally friendly practices in manufacturing (Tuncel et.al. 2014). Research done in the context of I4.0 must not overlook this aspect. Therefore, this characteristic concerns the use of new technologies to create environment-centric processes. For example, optimizing the disassembly scheduling process to maxi-mize the number of components that can be recycled. 

7. 
Knowledge Discovery and Generation: most of the com-panies have been computerized for a long time, which has eased the collection of data. Despite the access to a plethora of information systems, generating knowledge from raw data still supposes a major industrial and aca-demic challenge. Besides, the generation of knowledge is a mandatory step to improve the adoption of BDA by companies (Grabot 2018). In fact, knowledge could be considered as one of the most valuable assets in manu-facturing (Harding et.al. 2006), the reason why generat-
ing it represents an important gain behind the adoption of BDA. Therefore, as I4.0 is characterized by allowing 


knowledge creation, research efforts must include it to generate value. One example of this is harnessing data from maintenance reports to provide the production of responsible real-time information about the root causes of machine breakdowns. 
8. Smart Human Interaction: even with the advent of multi-ple I4.0 technologies, its adoption would be significantly hindered by not keeping humans in the loop or not con-sidering their interaction with the proposed solutions. For instance, Thomas et.al. (2018a) experienced the case of a company that was not willing to introduce an improved version of a quality control system because it somehow excluded the person from the process. There-fore, this characteristic concerns the consideration and/ or inclusion of a human being when implementing new technologies. Examples of this would be a worker behavior recognition system based on computer vision or software interacting with operators through NLP. 
Figure 5 summarizes this section. It also presents the relationship between the Research Questions (RQ), the analytical framework axes, the research objectives, and the expected outputs of this study. 



RESULTS

First research question: activities employed in.ML.PPC 
To identify the activities, the tasks used to implement a ML-PPC in each of the 93 communications were identified. Afterwards, these tasks were grouped into categories to ease the information analysis. These groups of activities were analyzed by two experts to keep the most meaningful ones. Results suggest eleven standard and recurrent activities: 
1. 
Data Acquisition system design and integration (DA): design and implementation of IoT systems to collect data. This activity also encompasses the data storage and communication protocols. 

2. 
Data Exploration (DE): use of data visualization tech-niques, inferential statistics, and others to derive initial insights and conclusions about the dataset. 

3. 
Data Cleaning and formatting (DC): data preparation from the raw data to make it exploitable by the ML-PPC model. It concerns tasks such as outlier removal or missing values handling. 

4. 
Feature Selection (FS): choice of the most suitable inputs to the ML-PPC model. It can be done through statistical techniques, e.g. stepwise regression or by means of expert insight. 




Fig. 5 Relationship between the building blocks, research objectives and expected outputs of this study 
5. 
Feature Extraction (FE): use of variables from the ini-tial dataset to calculate more meaningful features. 

6. 
Feature Transformation (FT): representation of the initial features into different spaces or scales using techniques such as normalization, standardization or kernel transformations. 

7. 
Hyperparameter Tuning and architecture design (HT): definition of the ML model architecture and adjustment of its hyperparameters to improve the performance. For instance, optimizing the learning rate and defining the activation function in a neural network. 

8. 
Model Training, validation, testing, and assessment (MT): using the data to perform the training, validation and testing process. It can be done through techniques such as k-fold cross-validation. It also encompasses the choice of the training/validation/testing set split and the model¡¯s performance assessment. 

9. 
Model Comparison and selection (MC): several ML techniques can be used to achieve a certain task. This activity concerns the comparison of multiple ML mod-els to choose the one that better suits the needs. 

10. 
Contextualized Analysis or application (CA): going further than just assessing the model¡¯s performance. It concerns the actual implementation of the ML-PPC model or the analysis of its results in the context of the problem that is addressed by the study. 


11. Model Update (MU): data used to train ML models represents the context of the studied environment at a given moment. However, this context is dynamic, hence the ML-PPC model must be adapted. Therefore, this task concerns the model update through new data. 
To address this research question, the percentage of papers using each activity was measured. These results are summa-rized in Fig..6. Findings suggests that four groups of activi-ties can be proposed following their usage: 


These groups show that a considerable amount of research papers only focus on the architecture design, training, and assessment of ML-PPC models (CUAs cluster), while not employing or documenting the use of other activities. Con-sidering OUAs, it is surprising to find that only half of the communications used the CA, which corresponds to an actual implementation of the proposed model in the context of the study. This suggests that half of the studies go no further than just training and evaluating the performance of the model. 
MUAs group encompasses data pre-processing tasks, which are capital to any ML implementation. Even if these activities are frequently employed in practice, their low usage is probably because researchers do not mention them, implying a lack of documentation. Moreover, as one of the characteristics of big data is the variety (in type, nature, for-mat, etc.) (Zhou et.al. 2017), it is crucial to employ data pre-processing activities to ensure the quality of the final models. Consequently, this lack of documentation can repre-sent a pitfall to practitioners willing to apply ML-PPC based on research papers. 
Finally, SUAs cluster highlights the most important research gaps in scientific literature. Three key findings can be inferred from activities in this group: firstly, the low usage of DA highlights the challenge of coupling IoT technolo-gies with ML-PPC. This is a major pitfall to deploy ML-PPC in companies, as they normally need real-time data or statuses from their manufacturing systems. Secondly, the lack of DE utilization could mean that ML-PPC applica-tions tend to jump directly to activities in the CUAs cluster while overlooking descriptive and basic inferential statis-tics techniques. This represents an obstacle to generating knowledge from data, as DE can draw conclusions easily interpretable by non-ML specialists. Finally, the rare use of MU implies that adapting the ML-PPC model to a dynamic manufacturing context is seldom addressed. This unpredict-able change of the statistical properties and relationships between variables over time is known as concept drift (Ham-mami et.al. 2017). Not addressing this issue can be harmful for the model reliability in the long term. 


Second research question: techniques and.tools used in.ML.PPC 
Concerning the techniques, results present the number of times a given ML model is used. In the case of communica-tions comparing several techniques, only the one chosen by the authors because of its better performance was consid-ered. If this best-performing model employs several tech-niques, each of them is counted as used once. 
There are numerous ML techniques in scientific litera-ture. Therefore, to ease the analysis of results, a grouping of techniques in families is proposed is Table.1. These families were determined with the help of a ML expert. It is impor-tant to mention that the column ¡°Concerned techniques¡± in Table 1 is not an exhaustive list, it is limited to techniques found in the systematic literature review. 
Results are presented in Fig..7. They suggest that NN, Q-Learning, and DT are the most used techniques in ML-PPC. The extensive use of NN is probably due to their abil-ity to learn complex non-linear relationships between vari-ables, often delivering good performance when compared to other techniques. Even if Q-Learning remains, by far, the most used RL technique, other RL models such as Sarsa or R-Learning are used, which points an interest in agent-based modeling in ML-PPC. Finally, the attention drawn by DT techniques is probably linked to their excellent trade-off between accuracy and interpretability, allowing knowledge generation. 
The high use of Clustering techniques could be explained by the fact that data in manufacturing systems is normally unlabeled and can contain meaningful unknown patterns. Therefore, clustering can be employed to discover groups as well as hidden structures in datasets. 
The usage evolution of the six most used technique fam-ilies was also measured. Figures representing this can be found in ¡°Appendix II¡±. Due to an imbalance in the amount of articles over the different years, results are presented as relative frequencies. For example, if the NN achieved a usage of 27% in 2018, it means that 27% of all the techniques used in that year corresponded to such a model. Results sug-gest that there is a strong growth in the use of NN since 2015, this is possibly due to the growing computing power, recent findings in terms of architectures such as CNNs or LSTMs, and the development of specialized frameworks like PyTorch, TensorFlow, Keras, etc. which ease the task of implementing such models. Moreover, results show a grow-ing interest on Ensemble learning techniques which evolved from not being used between 2011 and 2013 to accounting for 14% of applications in 2018. This can possibly explain the loss of interest on DT since 2017, as Random forests (a type of Ensemble learning) can achieve better performance by using committees of decision trees. 
Table 1 Technique families with their respective ML models 
Family Concerned techniques 
As NN and Ensemble learning families seem to be recently attracting the research community, a detailed view of their encompassed techniques is presented in ¡°Appen-
dix III¡±. Concerning NN, the most used technique is the Multi-layer perceptron, which is the classic architecture of a NN. However, more specialized architectures belonging to deep learning are starting to appear in PPC research. Such is the case of the CNNs, LSTMs, and Deep Belief Networks. 
These techniques have presented good performance when dealing with specific problems, such as image recognition for CNNs, time series analysis for LSTMs or feature extrac-tion for Deep Belief Networks. In the case of Ensemble learning, the most used technique is, by far, the Random forests. They seem to provide excellent results while ena-bling knowledge generation. In fact, they allow the most meaningful variables to be easily identified in the SL task, which is the reason why researchers tend to use them to both attain accuracy and model interpretability. 
To measure the utilization of the learning types, each paper was analyzed, and the learning types used were iden-tified and counted. As a given model can use several ML techniques, it can refer to several learning types at the same time. Hence, the different synergies between these were also considered. Results are presented in Fig..8. 
Findings show that the most used learning type is SL. This is probably because SL addresses two recurrent needs in applied research: classification and regression. In fact, SL can be used to learn the relationship between an input X and an output Y that can be either discrete in the case of classification or continuous for regression. Furthermore, it was found that RL techniques are extensively used, which confirms the interest behind agent-based models. 
Concerning UL, it seems to be especially used with SL (SL-UL), which suggests a strong synergy between these 
Learning type 

52 Number of uses 
Fig. 8 Number of uses by learning type 
C++ WEKA Java Python RapidMiner R Others MATLAB Not Mentioned 

41 Number of uses 
Fig. 9 Number of uses by tool 
two learning types. The reason behind this could be that UL techniques are normally used to perform data pre-process-ing, as with Principal Component Analysis, or discovery of hidden patterns in datasets, e.g. with Clustering. There are 6 papers using just UL, however, this learning type seems to unlock all of its potential when used in synergies, allowing for the design of more complex models. 
Even if there are some SL-RL synergies, they are not very common. This is probably because SL is normally coupled with RL when there is a need of performing rapid estimations of functions to save computing time. However, it seems that most of the applications do not reach a scale that needs this kind of configuration. Finally, it was found that using UL-RL and SL-UL-RL is rare in the scientific litera-ture. This does not mean that their synergy does not provide advantages, it is just that there may not be a current need for it. Also, it could be that coupling these learning types over-complexifies the model design, which prevents its use. 
Concerning the tools, only programming languages or software used to implement the ML model were considered. Therefore, other tools such as discrete event simulation soft-ware are out of the scope of this research. Results are pre-sented in Fig..9. 
For clarity sake, tools being used only once were grouped in the category denominated as ¡°Others¡±. These tools were: ACE Datamining System, C#, Clementine, GeNIe Modeler, Hugin 8.1, NetLogo, Neural-SIM, Visual C++, and Xelopes Library. Additionally, it is important to mention that most of the researchers do not mention the tool the use to imple-ment the models. 
It could be said that MATLAB is, by far, the most used tool to perform ML-PPC in research. Besides its robust cal-culation capacity, the reason behind this could be that uni-versities often invest in licenses for this software; therefore, they expect their researchers to use this tool. R is the second most used tool, which may be because it is a free software targeting statistical applications, including ML. Finally, the third most used tools are both RapidMiner and Python. The former eases the implementation of ML models thanks to its visual programming logic, while the latter is a multipur-pose programming language recently characterized by its ML libraries and frameworks such as Scikit-learn, PyToch, Keras, etc. 


Third research question: used data sources to.implement a.ML.PPC 
To answer this question, the data sources used by each of the analyzed papers were identified. These results are sum-marized in Table.2. The column ¡°Identification¡± (ID) will assign a number to each communication. This will be used later to establish a mapping of the scientific literature. 
Results show that ¡°Artificial data¡± is the most used data source in recent scientific literature. This probably highlights the difficulty of accessing data coming from companies. Additionally, it is important to remember the extensive use of RL techniques. These models normally require constant access to data concerning the real-time status of the produc-tion system, which can be difficult to find in real factories. Therefore, researchers normally use Artificial or Public data to test their models. This issue could be addressed by creating digital twins, but this still represents a research challenge. 
The extensive use of artificial data suggests that there are data availability issues. This poses two main challenges: firstly, dealing with highly unbalanced datasets when train-ing, for instance, SL algorithms for classification, and sec-ondly, accessing enough data to enable good generalization capacity, especially in deep learning models. 
The first challenge is common when training ML mod-els to identify disruptions. In fact, disruptive events in PPC such as machine breakdowns or quality problems tend to be scarce when compared to the total size of the dataset. Thus, ML techniques struggle to learn these events. To tackle this issue, some authors have proposed solutions such as data augmentation, a common practice in computer vision that consists of artificially creating new training examples by modifying existent observations (Perez and Wang 2017; Miko.ajczyk and Grochowski 2018). Another approach is to use crafted algorithms adapted to class-imbalance. Bi and Zhang (2018) performed a comprehensive comparison of 
Table 2 Data sources used by 



Fourth research question: addressed use cases by.recent scientific literature 
To answer this question, each analyzed article was allo-cated to one of the seven proposed types of use cases. This allows to measure their importance in the scientific literature (Fig..10). 
Results point out that Smart Planning and Scheduling is the most addressed use case in recent scientific literature, with nearly half of the communications discussing it. This result may come from two main reasons: firstly, the string chains used in the methodology are closely related to this use case; secondly, it normally uses structured data relatively easy to get from information systems, which eases the task of implementing a data-driven approach. The strong use of Time Estimation in ML-PPC (14% of the papers) suggests that classical time measurement methods are not compliant with the growing complexity of the manufacturing systems, which may represent a pitfall to perform a reliable planning. Therefore, ML models considering more diverse variables


as inputs are being adopted. Moreover, some researchers have addressed the coupling of Smart Maintenance, Process Control and Monitoring, and Quality Control with the PPC. However, there is still effort to be made, as the share of these use cases was no higher than 10%. 
Finally, two use cases are targeted as critical: The Inven-tory and Distribution Control (6%) and the Smart Design of Products and Processes (4%). These findings suggest two things: first, a lack of integration of the logistic functions into the ML-PPC, and secondly, a difficulty for harnessing insights from data to serve product and process design. This difficulty is probably because data employed in design is highly unstructured (text data, image data, etc.) and greatly depends on people¡¯s experience. 


Fifth research question: the.characteristics of.I4.0 
To quantify their usage, the addressed characteristics in each of the 93 analyzed papers were identified and counted. Results are summarized in Fig..11. In this figure, the sum of all the totals is higher than 93 as one ML-PPC model can satisfy several characteristics. 
Findings show that the Self-Organization of Resources is, by far, the most addressed characteristic (56 uses) in ML-PPC applications. This result was expected, as this charac-teristic can be achieved through production planning and scheduling, two functions directly related to the PPC and found to be extensively employed in the use cases. There-fore, it can be concluded that the ML-PPC based models effectively enable this characteristic. 

Smart Human Interaction 
Knowledge Discovery and Generation Environment-Centric Processes Self-Learning of the Production Process 
Self-Regulation of the Production Process Self-Execution of Resources and Processes Self-Organization of Resources Customer-Centric Product Development 
Number of papers 

papers), as well as the Knowledge Discovery and Genera-tion (26 papers) appear to be moderately boarded. This leads to two main conclusions: first, ML-PPC models effectively endow manufacturing systems with the capacity of adapting to unexpected events and predicting production problems. This is suitable to handle the stochastic nature of production environments. Secondly, ML is suitable to generate knowl-edge from PPC data, which is crucial in I4.0, where data is abundant, and it can provide useful guidelines to improve the company¡¯s know-how. 
Four characteristics were rarely satisfied: The Customer-Centric Product Development (3 papers), the Self-Execution of Resources and Processes (4 papers), the Smart Human Interaction (7 papers), and the Environment-Centric Pro-cesses (8 papers), which points to strong research perspec-tives of ML-PPC applications enabling these features. Con-cerning the Customer-Centric Product Development, it was rare to find papers including customer-related variables into their PPC. This can be due to the difficulty to access data from customers or end users. For instance, as observed in the data sources section, user data was seldom employed. 
The low number of papers dealing with Self-Execution of Resources and Processes suggests that it is unusual to couple the PPC with autonomous physical resources. This can be due to the complexity of such systems as they require important capital investments as well as multi-disciplinary knowledge in production systems, mechatronics, and control theory. 
It was very surprising to find that the Smart Human Interaction (7 papers) and the Environment-Centric Pro-cesses (8 papers) are rarely addressed. Indeed, manufactur-ing systems can be human based in several steps such as during the execution in the shop floor or during the tactical planning definition. Not considering the interaction of the proposed ML-PPC models with humans can be harmful for the deployment of the proposed system, as it may worsen the working conditions. Therefore, thinking about this human-ML interaction is the cornerstone for a successful adoption. Concerning the Environment-Centric Processes, scarce applications tried to minimize the environmental impact of production processes through ML-PPC. In a world where natural resources are becoming rare, this is a non-negligible aspect that must be considered, not only because of the tightening of environmental laws by gov-ernments but also because of the ethical responsibility of companies. 


Cross.axes analysis: mapping the.scientific literature through.use cases, I4.0 characteristics, and.learning types 
To address the second objective of this study, a mapping of the scientific literature in ML-PPC is proposed. This is achieved through a cross-analysis employing the use cases, characteristics of I4.0, and learning types. Results are repre-sented via a cross-matrix having the use cases in the vertical axis and the characteristics of I4.0 in the horizontal axis. This matrix also allows the maturity of a given use case to be assessed. For instance, a mature use case in the scientific literature will tend to satisfy more I4.0 characteristics. From this point of view, the crossing between a characteristic of I4.0 and a use case will be referred as a domain. 
The ID numbers defined on Table.2 are employed to place the analyzed articles in the matrix. Additionally, the learn-ing types employed by each communication are represented using a color code. Figure.12 provides a summarized view of this matrix, allowing for a high-level analysis that will help to identify research gaps and trends in ML-PPC. Figure.13 is 



a detailed view of the matrix indicating the scientific articles with their respective learning types found in each domain. 
Figure.12 shows that among the 56 possible domains, 18 (32%) were not addressed at all. Furthermore, 24 (43%) domains lie in the range of 1 to 3 papers. This means that nearly half of the domains are in an exploration phase. These two remarks lead to conclude that ML-PPC in the I4.0 is still an active research topic with strong perspectives. 
From Fig. 13, it can be said that there is a strong trend of using multiple synergies between learning types across all of the different use cases. However, there are no applications of RL in Time Estimation and in Smart Design of Products and Processes. The reason for this may be that these use cases have strong strategic impacts. Therefore, current ML implementations in such applications aim to support deci-sions rather than automating them such as with agent-based systems driven by RL. 
There are two use cases achieving a high maturity: Smart Planning and Scheduling and Process Control and Moni-toring. They both cover all but one of the characteristics of I4.0. In the case of Smart Planning and Scheduling, it fails to address the Self-Execution of Resources and Pro-cesses, which suggests that there are research perspectives in coupling the production planning and scheduling with autonomous physical resources. For the Process Control and Monitoring, there is a lack of applications satisfying the Customer-Centric Product Development, which would be an automatic optimization of physical resources from the analysis of customer-related variables. 
Knowledge Discovery and Generation is the only char-acteristic addressed by all the use cases, which denotes an intense interest in knowledge creation from data. Further-more, there is a strong presence of SL, UL, and SL-UL in this characteristic. This implies an important affinity between these learning types and the generation of useful information from raw data. Following a similar trend, there seems to be a generalized interest in Environment-Centric Processes, a characteristic that is addressed by almost all of the use cases. However, its low number of papers implies that there are strong research avenues to be explored. 
Communications addressing the Self-Execution of Resources and Processes focused exclusively on Process Control and Monitoring applications. This shows that the dynamic optimization of working parameters of the machines allows data-driven intelligent resources to be created. However, this characteristic has further potential to be explored in PPC research with other use cases, such as in Inventory and Distribution Control with autonomous AGVs to serve logistic needs or in quality, by automating processes. 

CONCLUSIONS

Conclusion and.further research perspectives 
This state-of-the-art analysis studied 93 research articles chosen through the logic of a systematic literature review. These papers were analyzed by means of an analytical framework composed of four axes. First, the elements of a method were reviewed, which enabled an analysis of activi-ties, techniques, and tools to perform a ML-PPC. Secondly, the data sources employed to implement a ML-PPC model were recognized and assessed. Thirdly, an analysis of the use cases enabled the recognition of the applications of data-driven models in the 4.0. Fourthly, the characteristics of I4.0 were identified and assessed through their usage. Additionally, a mapping of the scientific literature was pro-posed by means of the use cases, characteristics of I4.0 and ML learning types. 
Results concerning the activities allowed the recogni-tion of eleven recurrent tasks that are employed to create a ML-PPC model. They were grouped in four clusters follow-ing their use percentage: CUAs (Commonly Used Activi-ties), OUAs (Often Used Activities), MUAs (Medium Use Activities), and SUAs (Seldom Use Activities). From these clusters, it can be concluded that activities belonging to the CUAs and OUAs clusters are well documented in the scien-tific literature. MUAs activities mainly contain data pre-pro-cessing tasks, which are necessary but not commonly docu-mented by researchers. Finally, the SUAs cluster suggests that there are three activities rarely addressed in literature: the design and implementation of data acquisition methods from the manufacturing system, the exploration of data to get insights, and the constant adaptation of the proposed ML-PPC model to the environment dynamics. 
An extensive review of the techniques identified the most used families in scientific literature. These were found to be the NN, Q-Learning, DT, Clustering, Regression, and Ensemble learning. From these results, a temporal evolu-tion analysis of the top 6 most used families was performed. Findings suggested a growing interest in NN and Ensemble learning, which motivated a focused study on the detailed techniques encompassed by these families. Concerning the NN, the Multi-layer perceptron was the most used technique. Nevertheless, more specialized deep learning techniques such as CNNs, LSTMs, and Deep Belief Networks are starting to be employed. With respect to Ensemble learn-ing, the most used technique was Random forests. 
The ML learning types were also reviewed. Findings showed that scientific literature mainly focused on the individual use of SL and RL. However, synergies between learning types are also employed. For instance, the most used synergy was SL-UL, which allows to explore and pre-process the data through UL to improve the SL training. The UL-RL and SL-UL-RL synergies had only one use each, which could be considered as a research gap, advising improvements in its integration. In fact, each learning type has its advantages and limitations. Hence, it is important to explore more synergy possibilities, as they may help over-come individual limits. 
Other than increasing data availability, one option to encourage the utilization of UL-RL and SL-UL-RL is to boost the development of specialized libraries to build complex models coupling several learning types. Examples of this are deep learning frameworks such as TensorFlow, Keras, PyTorch, etc. which have eased the implementation of deep learning applications. This has allowed researchers to spend more time on the addressed problem than on the coding stage. 
Results concerning the tools showed that MATLAB, R, Python, and RapidMiner are the most used tools in develop-ing ML-PPC models in research. However, most authors did not mention the tool used, which is a limit of this study. Fur-thermore, it is important to mention that these results come from a sample of scientific articles, meaning that results are mainly valid in an academic context. If there are practition-ers willing to implement ML-PPC models in companies, other aspects need to be analyzed such as the cost of the software, its scalability, skill availability in the labor market, compatibility with existing information systems, etc. 
The current horizon of data sources used is dominated by Artificial and Management data. The former points to a difficulty in collecting all of the data required to implement ML-PPC models, while the latter suggest that companies are interested in valuing their data stored in information sys-tems. Data coming from IoT sources such as Equipment and Product data was moderately used, nevertheless showing an interest in these technologies to collect data. Finally, ML-PPC models failed to integrate User data, probably because it is complex to collect and it engages an important respon-sibility concerning data privacy. 
The most addressed use cases were Smart Planning and Scheduling and Time Estimation, probably because they are directly concerned by the PPC, which may lead to its high utilization. The fact that there are research articles in all of the use cases suggests that the PPC is a transversal func-tion that benefits from several applications. Therefore, when designing a ML-PPC system for a company, the impact on all of the use cases must be assessed. Finally, it was found that Inventory and Distribution Control, as well as Smart Design of Products and Processes, are seldom addressed. This suggests that there is still a lot of progress to be made when coupling the PPC to logistics as well as product and process design through ML. 
Concerning the characteristics of I4.0, results suggest that scientific literature in ML-PPC is extremely focused on satisfying the Self-Organization of Resources, which was expected, as one of the main goals of the PPC is resource management to satisfy the commercial plan. At a second level, the Self-Regulation of the Production Process, the Self-Learning of the Production Process, and the Knowl-edge Discovery and Generation seem to be more frequently addressed. However, Fig..13 showed that they are mainly employed for Smart Planning and Scheduling, implying a lack of research in the other applications. Finally, there are three characteristics that are partially overlooked by researchers: Environment-Centric Processes, Smart Human Interaction, and Customer-Centric Product Development. The first two are essential characteristics of building more responsible production systems as they aim to include human beings and reduce the environmental impact of manufactur-ing processes. The latter relates to the alignment of the PPC to the customer¡¯s needs. Hence, it appears that recent ML-PPC research ignores the influence of the customer in the manufacturing process. 
As illustrated in the proposed cross-matrix, 75% of the possible research domains are barely addressed or were not explored at all. This means that the ML-PPC is still a key topic for the enablement of I4.0, which presents strong research avenues. The main future research perspectives could be summarized in the following three key items: 
1. Reinforce the role of IoT in ML-PPC: this would allow an improvement to the data acquisition system¡¯s design and would provide a means to perform a model update to tackle the concept drift issue. To do so, the ML mindset and workflow should be shifted from a linear to a cir-cular process, considering the need to constantly retrain through new data. This way of thinking would enable the identification, from an early development stage, of the retraining policy and the necessary variables that could be measured again at a sensitive cost. By defining these two aspects, the data acquisition system design will be less complex to conceive, as the needs will be clearer. 
This would avoid investment in sensors and resources and architecture that would not be exploited. Concern-ing the retraining policy, a review in the context of PPC reporting common practices, advantages and pitfalls seems to be missing in the scientific literature. 
2. 
Improve the integration between the PPC, logistics, and design: it was stated that the PPC benefits from different use cases. However, recent literature seems to overlook logistics as well as product and process design applica-tions coupled with the PPC. To tackle this challenge, it is necessary enable data availability, continuity and sharing over the design, logistics, and production depart-ments. This could be achieved through interoperability as well as communication of intra-organizational sys-tems such as the PLM, ERP, and MES. Even if projects that are meant to couple such systems are costly, they are necessary to ensure data availability and quality. One way to achieve this is the use of data lakes, which have been recognized as suitable to handle big data reposito-ries of a structured and unstructured nature (Llave 2018; Lo Giudice et.al. 2019). For instance, Llave (2018) con-cluded, through expert interviews, that one of the key purposes of data lakes is to serve as experimentation platforms for data scientists. 

3. 
Set human interaction and environmental aspect as pri-orities to ensure the development of ethical manufactur-ing in I4.0: exploring the interaction of humans with the proposed ML-PPC models is paramount to build-ing inclusive technologies at the service of society. To achieve this, the short-and long-term impact of ML-PPC systems on employees¡¯ working conditions must be assessed. If the system degrades them, it must be redesigned. Concerning the second aspect, seeking a reduction in the environmental impact of manufactur-ing through ML could provide important developments. This can be addressed from a purely PPC approach by optimizing, for instance, the scheduling of disassembly processes or by improving the prediction of production times to avoid energy waste. Other approaches could be the optimization of the supply chain. Even though the supply chain was not covered in this review, it is an appropriate domain for researchers to implement ML applications. For instance, by considering environmental criteria when choosing suppliers, as in (Hosseini and Barker 2016). 


Some of the research gaps indicated in this review could motivate future work. Future work will be focused on the following aspects: 
1. 
The proposed activities will be reviewed to determine an order between them, creating a procedure: this would help shift from a linear to a circular workflow when implementing ML-PPC models. 

2. 
The most suitable techniques and tools will be linked to each of the activities with sectorial information: linking techniques, tools, and activities is the key to creating good practices that could be helpful to new practitioners, both in research and industry. Furthermore, according to Kusiak (2017, 2019), there are profound differences in the volume of data generation and usage across different industries. Therefore, future work will aim to identify trends categorized by sectorial information. 

3. 
The current state of data availability solutions and work-arounds will be explored: as data availability was found to be a main issue, a review of techniques to tackle the class-imbalance problem and the use of transfer learning in the context of PPC will be performed. Additionally, the utilization of data lakes for ML-PPC will also be explored. 

4. 
Future research avenues will be proposed through an NLP analysis: NLP may enable the discovery of non-trivial trends present in the corpus of the 93 sampled articles. This will complement the results of the system-atic literature review. 

Machine learning applied in.production planning and.control: a.state.of.the.art in.the.era of.industry 4.0 
Juan.Pablo.Usuga.Cadavid1

.¡¤ Samir.Lamouri1.¡¤ Bernard.Grabot2.¡¤ Robert.Pellerin3.¡¤ Arnaud.Fortin4 
Received: 15 July 2019 / Accepted: 30 December 2019 / Published online: 11 January 2020 . Springer Science+Business Media, LLC, part of Springer Nature 2020 
Abstract 

Because of their cross-functional nature in the company, enhancing Production Planning and Control (PPC) functions can lead to a global improvement of manufacturing systems. With the advent of the Industry 4.0 (I4.0), copious availability of data, high-computing power and large storage capacity have made of Machine Learning (ML) approaches an appealing solution to tackle manufacturing challenges. As such, this paper presents a state-of-the-art of ML-aided PPC (ML-PPC) done through a systematic literature review analyzing 93 recent research application articles. This study has two main objec-tives: contribute to the definition of a methodology to implement ML-PPC and propose a mapping to classify the scientific literature to identify further research perspectives. To achieve the first objective, ML techniques, tools, activities, and data sources which are required to implement a ML-PPC are reviewed. The second objective is developed through the analysis of the use cases and the addressed characteristics of the I4.0. Results suggest that 75% of the possible research domains in ML-PPC are barely explored or not addressed at all. This lack of research originates from two possible causes: firstly, scientific literature rarely considers customer, environmental, and human-in-the-loop aspects when linking ML to PPC. Secondly, recent applications seldom couple PPC to logistics as well as to design of products and processes. Finally, two key pitfalls are identified in the implementation of ML-PPC models: the complexity of using Internet of Things technologies to collect data and the difficulty of updating the ML model to adapt it to the manufacturing system changes. 
Keywords Machine learning.¡¤ Industry 4.0.¡¤ Smart manufacturing.¡¤ Production planning and control.¡¤ State-of-the-art.¡¤ 
Systematic literature review 
. Juan Pablo Usuga Cadavid juan_pablo.usuga_cadavid@ensam.eu 
Samir Lamouri samir.lamouri@ensam.eu 
Bernard Grabot bernard.grabot@enit.fr 
Robert Pellerin robert.pellerin@polymtl.ca 
Arnaud Fortin a.fortin@ifakt.com 
1 
LAMIH, Arts et M¨¦tiers ParisTech, Paris, France 
2 
LGP, INP/ENIT, Tarbes, France 
3 
Mathematics and.Industrial Engineering Department, Polytechnique de Montreal, Montreal, Canada 



Introduction 
The current manufacturing environment is characterized by high complexity, dynamic production conditions and volatile markets. Additionally, companies must offer cus-tomized products while engaging low costs and reducing the time-to-market if they want to remain competitive in a globalized world (Schuh et.al. 2017b; Carvajal Soto et.al. 2019). This situation poses tremendous challenges for manufacturers who seek to implement new technologies to meet their objectives while expecting a return on invest-ment. Several countries have developed projects that aim to help companies adapt their industries to new production technologies. For instance, Germany created Industry 4.0 (I4.0), the United States proposed the Smart Manufacturing Leadership Coalition, and China introduced the plan called China Manufacturing 2025 (Wang et.al. 2018a). This has led to significant financial support for manufacturing research; 
iFAKT France SAS, Toulouse, France 
for example in the European Union around €7 billion will be invested by 2020 in Factories of the Future (Kusiak 2017). 
Among the Industry 4.0 groups of technologies (Ruess-mann et.al. 2015), Big Data and Analytics (BDA) allows the constantly growing mass of produced data to be harnessed to generate added value. In fact, data generation in modern manufacturing has undergone explosive growth, reaching around 1000 Exabytes per year (Tao et.al. 2018). However, the potential of this data has been found to be insufficiently exploited by companies (Manns et.al. 2015; Moeuf et.al. 2018). As BDA enables the exploitation of data, the scope of this review will focus on this technology, and more spe-cifically ML applied in Production Planning and Control. 
In the context of I4.0, Production Planning and Control (PPC) can be defined as the function determining the global quantities to be produced (production plan) to satisfy the commercial plan and to meet the profitability, productivity and delivery time objectives. It also encompasses the con-trol of production process, allowing real-time synchroniza-tion of resources as well as product customization (Tony Arnold et.al. 2012; Moeuf et.al. 2018). In this review, I4.0 is considered a synonym of Smart Manufacturing, as they both refer to technological advances that value data to draw improvements in production. For example, Ruessmann et.al. (2015) proposed nine technologies for I4.0 while Kusiak (2019) suggested six, but for Smart Manufacturing. Both proposals tend to refer to similar technologies and variations depend on the authors¡¯ focus. Hence, as the PPC is a core function of manufacturing, this paper regards its improve-ment through I4.0 technologies, namely ML, which concerns BDA. Regarding ML, the definition that will be retained is the one of a computer program capable of learning from experience to improve a performance measure at a given task (Mitchell 1997). 
Classical approaches to performing PPC include analyti-cal methods and precise simulations, providing solutions that may rapidly become unfeasible in the execution phase due to the stochastic nature of the production system and uncertain-ties such as machine breakdowns, scrap rate, delayed deliv-eries, etc. Moreover, Enterprise Resource Planning (ERP) systems perform poorly at the operative level (Gyulai et.al. 2015). To tackle this issue, ML can endow the PPC with the capacity of learning from historical or real-time data to react to predictable and unpredictable events. Even though this may suggest that organizations must invest in data ware-housing to handle the mass amount of collected data, stud-ies have reported that enterprises successfully implementing data-driven solutions have experienced a payback of 10¨C70 times their investment in data warehousing (Rainer 2013). 
Having introduced the synergism between ML and PPC, this study aims to provide an analysis of its state-of-the-art through a systematic literature review. This will contribute to the definition of a methodology to implement a ML-PPC and to the proposal of a map to classify scientific literature. This paper analyzes research produced in the context of the I4.0 and is guided by five research questions: 
1. 
Which are the activities employed to perform a ML-PPC? 

2. 
Which are the techniques and tools used to implement a ML-PPC? 

3. 
Which are the currently harnessed data sources to imple-ment a ML-PPC? 

4. 
Which are the addressed use cases by the recent scien-tific literature in ML-PPC? 

5. 
Which are the characteristics of the I4.0 targeted by the recent scientific literature in ML-PPC? 


The first three questions are related to the first objective of this research. They will contribute to the definition of a methodology to implement a ML-PPC. The last two ques-tions address the second objective, as they will provide the basis to create a classification map. 
The reminder of this paper is organized as follows: sec-tion ¡°Research methodology and contribution¡± will explain the systematic literature review methodology employed to search and choose the sample of scientific articles. Addition-ally, the contribution of this paper with respect to similar studies will be briefly highlighted and a short bibliometric analysis is presented to assess the keywords used as string chains. The ¡°Analytical framework¡± section will explain the four axes encompassed by the analytical framework. After-wards, the ¡°Results¡± section will focus on the results of the systematic literature review and an analysis of it. Finally, the ¡°Conclusion and further research perspectives¡± sec-tion will conclude this study and provide further research perspectives. 


Research methodology and.contribution 
To meet the two objectives of this study, a systematic litera-ture review was carried out following the method proposed by Tranfield et.al. (2003) who extended research methods from the medical sector to the management sciences. This method has been successfully employed by other authors to draw insights from the scientific literature (Garengo et.al. 2005; Moeuf et.al. 2018). This literature review focuses exclusively on applications of ML in PPC in the context of I4.0. 
In another domain, Zhong et.al. (2016), proposed a bibli-ometric analysis of big data applications on different sectors such as healthcare, supply chain, finance, etc. but its focus on manufacturing was limited. (Kusiak 2017; Tao et.al. 2018; Wang et.al. 2018a) provided a literature analysis of data-driven smart manufacturing, citing representative references. 
However, these references were not chosen through a sys-tematic literature review. Finally, (Sharp et.al. 2018) could be considered as a study close to this paper as the authors used a pre-defined methodology to select the articles to ana-lyze. Nevertheless, they employed Natural Language Pro-cessing (NLP) to analyze around 4000 unique articles and provide insights about the scientific literature of ML applied in I4.0. The use of NLP can be useful to identify important trends, but it does not allow the authors to analyze the detail of the reviewed papers, where it is likely to find interesting research gaps and insights. On the other hand, a systematic review allows the authors to both follow a rigorous method-ology and perform a detailed study of each chosen article. 
Even though the PPC is closely related to the domain of supply chain, the latter is not included in the scope of this review as its vastness would increase the risk of stray-ing from the focus on PPC. Therefore, to learn about recent trends on this topic, the authors invite readers to refer to Hosseini et.al. (2019), who performed a comprehensive review of quantitative methods, technologies, definitions, and key drivers of supply chain resilience. In fact, supply chain resilience is a growing research area that examines the ability of a supply chain to respond to disruptive events (Hosseini et.al. 2019). Applications of this topic have been done by Hosseini and Barker (2016), who applied Bayesian networks to perform supplier selection based on primary, green, and resilience criteria; and Hosseini and Ivanov (2019), who proposed a method using Bayesian networks to assess the resilience of suppliers in identifying critical links in a supply network. 
The queries were performed between 10/10/2018 and 24/03/2019 in two scientific databases: ScienceDirect and SCOPUS. The following keywords conducted the research: 
. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction scheduling¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction planning¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction control¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Line balancing¡±) 


To consider the context of I4.0, only papers published since 2011 were considered because this year corresponds to the formal introduction of I4.0 at the Hannover Fair. Addition-ally, only communications labeled as ¡°Research Articles¡± in ScienceDirect and ¡°Conference paper¡± OR ¡°Article¡± in SCOPUS were included to solely capture articles present-ing application models. Subsequently, a review of titles and abstracts allowed for the exclusion of articles not related to ML-PPC. After the removal of duplicates, a full text analysis allowed a final selection that excluded papers that did not fit with research questions. The sample size obtained encom-passes 93 scientific papers. The article selection methodol-ogy with its Restrictions (R) is described in Fig..1. 

A brief focus on.the.query keywords 
The used string chains represent a core strategic choice for review. Therefore, this sub section aims to provide an analy-sis of the employed keywords. 
Concerning the keywords used in the first parenthesis of the string chains, the use of ¡°Deep Learning¡± and ¡°Machine Learning¡± was done for two reasons: firstly, they are rela-tively new terms, which eases the identification of recent trends in the literature; and secondly, they are directly related to one of the two core subjects in this study, which is ML. Other terms such as ¡°Data Mining¡± or ¡°Statistical Learn-ing¡± could have been sensible choices too, as they are often used interchangeably with ¡°Machine Learning¡± and ¡°Deep Learning¡±. Nevertheless, using these two terms might have deviated this study from its core topic. In fact, a recent study suggests that the differences between ML and Data Min-ing are not consistently defined in the literature. Thus, Data Mining is mostly considered to be the process of generat-ing useful knowledge from data (Schuh et.al. 2019). To do so, it draws from other fields such as Artificial Intelligence, Statistics, ML, and Data Analytics. Therefore, Data Mining can be a vast topic, and does not exclusively concern ML, which could potentially affect the focus of this study. As there seems to be no clear boundary between these terms, a short bibliometric analysis was performed to assess the chosen keywords. The analysis was done using VOSviewer, software developed by the University of Leiden to draw insights from scientific literature. Furthermore, using key-words related to specific ML techniques such as ¡°Random Forest¡± or ¡°k-means¡± did not seem appropriate due to the risk of introducing a bias when answering the second research question. In fact, this could have artificially boosted the results of the queried techniques. 
The bibliometric analysis followed a similar methodology to that used to choose the final article sample (cf. Fig..1). The objective was to briefly assess the influence of differ-ent keywords on the queries¡¯ results. For the analysis, three different string chains were considered: ¡°Deep Learning¡± OR ¡°Machine Learning¡±, ¡°Data Mining¡±, and ¡°Statistical Learning¡±. The queries were performed on 06/10/2019 and the detail of the search strategy can be found in ¡°Appendix I¡±. Finally, as the aim is to analyze the available literature when querying with a certain string chain, no Title and abstract review was done as this could introduce a bias into the results due to the authors¡¯ influence. 
The bibliometric analysis focused on the keywords defined by the authors for all of the papers of each of the three samples. To represent the results, the network visualization from VOSviewer was employed. In such a net-work, the nodes represent the keywords or items, their sizes represent the keyword importance determined by the number of occurrences, and the links between the nodes represent their co-occurrence. Furthermore, the relatedness between two terms is represented through their spatial distance in the network: two keywords closely related will be spatially closer. For this review, the obtained networks were displayed under the ¡°overlay visualization,¡± which shows the average publication year for each of the keywords through a color scale. For clarity reasons, a filter was applied on the mini-mum number of occurrences to display; at most, 50 items per graph. Also, the queried keywords were highlighted with a red frame to assist in their identification. The networks are presented on Figs..2, 3, 4. 


Results from the bibliometric study suggest that ¡°Statisti-cal Learning¡± may not be a common keyword to find in ML-PPC research because the size of the obtained article sample (241 articles) is far below the results obtained with the other two queries. In fact, ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± and ¡°Data Mining¡± provided 2862 and 2166 articles, respectively (cf. ¡°Appendix I¡±). This is also stated in all of the networks, in which the item ¡°Statistical Learning¡± does not appear, probably due to the filter excluding keywords with a low number of occurrences. 
Analyzing the relatedness between ¡°Data Mining¡± and ¡°Machine Learning¡± by their spatial distance on the net-works provides an idea of how these concepts are associ-ated: they are spatially closer on the ¡°Data Mining¡± Network (Fig..3) than on the ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± network (Fig..2). This suggests that Data Mining tends to relate more often to ML, rather than ML to Data Mining. Such a relation may support what is said in (Schuh et.al. 2019), in which Data Mining is considered a field drawing from ML, Artificial Intelligence, Statistics, etc. to produce useful insights. 
Findings from the network visualizations show that the item ¡°Machine Learning¡± is always associated with a more recent average publication year than ¡°Data Mining¡±. This supports the idea that ¡°Machine Learning¡± is a relatively new term, which can lead to the identification of recent trends in literature. Furthermore, querying with ¡°Deep 


Fig. 2 Network visualization with the average publication year for ¡°Deep Learning¡± OR ¡°Machine Learning¡± 

Fig. 3 Network visualization with the average publication year for ¡°Data Mining¡± Fig. 4 Network visualization with the average publication year for ¡°Statistical Learning¡± 

Learning¡± OR ¡°Machine Learning¡± provides a more recent average publication year (2017.06) for the item ¡°Machine Learning¡± than with the other two queries: 2016.95 when querying with ¡°Data Mining¡± and 2016.41 when querying with ¡°Statistical Learning¡±. Finally, ¡°Deep Learning¡± OR ¡°Machine Learning¡± was the only query enabling the inclu-sion of the item ¡°Deep Learning¡± with enough occurrences 
(25) to pass the filter, which is a recent research topic with an average publication year of 2018.28. 
From the bibliometric analysis, it could be concluded that using ¡°Deep Learning¡± OR ¡°Machine Learning¡± as part of the query keywords is appropriate enough, as this allows for the identification of a big sample of recent papers, enabling the identification of new trends. It seems that ¡°Statistical Learning¡± does not provide enough recent results to be con-sidered. Finally, even if ¡°Data Mining¡± is closely related to ¡°Machine Learning,¡± it covers a vast domain that can deviate from the focus of this review. 
Regarding the keywords employed in the second paren-thesis of the string chains, the objective was to represent the main functions of the PPC under the definition provided in the introduction section. Consequently, a determination of the global production quantities was represented by ¡°Produc-tion Planning¡± and the aspect of the main objectives (i.e. profitability, productivity, and delivery time) was depicted by ¡°Production Control¡±. Finally, the real time synchroni-zation of resources as well as product customizations were represented by both ¡°Production Scheduling¡± and ¡°Line Balancing,¡± given the fact that companies should be able to perform balanced scheduling even when facing customized client orders. 
As the PPC is a transverse topic tangled with other func-tions such as maintenance, quality control, logistics, etc., the challenge was to decide whether or not these related subjects should be included as explicit keywords for the queries. The final choice was to not include them through keywords, as this would broaden the perimeter of the research too much, losing a focus on PPC. Nevertheless, it was decided to include, in the final article sample, the studies dealing with other functions only if they were related to the PPC. 



Analytical framework 
This section presents the four axes that build the analytical framework that will be employed to harness knowledge and insight from the final sample of 93 scientific articles. 
First axis of.the.analytical framework: the.elements of.a.method 
This axis concerns the first and second research questions: the activities, techniques, and tools to implement a ML-PPC model. To link these three elements, the concept of ¡°Man-datory Elements of a Method¡± (MEM) proposed by Zellner (2011) is used. In fact, this concept has been successfully employed by other authors to propose methodologies in research domains such as product development (Lemieux et.al. 2015) and lean in hospitals (Curatolo et.al. 2014). Moreover, (Talhi et.al. 2017) suggested its use to develop a methodology in the context of cloud manufacturing applied to product lifecycle management. Thus, the MEM suits the first objective of this study, which concerns the definition of a methodology to implement a ML-PPC. There are five elements in the MEM: 
1. 
A procedure: order of activities to be followed when the method is employed. 

2. 
Techniques: the means to generate the results. Activities from the procedure are supported by techniques, while the latter is supported by tools. 

3. 
Results: they correspond to the output of an activity. 

4. 
Role: the point of view adopted by the person who per-forms the activity and is responsible for it. 

5. 
Information model: this refers to the relation between the first four mandatory elements. 


In the scope of this study, the first two elements are the con-cern. Firstly, to evaluate the procedure, the activities used to perform a ML-PPC implementation will be recognized and their use will be measured. By activities, this research refers to tasks such as ¡°model comparison and selection¡± or ¡°data cleaning¡±. Secondly, to address the techniques, ML models and tools will be identified, and their use will be measured. ML models point to elements such as Support Vector Machines or Neural Networks, while tools relate to programming languages or software used to implement these ML models. 
To provide further insight concerning the ML techniques, the learning types will also be measured. This will be used to summarize the information regarding the techniques as well as to ease the identification of trends and research perspec-tives. Additionally, the learning types will serve as a bridge between the first and second objectives of this study, as they will be used in the mapping to classify scientific literature. Based on the work of Jordan and Mitchell (2015), three main learning types can be identified: 
1. Supervised Learning (SL), which concerns ML tech-niques approximating a function f (X)= Y by learning the relationship between the inputs X and the outputs 
Y. For instance, learning the mapping between the Red Green, and Blue (RGB) codes (input X) in an image and the objects in it (output Y) to determine if a certain picture contains a misplaced product in a stock rack. 
2. Unsupervised Learning (UL), which encompasses tech-niques allowing data exploration to find patterns and hidden structures in a given dataset X. For instance, finding categories in maintenance reports by using the description of the problem and the duration of the main-tenance intervention. 
3. Reinforcement Learning (RL), which are techniques allowing the learning of actions to be performed by an agent interacting with a certain environment to maxi-mize a reward. For example, teaching an Automated Guided Vehicle (AGV) in a warehouse how to avoid obstacles to maximize the number of delivered pack-ages. 

Second axis of.the.analytical framework: employed data sources 
This axis addresses the third research question: the har-nessed data sources. Identifying which are the data sources used to perform a ML-PPC is capital. In fact, data could be considered as the raw material allowing ML models to develop autonomous computer knowledge gain (Sharp et.al. 2018). Moreover, the quality of the final model will depend to a great extent on the quality and appropriateness of the used data. Therefore, the choice of the data source is an important decision when training a ML model. To address this axis of the analytical framework, the data source types proposed by Tao et.al. (2018) will be used. They mention that there are five main data sources used in the data-driven smart manufacturing: 
1. 
Management data (M): historical data coming from com-pany¡¯s information systems such as the ERP, Manufac-turing Execution System (MES), Customer Relationship Management system (CRM), etc. M data will concern production planning, maintenance, logistics, customer information, etc. 

2. 
Equipment data (E): data coming from Internet of Things (IoT) technologies implemented in the factory. It refers to sensors installed in physical resources such as machines, places such as workstations or human resources such as workers. In the case of workers, data is collected passively, such as by RFID sensors installed on helmets. 

3. 
User data (U): consumer information collected from e-commerce platforms, social media, etc. It also encom-passes feedback given by workers or experts that will be used to train the ML-PPC model. User data coming from workers is collected actively, for example through interviews or questionnaires. 

4. 
Product data (P): data originating from products or ser-vices either during the production process or from the final consumer. 

5. 
Public data (Pb): data available in public databases from universities, governments or from other researchers. 


The analysis of the 93 shortlisted articles suggested that some of them did not fit into the five data sources proposed by Tao et.al. (2018): these communications used artificially generated data through computer simulations. Therefore, a sixth data source is proposed, which corresponds to the first contribution of this paper to the scientific literature: 
6. Artificial data (A): data generated by computers (e.g. simulations) to assess ML-PPC implementations. 

Third axis of.the.analytical framework: the.use cases of.the.ML.PPC in.the.I4.0 
This axis concerns the fourth question: it aims to show which applications can be achieved when applying a ML-PPC. Moreover, identifying the use cases and quantifying their use frequency is important to detect trends as well as further research gaps. By use cases, this study refers to the different possible applications in a certain domain, such as maintenance, quality control, distribution, etc. In fact, as the PPC is entwined with several manufacturing subjects, is difficult to perform a complete review on PPC if these topics are ignored. For example, if there were a predictive maintenance study meant to enable a more robust produc-tion scheduling, such application would be directly related to the PPC through maintenance. To start this analysis, the use cases of I4.0 initially proposed by Tao et.al. (2018) were considered. They identified six of them: 
1. 
Smart Maintenance: harnessing data to perform preven-tive and predictive maintenance. For instance, monitor-ing machine components to estimate the best date to perform a maintenance intervention. 

2. 
Quality Control: applying BDA to supervise the manu-facturing process or products, seeking for possible qual-ity problems and/or allowing the identification of root causes. 

3. 
Process Control and Monitoring: constantly analyzing data coming from the shop floor to perform a smart adjustment of the functioning parameters of physical resources (machines, AGVs, etc.). The objective is to automatically control these physical resources and/or optimize their parameters with respect to the working conditions. 

4. 
Inventory and Distribution Control: stock management, parts and tools tracking, and distribution control with the use real-time and/or historical data. 

5. 
Smart Planning and Scheduling: considering produc-tion uncertainties to perform a production planning and scheduling closer to the current state of the production system. For instance, considering unexpected mainte-nance problems to reschedule a production order and minimize the delay. 


6. 
Smart Design of Products and Processes: using BDA to support new products and processes development. For instance, using NLP to analyze the technical require-ments of a new product and then to propose the poten-tially suitable manufacturing process. 

The analysis of the 93 scientific articles suggests that these six use cases are not enough to fully characterize the recent publications. Additionally, papers not fitting in the initially proposed use cases shared the same application: time esti-mation (cycle time, operation time, etc.). Consequently, a seventh use case is proposed: 

7. 
Time estimation: adaptation of different manufactur-ing related times to current working conditions. For instance, estimating the operation times to the actual work rate of each employee instead of using the data from the Method Time Measurement (MTM) approach. 



Fourth axis of.the.analytical framework: the.characteristics of.I4.0 
The I4.0 aims to transform the collected data during the product¡¯s lifecycle into ¡°intelligence¡± to enhance the manu-facturing process (Tao et.al. 2018). With this transforma-tion, the objective is to reduce costs while improving the quality, productivity, sustainability of the production system (Wang et.al. 2018a). However, what specific benefits could be expected when embracing the I4.0? To answer this ques-tion, the characteristics of I4.0 need to be identified. Tao et.al. (2018) argue that I4.0 enables the following paradigms: 
1. 
Customer-Centric Product Development: production systems in the I4.0 should be able to adjust their param-eters by considering variables coming from customers such as their behavior, their needs, the way they use the products, inter alia. It is the case of manufacturing per-sonalized products, designing processes from the cus-tomer requirements or proposing a target manufacturing cost for each consumer profile. 

2. 
Self-Organization of Resources: I4.0 should endow pro-duction systems with the capacity of considering data coming from the manufacturing process to better engage the available resources. Additionally, this data should also be used to plan capital and operational expendi-tures. For example, updating the scheduling of machines the shop floor after new urgent order is released. 

3. 
Self-Execution of Resources and Processes: in the I4.0, resources should become ¡°smart¡± by providing them a real-time awareness and interaction capacity with the manufacturing environment (Huang et.al. 2019). There-fore, the self-execution of resources concerns their fac-ulty of making decisions depending on the received 


information or measured data. It is the case of machines automatically adapting their functioning parameters to work optimally or trolleys automatically replenishing workstations when these reach a certain level of security stock. 
4. 
Self-Regulation of the Production Process: unexpected events should be effectively handled in the I4.0. Thus, this characteristic concerns the capability to perform the required adjustments to respond to unpredicted prob-lems. For example, relaunching the scheduling process for a certain production line when one of the machines experienced a breakdown. 

5. 
Self-Learning of the Production Process: this charac-teristic follows a similar logic as the self-regulation of processes in terms of adjustability. However, it relates to the capacity of the production system to adapt to pre-dicted events. It is the case of predictive maintenance, which uses BDA to estimate the remaining useful life of machine¡¯s components. Afterwards, the manufacturing system can adapt to the results of this prediction. 

After concluding the analysis of the 93 articles, three char-acteristics seem to be overlooked: the environmental dimen-sion, the knowledge generation, and the inclusion of the human being. To consider these dimensions that seem to not be explicitly raised in the work of Tao et.al. (2018), three new characteristics are proposed: 

6. 
Environment-Centric Processes: estimations suggest that the electronics and home appliances industry scrapped around 100 million goods in China in 2012 (Tian et.al. 2013). As exemplified, the environmental impact of industry is far from being negligible, which is the reason why industrialized countries have started to tighten regu-lations and engage environmentally friendly practices in manufacturing (Tuncel et.al. 2014). Research done in the context of I4.0 must not overlook this aspect. Therefore, this characteristic concerns the use of new technologies to create environment-centric processes. For example, optimizing the disassembly scheduling process to maxi-mize the number of components that can be recycled. 

7. 
Knowledge Discovery and Generation: most of the com-panies have been computerized for a long time, which has eased the collection of data. Despite the access to a plethora of information systems, generating knowledge from raw data still supposes a major industrial and aca-demic challenge. Besides, the generation of knowledge is a mandatory step to improve the adoption of BDA by companies (Grabot 2018). In fact, knowledge could be considered as one of the most valuable assets in manu-facturing (Harding et.al. 2006), the reason why generat-
ing it represents an important gain behind the adoption of BDA. Therefore, as I4.0 is characterized by allowing 


knowledge creation, research efforts must include it to generate value. One example of this is harnessing data from maintenance reports to provide the production of responsible real-time information about the root causes of machine breakdowns. 
8. Smart Human Interaction: even with the advent of multi-ple I4.0 technologies, its adoption would be significantly hindered by not keeping humans in the loop or not con-sidering their interaction with the proposed solutions. For instance, Thomas et.al. (2018a) experienced the case of a company that was not willing to introduce an improved version of a quality control system because it somehow excluded the person from the process. There-fore, this characteristic concerns the consideration and/ or inclusion of a human being when implementing new technologies. Examples of this would be a worker behavior recognition system based on computer vision or software interacting with operators through NLP. 
Figure 5 summarizes this section. It also presents the relationship between the Research Questions (RQ), the analytical framework axes, the research objectives, and the expected outputs of this study. 



Results 

First research question: activities employed in.ML.PPC 
To identify the activities, the tasks used to implement a ML-PPC in each of the 93 communications were identified. Afterwards, these tasks were grouped into categories to ease the information analysis. These groups of activities were analyzed by two experts to keep the most meaningful ones. Results suggest eleven standard and recurrent activities: 
1. 
Data Acquisition system design and integration (DA): design and implementation of IoT systems to collect data. This activity also encompasses the data storage and communication protocols. 

2. 
Data Exploration (DE): use of data visualization tech-niques, inferential statistics, and others to derive initial insights and conclusions about the dataset. 

3. 
Data Cleaning and formatting (DC): data preparation from the raw data to make it exploitable by the ML-PPC model. It concerns tasks such as outlier removal or missing values handling. 

4. 
Feature Selection (FS): choice of the most suitable inputs to the ML-PPC model. It can be done through statistical techniques, e.g. stepwise regression or by means of expert insight. 




Fig. 5 Relationship between the building blocks, research objectives and expected outputs of this study 
5. 
Feature Extraction (FE): use of variables from the ini-tial dataset to calculate more meaningful features. 

6. 
Feature Transformation (FT): representation of the initial features into different spaces or scales using techniques such as normalization, standardization or kernel transformations. 

7. 
Hyperparameter Tuning and architecture design (HT): definition of the ML model architecture and adjustment of its hyperparameters to improve the performance. For instance, optimizing the learning rate and defining the activation function in a neural network. 

8. 
Model Training, validation, testing, and assessment (MT): using the data to perform the training, validation and testing process. It can be done through techniques such as k-fold cross-validation. It also encompasses the choice of the training/validation/testing set split and the model¡¯s performance assessment. 

9. 
Model Comparison and selection (MC): several ML techniques can be used to achieve a certain task. This activity concerns the comparison of multiple ML mod-els to choose the one that better suits the needs. 

10. 
Contextualized Analysis or application (CA): going further than just assessing the model¡¯s performance. It concerns the actual implementation of the ML-PPC model or the analysis of its results in the context of the problem that is addressed by the study. 


11. Model Update (MU): data used to train ML models represents the context of the studied environment at a given moment. However, this context is dynamic, hence the ML-PPC model must be adapted. Therefore, this task concerns the model update through new data. 
To address this research question, the percentage of papers using each activity was measured. These results are summa-rized in Fig..6. Findings suggests that four groups of activi-ties can be proposed following their usage: 
11. MU 
10. CA 
9. MC 
8. MT 84% 
7. HT 
6. FT 
5. FE 
4. FS 
3. DC 
2. DE 
1. DA 
Use percentage 
Fig. 6 Use percentage by activity. CUAs in green, OUAs in blue, MUAs in purple, and SUAs in red 
1. 
Commonly Used Activities (CUAs), applied in more than 60% of the analyzed papers (no. 7, 8). 

2. 
Often Used Activities (OUAs), harnessed in 40 to 60% of the communications (no. 9, 10). 

3. 
Medium Use Activities (MUAs), employed in 20 to 40% of the cases (no. 3¨C6). 

4. 
Seldom Use Activities (SUAs), used in less than 20% of the reviewed articles (no. 1, 2, 11). 


These groups show that a considerable amount of research papers only focus on the architecture design, training, and assessment of ML-PPC models (CUAs cluster), while not employing or documenting the use of other activities. Con-sidering OUAs, it is surprising to find that only half of the communications used the CA, which corresponds to an actual implementation of the proposed model in the context of the study. This suggests that half of the studies go no further than just training and evaluating the performance of the model. 
MUAs group encompasses data pre-processing tasks, which are capital to any ML implementation. Even if these activities are frequently employed in practice, their low usage is probably because researchers do not mention them, implying a lack of documentation. Moreover, as one of the characteristics of big data is the variety (in type, nature, for-mat, etc.) (Zhou et.al. 2017), it is crucial to employ data pre-processing activities to ensure the quality of the final models. Consequently, this lack of documentation can repre-sent a pitfall to practitioners willing to apply ML-PPC based on research papers. 
Finally, SUAs cluster highlights the most important research gaps in scientific literature. Three key findings can be inferred from activities in this group: firstly, the low usage of DA highlights the challenge of coupling IoT technolo-gies with ML-PPC. This is a major pitfall to deploy ML-PPC in companies, as they normally need real-time data or statuses from their manufacturing systems. Secondly, the lack of DE utilization could mean that ML-PPC applica-tions tend to jump directly to activities in the CUAs cluster while overlooking descriptive and basic inferential statis-tics techniques. This represents an obstacle to generating knowledge from data, as DE can draw conclusions easily interpretable by non-ML specialists. Finally, the rare use of MU implies that adapting the ML-PPC model to a dynamic manufacturing context is seldom addressed. This unpredict-able change of the statistical properties and relationships between variables over time is known as concept drift (Ham-mami et.al. 2017). Not addressing this issue can be harmful for the model reliability in the long term. 


Second research question: techniques and.tools used in.ML.PPC 
Concerning the techniques, results present the number of times a given ML model is used. In the case of communica-tions comparing several techniques, only the one chosen by the authors because of its better performance was consid-ered. If this best-performing model employs several tech-niques, each of them is counted as used once. 
There are numerous ML techniques in scientific litera-ture. Therefore, to ease the analysis of results, a grouping of techniques in families is proposed is Table.1. These families were determined with the help of a ML expert. It is impor-tant to mention that the column ¡°Concerned techniques¡± in Table 1 is not an exhaustive list, it is limited to techniques found in the systematic literature review. 
Results are presented in Fig..7. They suggest that NN, Q-Learning, and DT are the most used techniques in ML-PPC. The extensive use of NN is probably due to their abil-ity to learn complex non-linear relationships between vari-ables, often delivering good performance when compared to other techniques. Even if Q-Learning remains, by far, the most used RL technique, other RL models such as Sarsa or R-Learning are used, which points an interest in agent-based modeling in ML-PPC. Finally, the attention drawn by DT techniques is probably linked to their excellent trade-off between accuracy and interpretability, allowing knowledge generation. 
The high use of Clustering techniques could be explained by the fact that data in manufacturing systems is normally unlabeled and can contain meaningful unknown patterns. Therefore, clustering can be employed to discover groups as well as hidden structures in datasets. 
The usage evolution of the six most used technique fam-ilies was also measured. Figures representing this can be found in ¡°Appendix II¡±. Due to an imbalance in the amount of articles over the different years, results are presented as relative frequencies. For example, if the NN achieved a usage of 27% in 2018, it means that 27% of all the techniques used in that year corresponded to such a model. Results sug-gest that there is a strong growth in the use of NN since 2015, this is possibly due to the growing computing power, recent findings in terms of architectures such as CNNs or LSTMs, and the development of specialized frameworks like PyTorch, TensorFlow, Keras, etc. which ease the task of implementing such models. Moreover, results show a grow-ing interest on Ensemble learning techniques which evolved from not being used between 2011 and 2013 to accounting for 14% of applications in 2018. This can possibly explain the loss of interest on DT since 2017, as Random forests (a type of Ensemble learning) can achieve better performance by using committees of decision trees. 
Table 1 Technique families with their respective ML models 
Family Concerned techniques 

Association rule  Association rule  
Bayesian models  Bayesian networks, Na.ve Bayes  
Canonical variable analysis  Canonical variable analysis  
Clustering  c-Means, density peak clustering, hierarchical clustering, k-means  
Ensemble learning  Bagging, gradient boosting, machine learner fusion-regression, random forests, stacking  
FURIA  FURIA  
k-NN  K-nearest neighbors (k-NN), neighborhood component feature selection  
Neural network (NN)  Artificial neural agent, autoencoders, convolutional neural network (CNN), deep belief networks, extreme learning  
machine, long short-term memory (LSTM), multi-layer perceptron, self-organizing maps, stacked denoising  
autoencoders  
Principal component  Principal component analysis  
analysis (Princip. Comp.  
analysis)  
Q-learning  Q-learning  
Regression  Gaussian process regression, linear regression, logistic regression, polynomial regression, radial basis function  
approximation  
R-learning  R-learning  
Sarsa  Sarsa  
Supervised locally linear  Supervised locally linear embedding  
embedding (Sup. Locally  
Linear Embed.)  
Support vector machines  Support vector machines (SVM)  
Decision trees (DT)  Decision trees  

FURIA  1  
Sup. Locally Linear Embed.  1  
Canonical Variable Analysis  1  
Princ. Comp. Analysis  2  
R-Learning  2  
Sarsa  2  
Association Rule  3  
Bayesian Models  5  
k-NN  8  
SVM  9  
Ensemble Learning  10  
Regression  11  
Clustering  12  
DT  15  
Q-Learning  19  
NN  29  

Number of uses 
Fig. 7 Number of uses by technique family 
As NN and Ensemble learning families seem to be recently attracting the research community, a detailed view of their encompassed techniques is presented in ¡°Appen-
dix III¡±. Concerning NN, the most used technique is the Multi-layer perceptron, which is the classic architecture of a NN. However, more specialized architectures belonging to deep learning are starting to appear in PPC research. Such is the case of the CNNs, LSTMs, and Deep Belief Networks. 
These techniques have presented good performance when dealing with specific problems, such as image recognition for CNNs, time series analysis for LSTMs or feature extrac-tion for Deep Belief Networks. In the case of Ensemble learning, the most used technique is, by far, the Random forests. They seem to provide excellent results while ena-bling knowledge generation. In fact, they allow the most meaningful variables to be easily identified in the SL task, which is the reason why researchers tend to use them to both attain accuracy and model interpretability. 
To measure the utilization of the learning types, each paper was analyzed, and the learning types used were iden-tified and counted. As a given model can use several ML techniques, it can refer to several learning types at the same time. Hence, the different synergies between these were also considered. Results are presented in Fig..8. 
Findings show that the most used learning type is SL. This is probably because SL addresses two recurrent needs in applied research: classification and regression. In fact, SL can be used to learn the relationship between an input X and an output Y that can be either discrete in the case of classification or continuous for regression. Furthermore, it was found that RL techniques are extensively used, which confirms the interest behind agent-based models. 
Concerning UL, it seems to be especially used with SL (SL-UL), which suggests a strong synergy between these 
Learning type 
SL-UL-RL 
UL-RL 
SL-RL 
UL 
SL-UL 
RL 
SL 

52 Number of uses 
Fig. 8 Number of uses by learning type 
C++ WEKA Java Python RapidMiner R Others MATLAB Not Mentioned 

41 Number of uses 
Fig. 9 Number of uses by tool 
two learning types. The reason behind this could be that UL techniques are normally used to perform data pre-process-ing, as with Principal Component Analysis, or discovery of hidden patterns in datasets, e.g. with Clustering. There are 6 papers using just UL, however, this learning type seems to unlock all of its potential when used in synergies, allowing for the design of more complex models. 
Even if there are some SL-RL synergies, they are not very common. This is probably because SL is normally coupled with RL when there is a need of performing rapid estimations of functions to save computing time. However, it seems that most of the applications do not reach a scale that needs this kind of configuration. Finally, it was found that using UL-RL and SL-UL-RL is rare in the scientific litera-ture. This does not mean that their synergy does not provide advantages, it is just that there may not be a current need for it. Also, it could be that coupling these learning types over-complexifies the model design, which prevents its use. 
Concerning the tools, only programming languages or software used to implement the ML model were considered. Therefore, other tools such as discrete event simulation soft-ware are out of the scope of this research. Results are pre-sented in Fig..9. 
For clarity sake, tools being used only once were grouped in the category denominated as ¡°Others¡±. These tools were: ACE Datamining System, C#, Clementine, GeNIe Modeler, Hugin 8.1, NetLogo, Neural-SIM, Visual C++, and Xelopes Library. Additionally, it is important to mention that most of the researchers do not mention the tool the use to imple-ment the models. 
It could be said that MATLAB is, by far, the most used tool to perform ML-PPC in research. Besides its robust cal-culation capacity, the reason behind this could be that uni-versities often invest in licenses for this software; therefore, they expect their researchers to use this tool. R is the second most used tool, which may be because it is a free software targeting statistical applications, including ML. Finally, the third most used tools are both RapidMiner and Python. The former eases the implementation of ML models thanks to its visual programming logic, while the latter is a multipur-pose programming language recently characterized by its ML libraries and frameworks such as Scikit-learn, PyToch, Keras, etc. 


Third research question: used data sources to.implement a.ML.PPC 
To answer this question, the data sources used by each of the analyzed papers were identified. These results are sum-marized in Table.2. The column ¡°Identification¡± (ID) will assign a number to each communication. This will be used later to establish a mapping of the scientific literature. 
Results show that ¡°Artificial data¡± is the most used data source in recent scientific literature. This probably highlights the difficulty of accessing data coming from companies. Additionally, it is important to remember the extensive use of RL techniques. These models normally require constant access to data concerning the real-time status of the produc-tion system, which can be difficult to find in real factories. Therefore, researchers normally use Artificial or Public data to test their models. This issue could be addressed by creating digital twins, but this still represents a research challenge. 
The extensive use of artificial data suggests that there are data availability issues. This poses two main challenges: firstly, dealing with highly unbalanced datasets when train-ing, for instance, SL algorithms for classification, and sec-ondly, accessing enough data to enable good generalization capacity, especially in deep learning models. 
The first challenge is common when training ML mod-els to identify disruptions. In fact, disruptive events in PPC such as machine breakdowns or quality problems tend to be scarce when compared to the total size of the dataset. Thus, ML techniques struggle to learn these events. To tackle this issue, some authors have proposed solutions such as data augmentation, a common practice in computer vision that consists of artificially creating new training examples by modifying existent observations (Perez and Wang 2017; Miko.ajczyk and Grochowski 2018). Another approach is to use crafted algorithms adapted to class-imbalance. Bi and Zhang (2018) performed a comprehensive comparison of 
Table 2 Data sources used by 

IDReferences M E U P PbAeach of the analyzed scientific 
articles 1 Aissani et.al. (2012) X 2 Altaf et.al. (2018) X 3 Bergmann et.al. (2016) X 4 Cai et.al. (2016) X X 
Cao et.al. (2019) X 6 Carvajal Soto et.al. (2019) X 7 Chen et.al. (2015) X 8 Diaz-Rozo et.al. (2017) X 9 Ding and Jiang (2018) X X X 
Dinis et.al. (2019) X X 11 Dolgui et.al. (2018) X 12 Doltsinis et.al. (2014) X 13 Fotuhi et.al. (2013) X X 14 Gao et.al. (2014) X 
Gyulai et.al. (2014) X X 16 Gyulai et.al. (2015) X X X 17 Gyulai et.al. (2018a) X X 18 Gyulai et.al. (2018b) X X 19 Habib Zahmani and Atmani (2018) X 
Hammami et.al. (2016) X 21 Heger et.al. (2016) X 22 Huang et.al. (2019) X X X 23 Ji and Wang (2017) X 24 Jiang et.al. (2016) X X 
Jurkovic et.al. (2018) X 26 Kartal et.al. (2016) X 27 Khader and Yoon (2018) X 28 Kho et.al. (2018) X 29 Kim and Lim (2018) X 
Kim and Nembhard (2013) X 31 Kosmopoulos et.al. (2012) X 32 Kretschmer et.al. (2017) X X 33 Kruger et.al. (2011) X 34 Kumar et.al. (2018) X 
Lai and Liu (2012) X 36 Lai et.al. (2018) X 37 Leng et.al. (2018) X X 38 Li et.al. (2012a) X 39 Li et.al. (2012b) X 
Li et.al. (2013) X 41 Li et.al. (2018) X 42 Liao (2018) X 43 Lieber et.al. (2013) X X 44 Lingitz et.al. (2018) X 
Lubosch et.al. (2018) X X 46 Lv et.al. (2018b) X 47 Lv et.al. (2018a) X X X 48 Ma et.al. (2017) X X 49 Maghrebi et.al. (2016) X 
Manns et.al. (2015) X X 51 Manupati et.al. (2013) X Table 2 (continued) 
IDReferences M E U P PbA 
52 Mori and Mahalec (2015) X 53 Ou et.al. (2018) X X 54 Ou et.al. (2019) X 55 Palombarini and Mart¨ªnez (2012) X 56 Priore et.al. (2018) X X 57 Qu et.al. (2015) X 58 Qu et.al. (2016a) X 59 Qu et.al. (2016b) X 60 Reboiro-Jato et.al. (2011) X 61 Reuter et.al. (2016) X 62 Rostami et.al. (2018) X 63 Sahebjamnia et.al. (2016) X X 64 Schuh et.al. (2017a) X 65 Shahzad and Mebarki (2012) X 66 Shiue et.al. (2012) X 67 Shiue et.al. (2018) X X 68 Solti et.al. (2018) X X 69 Stein et.al. (2018) X 70 Stricker et.al. (2018) X 71 Thomas et.al. (2018a) X 72 Thomas et.al. (2018b) X 73 Tian et.al. (2013) X 74 Tong et.al. (2016) X 75 Tuncel et.al. (2014) X 76 Wang and Jiang (2018) X 77 Wang and Jiang (2019) X X 78 Wang and Yan (2016) X 79 Wang et.al. (2015) X 80 Wang et.al. (2017) X 81 Wang et.al. (2018b) X X 82 Wang et.al. (2018c) X 83 Waschneck et.al. (2018) X 84 Wauters et.al. (2012) X 85 Wu et.al. (2015) X X 86 Xanthopoulos et.al. (2017) X 87 Yang et.al. (2016) X 88 Yeh et.al. (2011) X 89 Yuan et.al. (2014) X 90 Zhang et.al. (2011) X X 91 Zhang et.al. (2012) X 92 Zhong et.al. (2014) X 93 Zhou et.al. (2018) X X 
Totals 33 20 2 12 14 43 
state-of-the-art ML techniques adapted to this issue. The second challenge normally concerns the training of deep learning models as they need voluminous data to learn meaningful representations. This issue is normally tackled by transfer learning, which is the use of models already trained on a source task to perform another related task (Wang et.al. 2018a), for instance, using a CNN trained to recognize pedestrians in the street to recognize operators on the shop floor. A comprehensive survey of transfer learning can be found in Pan and Yang (2010). 
Management is the second most used data source. Hence, there seems to be a strong interest in valuing enterprise data stored in information systems by making it available for researchers and practitioners. Furthermore, the use of Equip-ment and Product data suggests that recent applications are starting to employ data coming from IoT technologies installed in machines or semi-finished products. However, there are still tremendous research gaps when harnessing user data to implement ML-PPC models. Two studies used this data source, but only under the form of expert feedback to train the ML model. No study included consumer feed-back from e-commerce platforms or social media to influ-ence the PPC. 


Fourth research question: addressed use cases by.recent scientific literature 
To answer this question, each analyzed article was allo-cated to one of the seven proposed types of use cases. This allows to measure their importance in the scientific literature (Fig..10). 
Results point out that Smart Planning and Scheduling is the most addressed use case in recent scientific literature, with nearly half of the communications discussing it. This result may come from two main reasons: firstly, the string chains used in the methodology are closely related to this use case; secondly, it normally uses structured data relatively easy to get from information systems, which eases the task of implementing a data-driven approach. The strong use of Time Estimation in ML-PPC (14% of the papers) suggests that classical time measurement methods are not compliant with the growing complexity of the manufacturing systems, which may represent a pitfall to perform a reliable planning. Therefore, ML models considering more diverse variables 
Fig. 10 Share of the analyzed sample by proposed use case 
10% 
9% 
9% 
6% 




as inputs are being adopted. Moreover, some researchers have addressed the coupling of Smart Maintenance, Process Control and Monitoring, and Quality Control with the PPC. However, there is still effort to be made, as the share of these use cases was no higher than 10%. 
Finally, two use cases are targeted as critical: The Inven-tory and Distribution Control (6%) and the Smart Design of Products and Processes (4%). These findings suggest two things: first, a lack of integration of the logistic functions into the ML-PPC, and secondly, a difficulty for harnessing insights from data to serve product and process design. This difficulty is probably because data employed in design is highly unstructured (text data, image data, etc.) and greatly depends on people¡¯s experience. 


Fifth research question: the.characteristics of.I4.0 
To quantify their usage, the addressed characteristics in each of the 93 analyzed papers were identified and counted. Results are summarized in Fig..11. In this figure, the sum of all the totals is higher than 93 as one ML-PPC model can satisfy several characteristics. 
Findings show that the Self-Organization of Resources is, by far, the most addressed characteristic (56 uses) in ML-PPC applications. This result was expected, as this charac-teristic can be achieved through production planning and scheduling, two functions directly related to the PPC and found to be extensively employed in the use cases. There-fore, it can be concluded that the ML-PPC based models effectively enable this characteristic. 
The Self-Regulation of the Production Process (33 papers), the Self-Learning of the Production Process (26 
Share of papers by use case 
14% 
Time Estimation 
Smart Design of Products and
4% 
Processes 
Smart Planning and scheduling 
Inventory and Distribution Control 
Process Control and Monitoring 
Quality Control 
Smart Maintenance 
Fig. 11 Number of papers by I4.0 characteristic 

Smart Human Interaction 
Knowledge Discovery and Generation Environment-Centric Processes Self-Learning of the Production Process 
Self-Regulation of the Production Process Self-Execution of Resources and Processes Self-Organization of Resources Customer-Centric Product Development 
Number of papers 

papers), as well as the Knowledge Discovery and Genera-tion (26 papers) appear to be moderately boarded. This leads to two main conclusions: first, ML-PPC models effectively endow manufacturing systems with the capacity of adapting to unexpected events and predicting production problems. This is suitable to handle the stochastic nature of production environments. Secondly, ML is suitable to generate knowl-edge from PPC data, which is crucial in I4.0, where data is abundant, and it can provide useful guidelines to improve the company¡¯s know-how. 
Four characteristics were rarely satisfied: The Customer-Centric Product Development (3 papers), the Self-Execution of Resources and Processes (4 papers), the Smart Human Interaction (7 papers), and the Environment-Centric Pro-cesses (8 papers), which points to strong research perspec-tives of ML-PPC applications enabling these features. Con-cerning the Customer-Centric Product Development, it was rare to find papers including customer-related variables into their PPC. This can be due to the difficulty to access data from customers or end users. For instance, as observed in the data sources section, user data was seldom employed. 
The low number of papers dealing with Self-Execution of Resources and Processes suggests that it is unusual to couple the PPC with autonomous physical resources. This can be due to the complexity of such systems as they require important capital investments as well as multi-disciplinary knowledge in production systems, mechatronics, and control theory. 
It was very surprising to find that the Smart Human Interaction (7 papers) and the Environment-Centric Pro-cesses (8 papers) are rarely addressed. Indeed, manufactur-ing systems can be human based in several steps such as during the execution in the shop floor or during the tactical planning definition. Not considering the interaction of the proposed ML-PPC models with humans can be harmful for the deployment of the proposed system, as it may worsen the working conditions. Therefore, thinking about this human-ML interaction is the cornerstone for a successful adoption. Concerning the Environment-Centric Processes, scarce applications tried to minimize the environmental impact of production processes through ML-PPC. In a world where natural resources are becoming rare, this is a non-negligible aspect that must be considered, not only because of the tightening of environmental laws by gov-ernments but also because of the ethical responsibility of companies. 


Cross.axes analysis: mapping the.scientific literature through.use cases, I4.0 characteristics, and.learning types 
To address the second objective of this study, a mapping of the scientific literature in ML-PPC is proposed. This is achieved through a cross-analysis employing the use cases, characteristics of I4.0, and learning types. Results are repre-sented via a cross-matrix having the use cases in the vertical axis and the characteristics of I4.0 in the horizontal axis. This matrix also allows the maturity of a given use case to be assessed. For instance, a mature use case in the scientific literature will tend to satisfy more I4.0 characteristics. From this point of view, the crossing between a characteristic of I4.0 and a use case will be referred as a domain. 
The ID numbers defined on Table.2 are employed to place the analyzed articles in the matrix. Additionally, the learn-ing types employed by each communication are represented using a color code. Figure.12 provides a summarized view of this matrix, allowing for a high-level analysis that will help to identify research gaps and trends in ML-PPC. Figure.13 is 



a detailed view of the matrix indicating the scientific articles with their respective learning types found in each domain. 
Figure.12 shows that among the 56 possible domains, 18 (32%) were not addressed at all. Furthermore, 24 (43%) domains lie in the range of 1 to 3 papers. This means that nearly half of the domains are in an exploration phase. These two remarks lead to conclude that ML-PPC in the I4.0 is still an active research topic with strong perspectives. 
From Fig. 13, it can be said that there is a strong trend of using multiple synergies between learning types across all of the different use cases. However, there are no applications of RL in Time Estimation and in Smart Design of Products and Processes. The reason for this may be that these use cases have strong strategic impacts. Therefore, current ML implementations in such applications aim to support deci-sions rather than automating them such as with agent-based systems driven by RL. 
There are two use cases achieving a high maturity: Smart Planning and Scheduling and Process Control and Moni-toring. They both cover all but one of the characteristics of I4.0. In the case of Smart Planning and Scheduling, it fails to address the Self-Execution of Resources and Pro-cesses, which suggests that there are research perspectives in coupling the production planning and scheduling with autonomous physical resources. For the Process Control and Monitoring, there is a lack of applications satisfying the Customer-Centric Product Development, which would be an automatic optimization of physical resources from the analysis of customer-related variables. 
Knowledge Discovery and Generation is the only char-acteristic addressed by all the use cases, which denotes an intense interest in knowledge creation from data. Further-more, there is a strong presence of SL, UL, and SL-UL in this characteristic. This implies an important affinity between these learning types and the generation of useful information from raw data. Following a similar trend, there seems to be a generalized interest in Environment-Centric Processes, a characteristic that is addressed by almost all of the use cases. However, its low number of papers implies that there are strong research avenues to be explored. 
Communications addressing the Self-Execution of Resources and Processes focused exclusively on Process Control and Monitoring applications. This shows that the dynamic optimization of working parameters of the machines allows data-driven intelligent resources to be created. However, this characteristic has further potential to be explored in PPC research with other use cases, such as in Inventory and Distribution Control with autonomous AGVs to serve logistic needs or in quality, by automating processes. 



Conclusion and.further research perspectives 
This state-of-the-art analysis studied 93 research articles chosen through the logic of a systematic literature review. These papers were analyzed by means of an analytical framework composed of four axes. First, the elements of a method were reviewed, which enabled an analysis of activi-ties, techniques, and tools to perform a ML-PPC. Secondly, the data sources employed to implement a ML-PPC model were recognized and assessed. Thirdly, an analysis of the use cases enabled the recognition of the applications of data-driven models in the 4.0. Fourthly, the characteristics of I4.0 were identified and assessed through their usage. Additionally, a mapping of the scientific literature was pro-posed by means of the use cases, characteristics of I4.0 and ML learning types. 
Results concerning the activities allowed the recogni-tion of eleven recurrent tasks that are employed to create a ML-PPC model. They were grouped in four clusters follow-ing their use percentage: CUAs (Commonly Used Activi-ties), OUAs (Often Used Activities), MUAs (Medium Use Activities), and SUAs (Seldom Use Activities). From these clusters, it can be concluded that activities belonging to the CUAs and OUAs clusters are well documented in the scien-tific literature. MUAs activities mainly contain data pre-pro-cessing tasks, which are necessary but not commonly docu-mented by researchers. Finally, the SUAs cluster suggests that there are three activities rarely addressed in literature: the design and implementation of data acquisition methods from the manufacturing system, the exploration of data to get insights, and the constant adaptation of the proposed ML-PPC model to the environment dynamics. 
An extensive review of the techniques identified the most used families in scientific literature. These were found to be the NN, Q-Learning, DT, Clustering, Regression, and Ensemble learning. From these results, a temporal evolu-tion analysis of the top 6 most used families was performed. Findings suggested a growing interest in NN and Ensemble learning, which motivated a focused study on the detailed techniques encompassed by these families. Concerning the NN, the Multi-layer perceptron was the most used technique. Nevertheless, more specialized deep learning techniques such as CNNs, LSTMs, and Deep Belief Networks are starting to be employed. With respect to Ensemble learn-ing, the most used technique was Random forests. 
The ML learning types were also reviewed. Findings showed that scientific literature mainly focused on the individual use of SL and RL. However, synergies between learning types are also employed. For instance, the most used synergy was SL-UL, which allows to explore and pre-process the data through UL to improve the SL training. The UL-RL and SL-UL-RL synergies had only one use each, which could be considered as a research gap, advising improvements in its integration. In fact, each learning type has its advantages and limitations. Hence, it is important to explore more synergy possibilities, as they may help over-come individual limits. 
Other than increasing data availability, one option to encourage the utilization of UL-RL and SL-UL-RL is to boost the development of specialized libraries to build complex models coupling several learning types. Examples of this are deep learning frameworks such as TensorFlow, Keras, PyTorch, etc. which have eased the implementation of deep learning applications. This has allowed researchers to spend more time on the addressed problem than on the coding stage. 
Results concerning the tools showed that MATLAB, R, Python, and RapidMiner are the most used tools in develop-ing ML-PPC models in research. However, most authors did not mention the tool used, which is a limit of this study. Fur-thermore, it is important to mention that these results come from a sample of scientific articles, meaning that results are mainly valid in an academic context. If there are practition-ers willing to implement ML-PPC models in companies, other aspects need to be analyzed such as the cost of the software, its scalability, skill availability in the labor market, compatibility with existing information systems, etc. 
The current horizon of data sources used is dominated by Artificial and Management data. The former points to a difficulty in collecting all of the data required to implement ML-PPC models, while the latter suggest that companies are interested in valuing their data stored in information sys-tems. Data coming from IoT sources such as Equipment and Product data was moderately used, nevertheless showing an interest in these technologies to collect data. Finally, ML-PPC models failed to integrate User data, probably because it is complex to collect and it engages an important respon-sibility concerning data privacy. 
The most addressed use cases were Smart Planning and Scheduling and Time Estimation, probably because they are directly concerned by the PPC, which may lead to its high utilization. The fact that there are research articles in all of the use cases suggests that the PPC is a transversal func-tion that benefits from several applications. Therefore, when designing a ML-PPC system for a company, the impact on all of the use cases must be assessed. Finally, it was found that Inventory and Distribution Control, as well as Smart Design of Products and Processes, are seldom addressed. This suggests that there is still a lot of progress to be made when coupling the PPC to logistics as well as product and process design through ML. 
Concerning the characteristics of I4.0, results suggest that scientific literature in ML-PPC is extremely focused on satisfying the Self-Organization of Resources, which was expected, as one of the main goals of the PPC is resource management to satisfy the commercial plan. At a second level, the Self-Regulation of the Production Process, the Self-Learning of the Production Process, and the Knowl-edge Discovery and Generation seem to be more frequently addressed. However, Fig..13 showed that they are mainly employed for Smart Planning and Scheduling, implying a lack of research in the other applications. Finally, there are three characteristics that are partially overlooked by researchers: Environment-Centric Processes, Smart Human Interaction, and Customer-Centric Product Development. The first two are essential characteristics of building more responsible production systems as they aim to include human beings and reduce the environmental impact of manufactur-ing processes. The latter relates to the alignment of the PPC to the customer¡¯s needs. Hence, it appears that recent ML-PPC research ignores the influence of the customer in the manufacturing process. 
As illustrated in the proposed cross-matrix, 75% of the possible research domains are barely addressed or were not explored at all. This means that the ML-PPC is still a key topic for the enablement of I4.0, which presents strong research avenues. The main future research perspectives could be summarized in the following three key items: 
1. Reinforce the role of IoT in ML-PPC: this would allow an improvement to the data acquisition system¡¯s design and would provide a means to perform a model update to tackle the concept drift issue. To do so, the ML mindset and workflow should be shifted from a linear to a cir-cular process, considering the need to constantly retrain through new data. This way of thinking would enable the identification, from an early development stage, of the retraining policy and the necessary variables that could be measured again at a sensitive cost. By defining these two aspects, the data acquisition system design will be less complex to conceive, as the needs will be clearer. 
This would avoid investment in sensors and resources and architecture that would not be exploited. Concern-ing the retraining policy, a review in the context of PPC reporting common practices, advantages and pitfalls seems to be missing in the scientific literature. 
2. 
Improve the integration between the PPC, logistics, and design: it was stated that the PPC benefits from different use cases. However, recent literature seems to overlook logistics as well as product and process design applica-tions coupled with the PPC. To tackle this challenge, it is necessary enable data availability, continuity and sharing over the design, logistics, and production depart-ments. This could be achieved through interoperability as well as communication of intra-organizational sys-tems such as the PLM, ERP, and MES. Even if projects that are meant to couple such systems are costly, they are necessary to ensure data availability and quality. One way to achieve this is the use of data lakes, which have been recognized as suitable to handle big data reposito-ries of a structured and unstructured nature (Llave 2018; Lo Giudice et.al. 2019). For instance, Llave (2018) con-cluded, through expert interviews, that one of the key purposes of data lakes is to serve as experimentation platforms for data scientists. 

3. 
Set human interaction and environmental aspect as pri-orities to ensure the development of ethical manufactur-ing in I4.0: exploring the interaction of humans with the proposed ML-PPC models is paramount to build-ing inclusive technologies at the service of society. To achieve this, the short-and long-term impact of ML-PPC systems on employees¡¯ working conditions must be assessed. If the system degrades them, it must be redesigned. Concerning the second aspect, seeking a reduction in the environmental impact of manufactur-ing through ML could provide important developments. This can be addressed from a purely PPC approach by optimizing, for instance, the scheduling of disassembly processes or by improving the prediction of production times to avoid energy waste. Other approaches could be the optimization of the supply chain. Even though the supply chain was not covered in this review, it is an appropriate domain for researchers to implement ML applications. For instance, by considering environmental criteria when choosing suppliers, as in (Hosseini and Barker 2016). 


Some of the research gaps indicated in this review could motivate future work. Future work will be focused on the following aspects: 
1. 
The proposed activities will be reviewed to determine an order between them, creating a procedure: this would help shift from a linear to a circular workflow when implementing ML-PPC models. 

2. 
The most suitable techniques and tools will be linked to each of the activities with sectorial information: linking techniques, tools, and activities is the key to creating good practices that could be helpful to new practitioners, both in research and industry. Furthermore, according to Kusiak (2017, 2019), there are profound differences in the volume of data generation and usage across different industries. Therefore, future work will aim to identify trends categorized by sectorial information. 

3. 
The current state of data availability solutions and work-arounds will be explored: as data availability was found to be a main issue, a review of techniques to tackle the class-imbalance problem and the use of transfer learning in the context of PPC will be performed. Additionally, the utilization of data lakes for ML-PPC will also be explored. 

4. 
Future research avenues will be proposed through an NLP analysis: NLP may enable the discovery of non-trivial trends present in the corpus of the 93 sampled articles. This will complement the results of the system-atic literature review. 


Acknowledgements This work was financially supported by a part-nership between the company iFAKT France SAS and the ANRT(Association Nationale de la Recherche et de la Technologie) underthe Grant 2018/1266. Furthermore, the authors thank the Editor-in-chief and three anonymous referees who helped improve the quality ofthis paper through their comments and suggestions. 
Compliance with ethical standards 
Conflict of interest The authors declare that they have no conflict of interest. 



Appendix 

Appendix I: Detail on.the.search strategy for.the.bibliometric analysis 
See Figs..14, 15 and 16. 




38% 
29% 

13% 27% 33% 13% 0% 14% 14% 27% 44%Relative frequency Fig. 17 Usage evolution for Neural Networks Relative frequency Fig. 18 Usage evolution for Q-Learning 25% 27% 0% 13% 27% 7% 29% 4% 0%Relative frequency Fig. 19 Usage evolution for decision trees Appendix II: Usage evolution of.the.top 6 most used 


techniques 
18% 13% 11% 12% 11% 
0%
See Figs..17, 18, 19, 20, 21 and 22. 
0% 

29% Appendix III: Detail on.NN and.Ensemble learning techniques 

See Fig..23. 
Machine learner fusion-regression 
1 
Ensemble 
NN 
Learning
Bagging  1  
Stacking  1  
Gradient boosting  2  
Random Forests  5  

1 
Autoencoder 
1 
Artificial Neural Agents 
1 
CNN 
1 
Stacked Denoising Autoencoders 
1 

13% 13% 14% 
Deep Belief Networks  2  
LSTM  3  
Self-Organizing Maps  3  
Multi-layer perceptron  16  

Number of uses 
Fig. 23 Detail on the techniques of the NN and ensemble learning families 



References 
Aissani, N., Bekrar, A., Trentesaux, D., & Beldjilali, B. (2012). 

Dynamic scheduling for multi-site companies: A decisional 
14% 

13% approach based on reinforcement multi-agent learning. Journal of Intelligent Manufacturing, 23(6), 2513¨C2529.
Altaf, M. S., Bouferguene, A., Liu, H., Al-Hussein, M., & Yu, H.(2018). Integrated production planning and control system for a panelized home prefabrication facility using simulation and 
RFID. Automation in Construction, 85, 369¨C383. 
Bergmann, S., Feldkamp, N., & Strassburger, S. (2016). Approxima-tion of dispatching rules for manufacturing simulation usingdata mining methods. In 2015 winter simulation conference (pp.2329¨C2340). Huntington Beach, USA.
Bi, J., & Zhang, C. (2018). An empirical comparison on state-of-the-artmulti-class imbalance learning algorithms and a new diversi-fied ensemble learning scheme. Knowledge-Based Systems, 158, 81¨C93.
Cai, B., Liu, H., & Xie, M. (2016). A real-time fault diagnosis meth-odology of complex systems using object-oriented Bayesian net-works. Mechanical Systems and Signal Processing, 80, 31¨C44. 
Cao, X. C., Chen, B. Q., Yao, B., & He, W. P. (2019). Combiningtranslation-invariant wavelet frames and convolutional neural network for intelligent tool wear state identification. Computers in Industry, 106, 71¨C84. 
Carvajal Soto, J. A., Tavakolizadeh, F., & Gyulai, D. (2019). An onlinemachine learning framework for early detection of product fail-ures in an Industry 4.0 context. International Journal of Com-puter Integrated Manufacturing, 32(4¨C5), 452¨C465. https://doi. org/10.1080/0951192X.2019.1571238. 
Chen, C., Xia, B., Zhou, B. H., & Xi, L. (2015). A reinforcement learn-ing based approach for a multiple-load carrier scheduling prob-lem. Journal of Intelligent Manufacturing, 26(6), 1233¨C1245.
Curatolo, N., Lamouri, S., Huet, J. C., & Rieutord, A. (2014). A criti-cal analysis of Lean approach structuring in hospitals. Business Process Management Journal, 20(3), 433¨C454.
Diaz-Rozo, J., Bielza, C., & Larra.aga, P. (2017). Machine learning-based CPS for clustering high throughput machining cycle con-ditions. In: 45th SME North American manufacturing research conference (pp. 997¨C1008). Los Angeles, USA.
Ding, K., & Jiang, P. (2018). RFID-based production data analysis in anIoT-enabled smart job-shop. IEEE/CAA Journal of Automatica Sinica, 5(1), 128¨C138.
Dinis, D., Barbosa-P¨®voa, A., & Teixeira, .. P. (2019). Valuing data inaircraft maintenance through big data analytics: A probabilisticapproach for capacity planning using Bayesian networks. Com-puters & Industrial Engineering, 128, 920¨C936. 
Dolgui, A., Bakhtadze, N., Pyatetsky, V., Sabitov, R., Smirnova, G.,Elpashev, D., & Zakharov, E. (2018). Data mining-based pre-diction of manufacturing situations data mining-based. In: 16th IFAC symposium on information control problems in manufac-turing (pp. 316¨C321). Bergamo, Italy: Elsevier B.V.
Doltsinis, S., Ferreira, P., & Lohse, N. (2014). An MDP model-basedreinforcement learning approach for production station ramp-upoptimization: Q-learning analysis. IEEE Transactions on Sys-tems, Man, and Cybernetics: Systems, 44(9), 1125¨C1138.
Fotuhi, F., Huynh, N., Vidal, J. M., & Xie, Y. (2013). Modeling yardcrane operators as reinforcement learning agents. Research in Transportation Economics, 42(1), 3¨C12.
Gao, X., Shang, C., Jiang, Y., Huang, D., & Chen, T. (2014). Refin-ery scheduling with varying crude: A deep belief network clas-sification and multimodel approach. AIChE Journal, 60(7),2525¨C2532. 
Garengo, P., Biazzo, S., & Bititci, U. S. (2005). Performance measure-ment systems in SMEs: A review for a research agenda. Interna-tional Journal of Management Reviews, 7(1), 25¨C47.
Grabot, B. (2018). Rule mining in maintenance: Analysing large knowl-edge bases. Computers and Industrial Engineering, 139, 1¨C15. 
Gyulai, D., K¨¢d¨¢r, B., & Monosotori, L. (2015). Robust productionplanning and capacity control for flexible assembly lines. In 15th IFAC symposium on information control problems in manufac-turing (pp. 2312¨C2317). Ottawa, Canada: Elsevier Ltd.
Gyulai, D., K¨¢d¨¢r, B., & Monostori, L. (2014). Capacity planning andresource allocation in assembly systems consisting of dedicatedand reconfigurable lines. In 8th international conference on dig-ital enterprise technology (pp. 185¨C191). Stuttgart, Germany: Elsevier B.V. 
Gyulai, D., Pfeiffer, A., Bergmann, J., & Gallina, V. (2018a). Onlinelead time prediction supporting situation-aware production con-trol. In 6th CIRP global web conference¡ªEnvisaging the future manufacturing, design, technologies and systems in innovation era (pp. 190¨C195).
Gyulai, D., Pfeiffer, A., Nick, G., Gallina, V., Sihn, W., & Monostori,
L. (2018b). Lead time prediction in a flow-shop environment 
with analytical and machine learning approaches. In 16th IFAC symposium on information control problems in manufacturing 
(pp. 1029¨C1034). Bergamo, Italy.
Habib Zahmani, M., & Atmani, B. (2018). Extraction of dispatchingrules for single machine total weighted tardiness using a modi-fied genetic algorithm and data mining. International Journal of Manufacturing Research, 13(1), 1¨C25.
Hammami, Z., Mouelhi, W., & Ben Said, L. (2017). On-line self-adaptive framework for tailoring a neural-agent learning modeladdressing dynamic real-time scheduling problems. Journal of Manufacturing Systems, 45, 97¨C108. 
Hammami, Z., Mouelhi, W., & Said, L. B. (2016). A self adaptiveneural agent based decision support system for solving dynamicreal time scheduling problems. In 10th international conference on intelligent systems and knowledge engineering (pp. 494¨C501).Taipei, Taiwan.
Harding, J. A., Shahbaz, M., Srinivas, & Kusiak, A. (2006). DataMining in Manufacturing: A Review. Journal of Manufactur-ing Science and Engineering-Transactions of the ASME, 128(4),969¨C976. 
Heger, J., Branke, J., Hildebrandt, T., & Scholz-Reiter, B. (2016).Dynamic adjustment of dispatching rule parameters in flow shopswith sequence-dependent set-up times. International Journal of Production Research, 54(22), 6812¨C6824.
Hosseini, S., & Barker, K. (2016). A Bayesian network model for resil-ience-based supplier selection. International Journal of Produc-tion Economics, 180, 68¨C87. 
Hosseini, S., & Ivanov, D. (2019). A new resilience measure for sup-ply networks with the ripple effect considerations: A Bayesiannetwork approach. Annals of Operations Research, 1¨C27. https://doi.org/10.1007/s10479-019-03350-8. 
Hosseini, S., Ivanov, D., & Dolgui, A. (2019). Review of quantita-tive methods for supply chain resilience analysis. Transporta-tion Research Part E: Logistics and Transportation Review, 125(December 2018), 285¨C307.
Huang, B., Wang, W., Ren, S., Zhong, R. Y., & Jiang, J. (2019). Aproactive task dispatching method based on future bottleneckprediction for the smart factory. International Journal of Com-puter Integrated Manufacturing, 32(3), 278¨C293.
Ji, W., & Wang, L. (2017). Big data analytics based fault predictionfor shop floor scheduling. Journal of Manufacturing Systems, 43, 187¨C194. 
Jiang, S. L., Liu, M., Lin, J. H., & Zhong, H. X. (2016). A predic-tion-based online soft scheduling algorithm for the real-worldsteelmaking-continuous casting production. Knowledge-Based Systems, 111, 159¨C172. 
Jordan, M. I., & Mitchell, T. M. (2015). Machine learning: Trends,perspectives, and prospects. Science, 349(6245), 255¨C260.
Jurkovic, Z., Cukor, G., Brezocnik, M., & Brajkovic, T. (2018). Acomparison of machine learning methods for cutting parametersprediction in high speed turning process. Journal of Intelligent Manufacturing, 29(8), 1683¨C1693.
Kartal, H., Oztekin, A., Gunasekaran, A., & Cebi, F. (2016). An inte-grated decision analytic framework of machine learning withmulti-criteria decision making for multi-attribute inventory clas-sification. Computers & Industrial Engineering, 101, 599¨C613. 
Khader, N., & Yoon, S. W. (2018). Online control of stencil print-ing parameters using reinforcement learning approach. In 28th international conference on flexible automation and intelligent manufacturing (pp. 94¨C101). Columbus, USA: Elsevier B.V. 
Kho, D. D., Lee, S., Zhong, R. Y. (2018). Big data analytics for pro-cessing time analysis in an IoT-enabled manufacturing shop floor.In 46th SME North American manufacturing research conference (pp. 1411¨C1420). Texas, USA: Elsevier B.V. 
Kim, H., & Lim, D.-E. (2018). Deep-learning-based storage-allocationapproach to improve the AMHS throughput capacity in a semi-conductor fabrication facility. In Communications in computer and information science (pp. 232¨C240). Springer Singapore.
Kim, S., & Nembhard, D. A. (2013). Rule mining for scheduling crosstraining with a heterogeneous workforce. International Journal of Production Research, 51(8), 2281¨C2300.
Kosmopoulos, D. I., Doulamis, N. D., & Voulodimos, A. S. (2012).Bayesian filter based behavior recognition in workflows allowingfor user feedback. Computer Vision and Image Understanding, 116(3), 422¨C434.
Kretschmer, R., Pfouga, A., Rulhoff, S., & Stjepandi., J. (2017).Knowledge-based design for assembly in agile manufacturingby using data mining methods. Advanced Engineering Informat-ics, 33, 285¨C299. 
Kruger, G. H., Shih, A. J., Hattingh, D. G., & Van Niekerk, T. I. (2011).Intelligent machine agent architecture for adaptive control opti-mization of manufacturing processes. Advanced Engineering Informatics, 25(4), 783¨C796.
Kumar, A., Shankar, R., & Thakur, L. S. (2018). A big data drivensustainable manufacturing framework for condition-based maintenance prediction. Journal of Computational Science, 27, 428¨C439. 
Kusiak, A. (2017). Smart manufacturing must embrace big data.Nature, 544(7648), 23¨C25.
Kusiak, A. (2019). Fundamentals of smart manufacturing: A multi-thread perspective. Annual Reviews in Control, 47, 214¨C220. 
Lai, L. K. C., & Liu, J. N. K. (2012). WIPA: Neural network and casebase reasoning models for allocating work in progress. Journal of Intelligent Manufacturing, 23(3), 409¨C421.
Lai, X., Shui, H., & Ni, J. (2018). A two-layer long short-term memorynetwork for bottleneck prediction in multi-job manufacturing sys-tems. In 13th international manufacturing science and engineer-ing conference (p. V003T02A014). Texas, USA.
Lemieux, A. A., Lamouri, S., Pellerin, R., & Tamayo, S. (2015).Development of a leagile transformation methodology for prod-uct development. Business Process Management Journal, 21(4), 791¨C819. 
Leng, J., Chen, Q., Mao, N., & Jiang, P. (2018). Combining granularcomputing technique with deep learning for service planningunder social manufacturing contexts. Knowledge-Based Systems, 143, 295¨C306. 
Li, D. C., Chen, C. C., Chen, W. C., & Chang, C. J. (2012a). Employ-ing dependent virtual samples to obtain more manufacturing information in pilot runs. International Journal of Production Research, 50(23), 6886¨C6903.
Li, X., Duan, F., Loukopoulos, P., Bennett, I., & Mba, D. (2018).Canonical variable analysis and long short-term memory for faultdiagnosis and performance estimation of a centrifugal compres-sor. Control Engineering Practice, 72(January), 177¨C191. 
Li, X., Wang, J., & Sawhney, R. (2012b). Reinforcement learningfor joint pricing, lead-time and scheduling decisions in make-to-order systems. European Journal of Operational Research, 221(1), 99¨C109.
Li, L., Zijin, S., Jiacheng, N., & Fei, Q. (2013). Data-based schedulingframework and adaptive dispatching rule of complex manufactur-ing systems. International Journal of Advanced Manufacturing Technology, 66(9¨C12), 1891¨C1905.
Liao, Q. (2018). Study of SVM-based intelligent dispatcher for paral-lel machines scheduling with sequence-dependent setup times.In 6th international conference on mechanical, automotive and materials engineering, CMAME 2018 (pp. 46¨C50). Hong Kong: IEEE. 
Lieber, D., Stolpe, M., Konrad, B., Deuse, J., Morik, K. (2013). Qual-ity prediction in interlinked manufacturing processes based onsupervised & unsupervised machine learning. In 46th CIRP conference on manufacturing systems 2013 (pp. 193¨C198).Set¨²bal, Portugal: Elsevier B.V.
Lingitz, L., Gallina, V., Ansari, F., Gyulai, D., Pfeiffer, A., & Sihn,
W. (2018). Lead time prediction using machine learning algo-rithms: A case study by a semiconductor manufacturer. In 51st CIRP conference on manufacturing systems (pp. 1051¨C1056).Stockholm, Sweden. 
Llave, M. R. (2018). Data lakes in business intelligence: Reporting from the trenches. In CENTERIS/ProjMAN/HCist 2018 (pp.516¨C524). Lisbon, Portugal: Elsevier B.V.
Lo Giudice, P., Musarella, L., Sofo, G., & Ursino, D. (2019). Anapproach to extracting complex knowledge patterns among con-cepts belonging to structured, semi-structured and unstructuredsources in a data lake. Information Sciences, 478, 606¨C626. 
Lubosch, M., Kunath, M., & Winkler, H. (2018). Industrial schedul-ing with Monte Carlo tree search and machine learning. In 51st CIRP conference on manufacturing systems (pp. 1283¨C1287).Stockholm, Sweden: Elsevier B.V. 
Lv, Y., Qin, W., Yang, J., & Zhang, J. (2018a). Adjustment modedecision based on support vector data description and evidencetheory for assembly lines. Industrial Management and Data Sys-tems, 118(8), 1711¨C1726.
Lv, S., Zheng, B., Kim, H., & Yue, Q. (2018b). Data mining for mate-rial feeding optimization of printed circuit board template pro-duction. Journal of Electrical and Computer Engineering. https ://doi.org/10.1155/2018/1852938. 
Ma, Y., Qiao, F., Zhao, F., & Sutherland, J. (2017). Dynamic schedul-ing of a semiconductor production line based on a composite rule set. Applied Sciences, 7(10), 1052.
Maghrebi, M., Shamsoddini, A., & Waller, S. T. (2016). Fusion basedlearning approach for predicting concrete pouring productivitybased on construction and supply parameters. Construction Inno-vation, 16(2), 185¨C202.
Manns, M., Wallis, R., & Deuse, J. (2015). Automatic proposal ofassembly work plans with a controlled natural language. In 9th CIRP conference on intelligent computation in manufacturing engineering (pp. 345¨C350). Capri, Italy.
Manupati, V. K., Anand, R., Thakkar, J. J., Benyoucef, L., Garsia, F.P., & Tiwari, M. K. (2013). Adaptive production control systemfor a flexible manufacturing cell using support vector machine-based approach. International Journal of Advanced Manufactur-ing Technology, 67(1¨C4), 969¨C981.
Miko.ajczyk, A., & Grochowski, M. (2018). Data augmentation forimproving deep learning in image classification problem. In 2018 international interdisciplinary PhD workshop, IIPhDW 2018 (pp. 117¨C122). Swinouj.cie, Poland: IEEE.
Mitchell, T. (1997). Machine learning (Vol. 2). New York: McGraw-Hill. 
Moeuf, A., Pellerin, R., Lamouri, S., Tamayo-Giraldo, S., & Barbaray,
R. (2018). The industrial management of SMEs in the era ofIndustry 4.0. International Journal of Production Research, 56(3), 1118¨C1136.
Mori, J., & Mahalec, V. (2015). Planning and scheduling of steel platesproduction. Part I: Estimation of production times via hybridBayesian networks for large domain of discrete variables. Com-puters & Chemical Engineering, 79, 113¨C134. 
Ou, X., Chang, Q., Arinez, J., & Zou, J. (2018). Gantry work cellscheduling through reinforcement learning with knowledge-guided reward setting. IEEE Access, 6, 14699¨C14709. 
Ou, X., Chang, Q., & Chakraborty, N. (2019). Simulation study onreward function of reinforcement learning in gantry work cellscheduling. Journal of Manufacturing Systems, 50, 1¨C8. 
Palombarini, J., & Mart¨ªnez, E. (2012). SmartGantt¡ªAn intelligentsystem for real time rescheduling based on relational rein-forcement learning. Expert Systems with Applications, 39(11),10251¨C10268. 
Pan, S. J., & Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10),1345¨C1359. 
Perez, L., & Wang, J. (2017). The effectiveness of data augmentationin image classification using deep learning. arXiv e-prints, arXiv :1712.04621. 
Priore, P., Ponte, B., Puente, J., & G¨®mez, A. (2018). Learning-basedscheduling of flexible manufacturing systems using ensemblemethods. Computers & Industrial Engineering, 126, 282¨C291. 
Qu, S., Chu, T., Wang, J., Leckie, J., & Jian, W. (2015). A central-ized reinforcement learning approach for proactive scheduling in manufacturing. In IEEE international conference on emerging technologies and factory automation. Luxembourg, Luxembourg.
Qu, S., Jie, W., & Shivani, G. (2016a). Learning adaptive dispatchingrules for a manufacturing process system by using reinforcementlearning approach. In IEEE international conference on emerg-ing technologies and factory automation. Berlin, Germany.
Qu, S., Wang, J., Govil, S., & Leckie, J. O. (2016b). Optimized adap-tive scheduling of a manufacturing process system with multi-skill workforce and multiple machine types: An ontology-based,multi-agent reinforcement learning approach. In 49th CIRP conference on manufacturing systems (CIRP-CMS 2016) (pp.55¨C60). Stuttgart, Germany: Elsevier B.V.
Rainer, C. (2013). Data mining as technique to generate planning rules for manufacturing control in a complex production system. In
K. Windt (Ed.), Robust manufacturing control. Heidelberg: Springer.
Reboiro-Jato, M., Glez-Dopazo, J., Glez, D., Laza, R., G¨¢lvez, J. F.,Pav¨®n, R., et.al. (2011). Using inductive learning to assess com-pound feed production in cooperative poultry farms. Expert Sys-tems with Applications, 38(11), 14169¨C14177.
Reuter, C., Brambring, F., Weirich, J., & Kleines, A. (2016). Improv-ing data consistency in production control by adaptation of datamining algorithms. In 9th international conference on digital enterprise technology (pp. 545¨C550). Nanjing, China.
Rostami, H., Blue, J., & Yugma, C. (2018). Automatic equipment fault fingerprint extraction for the fault diagnostic on the batch processdata. Applied Soft Computing, 68, 972¨C989. 
Ruessmann, M., Lorenz, M., Gerbert, P., Waldner, M., Justus, J., Engel, P., et.al. (2015). Industry 4.0: The future of productivityand growth in manufacturing. The Boston Consulting Group, 9, 54¨C89. 
Sahebjamnia, N., Tavakkoli-Moghaddam, R., & Ghorbani, N. (2016).Designing a fuzzy Q-learning multi-agent quality control systemfor a continuous chemical production line¡ªA case study. Com-puters & Industrial Engineering, 93, 215¨C226. 
Schuh, G., Prote, J. P., Luckert, M., & H¨¹nnekes, P. (2017a). Knowl-edge discovery approach for automated process planning. In50th CIRP conference on manufacturing systems knowledge (pp.539¨C544). Taichung, Taiwan.
Schuh, G., Reinhart, G., Prote, J. P., Sauermann, F., Horsthofer, J., Oppolzer, F., & Knoll, D. (2019). Data mining definitions andapplications for the management of production complexity. In 52nd CIRP conference on manufacturing systems (pp. 874¨C879).Ljubljana, Slovenia: Elsevier B.V.
Schuh, G., Reuter, C., Prote, J. P., Brambring, F., & Ays, J. (2017b).Increasing data integrity for improving decision making in pro-duction planning and control. CIRP Annals¡ªManufacturing Technology, 66(1), 425¨C428.
Shahzad, A., & Mebarki, N. (2012). Data mining based job dispatchingusing hybrid simulation-optimization approach for shop schedul-ing problem. Engineering Applications of Artificial Intelligence, 25(6), 1173¨C1181.
Sharp, M., Ak, R., & Hedberg, T. (2018). A survey of the advancinguse and development of machine learning in smart manufactur-ing. Journal of Manufacturing Systems, 48, 170¨C179. 
Shiue, Y. R., Guh, R. S., & Tseng, T. Y. (2012). Study on shop floorcontrol system in semiconductor fabrication by self-organizingmap-based intelligent multi-controller. Computers & Industrial Engineering, 62(4), 1119¨C1129.
Shiue, Y. R., Lee, K. C., & Su, C. T. (2018). Real-time scheduling for asmart factory using a reinforcement learning approach. Comput-ers & Industrial Engineering, 125(101), 604¨C614.
Solti, A., Raffel, M., Romagnoli, G., & Mendling, J. (2018). Misplacedproduct detection using sensor data without planograms. Deci-sion Support Systems, 112, 76¨C87. 
Stein, N., Meller, J., & Flath, C. M. (2018). Big data on the shop-floor: Sensor-based decision-support for manual processes. Journal of Business Economics, 88(5), 593¨C616.
Stricker, N., Kuhnle, A., Sturm, R., & Friess, S. (2018). Reinforce-ment learning for adaptive order dispatching in the semiconduc-tor industry. CIRP Annals¡ªManufacturing Technology, 67(1), 511¨C514. 
Talhi, A., Fortineau, V., Huet, J. C., & Lamouri, S. (2017). Ontologyfor cloud manufacturing based product lifecycle management.Journal of Intelligent Manufacturing, 30(5), 1¨C22.
Tao, F., Qi, Q., Liu, A., & Kusiak, A. (2018). Data-driven smart manu-facturing. Journal of Manufacturing Systems, 48, 157¨C169. 
Thomas, T. E., Koo, J., Chaterji, S., & Bagchi, S. (2018b). Minerva:A reinforcement learning-based technique for optimal schedul-ing and bottleneck detection in distributed factory operations. In10th international conference on communication systems and networks (pp. 129¨C136). Bengaluru, India.
Thomas, A., Noyel, M., Zimmermann, E., Suhner, M.-C., Bril ElHaouzi, H., & Thomas, P. (2018a). Using a classifier ensemble for proactive quality monitoring and control: The impact of thechoice of classifiers types, selection criterion, and fusion process.Computers in Industry, 99(March), 193¨C204.
Tian, G., Zhou, M., & Chu, J. (2013). A chance constrained program-ming approach to determine the optimal disassembly sequence.IEEE Transactions on Automation Science and Engineering, 10(4), 1004¨C1013.
Tong, Y., Li, J., Li, S., & Li, D. (2016). Research on energy-saving pro-duction scheduling based on a clustering algorithm for a forgingenterprise. Sustainability, 8(2), Article number 136.
Tony Arnold, J. R., Chapman, S. N., & Clive, L. M. (2012). Introduc-tion to materials management (Vol. 118). Pearson: London.
Tranfield, D., Denyer, D., & Smart, P. (2003). Towards a methodol-ogy for developing evidence-informed management knowledgeby means of systematic review. British Journal of Management, 14, 207¨C222. 
Tuncel, E., Zeid, A., & Kamarthi, S. (2014). Solving large scale dis-assembly line balancing problem with uncertainty using rein-forcement learning. Journal of Intelligent Manufacturing, 25(4),647¨C659. 
Wang, C., & Jiang, P. (2018). Manifold learning based reschedul-ing decision mechanism for recessive disturbances in RFID-driven job shops. Journal of Intelligent Manufacturing, 29(7),1485¨C1500. 
Wang, C., & Jiang, P. (2019). Deep neural networks based order com-pletion time prediction by using real-time job shop RFID data. Journal of Intelligent Manufacturing, 30(3), 1303¨C1318.
Wang, H., Jiang, Z., Zhang, X., Wang, Y., & Wang, Y. (2017). A faultfeature characterization based method for remanufacturing pro-cess planning optimization. Journal of Cleaner Production, 161, 708¨C719. 
Wang, J., Ma, Y., Zhang, L., Gao, R. X., & Wu, D. (2018a). Deep learn-ing for smart manufacturing: Methods and applications. Journal of Manufacturing Systems, 48, 144¨C156. 
Wang, C. L., Rong, G., Weng, W., & Feng, Y. P. (2015). Mining sched-uling knowledge for job shop scheduling problem. In 15th IFAC symposium on information control problems in manufacturing (pp. 800¨C805). Ottawa, Canada: Elsevier Ltd.
Wang, H. X., & Yan, H. Sen. (2016). An interoperable adaptivescheduling strategy for knowledgeable manufacturing based onSMGWQ-learning. Journal of Intelligent Manufacturing, 27(5),1085¨C1095. 
Wang, J., Yang, J., Zhang, J., Wang, X., & Zhang, W. (2018b). Bigdata driven cycle time parallel prediction for production plan-ning in wafer manufacturing. Enterprise Information Systems, 12(6), 714¨C732.
Wang, J., Zhang, J., & Wang, X. (2018c). Bilateral LSTM: A two-dimensional long short-term memory model with multiplymemory units for short-term cycle time forecasting in re-entrantmanufacturing systems. IEEE Transactions on Industrial Infor-matics, 14(2), 748¨C758.
Waschneck, B., Bauernhansl, T., Knapp, A., Kyek, A. (2018). Optimi-zation of global production scheduling with deep reinforcementlearning. In 51st CIRP conference on manufacturing systems (pp.1264¨C1269). Stockholm, Sweden.
Wauters, T., Verbeeck, K., Verstraete, P., Vanden Berghe, G., & DeCausmaecker, P. (2012). Real-world production scheduling forthe food industry: An integrated approach. Engineering Applica-tions of Artificial Intelligence, 25(2), 222¨C228. 
Wu, W., Ma, Y., Qiao, F., & Gu, X. (2015). Data mining based dynamicscheduling approach for semiconductor manufacturing system.In 34th Chinese control conference (pp. 2603¨C2608). Hangzhou, China. 
Xanthopoulos, A. S., Kiatipis, A., Koulouriotis, D. E., & Stieger, S.(2017). Reinforcement learning-based and parametric produc-tion-maintenance control policies for a deteriorating manufactur-ing system. IEEE Access, 6, 576¨C588. 
Yang, Z., Zhang, P., & Chen, L. (2016). RFID-enabled indoor position-ing method for a real-time manufacturing execution system usingOS-ELM. Neurocomputing, 174, 121¨C133. 
Yeh, D. Y., Cheng, C. H., & Hsiao, S. C. (2011). Classification knowl-edge discovery in mold tooling test using decision tree algorithm.Journal of Intelligent Manufacturing, 22(4), 585¨C595.
Yuan, B., Wang, L., & Jiang, Z. (2014). Dynamic parallel machinescheduling using the learning agent. In 2013 IEEE international conference on industrial engineering and engineering manage-ment (pp. 1565¨C1569). Bangkok, Thailand.
Zellner, G. (2011). A structured evaluation of business processimprovement approaches. Business Process Management Jour-nal, 17(2), 203¨C237.
Zhang, Z., Zheng, L., Hou, F., & Li, N. (2011). Semiconductor finaltest scheduling with Sarsa(¦Ë, k) algorithm. European Journal of Operational Research, 215(2), 446¨C458.
Zhang, Z., Zheng, L., Li, N., Wang, W., Zhong, S., & Hu, K. (2012).Minimizing mean weighted tardiness in unrelated parallelmachine scheduling with reinforcement learning. Computers & Operations Research, 39(7), 1315¨C1324.
Zhong, R. Y., Huang, G. Q., Dai, Q. Y., & Zhang, T. (2014). Min-ing SOTs and dispatching rules from RFID-enabled real-timeshopfloor production data. Journal of Intelligent Manufacturing, 25(4), 825¨C843.
Zhong, R. Y., Newman, S. T., Huang, G. Q., & Lan, S. (2016). Big Datafor supply chain management in the service and manufacturingsectors: Challenges, opportunities, and future perspectives. Com-puters & Industrial Engineering, 101, 572¨C591. 
Zhou, P., Guo, D., & Chai, T. (2018). Data-driven predictive controlof molten iron quality in blast furnace ironmaking using multi-output LS-SVR based inverse system identification. Neurocom-puting, 308, 101¨C110. 
Zhou, L., Pan, S., Wang, J., & Vasilakos, A. V. (2017). Machine learn-ing on big data: Opportunities and challenges. Neurocomputing, 237(January), 350¨C361. 
Publisher¡¯s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. 



Manufacturing paradigm-oriented PHM methodologies for cyber-physical systems 
Tangbin Xia1 ﹞ Lifeng Xi1 

Received: 25 May 2016 / Accepted: 2 July 2017 / Published online: 1 August 2017 . Springer Science+Business Media, LLC 2017 
Abstract In today＊s competitive environment of Indus-try 4.0, cyber-physical systems (CPS) of various advanced manufacturing paradigms have brought new challenges to maintenance managements. Effcient prognostics and health management (PHM) policies, which can integrate both indi-vidual machine deteriorations and different manufacturing paradigms, are urgently needed. Newly proposed PHM methodologies are systematically reviewed in this paper: as the decision basis, an operating load based forecast-ing algorithm is proposed for machine health prognosis; at the machine level, a dynamic multi-attribute mainte-nance model is studied for diverse machines in CPS; at the system level, novel opportunistic maintenance poli-cies are developed for complex fow-line production, mass customization and reconfgurable manufacturing systems, respectively. This framework of PHM methodologies has been validated in industrial implementations. 
Keywords Maintenance ﹞ Dynamic programming ﹞ Manufacturing paradigms ﹞ Cyber-physical system 


Introduction 
In the global competition and technique innovation, many manufacturing enterprises are pursuing a shift to cyber-physical systems (CPS) of advanced manufacturing para-digms (Lee et al. 2014a). In practice, complex fow-line 
B Lifeng Xi lfxi@sjtu.edu.cn 
State Key Laboratory of Mechanical System and Vibration, Department of Industrial Engineering, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai 200240, China 
production, mass customization and reconfgurable manu-facturing paradigms have been applied to satisfy changeable customer demands and keep enterprise core competitive-ness (Al-Zaher and ElMaraghy 2013; Lin et al. 2015; Jardim-Goncalves et al. 2016). However, these CPS systems, machines and accessorial sensors have also become techno-logically more advanced, and more diffcult to manage. This transformation provides motivation for improving mainte-nance methodologies. It is important to effciently predict machine health statuses, eliminate unnecessary production breaks, achieve maintenance cost reduction and decrease systemic decision-making complexity (Rafee et al. 2014; Benkedjouh et al. 2015). 
In the recent decades, numerous valuable studies have been devoted to the maintenance scheduling (Arab et al. 2013; Mirabi et al. 2013; Zied et al. 2014). Prognostics and health management (PHM) has been crucial to keep CPS systems and their machines in good condition (Liu et al. 2013). Cyber-physical systems usually consist of diverse machines, which have different degrading processes that will fnally lead to failures and interrupt the normal pro-duction (Sheikhalishahi et al. 2014; Chouikhi et al. 2014; Ebrahimipour et al. 2015). Considering CPS characters of integrated computational and physical capabilities such as actuation, sensing and communication to physical world, PHM should provide a systematical view of the machine health prognosis, the machine-level maintenance scheduling and the system-level maintenance optimization. To develop proper PHM methodologies for advanced manufacturing paradigms, it is necessaryto comprehensively consider main-tenance opportunities and manufacturing characters to make maintenance schedules in a cost-effective manner. However, classical opportunistic maintenance policies are insuffcient to provide feasible solutions because of complex series-parallel structures, changeable batch orders and open-ended 
123 

system reconfgurations (Chang et al. 2007; Derigent et al. 2009; Zhou et al. 2009; Lee et al. 2013). Thus, PHM poli-cies that can decrease decision-making complexity, avoid breakdowns of batch production, and adapt to diverse recon-fgurations are urgently needed. 
PHM methodologies for advanced manufacturing para-digms are complex due to the hierarchical levels of system-atical maintenance decision-making: (a) accurate machine health prediction at the physical level; (b) dynamic mainte-nance scheduling at the machine level; (c) effective oppor-tunistic maintenance policies at the system level. In a CPS system, recent advances in sensing and information tech-nologies enable enterprises to on-line collect, store and process information that characterizes machine health sta-tuses (Lee et al. 2014b). Thus, these statuses are utilized to predict machine deteriorations for supporting PHM decision-making.Furthermore,designedinformationtransferbetween the machine level and the system level should not be a ※push§ process, but a ※pull§ process. By pulling machine-level outputs, this interactive scheduling mode promotes opportunistic maintenance policies to dynamically optimize system-level schedules by integrating maintenance opportu-nities and manufacturing paradigms. 
The remainder of this paper is organized as follows: ※A systematical framework of PHM methodologies§ sec-tion presents a systematical PHM framework for advanced manufacturing paradigms. ※WFRGM algorithm for machine health prognosis§ section proposes the W-variable fore-casted-state rolling grey model (WFRGM) by considering the effect of operating loads. ※MAM method for machine-level maintenance scheduling§ section develops the multi-attribute model (MAM) by utilizing the multiple attribute value theory and imperfect maintenance. ※Opportunistic maintenance for various cyber-physical systems§ section dis-cusses the maintenance time window (MTW) for complex fow-line production, the advance-postpone balancing (APB) for mass customization, and the reconfgurable maintenance time window (RMTW) for reconfgurable manufacturing, respectively. Finally, conclusions and perspectives are drawn in ※Case study of PHM methodologies§ section. 


A systematical framework of PHM methodologies 
Cyber-physical systems (CPS) are defned as transformative technologies for managing interconnected systems between its physical assets and computational capabilities (Lee et al. 2015). Recent advances in manufacturing industry have paved way for a systematical deployment of CPS, within which information from all related perspectives is closely monitored and synchronized between the physical factory foor and the cyber computational space. Moreover, by uti-lizing advanced information analytics, networked machines will be able to perform more effciently, collaboratively and resiliently. Cyber-physical systems are ubiquitous in power systems, transportation networks, industrial control processes, and critical infrastructures. These systems need to operate reliably in the face of unforeseen failures (Pasqualetti et al. 2013). 
For understanding the impact of CPS and the relation to the manufacturing feld, Monostori et al. (2016) compre-hensively studied cyber-physical systems in manufacturing. This important survey can help us: (1) to identify potentially impactful articles that are related to CPS and (2) to fnd out how CPS has evolved with respect to problems, appli-cations and techniques. Wang et al. (2015) presented the current status and advancement of cyber-physical systems and their future research directions when applied to manu-facturing. The characteristics of CPS were outlined together with those of Systems of Systems (SoS), Internet of Things (IoT), Big Data and Cloud technology. Like cloud-enabled prognosis can leverage advanced manufacturing by using data and information from across the manufacturing hierar-chy(Gaoetal.2015),PHMmethodologiesforCPShavebeen designed to improve effciency, productivity, and proftabil-ity by integrating monitored information, failure prediction, system structure and manufacturing characteristics. 
In industry, modern manufacturing systems with CPS technologies could be widely used in advanced manufactur-ing paradigms, such as complex fow-line production, mass customization and reconfgurable manufacturing paradigms. Since machine statuses are available from sensors within the cyber computational space, PHM decisions to optimize maintenance arrangements should be made in the physical factory foor by considering different manufacturing char-acters. Without properly integrating the special characters of advanced manufacturing paradigms, valuable information collected by CPS technologies can achieve rapid respon-siveness and cost effectiveness for modern manufacturing systems. At this point, several key issues need to be addressed in the developed PHM methodologies for CPS: 
(1) Based on monitored and synchronized information, it will improves forecast accuracy by incorporating real-time infuencing factors (i.e., operating load) for machine health prognosis; (2) with failure frequency predictions, it is important to accurately describe hazard rate evolutions of individual machines and model machine-level maintenance operations with multiple objectives; (3) by pulling machine-level outputs, cost-effective system schedules should be studied to avoid decision-making complexity caused by series-parallel structures for complex fow-line paradigm; (4) for mass customization paradigm, an opportunistic mainte-nance strategy is required to handle changeable batch orders due to customer demands and eliminate unnecessary produc-tion breaks; (5) for reconfgurable manufacturing paradigm, real-time maintenance schedules should be made to respond 
123 


Fig. 1 Scheme of hierarchical PHM decision-making for CPS 
rapidly to diverse open-ended reconfgurations and fexible system structures. 
The designed PHM framework consists of three levels, where CPS maintenance decisions are dynamically made through the machine health prognosis, the machine-level maintenance scheduling and the system-level maintenance optimization. The hierarchical scheme is shown in Fig. 1. 
. 
Physical level Cyber-physical systems of advanced man-ufacturing paradigms are defned as the decision objects. With rapid innovations of monitoring techniques and sen-soring tools, effcient prognostic algorithm is developed to forecast accurate machine health trends for supporting the PHM decision-making process in real time, rather than over time. 

. 
Machine level For each individual machine, preventive maintenance (PM) intervals are dynamically scheduled by considering multiple attribute value theory, imperfect maintenance assessment and sequential PM scheduling mode. If a machine fails between successive PM actions, 


minimal repair recovers it to the failure rate that it had when it failed. 
. System level By pulling PM intervals, novel opportunistic maintenancepoliciesarepresentedtoutilizemaintenance opportunities and manufacturing characters to make dynamic maintenance schedules in a cost-effective man-ner. The manufacturing characters of CPS are thoroughly investigated. Thus, the proposed PHM methodologies can adapt to advanced manufacturing paradigms and achieve signifcant reduction of maintenance cost, pro-duction downtime and decision-making complexity. 
The notation used in this paper is listed in Table 1. 


WFRGM algorithm for machine health prognosis 
Machine health prognosis plays an important role in PHM methodologies. For complex CPS consisting of multiple machines, it is necessary to utilize maintenance opportunities 
123 

1662  J Intell Manuf (2019) 30:1659每1672  
Table 1 Notation  
W : Generating coeffcient of grey model  L : Operating load change rate  
x(0) : Actual status of machine  .x(0) : Forecasted status of machine  
i : Index of PM cycles at machine level  j : Index of machine M j  
Aij : Availability of the ith PM cycle for M j  crij : Cost rate of the ith PM cycle for M j  
Tpij : Time duration of PM action  Tfij : Time duration of minimal repair  
Cpij : Cost of PM action  Cfij : Cost of minimal repair  
竹ij(t): Hazard rate function prior to the ithPM  Toij : PM interval of machine level  
aij : Age reduction factor  bij : Hazard increase factor  
Tw : Maintenance time window  k : Index of PM cycles at system level  
tjk : PM time point of M j at system level  tk : PM execution point at system level  
ETC : Excepted total system maintenance cost  cd j : Downtime cost rate  
u : Index of batch Bu  T Bu : Time duration of batch Bu  
tij : Time point of PM from machine level  tbu : Set-up time point after Bu at system level  
(j,tbu) : Maintenance decision at tbu  Gu : PM combination set after Bu  
SCAj(u+1) : Saved cost of PM advancement  SCPj(u+1) : Saved cost of PM postponement  
APBj(u+1) : Advance-postpone balancing  cs j : Set-up cost rate  
Tpu max : Maximum duration for PM actions  h : Index of manufacturing stage MSh  
TRh : Time duration of the hth reconfguration  tRh : Time point of the hth reconfguration  
TWh : Time width of RMTW in MSh  (j,tk) : Maintenance decision for M j at tk  

and avoid production losses by forecasting machine degrada-tions. Conventional forecasting methods can be categorized into quantitative forecasting and qualitative forecasting, including Delphi method, time series, exponential smooth-ing, linear regression, expert systems and neural networks (Wang and Hsu 2008; Yu and Xi 2008; Tian 2012). Gen-erally, large amounts of machine statuses are required to construct prognosis models, which limit their practical uses for CPS. In recent decades, grey model (GM) forecasting has achieved good prognosis accuracy with limited statuses by using approximate differential equations to describe future tendencies for a time series (Akay and Atak 2007; Xia et al. 2015b). The GM method, which was frst proposed by Deng(1982),focusesoninformationinsuffciencyandmodel uncertaintyin analyzing future trends through studies on con-ditional analysis, prediction and decision making based on scarce and fuzzy information. This forecasting model is suit-able for real-time prediction with limited data available. 
To further increase GM accuracy, the novel philosophy comprising of utilizing practical industrial infuencing fac-tors, besides the time series itself, is needed. This study tries to achieve the following GM improvements: (1) incorporat-ing real-time infuencing factors (such as operating loads) that affect machine health trends; (2) taking new statuses into consideration and avoiding too old ones that cannot refect current machine degradations; (3) dynamically eval-uating the generating coeffcient W values to overcome the shortage of static W = 0.5 in original GM(1,1). Thus, a W-variable Forecasted-state rolling grey model (WFRGM) is proposed to increase the accuracy of CPS health prognosis. This WFRGM algorithm includes the following steps: 
(1) 
Health data acquisition With sensing technology of CPS, health statuses of machine failure frequency at sequential time d are collected online as the in-sample testing data x(0) = (x(0)(1),x(0)(2),...,x(0)(d),..., x(0)(p)), p ≡ 4. 

(2) 
Dynamic W .tting In grey model, enumerate W val-ues and select optimal ones (W1,W2,W3,...,Wp) at time d = 1,2,3,..., p. Evaluate the correlation coef-fcient (CR)of W values and corresponding operating load change rate Lvalues (L1,L2,L3,...,Lp). Then construct the relationship of W = f (L). 


p  Ld . L
d=1 Wd . W 
CRWL =  (1) 
p 2 p 2Wd . W Ld . L
d=1 d=1 
(3) 
WFRGM reconstruction With forecasted W (Wp+1, Wp+2,Wp+3,...,Wp+q )related to real-time L(Lp+1, Lp+2,Lp+3,...,Lp+q ), WFRGM is reconstructed by taking advantages of forecasted-state rolling and gener-ated values calculating with dynamic W in Accumulat-ing Generation Operation (AGO). 

(4) 
Health trend prediction Then WFRGM is used to forecast the out-of-sample predictive data (x.(0)(p + 1),x.(0)(p+2),x.(0)(p+3),...,x.(0)(p+q)).Forecasted-state rolling process and dynamic W values ensure a high-precision prediction, which is essential for support-ing PHM scheduling. The rolling process reconstructs the grey model whenever a new status rolls in. It takes newer information into consideration and eliminates older statuses that cannot show the new machine health trend. Furthermore, in original rolling GM, the generat-


123 

ing coeffcient W is customarily given as 0.5. The static W value does not consider real-time infuencing fac-tors. Therefore, by analyzing the relationship between dynamic W values and variable L data, WFRGM can generate better forecasts. 
(5) Performance evaluation and application To evaluate the predicting performance, different error criteria are intro-duced and used, such as the mean absolute percentage error (MAPE) and the mean absolute error (MAE). 


MAM method for machine-level maintenance scheduling 
Based on the machine health prognosis, decision makers can make maintenance schedules. With age and usage, each machine undergoes increasing wear, which fnally leads to a failure and breaks the normal production. Conventional maintenance models usually suffer from a critical problem of setting periodic intervals to perform PM actions. However, it has been noticed that insuffcient maintenance inevitably leads to unnecessary downtime and huge cost; on the other hand, plethoric maintenance will increase maintenance cost and decrease manufacturing proft (Dekker et al. 1997). The innovative idea of this research is to incorporate the multiple attribute value theory, the imperfect maintenance assessment and the sequential PM scheduling mode. Proper machine-level PM intervals of diverse machines will be the solid base for the opportunistic maintenance policies at the system level (Xia et al. 2013). 
This research focuses on three crucial questions for optimally scheduling PM intervals: frstly, the traditional assumption of perfect PM that covers a machine to the ※as good as new§ status is plausible (Wang and Tsai 2014). For most machines, even though some components are replaced, the cumulative wear on adjacent components may deteriorate unnoticed. This leads to the imperfect effects of mainte-nance activities. In practice, a machine after PM is not as good as brand new one, that is, the hazard rate value is decreased while always greater than zero. Simultaneously, each machine tends to have more frequent maintenance since the hazard rate increases more quickly than it did in the pre-vious PM interval. To sum up, PM not only decreases the hazard rate to a certain value but also changes the slope of the hazard rate function. Secondly, most existing maintenance models were concerning cost. In fact, it should consider other machine-level PM objectives according to practical require-ments. Thus, this study utilizes the multiple attribute value theory in building the PM model. Last but by no means the least, for responding quickly to system-level PHM pulling, a dynamic model-iteration mode is proposed to output PM intervals cycle by cycle. Since conventional static long-time planning focuses on the maintenance modelling and analysis for the whole designed lifetime and arranges PM actions in advance without considering the real-time machine degrada-tion, which is usually not applicable in a practical factory. In our dynamic model-iteration mode, sequential PM intervals are obtained according to the real-time hazard rate evolution of the current cycles, not being relative to the whole lifetime. 
The multi-attribute model (MAM), which is illustrated in Fig. 2, provides real-time PM intervals Toij , even if there are L objectives (O1ij,O2ij,...,OLij). The comprehensive objective function is minimized to schedule optimal PM intervals. If a smaller Oli j (such as the maintenance cost crij )is preferred, l =0; if a larger Olij (such as the machine availability Aij)is preferred, then k =1. 
(.1)1 O1ij (.1)2 O2ijVij = w1ij +w2ij
O. O. 
1ij 2ij 
(.1)L OLij
+﹞﹞﹞+wLij (2)
O. 
Lij 
In this model, the machine availability Aij and the main-tenance cost rate crij may be considered as two objectives related to the effciency and the economy, respectively: 
Aij =  Taij  (3) 
 Taij
Taij + Tpij +Tfij 0 竹ij (t)dt 
 Tcij
Cpij +Cfij 0 竹ij (t)dt 
crij =  (4) 
 Tcij
Tcij + Tpij +Tfij 0 竹ij(t)dt 
For each next PM cycle, with the actual interval Tij from the system-level feedback, the relationship between hazard rates of consecutive cycles can be defned as: 
竹(i+1)j (t) =bij 竹ij (t +aijTij ),t ﹋(0,T(i+1)j ) (5) 
In imperfect maintenance effects, the age reduction fac-tor aij, aij ﹋ (0,1)indicates that imperfect PM causes the machine＊s initial failure rate to become 竹ij(aijTij); mean-while, the hazard increase factor bij > 1 refects that PM increases the failure rate bij竹ij(t). 


Opportunistic maintenance for various cyber-physical systems 
Nowadays,therehasbeenagrowinginterestinPHMmethod-ologies of multi-unit systems for leading enterprises. It is essential to investigate and model the complicated machine interactions and the diverse manufacturing characters, which provide maintenance opportunities for CPS of advanced manufacturing paradigms. Opportunistic maintenance refers 
123 


Fig. 2 Illustration of machine-level MAM method 
to the scheme where PM can be performed at opportuni-ties with the advantages of combining individual PM actions and saving much group maintenance cost (Xia et al. 2012; Gu et al. 2015). To overcome the exponential decision-making complexity with machine number increasing and apply the system-level PHM methods to advanced manufac-turing paradigms, novel opportunistic maintenance policies will be presented in detail. 


MTW policy for complex .ow-line system 
Complex series-parallel cyber-physical systems have been widely used to satisfy fow-line productions. In this article, a general PHM decision-making policy is proposed by consid-ering both machine degradation and system structure. This maintenance time window (MTW) policy can help enterprise managers to make dynamic maintenance schedules based on not only single-machine plans, but also the whole-system global programming. MTW programming is applied by pulling real-time machine-level PM intervals. A breakdown 
.
. cdj ﹞ Tpk max ETCkj = . Toij ..(tjk .tk )

Cpij + Cfij 0 竹ij (t)dt + cdj ﹞ Tpk max 
.
. 
0 
caused by one machine is utilized to carry out PM actions on non-failed ones, thus unnecessary breakdown of CPS could be avoided. This maintenance-driven opportunistic main-tenance policy aims to systematically obtain system-level maintenance schedules in a cost effective manner: 

(1) 
MTW-separation in parallel subsystem According to machine-level PM intervals, the MTW value Tw pro-vides a criterion to separate PM actions in subsystems. MTW-separations can avoid the unnecessary downtime of upstream and downstream machines. 

(2) 
MTW-combination in series subsystem Pulling the out-puts from MAM and MTW-separations cycle by cycle, MTW is defned as the criterion to combine PM actions within [tk, tk+Tw].Thetimepoint tk iswhenonemachine is preformed PM, which also means maintenance oppor-tunities for other machines in series. 

(3) 
System performance evaluation The total system mainte-nance cost (ETC) by using MAM policy can be evaluated based on system-level maintenance schedules. The total maintenance cost of the kth cycle for machine j can be evaluated by: 


( j, tk) = 0 ( j, tk) = 1 (6) ( j, tk) = 2 
123 


where ( j, tk) = 0 means no maintenance action is ini-tiated on M jat the time point tk, but this machine will be down; ( j, tk) = 1 means the PM action is combined to be performed in advance; ( j, tk) = 2 means no maintenance action is initiated and the machine continues to operate. Thus, the total system maintenance cost for the CPS in its mission lifetime can be obtained by: 
..
KJ ETC = . ETCkj.. (7) k=1 j=1 


APB policy for mass customization system 
As one of advanced manufacturing paradigms, mass cus-tomization is widely used to response quickly to changeable customer demands. In mass customization, batch orders are processed through CPS with following production charac-teristics: (1) batch orders are independent with diverse lot size; (2) batches are sequentially ordered only a transient time beforehand; (3) one set-up work happens when a batch switches to another; (4) it prefers no interruptions in each batch cycle to ensure product quality. To meet the require-ments of mass customization, there has been a great need to propose a new type of opportunistic maintenance that consid-ers machine degradations and manufacturing characteristics (Xia et al. 2015a). 
In this study, a production-driven opportunistic mainte-nance policy is presented to eliminate unnecessary produc-tion breaks and achieve signifcant cost reduction. According to sequential batch orders and machine-level PM intervals, the advance-postpone balancing (APB) policy utilizes the set-up works and analyzes the cost savings to schedule real-time PM adjustments. Each set-up time between successive batches is used to perform PM actions, thus unnecessary breakdown during batch productions can be avoided. We apply APB programming to analyze the cost savings of PM advancement and PM postponement, and then choose the better PM adjustment to ensure no-disruptions and reduce maintenance cost. 
This production-driven APB policy has the advantages for the mass customization paradigm: (1) APB satisfes no-disruption requirements during changeable batch orders, other than traditional constant production assumption; (2) it utilizes planned production downtimes as maintenance opportunities to advance or postpone PM actions, which signifcantly reduces the complexity of system-level schedul-ing; (3) by choosing the greater cost savings between PM advancement and PM postponement at each set-up opportu-nity, APB ensures that the maximization of ETC-saving can be achieved. The procedure of APB programming is illus-trated in Fig. 3. 
When each batch Bu has been fnished, and the next batch Bu+1 has not started, this moment tbu is utilized as the deci-sion time to schedule APB. On the one hand, if machineMj is prevented maintained now, the saved cost by advancing PM in batch Bu+1 can be evaluated as: 
SCAj(u+1) = SCAdj(u+1) + SCAjf (u+1) . SCApj(u+1) 
= Tpij (cdj . cs j ) 

 T .  T . 
oij oij .(tij .tbu ) 
+ 竹ij (t)dt . 竹ij (t)dt 
00 
123 

tij . tbuCfij . CpijT . oij . (tij . tbu)  (8)  
where SCAd  is the downtime cost saving, SCA f  is 

j(u+1) j(u+1) the minimal repair cost saving, SCAp is the PM cost
j(u+1) 
saving of PM advancement: 
On the other hand, if PM of machine Mj is postponed to the next set-up time point tbu+1, the minimal repair cost sav-ing will be a negative value (prolonged PM interval leads to increasing cumulative failure risk and more minimal repair cost) and the PM cost saving will be a positive value (longer intervals mean that less PM actions would be needed in the same scheduling horizon). Therefore, the saved cost by post-poning PM in batch Bu+1 can be evaluated as: 
SCPj(u+1) = SCPdj(u+1) . SCPjf (u+1) + SCPpj(u+1) 
= Tpij (cdj . cs j ) 

 T .  T . 
oij+(tbu+1.tij ) oij
. 竹ij (t)dt . 竹ij (t)dt 
00 
tbu+1 . tijCfij + Cpij (9)
T . 
oij + (tbu+1 . tij ) 
where SCPd is the downtime cost saving, SCP f is
j(u+1) j(u+1) the minimal repair cost saving and SCPp is the PM cost 
j(u+1) saving of PM postponement: According to the values of SCA and SCP, APBj(u+1) could be defned as the criterion to decide weather to advance or postpone this PM action: 
APBj(u+1) = SCAj(u+1) . SCPj(u+1). (10) 


RMTW policy for recon.gurable manufacturing system 
The system structure of reconfgurable manufacturing CPS can be adjusted to meet various future products and change-able market demands (Koren and Shpitalni 2010; Ni and Jin 2012). In other words, the main advantage of reconfgurable manufacturing is the adaptability to the uncertainties of the open system architecture with reconfgurable system struc-tures. For the entire system, those different reconfgurations are caused by the changing needs in terms of capacity and functionality, while the production process will be separated into sequential manufacturing stages. Each manufacturing stage (MSh) has its own system structure designed for its current production requirements. If the system-level main-tenance policy has to be rebuilt according to each different structure, its responsiveness and fexibility will be obviously weakened (Xia et al. 2016). 
By extending the previous research from both recon-fgurable structure and manufacturing paradigm aspects, this study presents a reconfguration-oriented opportunistic maintenance policy to achieve rapid responsiveness and cost effectiveness for future reconfgurable manufacturing. Other than rebuilding new system-level policies for different sta-tionary structures, the developed reconfgurable maintenance time window (RMTW) focuses on the structure analysis to extract reconfgured parallel subsystems and series sub-systems in each manufacturing stage. Faced with different system structures, the RMTW policy utilizes reconfgura-tion characters and maintenance opportunities to constantly redefne reconfguring scheduling criteria within a uniform method. This manner is more suitable for rapidly adapting to new system structures in reconfgurable manufacturing sys-tems (RMS). 
The production scenarios in Fig. 4 can be taken as an example to illustrate the RMTW scheduling for system-level reconfgurations. After the original design, the RMS enters service at time t = tR1 = 0 with its initial system structure (5 machines). In the frst manufacturing stage MS1, the time width value of RMTW TW1 is defned as a criteria to separate PM actions in parallel subsystems and combine PM actions in series subsystems based on machine-level PM intervals. 
At the reconfguration time tR2, the structure is redesigned for the second manufacturing stage MS2. In the time dura-tion of this reconfguration TR2, M1 is replaced with a new M6, and M7 is added in parallel with M5. Then, the RMS continues production with a new structure, while a rede-fned time width of RMTW TW2 is applied for reconfgured parallel/series subsystems to minimize the total system main-tenance cost. 
Similarly, in the next reconfguration before MS3,M3 is removed, while M8 is added in parallel with M2 and M4. In contrasted to the traditional manner of rebuilding new system-level policies for different structures, RMTW scheduling focuses on reconfguring scheduling criteria TWh within a uniform method for rapidly adapting to new struc-tures.Abovestructureanalysisofeachmanufacturingstageis essential for RMTW scheduling. Then, the process fowchart of the proposed RMTW programming is shown in Fig. 5. 


Case study of PHM methodologies 
Effectiveness of WFRGM algorithm 
To prove the prognosis accuracy of the proposed WFRGM algorithm, the increasing health statuses of a monitored machine＊s deterioration during a maintenance interval are collected. The twelve status data points of failure frequency from monitoring points 1每12 are regarded as the in-sample test data, which refects the increasing failure risk. The 
123 



Fig. 5 Flowchart of RMTW policy for reconfgurable manufacturing CPS 
remaining six states from cycles 13每18 are used for out-of-sample forecasting. Results of the linear regression model (LRM) with x.(0)(d) = 0.0362d + 0.0211, the original GM(1,1) model 
 
.0.147 (0)(1) + 0.0951 0.147(d.1)
with x.(0)(d) = 1 . ex e ,
0.147 the actual-state rolling grey model (ARGM), the forecasted-
state rolling grey model (FRGM) and the proposed WFRGM algorithm have been presented in Table 2. The plot of actual versus forecasted machine states from above fve models is shown in Fig. 6. 
From the result comparisons in Table 2, it can found that the MAPE (5.19%) and MAE (0.0395) of WFRGM are all lower than LRM (MAPE = 21.26%; MAE = 0.1608), GM (MAPE = 17.74%; MAE = 0.1415), FRGM (MAPE = 15.73%; MAE = 0.1263) and ARGM (MAPE = 9.80%; MAE = 0.0729), indicating the highly accurate forecast-ing ability. Thus, WFRGM algorithm can provide real-time machine health information to dynamic PHM decision-making. 


Effectiveness of MAM method 
A 5-unit series-parallel system with the initial system structure in Fig. 4 is selected as an example for numeri-cal experiments using the proposed MTW policy. In this manufacturing system, PM intervals of each machine are dynamically scheduled by the MAM method according to individual machine degradation. The reliability of each machine is formulated by a Weibull failure probability function: 竹1 j (t) = (mj /灰 j )(t/灰 j )mj .1, which has been widely used to ft repairable equipment in electronic and mechanical engineering. Machine e parameters are shown in Table 3. 
From the results of industrial implementations (Xia et al. 2012),theproposedmachine-level MAM method reveals fol-lowing conclusions: (1) the PM interval decreases while PM cycle increases, since the underlying hazard rate evolution 
123 

1668  J Intell Manuf (2019) 30:1659每1672  
Table 2 Forecasting results of different methods  Monitoring point Actual status LRM 1 0.0837 0.0573 2 0.0864 0.0935 3 0.1705 0.1297 4 0.1732 0.1659 5 0.2110 0.2021 6 0.2122 0.2383 7 0.2388 0.2745 8 0.2448 0.3107 9 0.2856 0.3469 10 0.4144 0.3831 11 0.4219 0.4193 12 0.5318 0.4555 In-sample testing statuses (1每12) 13 0.6316 0.4917 14 0.6401 0.5279 15 0.6812 0.5641 16 0.7483 0.6003 17 0.8540 0.6365 18 0.9026 0.6727 Out-of-sample forecasting statuses (13每18) MAPE (%) 21.26 MAE 0.1608  GM 0.0837 0.1157 0.1340 0.1552 0.1798 0.2082 0.2412 0.2794 0.3236 0.3749 0.4342 0.5030 0.5827 0.6749 0.7818 0.9056 1.0490 1.2151 17.74 0.1415  FRGM ARGM WFRGM 0.0837 0.0837 0.0837 0.1157 0.1157 0.1157 0.1340 0.1340 0.1340 0.1552 0.1552 0.1552 0.1798 0.1798 0.1798 0.2082 0.2082 0.2082 0.2412 0.2412 0.2412 0.2794 0.2794 0.2794 0.3236 0.3236 0.3236 0.3749 0.3749 0.3749 0.4342 0.4342 0.4342 0.5030 0.5030 0.5030 0.5827 0.5827 0.6207 0.6586 0.6857 0.6743 0.7649 0.7762 0.7074 0.8849 0.8421 0.8285 1.0315 0.9130 0.8589 1.1953 0.9976 0.9830 15.73 9.80 5.19 0.1263 0.0729 0.0395  


Table 3 Machine parameters 
effects; (3) ignoring the effects of a maintenance activity will lead to less availability and extra cost, and MAM contributes to more practicality of PM intervals. 


Effectiveness of MTW policy 
To validate the MTW policy for complex fow-line systems, we program the system-level maintenance schedule with machine parameters in Table 3. Taken Tw = 800 h for the MTW programming as an example, the CPS mission lifetime is 25,000 h. Table 4 provides the system-level maintenance schedule results. 
The infuence of MTW-value and the effectiveness of MTW programming is shown in Fig. 7.Itisproventhat MAM policy can reduce ETC up to 27% comparing with Individual maintenancemode(IMM)of Tw = 0andSimulta-neous maintenance mode (SMM) of Tw = 25,000. Besides, 

j  m j  灰 j  Tpij  Tfij  Cpij  Cfij  cdij  aij  bij  
1  3.0  8000  140  600  5000  35,000  80  i/(15i+5)  (17i+1)/(16i+1)  
2  2.0  7000  120  200  6000  18,000  40  0.03  1.04  
3  1.5  12,000  200  350  2000  15,000  30  i/(20i+20)  1.03  
4  3.0  13,000  80  300  7500  22,000  45  0.025  (16i+3)/(15i+3)  
5  2.5  16,000  300  800  2500  25,000  75  i/(16i+14)  1.05  

123 

Table 4 System-level maintenance schedule based on MTW 
j Time point of PM activity (h) 

1  3319  6911  10,020  13,121  15,134  17,940  20,212  22,789  
2  3319  6911  10,020  15,134  19,105  22,789  
3  5108  10,020  15,134  20,212  
4  6911  14,455  21,984  
5  5108  10,020  15,134  20,212  

Fig. 7 ETC of the fow-line 
1,100,000
CPS with various MTW 
1,000,000 900,000 800,000 700,000 600,000 
0 
400 ETC($) 1007149 886153 


ETC ($) 
it can be concluded that larger MTW value enables more machines to take advantage of maintenance opportunities, but too large MTW causes extra maintenance and more ETC will be needed for CPS. 
Moreover, traditional opportunistic maintenance policies calculatethecost-savingsofallpossiblecombinationsateach cycle with the exponential decision-making complexity of O(2(J.1)). For example, Zhou et al. (2009) took a 3-unit system to illustrate the opportunistic PM scheduling algo-rithm, while the cost savings for 4 possible combinations were calculated at each opportunity. For our presented MTW policy, since the numbers of parallel/series subsystems and their respective machines are all smaller than J, the maxi-mal decision-making complexity at each opportunity is less than 2J2. Thus, the MTW complexity is just polynomial with total machine number J, which means even a complex 
Table 5 APB results in 

500 
600 
700 
800 
900 
1000 
1100 
1200 
1300 
25000 

881590 875455 
815673 
734736 
794904 
826257 
829011 
832444 
850728 
921532 
fow-line CPS with a large number of machines can be han-dled. 


Effectiveness of APB policy 
Facedwithsequentialbatchorders,APBdynamicallyutilizes set-up works and analyzes the cost savings to reduce the total system maintenance cost. PM intervals and various batch orders are pulled to make opportunistic maintenances cycle by cycle. For a 7-unit mass customization CPS, results of production-driven opportunistic maintenance are presented in Table 5. 
The results from mass customization CPS (Fig. 8) reveal that the mechanism of APB policy can ensure the low-est ETC. On the one hand, huge downtime cost saving ensures that ETC of APB policy is lower than those of 

APB (cost) B1 B2 B3 B4 B5 B6 B7 B8 B9 B10 
sequential batch cycles M1 .9204 .3269 .2725 4022 M2 2262 33 1865 1746 2790 M3 392 .6332 105 .902 M4 .110 4033 4555 M5 .78 5762 5934 3687 1758 .1640 M6 526 .2332 336 151 M7 6490 1951 2250 
123 


Fig. 8 Results comparison of opportunistic maintenance policies 
maintenance-driven opportunistic maintenance policies (e.g. IMM, SMM and MTW). On the other hand, APB dynami-cally compares cost savings and chooses PM adjustment with 

Max SCAju, SC Pju , which is thus a more cost-effective policy than Advanced maintenance mode (AMM) and Post-poned maintenance mode (PMM). Therefore, APB policy achievessignifcant costreduction byconsideringbatch char-acteristics and making PM adjustment based on maximum cost saving for each machine at each set-up time. 


Effectiveness of RMTW policy 
The RMTW policy is performed on a reconfgurable manu-facturing CPS with changeable system structures shown in Fig. 4. In the frst manufacturing stage (MS1), TW1 = 800 is applied for the RMTW programming as an example, while 
= 600 and TW3 = 1000 are taken for MS2 and MS3
TW2 separately. Table 6 shows the RMTW scheduling results for reconfgured system structures. At each system-level PM execution point tk, ( j, tk) = 0 means no PM action but this machine will be down according to the system structure; ( j, tk) = 1 indicates a PM action is combined to be per-formed; while ( j, tk) = 2 evinces no PM and this machine 
Table 6 RMTW results for reconfgurable manufacturing CPS Fig. 9 ETC-saving rate comparison with various methods 

continues working. Newly added or removed machines are considered in each manufacturing stage. 
From the results of reconfgurable manufacturing CPS, we can fnd that different CPS structures with various machine reliabilities and changeable system-level reconfgurations would lead to different ETC-saving rates. However, RMTW policy is exactly designed to redefne the time width of TWh for minimizing the ETC in each manufacturing stage. Therefore, this optimization mechanism ensures that RMTW policy can not only be rapidly adapt to new diverse sys-tem structures, but also achieve cost effectiveness for the whole-CPS maintenance scheduling. In Fig. 9, results indi-cate that theETC-saving rate (28.105% comparing to IMM) achieved by RMTW scheduling is much higher than tradi-tional opportunistic maintenance policies (IMM, SMM and static MTW). It can be concluded that proposed RMTW policy is a viable and effective policy to achieve rapid responsiveness and cost reduction for future reconfgurable manufacturing. 


Conclusions and perspectives 
In this paper, we have presented systematic PHM method-ologies for cyber-physical systems of three advanced man-

( j, tk)  MS1  MS2  MS3  
tk  4587  6042  9033  10251  12563  16012  18437  19147  19978  21955  24188  27538  
M1  1  2  1  0  每  每  每  每  每  每  每  每  
M2  1  0  0  1  0  1  2  0  0  1  0  1  
M3  0  1  0  0  1  0  2  1  0  每  每  每  
M4  0  2  1  0  2  0  1  2  0  2  0  1  
M5  1  2  0  1  2  1  2  2  0  1  0  1  
M6  每  每  每  每  2  1  2  2  1  2  1  1  
M7  每  每  每  每  2  0  2  2  1  2  0  1  
M8  每  每  每  每  每  每  每  每  每  2  0  1  

123 

ufacturing paradigms. With monitored and synchronized information from the cyber computational space, PHM methodologies integrating manufacturing characters in the physical factory foor can improve the health management. These developed prognosis algorithm, scheduling model and opportunistic maintenance policies achieve signifcant improvements in following aspects: (1) WFRGM algorithm provides real-time and accurate health predictions by incor-porating updated information and infuencing factors; (2) MAM method can output sequential PM intervals based on individual machine health for supporting the system-level opportunistic maintenance; (3) MTW policy schedules PM separations/combinations according to series-parallel struc-tures for reducing maintenance cost and decision-making complexity; (4) APB policy achieves huge cost savings by utilizing set-up times to makes real-time PM optimizations and handle variable batch orders; (5) RMTW policy eff-ciently achieve rapid responsiveness and cost effectiveness for diverse open-ended reconfgurations and fexible system structures. 
In sum, both cyber factors (information technologies) and physical factors (manufacturing paradigms) are essential for the health management of future CPS. Some industrial enter-prises (i.e., port machinery manufacturers and automobile manufacturing companies) have already benefted from these novel PHM methodologies. Future work is needed to extend-ing this hierarchical PHM framework to other burgeoning manufacturing paradigms, such as sustainable manufactur-ing, green production and cloud manufacturing. 
Acknowledgements The research is funded partially by National Natural Science Foundation of China (51505288, 51535007), the Pro-gramme of Introducing Talents of Discipline to Universities (B06012) and Foundation for Innovative Research Groups of the National Natural Science Foundation of China (51421092). 


References 
Akay, D., & Atak, M. (2007). Grey prediction with rolling mechanism for electricity demand forecasting of Turkey. Energy, 32(9), 1670每 1675. 
Al-Zaher, A., & ElMaraghy, W. (2013). Design of reconfgurable auto-motive framing system. Journal of Manufacturing Systems, 32(3), 436每448. 
Arab, A., Ismail, N., & Lee, L. S. (2013). Maintenance scheduling incorporating dynamics of production system and real-time infor-mation from workstations. Journal of Intelligent Manufacturing, 24(4), 695每705. 
Benkedjouh, T., Medjaher, K., Zerhouni, N., & Rechak, S. (2015). Health assessment and life prediction of cutting tools based on support vector regression. Journal of Intelligent Manufacturing, 26(2), 213每223. 
Chang, Q., Ni, J., Bandyopadhyay, P., Biller, S., & Xiao, G. (2007). Maintenance opportunity planning system. Journal of Manufac-turing Science and Engineering, 129(3), 661每668. 
Chouikhi, H., Khatab, A., & Rezg, N. (2014). A condition-based maintenance policy for a production system under excessive environmental degradation. Journal of Intelligent Manufacturing, 25(4), 727每737. 
Dekker, R., Wildeman, R. E., & van der Duyn Schouten, F. A. (1997). A review of multi-component maintenance models with economic dependence. Mathematical Methods of Operational Research, 45(3), 411每435. 
Deng, J. L. (1982). Control problems of grey system. Systems and Con-trol Letters, 1(5), 288每294. 
Derigent, W., Thomas, E., Levrat, E., & lung, B. (2009). Opportunistic maintenance based on fuzzy modelling of component proximity. CIRP Annals-Manufacturing Technology, 58(1), 29每32. 
Ebrahimipour, V., Najjarbashi, A., & Sheikhalishahi, M. (2015). Multi-objective modeling for preventive maintenance scheduling in a multiple production line. Journal of Intelligent Manufacturing, 26(1), 111每122. 
Gao, R., Wang, L., Teti, R., Dornfeld, D., Kumara, S., Mori, M., et al. (2015).Cloud-enabledprognosisformanufacturing. CIRPAnnals-Manufacturing Technology, 64(2), 749每772. 
Gu, X., Jin, X., & Ni, J. (2015). Prediction of passive mainte-nance opportunity windows on bottleneck machines in complex manufacturing systems. Journal of Manufacturing Science and Engineering, 137(3), 31017每31024. 
Jardim-Goncalves, R., Grilo, A., & Popplewell, K. (2016). Novel strate-gies for global manufacturing systems interoperability. Journal of Intelligent Manufacturing, 27(1), 1每9. 
Koren,Y., &Shpitalni, M.(2010).Designofreconfgurablemanufactur-ing systems. Journal of Manufacturing Systems, 29(4), 130每141. 
Lee, J., Bagheri, B., & Kao, H. A. (2015). A cyber-physical systems architecture for industry 4.0-based manufacturing systems. Man-ufacturing Letters, 3, 18每23. 
Lee, J., Kao, H. A., & Yang, S. (2014a). Service innovation and smart analytics for Industry 4.0 and big data environment. Procedia CIRP, 16, 3每8. 
Lee, J., Wu, F., Zhao, W., Ghaffari, M., Liao, L., & Siegel, D. (2014b). Prognostics and health management design for rotary machin-ery systems〞reviews, methodology and applications. Mechanical Systems and Signal Processing, 42(1每2), 314每334. 
Lee, S., Gu, X., & Ni, J. (2013). Stochastic maintenance opportunity windows for unreliable two-machine one-buffer system. Expert Systems with Applications, 40(13), 5385每5394. 
Lin, Q. L., Liu, H. C., Wang, D. J., & Liu, L. (2015). Integrating sys-tematic layout planning with fuzzy constraint theory to design and optimize the facility layout for operating theatre in hospitals. Jour-nal of Intelligent Manufacturing, 26(1), 87每95. 
Liu, X., Li, J., Al-Khalifa, K. N., Hamouda, A. S., Coit, D. W., & Elsayed, E. A. (2013). Condition-based maintenance for continu-ously monitored degrading systems with multiple failure modes. IIE Transactions, 45(4), 422每435. 
Mirabi, M., Fatemi Ghomi, S. M. T., & Jolai, F. (2013). A two-stage hybrid fowshop scheduling problem in machine breakdown con-dition. Journal of Intelligent Manufacturing, 24(1), 193每199. 
Monostori, L., K芍d芍r, B., Bauernhansl, T., Kondoh, S., Kumara, S., Reinhart, G., et al. (2016). Cyber-physical systems in manufactur-ing. CIRP Annals-Manufacturing Technology, 65(2), 621每641. 
Ni, J., & Jin, X. (2012). Decision support systems for effective mainte-nance operations. CIRP Annals-Manufacturing Technology, 61(1), 411每414. 
Pasqualetti, F., Dfer, F., & Bullo, F. (2013). Attack detection and identifcation in cyber-physical systems. IEEE Transactions on Automatic Control, 58(11), 2715每2729. 
Rafee, K., Feng, Q., & Coit, D. W. (2014). Reliability modeling for dependent competing failure processes with changing degradation rate. IIE Transactions, 46(5), 483每496. 
123 

Sheikhalishahi, M., Ebrahimipour, V., & Hosseinabadi Farahani, M. (2014). An integrated GA-DEA algorithm for determining the most effective maintenance policy for a k-out-of-n problem. Jour-nal of Intelligent Manufacturing, 25(6), 1455每1462. 
Tian, Z. G. (2012). An artifcial neural network method for remaining usefullifepredictionofequipmentsubjecttoconditionmonitoring. Journal of Intelligent Manufacturing, 23(2), 227每237. 
Wang, C. H., & Hsu, L. C. (2008). Using genetic algorithms grey theory to forecast high technology industrialoutput. Applied Mathematics and Computation, 195(1), 256每263. 
Wang, C. H., & Tsai, S. W. (2014). Optimizing bi-objective imper-fect preventive maintenance model for series-parallel system using established hybrid genetic algorithm. Journal of Intelligent Man-ufacturing, 25(3), 603每616. 
Wang, L., Tngren, M., & Onori, M. (2015). Current status and advancement of cyber-physical systems in manufacturing. Journal of Intelligent Manufacturing. doi:10.1016/j.jmsy.2015.04.008. 
Xia, T., Jin, X., Xi, L., & Ni, J. (2015a). Production-driven opportunistic maintenance for batch production based on MAM每APB schedul-ing. European Journal of Operational Research, 240(3), 781每790. 
Xia, T., Jin, X., Xi, L., Zhang, Y., & Ni, J. (2015b). Operating load based real-time rolling grey forecasting for machine health prognosis in dynamic maintenance schedule. Journal of Intelligent Manufac-turing, 26(2), 269每280. 
Xia, T., Xi, L., Pan, E., & Ni, J. (2016). Reconfguration-oriented opportunistic maintenance policy for reconfgurable manufactur-ing systems. Reliability Engineering & System Safety,. doi:10. 1016/j.ress.2016.09.001. 
Xia, T., Xi, L., Zhou, X., & Lee, J. (2012). Dynamic maintenance decision-making for series-parallel manufacturing system based on MAM每MTW methodology. European Journal of Operational Research, 221(1), 231每240. 
Xia, T., Xi, L., Zhou, X., & Lee, J. (2013).Condition-based maintenance for intelligent monitored series system with independent machine failure modes. International Journal of Production Research, 51(15), 4585每4596. 
Yu, J., & Xi, L. (2008). Intelligent monitoring and diagnosis of manu-facturing process using an integrated approach of neural network ensemble and genetic algorithm. International Journal of Com-puter Applications in Technology, 33(2每3), 109每119. 
Zhou, X., Xi, L., & Lee, J. (2009). Opportunistic preventive mainte-nance scheduling for a multi-unit series system based on dynamic programming. International Journal of Production Economics, 118(2), 361每366. 
Zied, H., Sofene, D., & Nidhal, R. (2014). Joint optimisation of main-tenance and production policies with subcontracting and product returns. Journal of Intelligent Manufacturing, 25(3), 589每602. 
123 



Introduction 

Global recession over the last years changed the overview on the industrial sector, now looking at the real value-added that it creates. Companies that followed the trend to relocate activities by looking for low cost labor, are now committed to recover their competitiveness. 
German manufacturing strategy played a key role on this shift-ing, launching initiatives to maintaining and promoting its impor-tance as a ＆＆forerunner§ in the industrial sector [1]. The buzz word ＆＆Industry 4.0§ has been presented and with it big promises arose to face the latest challenges in manufacturing systems. The impeller Industry 4.0 (I4.0) is enabling and reinforcing this trend using its technologies, changing the way of living, creating new business models and new ways of manufacturing, renewing the industry for the so-called digital transformation. 
In 2011, the German government have brought into the world a new heading called Industrie 4.0 (I4.0), assumed as the fourth indus-trial revolution [2每6]. I4.0 aim is to work with a higher level of automatization achieving a higher level of operational productivity and ef.ciency [3,7],connecting the physical to the virtual world[8每 9].It will bring computerization and inter-connection into the tra-ditional industry [3]. According to several authors [3,5每6], I4.0 can be assumed as Cyber-Physical Systems (CPS) production, based on heterogeneous data and knowledge integration and it can be summed up as an interoperable manufacturing process, integrated, adapted, optimized, service-oriented which is correlated with algo-rithms, Big Data (BD) and high technologies such as the Internet of Things (IoT) and Services (IoS), Industrial Automation, Cybersecu-rity (CS), Cloud Computing (CC) or Intelligent Robotics [3,7,9]. From the production approach, Martin and Sch.ffer [8] de.ne I4.0 as the intelligent.owofthe workpieces machine-by-machineinafactory, ona real-time communication between machines.On this environ-ment, I4.0 will make manufacturing become smart and adaptive using .exible and collaborative systems to solve problems and make the best decisions [7].It bringsa good development for the industrial scenario focusing on creating smart products, smart pro-cesses and smart procedures [5]. Companies expected to increase the level of digitalization, working together in digital ecosystems with customers and suppliers [10]. 
Since I4.0 boom, the research community has experienced differ-entapproachestoI4.0 concept;however,thegeneralsocietymaybe confused basedonthelackof understandingonthis area. Thereisa needfor clari.cationofI4.0 related conceptsand technologies. 
This paper deals with the research of I4.0 in manufacturing environments on a literature review over the enabling technolo-gies, focusing on the state-of-the-art and future trends. The approach of I4.0 for manufacturing systems in this paper is based on the Smart Factory (SF) concept. The SF concept makes use of components such as IoT, IoS, the systems integration and Cyber-Physical Production System (CPPS) that is formed by several linked CPS (CPS may use up until nine key enabling technologies, widely assumed by research community). 
The paper is structured as follows: section 2 presents the Refence Architecture Model Industrie 4.0 (RAMI4.0) as the guid-ance for the I4.0 technologies implementation, section3 presents key enabling technologies of I4.0, section4reviews the Smart Fac-tory (SF) concept of the I4.0 structured with its components, and the .nal remarks are in section5 which introduces the summary and gives future outlooks. 
2. 
Reference Model of I4.0 
Several German associations and institutions cooperated on the creation of the reference model for I4.0. This 3D model in Fig.1 is the development of a shared language and a structured framework [11每12] that describes the fundamental bases of I4.0. It is intended to assist on the I4.0 technologies implementation [13]. 
Unlabelled image
The Reference Architecture Model Industrie 4.0 (RAMI4.0) should enable to identify the existing standards and among it, identify and close the gaps, loopholes and identify the overlaps [14]. 
On the left horizontal axis from the IEC 62,890 standard, facili-ties and product lifecycle with the correspondent value stream are showed [15]. RAMI4.0 clearly describes the difference between instance and type. When the design and prototyping is completed, the type becomes an instance, ready for production [14]. 
The hierarchy levels from the IEC 62264 standard are showed in the right axis, representing the different grouped entities by func-tional properties, de.ned to represent all hierarchical levels of the enterprise, from the ＆＆Product§ (e.g., a workpiece) to the ＆＆Connect World§ level. The ＆＆Connect World§ is the last stage of the I4.0 development enterprise environment using IoT and IoS to connect enterprises, customers and suppliers [13每14]. The hierarchy levels are discussed further insidetheSFin section4throughthe Fig.25. 
The layers on the vertical axis represent a reminder to integrate all aspects on the enterprise digitalization [11]. The functionallay-ers of the organized vertical axis describe: 
. 
＆＆Asset Layer§ represents reality, for instance, physical compo-nents including linear axes, robots, conveyor belts, PLC＊s, metal parts, documents, archives also persons that form a part of con-nection to the virtual world via the ＆＆Integration Layer§ [12,14每 15]. Also, non-physical objects such as software or ideas; 

. 
＆＆Integration Layer§ provides processed information for the dig-itization of the assets. Elements connect to Information Tech-nologies (IT) such as sensors, Radio Frequency IDenti.cation (RFID) readers, integration of Human-Machine Interface (HMI) and computer-aided controls the technical processes [12,14]. Persons via HMI also participate on this layer. In the virtual domain, each signi.cant event is mirrored through the enabler [12]; 

. 
＆＆Communication Layer§ with the function of communication standardization. It makes use of uniform data format and prede-.ned protocols, providing services for the ＆＆Integration Layer§ [12,14每15]; 

. 
＆＆Information Layer§ to process and integrate consistently the different available data into useful information [14]. Also receives and transforms events to match the data which are available for the next layer [15]; 

. 
＆＆Functional Layer§ to enable formal descriptions of functions. It creates an horizontal integration platform of several functions that can be with remote access, resulting of the necessity of data 

. ＆＆Business Layer§ enables mapping of the business model and links between different business models. It ensures, within the value stream, the integrity of the functions [14每15]. 
It＊s possible to map all crucial aspects of I4.0, allowing the clas-si.cation accordingto the model,of objects such as machines. This model allows the step-by-step migration from the actual to the future manufacturing environments [13]. 
The I4.0 essential technological elements are compiled at the .rst time as RAMI4.0 and it is registered in Germany in the DIN SPEC 91345 standard [14]. 
3. The Key Technologies of I4.0 
I4.0 is characterized on manufacturing and services by highly developed automation and digitalization processes, electronics and IT [3]. From the production and service management perspec-tive, I4.0 focus on establish intelligent and communicative systems such as Machine-to-Machine and Human-Machine Interaction, dealing with the data .ow from intelligent and distributed system interaction [16]. Among other features, I4.0 promotes autonomous interoperability, agility, .exibility, decision-making, ef.ciency or cost reductions [17]. 
The I4.0 implementation should be interdisciplinary in a closely between different key areas. Several authors [5,18每19] described nine pillars (also called the building blocks) of the I4.0 framework as follows in the subsections.A fundamental key point to achieve the integration of I4.0 framework is the human contribution that will be improved with the development of professional skills of the stakeholders. 
3.1. The Industrial Internet of Things 
On the IT, the IoT is the connection oftwo words i.e. ＆＆internet§ and ＆＆things§. ＆＆Internet§ as the network of the networks. A global system serving users worldwide with interconnected computer networks using Standard Internet Protocol suit (TCP/IP). As individ-ually distinguishable by the real world, the ＆＆things§ can be any-thing like an object or a person [20]. Today, IoT is widely used for instance, in transportation, healthcare or utilities [21]. Thing-to-Thing, Thing-to-Human and Human-to-Human form a network inside IoT, connected to the internet. Individually identi.able objects exchange information inside this network. [22每23]. 
IoT has been increase with the advancement of mobile devices. IoT can be achieved with connected RFID, Wireless Sensor Net-works (WSN), middleware, CC, IoT application software and Soft-ware De.ned Networking (SDN) as the key enabling technologies [23]. Fig.2 presents the associated technologiesin IoT. 
One simple de.nition of IoT described by Sezer et al. [21] is: ＆＆IoT allows people and things to be connected anytime, anyplace, with 
Unlabelled image
anything and anyone, ideally using any path/network and any ser-vice§. In other words, Bortolini et al. [24] de.ned IoT as an ubiqui-tous presence for a common purpose of various things or objects interacting and cooperating each other, digitalizing all physical systems. For different aims, the digitalized information can be used to adjust production patterns, with the use of a virtual copy of the physical world and using sensor data [7]. The entire production systems such as machinery and related resources can be the ＆＆things§ managed and virtualized by I4.0 [4,7]. In addition, the IoT nature as to be decentralized and heterogeneous [25]. 
Regarding to IoT design architecture, Trappey et al. [26] estab-lished a logical framework by layers to classify IoT technology and used to characterize and identify CPS. According to several authors [25,27每28], IoT architecture most common layering in a typical network, includes four main layers as represented in the Fig.3 as follows: 
1) ＆＆Sensing Layer§ to sense the ＆＆things§ status with a unique identity and to integrate, e.g., actuators, sensors, RFID tags as several types of ＆＆things§; 
2) ＆＆Network Layer§ to support the transferred information through wired or wireless network from the ＆＆Sensing Layer§ to ＆＆Service Layer§, being the support＊s infrastructure. This layer determines and maps ＆＆things§ automatically in the network enabling to connect all ＆＆things§ for sharing and exchange data; 
3) ＆＆Service Layer§ makes use of a middleware technology sup-porting services and applications, required by the users or applications. The interoperability among the heterogeneous devices is ensured by this layer, performing useful services, e.g., information search engines and communication, data storage, exchanging and management of data as well as the ontology database; 
4) ＆＆Interface Layer§ to make the interconnection and manage-ment of the ＆＆things§ easier and to display information allow-ing a clear and comprehensible interaction of the user with the system. 
Differing from IoT based users, regarding to industrialenviron-ments needing real-time data availability and high reliability [29], the Industrial InternetofThings(IIoT)isthe connectionof industrial products such as components and/or machines to the internet. For instance, linkingthe collected sensingdatainafactorywithIoTplat-form, IIoT increases production ef.ciency with the BD analysis [22]. 
AtypicalIIoTis showedinFig.4,withwireand wireless connec-tions, increasing value with additional monitoring, analysis and optimization. 
As a natural evolution of IoT, the IoS can be seen as the connec-tivity and interaction of the things creating valuable services and is one of the fundamental basis of the SF. IoS is discussed further in section 4. 

3.2. Cloud Computing 
Cloud Computing (CC) is an alternative technology for compa-nies who intent to invest in IT outsourcing resources [30]. Assante et al. [31] characterized CC for Small and Medium Enterprises (SMEs) as a resource pooling with rapid elasticity and measured service, on-demand self-service and broad network access. The adoption of CC has several advantages related to cost reduction, e.g., the direct and indirect costs on the removal of IT infrastructure in the organization, the resource rationalization service by the dynamically scalable users consuming only the computing resources they actually use or portability when using any type of device connected to the internet such as mobile phones or tablets accessing from any world location [30].Bythis, the cloud can have 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 3. Generic Service-oriented Architecture (SoA) for IoT [25]. 
Unlabelled image
Unlabelled image
Fig. 4. Typical IIoT network [137]. 
any of the four types of access: public (usually on a data center location, managed by vendors and available for all public [32]), pri-vate (same organization location and offering special bene.ts [32]), hybrid (combination of public and private clouds [32])and com-munity (shared by multi organizations and supported by a speci.c sharing of interests and concerns community [33]). Everything is treated as a service in CC. These services de.ne a layered system or types of service models structured for CC as in Fig. 5 and the management overviewis shownin Fig.6,as follows [31,33每34]: 
. Infrastructure as a Service (IaaS) is where cloud service provi-ders supply users with fundamentalcomputingresources, with virtual infrastructures, e.g., virtual servers, networks or storage and where users into the cloud can deploy and run arbitrary software, which can include, for instance, operating systems applications; 
. 
Platform as a Service (PaaS) is where users develop and run applications using programming languages on the cloud infras-tructures. Therefore, it can be achieved scalability, high speed server and storage. Users can build, run and deploy their own applications with the use of remote IT platforms. On this layer, there is no concern on the resource＊s availability and mainte-nance [35]; 

. 
Software as a Service (SaaS) is where applications reside and runs in a cloud infrastructure [34]. Accessible from various cli-ent devices through an interface such as a web browser and programs. The focus is to eliminate the service applications on local devices of individual user, achieving an high ef.ciency and performance for the users. This category enables software applications such as Computer-Aided-Design (CAD) software and Enterprise Resource Planning (ERP) software, with a lower total cost of ownership [35]. 



V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

All underlying Everything as a Service (XaaS) layers allows direct interactions with the user interface layer at the top. 
On manufacturing environments, Cloud Manufacturing (CMfg) concept was proposed to make use of CC technology, in order to improve the current manufacturing systems [36]. Cloud-related manufacturing has two approaches: 
1) CC in manufacturing industry as a manufacturing version of CC -using cloud applications in manufacturing industry directly, web-based manufacturing applications or computer-aided are examples of possible deployments in the CC system. These applications are implemented at two service levels of CC system, matching SaaS and PaaS levels [37]; 
2) CMfg systems as an entire new type of cloud service, based on Service-orientedArchitecture (SoA) in the cloud environ-ment that provides manufacturing capabilities [36].It re.ects the IaaS level on CC system [37]. 
With the combination of advanced technologies, it arises a new computing and service-oriented manufacturing mode as CMfg [38]. A solution such as CMfg enables users to request services fromall stagesofa product lifecycle rangingfrom design, manufac-turing, management and so on [38每39].Bythis meaning, the main characteristics of CMfg is the service-oriented approach [40] and its trend on shifting manufacturing approach from production-oriented to service-oriented [33,41].Abrief CMfg model is shown in Fig.7, consisting on three categories of stakeholders: providers, operators and consumers, with their cooperation to maintain sus-tainable operation of a CMfg system [42每43]: 
. 
Providers 每 own and provide the abilities and the manufactur-ing resources [43].Within the entire product lifecycle, for shar-ing purposes, providers publish manufacturing resources to the CMfg platform and also receive manufacturing tasks from the cloud platform. Everything is transformed into services, under the exclusive management ofthe operator [42]; 

. 
Operator/s -to operate CMfg platform and to deliver services to providers, consumers and even third parties [43]. In an on-demand manner, consumers from the cloud platform can achieve high-quality and sustainable manufacturing services. Providers have permission to publish their resources and capa-bilities with the use of tools provided by the cloud platform [42]; 

. 
Consumers -to subscribe the manufacturing computing ser-vices availability in a CMfg service platform [43]. Under the exclusive management of the operator, consumers, including enterprises consumers and individual consumers, submit their 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
Unlabelled image
Fig. 7. CMfg model [42]. 
requirement tasks to the CMfg platform, e.g., design, manufac-turing, test or simulation tasks and also receive the execution results of their orders [42]. 
CMfg is a manufacturing paradigm based on Knowledge. In the running process, the knowledge plays as the central role [44], e.g., models, standards, protocols, rules and algorithms as knowledge, indispensable in many process and activities within entire lifecycle services as service generation, service management and service applications [42]. 
The concept of CMfg makes use of CC, BD, IoT, CPS, the net-worked manufacturing, service-oriented manufacturing, virtual manufacturing and virtual enterprise [45每46]. Cooperation can be enabled and supported by CMfg, sharing and management of manufacturing resources such as fabrication capabilities, equip-ment, applications, software tools, know-how, etc., of companies [47每48] and these companies can be included into the cloud, becoming accessible to potential consumers, in a pay-as-you-go manner [39]. CMfg enables the recommendation and execution, intelligent mapping and search of a service [33]. CMfg can provide in a form of service scalable, .exible and cost-effective solutions with lower maintenance costs and supports. Manufacturing tasks can be obtained also as services into the CMfg service platform [41]. Cloud data center owns the computational resources and the different organizations, e.g., manufacturing enterprises, owns the manufacturing resources [45]. There is no need for manufac-turers and users to invest in high-tech computers, computer licenses or worrying about software updates or upgrades [48]. Mai et al. [46] in Fig. 8 discussed a CMfg platform integrating resources and services related to 3D printing, including, e.g., design, 3D printers, assembly, simulation, models, software, etc. It is important to consider model library management and the online-device integration on the construction of the 3D printing service CMfg platform, due to the close relation between 3D print-ing and 3D models. 
Usually with a short budget for the initial investment, lack of experience and related technical support, SMEs are seeking novel technologies such as cloud technologies. According to Wang et al. [49], SMEs needs high level of safety and security regarding their customer＊s requirements, i.e., all data and results as to be maintain within the boundaries of the own company. These facts indicate that public or community cloud services probably are not suitable in this scenario. To ful.ll this need, Wang et al. [49] proposed a CMfg system tailored to meet the requirements of SMEs, consider-ing a hybrid cloud structure. Within this, the sensitive data stays inside a private cloud, with integrated and managed hardware and software. Moreover, the data interoperability presence of the public and private clouds is identi.ed on the multiple levels in the CMfg. 
3.3. Big Data 
Huge amount of generated data from different types, can come from interconnected heterogeneous objects [24]. This huge amount of structured, semi-structured and unstructured data can describe Big Data (BD). In order to obtain the correspondent value, these data would need too much time and money to be store and to be analyzed [50]. Bringing value opportunities to industries in the era of Internet of Everything can be achieving with the connection of more physical devices to the internet and with the use of a gen-eration of novel technologies. 
Data collection or storage characterize BD, but the core charac-teristic of BD is the data analysis and without it, BD has no much value [51]. Systematic guidance can be provided by BD for related production activities within entire product lifecycle [52], achieving cost-ef.cient running of the process and fault-free [53], and help managers on decision-making and/or to solve problems related to operation [52]. The use of BD provides a business advantage through the opportunity of generated of value-added [54]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 8. Various services in CMfg [46]. 
Cemernek et al. [55] presentedBDde.nition of the TechAmer-ica Foundation, as ＆＆a ＆＆term§ describing large volumes of high velocity, complex and variable data requiring advanced techniques and tech-niques to enable the capture, storage, distribution, management and analysis of the information§. BD demands a cost-effective, innova-tive forms of information processing for enhanced insights. Accord-ing to the researched de.nitions of BD, differing from the traditional data processing [21], the .rst suggestion to characterize BD was related in terms of Volume, Variety, and Velocity, also named as the Three V＊s. These was the three dimensions that emerged as a common framework of challenges in data manage-ment [56].To 
process continuously large amounts of unstructured heteroge-neous data collected in formats such as video, audio, text, or others [51], additionally, other dimensions have also been attempted to assign for a better characterization such as: Veracity, Vision, Volatility, Veri.cation, Validation, Variability and Value [56]. According to several authors [21,51,56每57], the description of the dimensions as follows: 
. 
Volume 每 great data volume size consuming large storage or consist of enormous number of collections. BD sizes are men-tioned in multiple terabytes and petabytes; 

. 
Variety 每 various types of data, generated from a large sources and formats variety, and multi-dimensional data .elds contents. It refers to the structural heterogeneity in a dataset; 

. 
Velocity 每 rapid production. Generation, analysis, delivery, and data creation measured by its frequency. It refers to the data generation rate and the speed for analyzing and acting upon; 

. 
Veracity 每 represents the unreliability in some data sources. Some data requires BD analysis to gain reliable prediction; 

. 
Vision 每 only a purposeful process should send data generation. The likelihood of data generation process is addressed in this dimension; 


. 
Volatility 每 a limited useful life can characterize data generated. The data lifecycle concept is addressed by this dimension. It ensures the replenishment ofthe outdated data with new data; 

. 
Veri.cation 每 conformity of the data generated by a speci.ca-tion set. It ensures the conformity of the engineering measurements; 

. 
Validation 每 the vision conformity of the data generated. Behind the process, the transparency of assumptions and connections are ensured; 

. 
Variability 每data.ow rates measuredbyits variation. Variability and Complexity was added as two additional dimensions of BD; 

. 
Value 每 through extraction and transformation, de.nes how far BD generates economicallyworthy insights and bene.ts. Value as a de.ning BD attribute. 


On manufacturing domain and at the BD process comprehen-sion, it is the engineering aspects that give value to the BD analysis using its dimensions [51]. These dimensions are dependent from each other, related with the relativity of BD volumes applied to all dimensions [56]. 
To explore data, advanced data analysis is required. Using CC through the advanced analytics, methods and tools, off-line and real-time data are analyzed and mined, e.g., machine learning, forecasting models, among others. Knowledge is extracted from the huge data number enabling manufacturers on understanding the product lifecycle various stages [50]. Moreover, the advanced analytics of BD can be used as a facilitator, identifying and over-coming bottlenecks created by IoT generated data [58]. 
The mutation opportunity from today＊s manufacturing para-digm to smart manufacturing is offered by BD [59]. Therefore, BD can help manufacturers on more rational, informed and responsive decision-making way. Manufacturing competitiveness in the global market is enhanced by these BD characteristics. Various stages in data lifecycle where manufacturing data is exploited are depicted in Fig.9 consisting on the complete manufacturing data journey. 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 9. Manufacturing data lifecycle [59]. 
According to Mourtzis et al. [58],ina framework structuredby levelsofa manufacturing enterprise,the lower level generatesdata directly from machine tools and operators. For an enterprise, this data is very important, providing precious information when used and analyzed enabling adaptivity and .exibility on the higher levels of the enterprise. 
BD analytics is an essential key to digital manufacturing, play-ing as an enabler for technologies. Moreover, the scope of mass customization focusing on the needs of individualized markets, use BD analytics as foundation [58]. 
As mentioned above, IoT data converges to BD in order to ana-lyze it and take conclusions from collected datasets. In other words, IoT data will be a part of BD [21] and BD cannot be explored further without the IoT [57]. Furthermore, CC and BD are consid-ered as a coin with its two faces: BD is seen as the absorbent appli-cation of CC, while CC provides the IT infrastructure of BD [57]. 
3.4. 
Simulation 
For the successfulimplementation of the digital manufacturing [60], an indispensable and powerful tool, the computer simulation, is becoming a technology to better understand the dynamics of business systems [61]. Manufacturing industry current challenges can be approached by this technology [62], dealing with the com-plexity of the systems, with elements of uncertain problems that cannotbe resolvedwith usual mathematical models [63].Ona cus-tomized product manufacturing environment, the value of simula-tion is remarkable and evident. Simulation allows experiments for the validation of products, processes or systems design and con.g-uration [60]. Simulation modeling helps on cost reduction, decrease development cycles and increase product quality [61]. In order to analyze their operations and support decision-making, manufacturers have been using modeling and simulation [64]. Simulation technologies already proved its effectiveness in the approach of several practical real-world problems in manufac-turing sector [65]. Mourtzis et al. [60] presented on their research, the domain areas of simulation as shown in Fig. 10 with the focus on simulation methods and tools. Simulation is de.ned as an oper-ation imitation, over time, of a system or a real-world process. It uses a system＊s arti.cial history and its observation, drawing infer-ences over the operational features of the representation of the real system. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Simulation modeling is the method that makes use of a real models or imagined system models or imagined process models. It helps ona better estimating and understanding the modeled sys-tems or process through its behavioural analysis [61]. A model is an entity (generally a simpli.ed abstraction) used to represent other entity with a particular de.ned purpose [66]. Simulation modeling allows to gain insights into complex systems by the development ofcomplex and versatile products and make possible to test new concepts or systems, resource policies and new operat-ing before its real implementation, allowing to gather information and knowledge with no interference on the actual running system [60]. The Fig. 11 shows types of simulation models discussed by Mourtzis et al. [60] regarding to the classi.cation, dimensions, and differences. 
Choose and develop the best suitable type of simulation model to represent the real system is a multiparameter decision, e.g., sta-tic models for modelling a structure without activity and dynamic models for investigating the behaviour of a system evolving through time [67]. 
Simulation have been playing a spotlight role in design evalua-tion (referred to as off-line) and operational process performance (referred to as on-line) during a manufacturing system [65,67]. 
Its usual the existence of making long-term decisions on the design process [67] in, e.g., facility layouts, system capacity con.g-urations, material handling systems, .exible manufacturing sys-tems and cellular manufacturing systems [65]. Simulation runtimein off-lineisnot signi.cantonthe simulation process, offer-ing the advantages to study and analyze the what-if scenarios [67]. 
On the operational process of the manufacturing system, e.g., manufacturing operations planning and scheduling, real-time con-trol, operation policies and maintenance operations [65], the decision-making is short-term, making the simulation runtime a very important aspect. On-line simulation relates the number of entities belonging to the production system, the number of its gen-erated events, the activities complexity and simulation time hori-zon. If the IT system is integrated with the on-line simulation, for instance, it＊s possible to own the capacity to estimate the future shop .oor behaviour and to emulate and/or determinate the man-ufacturing system logic control [67]. 
Optimal or near-optimal system design is the goal for decision makers. This optimization is possible due a systematically search on a wide decision space without restrictions or pre-speci.ed requirements. This simulation optimization tool will search for the optimal design within a given system, according to the com-


V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
puter simulation model. On dynamic and uncertain environments, this tool has the potential on optimizing control decisions and on supporting real-time decision-making. This can be possible when the required computational ef.ciency is reached [68]. Compared to conventional simulation, real-time simulation, on-line, can ana-lyze the behaviour of user and system in milliseconds, allowing the user to develop and produce ＆＆virtually§a prototype for the product or service [69]. According to Cede.o et al. [69], a real-time simula-tion is when a computer runs at the same rate as the physical sys-tem, so the simulation model needs to be feed with real-time data that can be reached using IoT. 
Ahigh-.delity simulation of a manufacturing factory is de.ned as Virtual Factory (VF). An industrial collaboration environment focusing on Virtual Reality (VR) representation of a factory [70] or an emulationfacility [71] canbe consideredaVF.TheVF vision con-siders validated real factories simulation models to generate data andtobe workedin formatsofreal conditionsinareal factory [64]. 
The new simulation modeling paradigm is based on the concept of Digital Twin (DT) [61].An ultra-high-.delity simulationis pro-vided by the DT concept and it plays an important role in I4.0. It extends simulation to all product lifecycle phases, combining real-life data with simulation models for better performances in productivity and maintenance based on realistic data [61]. 
Technologies based on simulation are the core role in the digital factory approach, allowing experiments and validation upon differ-ent manufacturing system patterns, processes and products [72]. 
3.5. 
Augmented Reality 
New challenges are coming with Augmented Reality (AR) usage in everyday [73]. Increase human performances is the aim of AR, supplying the needed information toa given speci.c task [74]. This novel technology provides powerful tools, acting as an HMI [75].AR technology can be found on a wide range of sectors, e.g., entertain-ments, marketing, tourism, surgery, logistics, manufacturing, main-tenance, etc. [76].Asa growing evolving technology, recently,AR usage is spreading to different manufacturing .elds [77]. The use of AR on manufacturing processes regarding to simulation, assis-tance and guidance has been proven to be an ef.cient technology helping on problems [78]. AR technology increase reality operator＊s perception by making use of arti.cial information about the environment,wheretherealworldis ful.lledbyitsobjects [79每 80].As long as it interacts with human senses, AR can make use of any kind of hardware [74]. Using AR can help on closing some gaps, e.g., between product development and manufacturing operation, due to the ability to reproduce and reuse digital information and knowledge at the same time that supports assembly operations [78]. Fig. 12 shows the most relevant tasks related to industrial environments and manufacturing .elds where the AR brings value. 
The principle of AR is the combination of two scenarios: 1) dig-itally processed reality with2) digitally added arti.cial objects that could be 2D .at objects, or by other de.nitions that only considers 3D objects within the scene [73]. The authors [79每80] de.ned AR system features as: 1) the ability on combining real and virtual objects on a real environment, 2) the ability on align each other the real and the virtual objects, and 3) the ability on running inter-actively, in 3D, and on real-time. 
Making use of conventional hardware, the use of AR has a big advantage that can be minimal or even zero purchase expense. Some cases, the see-through glasses component can be more expensive [73]. On industrial environment, other key advantage was pointed by Blanco-Novoa et al. [81] about the assets: AR pro-vides dynamic real-time information,soitcan suppress mostofthe paperwork. 
The AR system software might be selected based on environ-ment＊s considerations, which obviosity differ among them, e.g., on the military environment the proper use is zero-connectivity to ensure CS, differing from commercial environment that requires providing remote assistance＊s connectivity [74]. 
The essential parts of an AR system make use of electronic devices to directly or indirectly view a real-world combination with virtual elements. According to Fraga-Lamas et al. [75], these elements can be: 
. 
Image capture element 每 web camera is suf.cient [73]; 

. 
Display 每 for projection of the virtual information on the images acquired by the image capture element. Basically, three device types with optical options can be used [80,82]: 1) hand-held (video and optical), 2) head-worn (video, optical, and retinal), and 3) spatial (projector and hologram); 

. 
Processing unit 每 to generate virtual information to be projected; 

. 
Activating elements 每 to trigger the display of virtual informa-tion, e.g., sensors,QR markers, GPS positions,images, etc. 



In order the user to visualize information, these AR devices use types of optics as follows [82]: 
. Video 每 merged worlds (real and virtual) into the same digital view; 


V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 



. 
Optical 每 real world with virtual objects overlaid directly on the view; 

. 
Retinal 每 direct projection of virtual objects onto the retina with the use of low-power laser light; 

. 
Hologram 每 real world mix with virtual objects using a photo-metric emulsion; 

. 
Projection 每 projection of virtual objects directly on real-world objects with the use of a digital projector. 


Related to the quality of products, Segovia et al. [83] proposed an AR system solution to production monitoring, based on Statisti-cal Process Control (SPC) and Six Sigma methodology. It uses AR in realtime reportsto assist qualitydata reportingby monitoringCpk indexes to support the decision-making process. The AR system was linked to a Computer-Aided-Quality (CAQ) to receive data. The CAQ used was Quality Data Analysis (QDA) software that allows the user to verify quality goals. The used measurement device was wireless connected to QDA software. The QDA software generatedreports and exported them automatically in a .le to the AR application. The mobile device used to run the AR application was a tablet. Fig. 13 shows the AR technology with the inside of the facilities and the displayed Key Performance Indicators (KPI) of each workstation. According to Segovia et al. [83], one of the big-gest bene.ts of this tool is the reduction on audit times. 
Maintenance is one of the most promising .elds of AR. It enhances human performances in technical maintenance tasks execution as also supports on maintenance decision-making [76]. One example of AR in maintenance is shown in Fig. 14 on a step-by-step assembly procedure of a consumer device, using Hand-Held Display (HHD) to carry out maintenance tasks. The AR appli-cation has text description of the task on the bottom, right and left arrows to go forward and backward on the procedure. 
Other example in the use of AR technology is on the diagnostics .eld.Ameaningful example is shown in Fig. 15, also with the use of an HHD. The defects inspection and mapping on the pipe was madewitha3Dimage.The defects positionis indicatedonthepipe anditcanbeseenaclearerimageofthe natureandscaleof defects. At the end, the operator can detect, locate and mark defects using a tablet and a marker [84]. 

3.6. Additive Manufacturing 
Products and services innovations needs hard and long research work and development that I4.0 with the novel technologies such as simulation via virtual reality are enabling it. However, on the next step, there is a manufacturing process with its related costs that can be a barrier to competitiveness. Additionally, at the end, there is a dilation of product or service lead time for markets. 
The Additive Manufacturing (AM) paradigm is being increas-ingly developed and it brings into real industry, high feasible appli-cations [85]. Jian et al. [86] discussed the potential of AM on the replacement of many conventional manufacturing processes. AM is an enabling technology helping on new products, new business 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

models and new supply chains.A set of technologies that enables ＆＆3D printing§ of physical objects form the collective term AM [87]. Products such as one-of-a-kind, can be manufactured without the conventional surpluses, so it is a big advantage. AM technolo-gies can be referred also with other synonyms such as rapid proto-typing, solid freeform manufacturing, layer manufacturing, digital manufacturing or 3D printing [88]. With AM it＊s possible to create prototypes to allow value chain elements independence, and therefore, achieving time reduction on design and manufacturing process. 
As follows in Fig. 16,AMprocesses are classi.ed into seven cat-egories according to the standard of the International Organization for Standardization (ISO)/American Society for Testing and Materi-als (ASTM) 52900:2015 (ASTM standard F2792). 
AM technology is de.ned by Kim [85] as a process of creating a 3D object-based on the deposition of materials on layer-by-layer or drop-by-drop undera computer-controlled system. Some potential bene.ts of AM can be summarized as follows [89]: 
. 
Manufactured parts directly from CAD data .les (.nal or near .nal parts with minimal to no additional processing); 

. 
Greater customization without extra tooling or manufacturing cost; 

. 
Manufacturing of complex geometries (some geometries cannot be achieved on conventional processes, otherwise, it is achieved by splitting it into several parts); 

. 
Manufacturing of hollow parts (achieving less weight) or lattice structures; 

. 
Maximization of the material utilization for the ＆＆zero waste§ approach; 

. 
Smaller operational foot-print towards manufacturing a large variety of parts; 

. 
On-demand manufacturing and excellent scalability. 



According to Shin et al. [90],AMwork.ow includes the geome-try design, computational tools and interfaces development, mate-rial design, process modeling and control tools, and it was also discussed the AM applications .elds such as nano-scale (bio-fabrication), micro-scale (electronics), macro-scale (personal prod-ucts, automotive), and large-scale (architecture and construction, aerospace and defense). 
For the next generation of AM processes, Chang et al. [91] dis-cussed novel processes such as micro/nano scale 3D printing, bio-printing (AM of biomaterials), and 4D printing (combination of AM with smart materials (stimulus-responsive that change their shape or functional properties)) to fabricate within high resolution a complex 3D features, in multi-materials, or multi-functionalities. 
On a near future, AM technology will expand eventually to super-advanced technology areas and substitute current technolo-gies [85]. 
3.7. Horizontal and Vertical Systems Integration 
Engineering, production, marketing, suppliers, and supply chain operations, everything connected must create a collaborative sce-nario of systems integration, according to the information .ow and considering the levels of automation [18].In general, the sys-tems integration of I4.0 has two approaches: horizontal and verti-cal integrations [10,92]. Real-time data sharing is enabled by these two types of integration [16]. 
Horizontal integration is the inter-company integration [92] and is the foundation for a close and high-level collaboration between several companies, using information systems to enrich product lifecycle [16], creating an inter-connected ecosystem within the same value creation network [10,92]. It is necessary an independent platform to achieve interoperability on the devel-opment of these systems, based on industrial standards, enabling exchanging data or information [92]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
Vertical integration is a networked manufacturing system [93], the intra-company integration [92] and is the foundation for exchanging information and collaboration among the different levels of the enterprise＊s hierarchy such as corporate planning, pro-duction scheduling or management [10,93]. Vertical integration ＆＆digitizes§ all the process within entire organization, considering all data from the manufacturing processes, e.g., quality manage-ment, process ef.ciency or operations planning that are available on real-time. By this, in a high level and .exible way, providing the small lot sizes production and customized products, the verti-cal integration enables the transformation to SF [16]. It＊s important to refer that standards must be the bases of the vertical integration [92]. 
According to several authors [16,93每96], the paradigm of I4.0 in manufacturing systems has another dimension between horizontal and vertical integration considering the entire product lifecycle. This kind of integration is based on vertical and horizontal integra-tions[93].Ina visionof holistic digital engineering,asthe natural .ow of a persistent and interactive digital model, the scope of the end-to-end digital integration is on closing gaps between product design and manufacturing and the customer [94], e.g., from the acquisition of raw material for the manufacturing system, product use and its end-of-life. The phase of end-of-life product contains reusing, remanufacturing, recovery and disposal, recycling, and the transport between all phases [95]. Fig. 17 shows the relation-ship between the three types of integration on a manufacturing system, considering vertical integration as the corporation(s), hor-izontal integration between corporations, and end-to-end integra-tion linking design, production and logistics as an example. 
3.8. Autonomous Robots 
Manufacturing paradigm is shifting rapidly production from mass towards customized production, requiring robots, for instance, as a recon.gurable automation technology. The impact on the production systems of the manufacturing companies is that this trend leads to the production adaptation for a wider product variation, focusing ideally on batch size one. Nowadays, to reach the .exibility demanded level, robots are essential on production systems [97]. Towards that, abilities on computing, communica-tion, control, autonomy and sociality are achieved terms when combining microprocessors and Arti.cial Intelligence (AI) with products, services, and machines to make them become smarter. Robots with AI, adaptive and .exible, can facilitate different prod-ucts manufacturing and consequently providing decreasing pro-duction costs [16].In addition,a robot also canbe seen as oneof the forms of AI [98]. 
Processes such as product development, manufacturing and assembling phases, are processes that adaptive robots are very use-ful on manufacturing systems [16]. It is important to refer that fully autonomous robots make their own decisions to perform tacks on a constantly changeable environments without operator＊s interaction [99]. Fig. 18 shows an overview, not strict, on the autonomous robot characterizations, considering industrial and non-industrial environments. 

Dirty or hazardous industrial applications on unstructured envi-ronments can be improved by an Autonomous Industrial Robot (AIR) or multiple in a close collaboration. Hassan et al. [100] pre-sented a multiple autonomous robot＊s collaboration approach in Fig. 19, consisting on robots with different capabilities performing grit-blasting and spray painting. 
According to Hassan et al. [100], with the deployment of multi-ple autonomous industrial robots working as a team, it＊s possible to have a larger range of manufacturing applications. Other approach in multi-robot systems can be seen in Fig. 20 during a 

Fig.19.Autonomous industrial robots performing grit-blasting or spray painting [100]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

sequence of collaborative assembly operations, dealing with robot con.gurations to grasp assembly parts and build complex struc-tures such as a chair [101]. 
Collaborative robots concept also introduces the proximity of robotswith humans [102].Onthe visionofSF, collaborative robots (cobots) and humans will work closely together. Cobots are a cat-egory of robots specially designed to interact directly and physi-cally with humans, in a close cooperation [103每104]. This is possible due to the safety existing limits on speed and forces that automatically restarts the cobot allowing to guide the cobot by hand [103]. By this, for manufacturing companies, human-robot barrier is break down offering bigger affordability and .exibility on solutions [104]. 
3.9. 
Cybersecurity 
Every year, increasingly, devices are connected to the global network: the internet. In a close future, the main source of data will be inanimate objects [105].Bythis, IoT, virtual environments, remote access, stored data on cloud systems, etc., are many open opportunities that represents increasing new vulnerabilities lead-ing to a compromised information for people and enterprises. The risk scenario becomes reality because the enterprise bound-aries are unclear and are vanishing [106]. Kannus and Ilvonen 
[107] de.ned Cybersecurity (CS) as a new term on a high level of information security, and through the word ＆＆cyber§ it spreads to apply also on industrial environments and IoT. CS is a technology laying on protecting, detecting and responding to attacks [108]. 
IoT has to be built based on safety communications on each point of the manufacturing process and safety interoperability has to be assured between facilities as basic elements of the supply chain value. I4.0 technologies must allow the creation of a safety cyber environment, bene.ting on CS. 
Direct attacks from evil persons and/or software can be hard jeopardies to Industrial Control Systems (ICS). These ICS of the industrial sectors are basically control such as Supervisory Control and Data Acquisition (SCADA), process control systems, distributed control systems, CPS or Programmable Logic Controllers (PLC) [109]. The increasing of connected devices means more possibili-ties of cyber-attacks. Benias and Markopoulos [110] discussed why industrial devices get hacked, the main reasons as follows: 
. 
Devices running for too much time (weeks or months) without updating security or anti-virus tools; 

. 
Considerable number of old controllers used in ICS networks, designed when CS was not a concern; 

. 
CS threats can enter bypassing CS measures due to the existence of multiple pathways from several ICS networks; 


. Quick spread of malware due to several ICS networks that still remains implemented as a .at network without physical or vir-tual isolation among other unrelated networks. 

I4.0 creates valuable information that needs to be protected. Information and data security are critical for the industry success. It is important that data is available just for authorized persons. Integrity and information sources must be ascertained. I4.0 has raised two demands for CS in order to secure smart manufacturing systems: Security Architecture and Security by Design. Hence, attacks, threats and malware must be automatically detected with zero-installation by the systems [106]. Manufacturing operations can be shut down by a cyber-attack, therefore, companies have money losses, but the main issue are cyber-attacks targeting sys-tems requiring safety operations and representing a serious risk for the safety of the operators [111]. Elhabashy et al. [112] dis-cussed other approach on manufacturing environments regarding to some potential attacks such as modifying product designs (related to CAD .les, tolerances), modifying manufacturing pro-cesses (Computer-Aided-Manufacturing (CAM) .les, machine parameters, used tools, tool paths) or manipulating process/pro-duct data (inspection results, indicators of machine maintenance). These attacks can delay a product＊s launch, cause the production of modi.ed products, can ruin customer trust or increase warranty costs. 
The cyber-attack could be internal and/or external source. According to Khalid et al. [113],in Fig. 21, a cyber-attack can come from an internal source such as an operator that physically access to a data port or an external source such as an outside communi-cation channel or also a wireless transmission. 
The ICS safety is time-sensitive so an automatic incident response is need it. For a variety of industrial attacks, Software-De.ned Networks (SDN) and Network-Function Virtualization (NFV) can facilitate automatic incident response. The incident response in ICS can be achieved using a private-cloud architecture (cost-effective investment). SDN and NFV makes automatic inci-dent response possible to rapidly detect and temporarily replace the failing systems with virtual implementations of those systems. SDN and NFV are technologies to improve the following aspects:1) network visibility, 2) network capabilities (enables network traf.c .ows with better management), and 3) network functions deploy-ment and control using software, instead of speci.c hardware mid-dleboxes [108]. However, the combination of SDN with NFV shows a capable approach in new defense solutions in depth for ICS [114]. 
The concept of defense-in-depth, as showed in Fig. 22, was dis-cussed by Jasen et al. [115], according to the international standard IEC/ISA-62433 with the incorporation of three measures as techno-logical, organizational, and human-centered, as multilayer approach for security ICS. Security controls at system level, net-work and plant must exist on this concept. 
Updating the implemented security controls continuously is obligatory, keeping the protection up-to-date [115], such as fol-lows on: 
. 
Device level -with the installation of new security patches; 

. 
Network level -with the .rewall signatures of new threats updated; 

. 
Plant/factory level -with the analysis and monitoring of the actual log sources. 



4. The Smart Factory of the I4.0 
According to several authors [2,4每8,116], the framework of the I4.0 is the development of the Smart Factory (SF). In conceptual terms, the SF is the heart of I4.0 [117]. CPS, IoT and IoS were assumed as the main components of I4.0 [1]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 


These components have very closely linked each other, enabling the SF and built on the concept of decentralized production system with a social network connecting persons, machines and resources [1]. Using cloud-based manufacturing in SF, both IoT and CPS tech-nologies converges to IoS to create, publish and share the manufac-turing processes, represented in services that could be supply by virtual enterprises [118]. 
Comparedto humans living in two worlds such as the physical and the cyber world, SF will work on the physical and on the DT, in the cyberspace. The DT will collect generated data from manual inputs and sensor networks, will process data on cyberspace and take the corrective actions on real-time to handle the physical world [29]. 
Based on the manufacturing process digitalization, I4.0 is the development of a new generation of SF＊s [24]. According to several authors [2,4,8,10],in this new generation of SF, the main key tech-nology is CPS. SF is the key feature of I4.0 and the core concept component, where vertical integration occurs, the horizontal inte-gration occurs in the SF value network and across different SF＊s, enabling end-to-end engineering integration across the entire value chain [119]. Fig. 23 identi.es the transformation technolo-giesof the current industrial productioninaSF framework. 

4.1. Cyber-Physical Systems 
Cyber-Physical Systems (CPS) has the potential to change our life with concepts that already emerged, e.g., robotic surgery, autonomous cars, intelligent buildings, smart manufacturing, smart electric grid, and implanted medical devices [120] (e.g., a pace maker in a smaller scale [121]). CPS represents the latest and signi.cative developments of Information and Communication Technologies (ICT) and computer science [120]. 
CPS is the merger of ＆＆cyber§ as electric and electronic systems with ＆＆physical§ things. The ＆＆cyber component§ allows the ＆＆physi-cal component§ (such as mechanical systems) to interact with the physical worldby creatinga virtualcopyofit.This virtualcopywill include the ＆＆physical component§ of the CPS (i.e., a cyber-representation) through the digitalization of data and information. By this, CPS can be assumed as a range of transformative technolo-gies to manage interconnected computational and physical capa-bilities [122]. CPS embraces smart elements or machines who has the augmented intelligence and ability to communicate each other to make part of planning, unique or non-repetitive tasks. These smart elements, for instance, can control the needs of workpieces, alter the manufacturing strategies for the optimal production, choose (if already exists) or .nd a new strategy all by themselves. These elements will build their own network [123].In other words, the CPS core is the embedded system to process information about the physical environment. This embedded system will perform tasks that were processed by dedicated computers. CPS model can be described as a control unit with one or more microcon-trollers, controlling sensors and actuators that interacts with the real world and processes the collected data [124每125].A commu-nication interface will enable this embedded system to exchange data with the cloud or with other embeddedsystems. CPS is asso-ciated with the IoT concept [126]. According to Humayed et al. [127], CPS mainly consists of three components such as: 1) com-munication; 2) computation and control and; 3) handling and monitoring. The CPS communication can be both wired or wireless 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 


and connects CPS to a higher level such as control systems, or lower-levels such as physical world components. The intelligence is embedded on the computation and control component with the exchange of control commands and received measures. CPS is connected to the physical world by the handling and monitoring component, using actuators to handle physical components and using sensors to monitor them [127]. 
Referring a manufacturing system and according to Keil [128], Fig. 24 shows a schematic representation of a CPS, an embedded system integrated in physical systems such as production lots or machines. The sensors collect physical data and the electronic hardware and software will save and analyze it. The interaction between data processing and other physical or digital systems are the CPS bases. it＊s also possible to identify an HMI in this CPS schematics for supervision and exchange information. 
Several CPS linked within digital networks formaCyber-Physical Production System (CPPS) [128],based on sub-systemsand autono-mous and cooperative elements linked across all levels of produc-tion [120]. According to Rojas et al. [129], CPS are the building blocksfortheSF, structuredasCPPS.The collecteddatawillbesent to BD and become accessible via CC. The CPPS interaction with the virtual world enables IoTin manufacturing [13,118].As the system are getting intelligence regarding to the so-called smart objects, the IoT creates the connect environment with smart objects to the glo-bal internet. Several authors [3,6,10,19,58,94,113,121,124,130] dis-cuss the level of cooperation and communication of CPPS in manufacturing. 
The implementation of CPPS in the SF leads to a fundamental design principle as the real-time management in industrial pro-duction scenarios. CPPS will make the automation pyramid approach on a different manner. The traditional automation pyra-mid,as showsthe Fig.25,is partly breakatthe PLC＊s level.The .eld level and control remain including closest PLC＊s of the technical processes to improve critical control loops, and the highest levels of the hierarchy will be decentralized [131]. 
In the CPS-based Automation of the Fig. 25, the squares repre-sent inputs/outputs devices, the lines represent service interac-tions and the blue, yellow, grey and black points represent the 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

corresponding functionalities of the .ve-layer architecture of the traditional automation pyramid [126]. 
Some researchers are developing a .ve C＊s structure for better analyzing I4.0. This .ve C＊s architecture can guide the development of I4.0 and it is dependent of CPS attributes. These .ve levels are: Connection Level (main attribute is self-con.gurable), Conversion Level (main attribute is early-aware), Cyber Level (main attribute is controllable), Cognition Level (main attribute is informational) and Con.guration Level (main attribute is communicable) [116,132每133]. 
4.2. Internet of Services 
Replacing physical things by services, the Internet of Services (IoS) is based on the concept that services are available through the internet so that private users and/or companies can create, combine and offer new kind of value-added services [1]. IoS can enable service vendors to offer their services on the internet. Thus, the manufacturing industry of product-oriented trend is rapidly shifting for service-oriented to enable gaining revenue through all lifecycle of a product service system. By this, high quality on products can be enable by SoA, and side-by-side, gives a strong competitive position for companies through the value-added ser-vices. IoS enables collecting product information, e.g., during its operation, for updates and for the development of new services, increasing the perceived product quality [29]. IoS is consider by Andulkar et al. [29] as the technology to monitor the product lifecycle. 

CONCLUSIONS

5. Conclusions and Outlooks 
As aforementioned, the foundations of the I4.0 are the advanced technologies ofautomation, and the ICT present across this review. Key challenge of I4.0 is to make the production systems more .ex-ible and collaborative. For this purpose, the use of enabling tech-nologies is the strategy that is behind of I4.0 paradigm. On an industrial context, each implemented technology in an individual manner will present a lower impact. On the other hand, when implemented together, it offers new possibilities to embrace the future. For instance, one of the I4.0 impact will be the elimination of monotonous work as well as physically demanding jobs. 
IoT is an in.nite world of possibilities on innovation and opti-mization, due to the combination of many advanced systems and technologies such as BD and analytics, AI, networks, clouds, intel-ligent objects, robotics, middleware, people, among others. 
The development of a CMfg service integration platform is pro-posed by Mai et al. [46] as a promising concept. It is an online tool consistingon builda processwith several sub-taskswitha seriesof modules sequentially connected each sub-task. This concept allows consumers to have customized products or even make products in the cloud. Even more, through CMfg, producers can create smart solutions to save costs and improve pro.ts.Acrucial note is the improvement of the safety and security regarding to online services that was mentioned at all examples. The develop-ment of CS technology deserves maximum efforts from all actors, since individual, professional users, and organizations that need to be safe and secured to face these rapid technological advances. 
The Systems integration of I4.0 has two major characteristics relying on vertical and horizontal integration. The vertical integra-tion of the manufacturing processes, breaks the traditional automation pyramid, focusing on distributed and collaborative architectures. The horizontal integration allows the creation of a new kind of value-added [129]. By this, there is an unavoidable surrounding of customers and suppliers that are involved just from the beginning of the product life cycle. 
A challenging scenario with the deployment of I4.0 will be the extinction of the centralized applications used in common manu-facturing environments, that leads to decentralized systems as one of the main I4.0 goals. By this meaning, distributed computing systems also plays a key role on I4.0 paradigm. It allows to save time on computing runtimes, allows working with more accurate details on smaller systems and for the overall system, and decreases the fail reaction time, e.g., if one computing system fails the others can continuing on computing. 
Providing a guideline for the interdisciplinary I4.0 technologies, the RAMI4.0 was developed, describingthe connection between IT, manufactures/plants and product lifecycle througha3Dspace. The integration of RAMI4.0 and I4.0 component (component as, e.g., a production system, an individual machine or an assembly inside the machine) close the gap between standards and I4.0 technolo-gies at the production level, leading to the emerge of CPPS [130]. 
Interoperability is one of the I4.0 design principles and can be found between BD and simulation as discussed by Shao and Jain [64]; BD on its analytics supports simulation by estimating the unknown input parameters and performing data calibration for simulation and its validation results. The return is the support of simulationfor BD analytics on various roles. Data analytics applica-tion can summarize and report production trends (e.g., product 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

variation cycle time or throughput average). Diagnostic data anal-ysis can respond to what has happened and what is happening, identifying causes. Diagnostic analysis can take advantage using of manufacturing system＊ simulation model that emulates the cur-rent operation. Predictive analytics estimates performance based on planned inputs, e. g., product cycle time and throughput estima-tion for several products based on current policies. It will take advantage from simulation models to execute the what-if scenar-ios. Prescriptive analytics can respond to how can we make it hap-pen and what will be the consequences.It uses simulation models to improve the production performancein future periodsby emu-lating operations under paralleled realities and these plans can be improved with the arrangement of simulation and optimization models. 
In the VF level, simulation can be seen as data generator allow-ing VF to generate for instance, streams of production data and resource utilization, and feed data to analytics applications. Can be seen also as supporting evaluation and validation giving an advantage to the real factory. 
Simulation technology on I4.0, using VR, is an integral process to simulate all industrial processes, from planning, design, manu-facturing, providing services, maintenance, try-outs or even quality controls. All processes can be simulated as modular [132]. It＊s pos-sible to simulate and virtual verifya factory manufacturing process before being realized. After approved, all physicals can be done. For instance, if it is considering the combination within simulation and AM, after product simulation, the production of prototypes allows the time reduction on design and production process, by reducing the value-added dependencies. These time reductions are particu-larly relevant on customized markets. 
Grieco et al. [4] presented an interesting case study in fashion manufacturing where a decision support system as a software is developed under the I4.0 concept, aiming the minimization of: 1) orders delivered later than due date, and 2) resource overload cases. 
Many researchers discuss that the data is the raw material of the XXI century and the real world will be a huge information sys-tem. According to this, Lu [3] discussed one of the major chal-lenges in I4.0 that will be the development of algorithms for dealing with data. 
According to Salkin et al. [16], there is no speci.c I4.0 de.nition, and therefore, there is no de.nitive utilization of the enabling tech-nologies to initiate the I4.0 transformation. 
But the fact that this fourth revolution is been announced before it takes place, opens several opportunities for co-working environments between academic researchers and industrial practi-tioners, shaping on the manufacturing future [134]. 


5.1. Looking Forward 
As mentioned by Rojas et al. [129], I4.0 is on its infancy and to make it a reality, several challenges and gaps must be addressed. By this, the roadmap for the I4.0 ful.llment is still not clear to date in both academia and industry [132]. Considering .ve fundamental manufacturing systems to conceive I4.0, Fig. 26 can represent the research gaps between the current manufacturing and the I4.0 requirements [132]. These .ve manufacturing systems are systems where is hard to achieve intelligent concepts, that are the goal of I4.0 development, neither I4.0 lower or upper levels. The closest to I4.0 is the Recon.gurable Manufacturing System. 
5.2. Executing I4.0 in SMEs 
Looking at European Union, SMEs represents the backbone of the economy and the key to competitivity. Inside this enterprise dimension, special approaches must be developed to introduce and apply I4.0 technologies [129]. The enabling technologies of I4.0 are the foundation for the integration of intelligent machines, humans, physical objects, production lines and processes to form a new kind of value chain across organizational boundaries, featur-ing intelligent, networked, and agile. By this, due to the increase level of complexity, manufacturing SMEs has doubts on the required .nancial effort for the transformation technologies and its impact on their business model [135]. 
The implementation of I4.0 in SMEs can be facilitated, for instance, on a SaaS approach, enabling technology acquisition for digital services with appealing investments. A clear example can be an SME integration on the supply chain of a product, allowing collaborative of project development, collaborative working on product＊s launch and time to market reduction, shared innovation, and consequently, minimizing the related risks. 
Acknowledgments 
Authors would like to acknowledge to the reviewers for their valu-able feedback. Special thanks to Freepik for providing vectors/icons for some .gures, available at www..aticon.com. 
Declaration of con.ict interests. 

The authors declare no potential con.ict of interests at all in this paper. 
Keywords:
Wind energy
Wind turbine monitoring Wind farm monitoring Control chart
SCADA monitoring Statistical inertia

Abstract

A method for monitoring wind turbine generators (WTG) using data provided by the SCADA system is proposed. This method relies mainly upon comparing one WTG with the average of all remaining WTGs on a wind farm. Because environmental conditions on a wind farm are roughly the same over its entirety, the difference between each WTG and the average of the remaining WTGs on the wind farm is constant over time. The statistical inertia of averaged conditions for the entire farm provides a good yardstick for WTG monitoring. The results of monitoring four aspects of a WTG are presented here: these are electrical energy produced; tower vibration; nacelle yaw; and gearbox temperature. Control charts are used to detect abnormal behaviour. With regard to the electrical energy produced, one accidental activation of a curtailment algorithm was found. For tower vibration, we describe an application for the detection of rotor imbalance. For yaw, an example showing detection of nacelle drift is covered. Lastly, for gearbox temperature, the proposed methodology succeeded in detecting an issue two months prior to failure. We have included limitations as to the minimum wind farm size required in order to use the wind farm average. A centralized control chart is also proposed.

1. Introduction

During the life time of a Wind Turbine Generator (WTG), many events can affect its performance. These events can be classi?ed according to the time scale on which they occur. Faults such as blade-angle asymmetry or generator over-speed generally affect the produced electrical energy for hours. Downtime resulting from changing or repairing a principal component such as a main bearing or a gearbox can last for days, even weeks [1]. Other events such as blade erosion build up over months and years. The effect of such events on power output can also be divided in two categories: events that partially reduce the production (e.g.: icing, blade erosion) and others that stop the WTG (e.g.: faults, downtime for repairs). Preventing these events or limiting their duration is an important aspect of wind farm operation and maintenance (O&M). The ageing of WTGs is now a timely topic for the wind industry, since many wind farms in Canada and around the world have been in service for decades. With time, failure of components is more frequent and underperformances can appear. Some authors are reporting a performance reduction rate of approximately 1.5% per year [2]. Also, some operators are even considering the option of repowering, as their farms are getting closer to the end of their planned lifetimes, or as new WTG models, signi?cantly larger than the ones built decades ago, become available [3]. Thus, various monitoring method are used to improve availability of WTGs and to achieve condition based maintenance. The ageing of the wind farms also motivates interest in the great amount of data available for the development of monitoring tools.
The objective of this paper is to propose a data-driven method to monitor wind farm WTGs base on the long term, which is also robust and suitable for the industry. Here, control charts are used for the generation of alarms. Unlike most other monitoring methods, the proposed methodology can be used to monitor a wide range of WTG components or aspects. It is also simple to under- stand and use: no advanced knowledge in data mining or modeling is required. Thus, the proposed methodology is suitable for indus- trial applications. Furthermore, this method allows for the moni- toring of various aspects of a WTG simultaneously, with the help of a centralized control chart. This method is suitable for medium and large wind farms (more than 25 WTGs). Since the number of units per wind farm is constantly rising, this method is can be applicable to most cases.
First, previous work on the monitoring of WTGs will be reviewed. Then the proposed method will be detailed, followed by the results of its application to industrial wind farms. The method used here for monitoring a wind farm is based on comparing a single WTG with the average of the other WTGs on the same wind farm. The effectiveness of this approach will be demonstrated by means of four separate industrial study cases.

2. Literature review

One way of increasing the reliability and availability of wind energy is by monitoring WTGs. With proper monitoring, failures can be avoided and their consequent down-times limited, all of which increases availability. Maintenance can be planned ef?- ciently, and replacement parts can be ordered before failure occurs. There are various ways of performing the monitoring of a WTG. These can be classi?ed as follows: condition monitoring systems (CMS); Supervisory Control and Data Acquisition (SCADA) moni- toring; power curve monitoring; and fault prediction.

2.1. Condition monitoring systems

Condition monitoring systems (CMS) involve the use of addi- tional sensors that evaluate the health of WTG components. They are often based on vibration analyses and use methods such as wavelet analysis or Fourier transformations. They can achieve great precision by predicting the failure of a component months before it happens [4] [5] [6]. However, installing additional sensors can be costly in large wind farms [7]. CMS may also include oil analysis, thermography, shock pulse methods, acoustic emissions and ul- trasonic techniques, as reviewed in Ref. [5].

2.2. SCADA monitoring

SCADA monitoring is the monitoring of a WTG using the data provided by its SCADA system and does not need the use of addi- tional sensors [7]. This monitoring method is limited by the data SCADA provides. Often it is component temperature that is analyzed via SCADA monitoring [8]. In some cases, models are used in order to predict component behaviour [4] [9] and in other cases, the monitoring can be based on the signal itself (mean values, standard deviation, slopes, root mean square, spectrum, etc.) Power curve monitoring can be viewed as a subset of SCADA monitoring. Power curve monitoring is based on the relationship  between wind speed and power output. A change in behaviour of a WTG may be re?ected by its power curve [10] [11] [12] [13]. This method of monitoring can be powerful for the detection of small, progres-
sive underperformances [14].
However, power curve monitoring relies upon measurement of wind speed. According to IEC (International Electrotechnical Commission) 64100-12-1 [15], wind speed shall be provided by a met mast in order to assess the performance of a WTG as a function of freestream speed. But since wind farms have only a few met masts, nacelle wind speed is used instead in power curve moni- toring. In fact, the important point is to obtain a reliable, repeatable and representative wind speed measurement. Therefore, the power curves, using nacelle wind speeds, can be used for monitoring. However, a major ?aw in power curve monitoring is that any change in nacelle anemometry can create a considerable shift in the power curve. Fig. 1 illustrates a change in nacelle power curve following various changes in nacelle anemometry. These power curves were obtained with the bin method described in IEC's 64100-12-1 standard, while using nacelle anemonetry [15].  A noteworthy dif?culty while monitoring a WTG's components using power curve monitoring, is that often, their failure will be seen in the power output after a critical point is reached.

2.3. Fault monitoring

Another type of monitoring is the prediction of faulty behav- iours in a WTG. As de?ned by Ref. [16], a fault occurs when a parameter of a system deviates from standard conditions, such as blade angle asymmetry, component over-temperature or generator over-speed. Operational data are analyzed by means of complex algorithms in order to predict or even avoid the shutdown of the WTG [17] [18] [19] [20] [21] [22]. Faults monitoring methods can be classi?ed into two categories: model-based and signal-based [16]. In the ?rst case, a model is used to predict the value of a parameter and the predicted value is compared to the observed value in order to ?nd abnormal behaviours. For the signal based approach, fea- tures of the signal are studied. These features may be in the time domain (mean, root mean squarre, gradient) or frequency domain (spectrum) [16] [23]. Fault monitoring also includes fault-tolerant control, where a system analyses the severity of the fault  and takes appropriate action (compensation, controller recon?guration, etc) [16].
Fault prediction is especially important for offshore wind farms, where access to the WTG is more dif?cult than onshore wind farms. High frequency data (~1 Hz) must be available for use in predicting faults; often, additional sensors or complex models, or algorithms are used as well. The types of conditions the control system of a WTG uses in order to detect faulty behaviour requires input to remain above a certain threshold for a few seconds or minutes. Some work has also been done on the development of a WTG controller that can be optimized according to the conditions of the WTGs subset in order to avoid faults. Used components or component subsets may act differently from their nominal behav- iour and thus, the optimal control strategy should be revised [22]. For more on fault monitoring and diagnosis, see Refs. [16] and [23]. The monitoring methods detailed above can all be useful in the O&M of a wind farm. Operators might consider using a combina- tion of these methods, as each evaluate the condition of a different
aspect of their WTG, based on different criteria.

3. Data source
The data used to develop and test the proposed method was taken from ?ve industrial wind farms located in Canada. All the WTGs were the same model and were MW class and pitch regu- lated. Each wind farm contained over 50 WTGs. While the data was recorded at a frequency of 1 Hz, ten-minute averages are used. This averaging limits the noise in the signals and is the norm in the wind power industry, as suggested by the IEC standard [15]. The database has been in service since 2009, which is of interest for long-term monitoring. No CMS were installed on the WTGs. Available values included (online, repair, maintenance, curtailment, etc.) Because there was no mea- surement of principal-component vibration, monitoring methods using these values could not be used. Extreme values correspond- ing to obvious instrumentation malfunction have been removed from the database. Fig. 2 illustrates the data available and the data acquisition process. Here the acquisition and archiving system is the PI system from OSISoft. There is a local server in each wind farm, linked with an Internet connection to a main server. This redundancy allows to archive data even in the eventuality of connection losses with the main server and the on-site server. The
Fig. 1. Example of shifts in power curve caused by instrumentation. On the left, the effect of a faulty anemometer and on the right, the effect of the anemometer type.
Fig. 2. Available data and data acquisition process.

4. Method

The proposed method for monitoring the WTGs on a wind farm compares each WTG's behaviour with the mean of the other WTGs' behaviour on the wind farm. This method is data-driven and empirical based. No models are used.
The behaviour of a WTG constantly ?uctuates due to the various productive WTGs. However, this difference should be constant in time, the same assumptions are made in Ref. [24], where a moni- toring method using wind farm power curves is presented. Thus, the difference between a speci?c WTG and the average of the remaining WTGs on the wind farm should be statistically constant in time.
the difference with the mean of the others,, can be
expressed by the following relation in its environment (wind speed, wind direction, air density,

ambient temperature, etc.). However, all WTGs on a wind farm are
relatively affected by the same environmental variations, since weather conditions are roughly the same over the entire wind farm. Some WTG will perform better, depending on the con?guration of the wind farm. Because physical quantities such as the components and structural vibrations and component temperatures are pro- portional to power output, their values will be higher in the most

This calculation is made at 10 min intervals for which the WTG is available. The rest of the ?eet's average, or the average of all other WTGs on the wind farm, is in fact the average of all the other available WTGs (N(t)). Units under maintenance, curtailment, repair or other production limiting state should not be included in calculating the average for the remaining WTGs on the wind farm, as these are not in operation at that time.
In the case of industrial wind farms, since there are many WTGs,
the ?eet average is a robust value. Small, instantaneous variations

of statistical hypotheses, this implies the need to test two hy- potheses. The ?rst test is on the expected value for Dxi,j(t). The null hypothesis (H1) is that the expected value, E  Dxi;j  t   , is a constant
(m?0?). The alternate hypothesis (H1) is that the expected value is not or change in behaviour of any one WTG will not affect the moni- toring of the other WTGs. Fig. 3 illustrates the calculation method
for the Dxi,j(t). The minimal size required by a wind farm will be constant. discussed in Section 6.
A change in Dxi,j(t) implies a change in the behaviour of the physical quantity j of the WTG i. It is easier to detect an abnormal wind  farm's  environment  or  by any seasonal  effect  are removed
while calculating the difference between the WTG and the average of all others WTGs. Due to the stochastic nature of wind, the out-

These two parameters are not functions of time since, in the

There are several ways of testing these two hypotheses. For the hypothesis, tests on the mean of a distribution (such as the Student t-test) may be used, and for the second, a test on the normality of distribution (such as the Kolmogorov-Smirnov test) might be appropriate. Here, however, rather than using statistical absence of abnormal behaviour,

As mentioned

tests to detect a change in Dxi,j(t) at any time, we will rely upon before and illustrated on Fig. 4 for the case of tower vibration, this distribution is normal. and, as mentioned above, this bias is constant in time. The standard devi- ation is a random noise caused by the ergodicity of the signal.
The detection of a change in behaviour of the physical quantity j of a WTG i in fact detects a change in distribution f(mi,j,si,j). In terms

control charts: these are more appropriate for continuous moni- toring of a quantity. The control chart methodology will be described in the following section.
Lebranchu et al. have presented in Ref. [8] a method for the monitoring of a main bearing using the temperature difference between it and the remaining bearings on its wind farm. Here we will present its application to a wider range of components of a WTG: we will also discuss this method's limitations.

Fig. 3. Illustration of the calculation of the difference between measurement of a single WTG and average of all other WTGs on the wind farm.
Fig. 4. Example of the distribution of the Dxi,j(t). Here the case of the Difference in tower vibration between a WTG and the wind farm's remaining WTGs.


4.1. Control charts

In order to decide whether or not the WTGs have sustained changes in their behaviour, an Exponentially Weighted Moving Average (EWMA) control chart was used. Control charts are used in Statistical Process Control (SPC). A time series of the data or a sta- tistic of a sample of this data (such as the mean, standard deviation, minimum, maximum) is plotted together with control limits [25]. Control limits are de?ned by the process's history or requirements, to 0.7 provided satisfactory results. However, l 0.3 seems to be the optimal value, and therefore, all our analyses were made using this value.
 The lower and upper control limits, LCL and UCL, were calculated based on the standard deviation of the process within a reference period. Here the reference period used was the ?rst year of avail- able data,  covering  any  seasonal  variations.  These  were  are expressed as follow expressions: and whenever the data steps outside of these limits, the process is viewed as out of control and thus, statistically different from its reference. There are a few rules that can be used to determine if the process is out of control or not.


In the present case, prior to using the control chart,



e main objective of this step was to reduce noise in the data. Here, data was taken at ten minute intervals, and the objective was to detect events building up on a daily to monthly time-scale. The use of a moving average as a high-frequency ?lter helped reduce the noise and improved detection performance. The period of the moving average depended on the quantity monitored and the time-scale over which problems could occur.
After the moving average was taken, the EWMA control chart was applied [26]. The expression for the EWMA Zi,j(t), for Dxi,j(t) was expressed as follows:
with Zi,j(0) is the target value for the process or its historical mean. The EWMA acted as a high-frequency ?lter with l as a smoothing value. l is from 0 to 1 and the smaller the l, the higher the smoothing. Depending on the level of variation in the variable of interest, the optimal lambda changes. For a large, sudden change, l must be relatively high, near 0.6e0.8 and for a small, progressive variation, l the optimal l falls somewhere between 0.2 and 0.4 [27]. Various values for this parameter have been studied and values up
with s, being the variance of the quantity monitored over a refer- ence period where the process has not experienced any abnormal issue. Usually, k 3 correspond to a three sigma or 99.9% con?- dence level. The EWMA control must be applied on data that is normally distributed. This requirement was satis?ed for each application presented.

5. Results

To illustrate the effectiveness of the proposed methodology, four study cases are presented here. In each of these cases, a change in behaviour of a WTG was observed and the use of a monitoring method could have reduced production losses caused by under- performances or prolonged downtime. The produced electrical energy is not directly impacted but nonetheless present. The study cases are from four different WTGs, each on a different wind farm.

5.1. Electrical energy produced

In this case, the monitored quantity is the electrical energy produced by a WTG. This kind of monitoring could re?ect problems such as underperformances or blade erosion. For each ten minute intervals where the WTG was available, the difference in produc- tion between it and the remaining available WTGs on the wind farm was calculated. This difference was then averaged on a monthly basis. Fig. 5 illustrates the application of a EWMA control chart to the difference in electrical energy production between one WTG and the rest of the ?eet.
 Following the application of the proposed methodology, it was found that between April 2012 and October 2012, on this WTG, a curtailment algorithm was mistakenly activated following major maintenance. If the proposed methodology had been used, the control chart would have detected a change in behaviour almost immediately. Instead, this step-type shift caused production losses of approximately 800 MWh per day over the whole period.

5.2. Tower vibration

Tower vibration can provide useful information on the health of the structure or highlight rotor or yaw problems. The tower vi- bration is measured directly under the drivetrain. In this applica- tion, the difference between this WTG and the ?eet average vibration level was calculated for each ten minute average while the WTG is available. Fig. 6 illustrates a case of a sudden change in vibration level: following the replacement of one of the blades on this WTG, the vibration level suddenly increased. This was caused by a mechanical imbalance of the rotor. A couple of weeks after the change, the rotor was rebalanced, and vibrations returned to their prior level. Rotor imbalance is a serious issue that can damage the drive-train's components if not treated rapidly.
For another WTG, a progressive increase in vibrations following preventive maintenance has been investigated (Fig. 7). During the maintenance of this turbine, the yaw system was over-torqued, causing excessive vibrations each time the WTG changed its orientation.
Analyzing the tower vibration can provide useful indications of damage the WTGs sustained in terms of fatigue analysis. This can

help plan preventive maintenance on wind farms and, over the long term perspective, prolong the lifetimes of WTGs.

5.3. Yaw

 Even though a WTG is designed to face the wind at all times, in some cases, like in imposed directional curtailment, yaw or nacelle azimuth is an important value to monitor. While it is assumed that the WTG is always facing the wind correctly, the value measured is not always accurate. In some cases, for two neighbouring WTGs, the SCADA will measure and archive major differences in yaw values. In the present case, since there is a discontinuity in the yaw (passing from 360¡ð to 0¡ð), the average and the difference must be calculated vectorially. Fig. 8 illustrates this issue, where the layout of one section of a wind farm at a certain time is shown. The position of each wind turbine is shown by a dot. Line color and length is an indicator of the turbine production level (blue is the lower and red is the higher), and line direction represents nacelle azimuth, or yaw, as provided by the SCADA system. As seen in Fig. 8, one of the WTGs appears to be drifting in a direction opposite to its neighbours. This situation is caused by an error in the measurement of the yaw itself: this value is measured using a proximity switch which counts the
number of metal teeth. If the teeth are worn, or if the grease con- tains metal particles, the tooth count can be erroneous, resulting in an apparent drift in yaw. Fig. 9 presents the case of an apparent drift of a WTG's nacelle. Since 2009, the difference between the yaw of this particular WTG and the mean yaw of the wind farm's remaining WTGs has been constantly increasing.
In general, this will not be an issue, because even in cases where measured yaw is incorrect, the WTG is, in reality, still facing the wind. However, when directional curtailment is applied to a WTG, if this curtailment is intended for a certain wind direction, yaw measurement must be reliable. The WTG's performance will be reduced in the wrong direction and the WTG will be unprotected in the proper direction.
As mentioned above, since the yaw is an angle, one must be careful with the mathematical operations. Here, sums and

Fig. 5. Case study of the application of a control chart to the difference between the production of a single WTG and the mean production of the remaining WTGs on the wind farm. This example shows the detection of a badly activated curtailment.

Fig. 6. Case study of the application of a control chart comparing a WTG's tower vibration of a WTG a with the mean vibration of the wind farm's remaining WTGs. This example shows detection of a rotor imbalance.
Fig. 7. Case study of the application of a control chart showing one WTG's the tower vibration as compared to the mean vibration of the wind farm remaining WTGs. This example shows the detection of a progressively rising tower vibration. subtractions must be done vectorially. Since yaw is an input parameter for a WTG, model monitoring cannot be used here and thus, proposed methodology of particular interest for monitoring yaw.

5.4. Gearbox temperature

It has been demonstrated that some failures in gearboxes or bearings can be detected using temperature analysis [8]. In these

cases, friction becomes greater and thus transfers energy by heat. Therefore, monitoring the temperature of gearboxes and bearings can possibly warn the operator of upcoming failures.
 Here, two different case studies will be presented on the application of the differences in gearbox temperature between a single WTG and the average of the wind farm's remaining WTGs. The gearbox temperature of the studied WTG is controlled in order to maintain its temperature around 55 ¡ðC. However, if the gearbox is near failure, its cooling system capacity may prove to be

Fig. 8. Partial layout of a wind farm at a certain time. Line color indicates the wind speed and line direction indicates wind direction. (For interpretation of the references to colour in this ?gure legend, the reader is referred to the web version of this article.)

Fig. 9. Case study of the application of a control chart to the yaw difference of WTG as compared to the mean yaw of wind farm's remaining WTGs. This example shows of slow drift.

insuf?cient. In February 2012, this gearbox failed and was replaced with a new one. In the weeks before the failure, the temperature had risen and the methodology proposed here would have raised an alarm in December 2011, two months before the failure occurred, as reported in Fig. 10. For wind farms in cold climates, as the ones studied here, it is important to avoid major repairs in wintertime.
A second issue occurred during winter 2015. Each winter, since, December 2013, the temperature of the gearbox oil of another WTG was found to be lower than the average temperature of the remaining WTGs on the wind farm, as reported by the control chart in Fig. 11. It was found that the oil temperature control system of the gearbox was faulty. Although, this situation is not as important as a gearbox failure, if the gearbox is not suf?ciently cooled or is overcooled, it will suffer damage. In winters of cold climates such as in Canada, the oil must be heated. If control over



6.1. Required wind farm size

 To use the mean of the wind farm as a reference, a certain minimum number of WTGs in the wind farm is required. The mean of the wind farm must be consistent. In the case of a wind farm that has too few WTGs if an important shift occurs on one of them, the average over the wind farm will be affected signi?cantly by that shift. Thus, the statistical inertia of the mean of a population (all WTGs of the wind farm) in this speci?c case must be evaluated.
 The effect on an important shift in the behaviour of one WTG on the average of a quantity is expressed by the following: overcooled gearbox.
 The temperature control of the gearbox makes the use of physical model-based monitoring dif?cult. A model that can pre- dict the temperature of the gearbox must take into account all parameters that could affect oil temperature. However, the power
   
Let's add a shift of dxi to one of the xi. We de?ne ¦Åx as the maximum acceptable impact on the average. Thus, we can re-write eq. (11) in this inequality:

of the cooling or heating system is not measured. Thus, it is now possible with the proposed methodology to provide a model that will adequately monitor a gearbox, based on available measurements.

which can simplify to:

Fig. 10.   Case study of the application of a control chart to the difference between one WTG's gearbox temperature and the mean temperature of the wind farm's remaining WTGs.   This example shows early detection of a gearbox failure.

Fig. 11. Case study of the application of a control chart to the gearbox temperature difference of one WTG as compared with the mean temperature of  the wind farm's  remaining WTGs. This example shows oil temperature control failure.

(13)
   
We choose to accept a maximum shift in the average of 1% of x, which sets ¦Å to 0.01. As for the maximum shift size to be observed that has no in?uence on the average, we choose 25% of x. This lead

In the present case, since all WTGs are in the same operating conditions and thus, each xi are similar, we can af?rm that x and xi are in the same order (x xi ). Thus, the precedent inequality becomes: to minimal wind farm size of around 25 WTGs. This limit is a guideline and the values for ¦Å and d could be different. However, ¦Å should be small, since the aim of this calculation is to limit the variation of x. As for d, smaller values would conclude to smaller wind farm size. However, a greater d would not be appropriate since we have made the assumption that x is similar to xi.
Here, the wind farms studied have each more than 50 WTGs.

Fig. 12. Centralized control chart for a WTG.

Thus, the proposed methodology can be used for all of them.

6.2. Centralized and normalized control chart

Since this methodology is optimal for use in large wind farms, and because several physical quantities are measured on each WTG, the number of control charts or ?gures needed for analysis can be great. One way to reduce the number of control charts is to group  all  the  physical  quantities  of  a  single  WTG  onto  a singlegure. In order to do this, Dxi,j(t) must be nondimensional. This means  reshaping  distribution  to  distribution  f(0,1). average of the remaining WTGs on its wind farm. Following this comparison, control charts are used in order to determine whether or not the WTG is behaving abnormally. Various cases based on the data available have been presented in order to illustrate the effec- tiveness of this monitoring approach.
 In comparison with model-based monitoring, the use of the wind farm as a reference is a simpler method that can be used by O&M engineers. While model-based monitoring can provide better results, it uses complex algorithm such as arti?cial neural networks, random forests, principal components analyses or other data- mining methods. The method we are presenting here is suitablefor industrial applications as it is simple and robust. It could easily be implemented online to continuously monitor WTGs. Moreover, it can be used for a wide range of turbine aspects and components. However, the proposed method can only be used on large wind farms (minimum size of around 25 WTGs) and if an issue is affecting all the WTGs in a wind farms, this method will not be able to generate an alarm.

One important aspect of a monitoring method is the manage- ment of alarms. A good method must maximize the detection rate and speed of changes in the behaviour of a WTG while minimizing the rate of false alarms. This kind of robustness required can be
are the average and the standard deviation of

provided by Control Charts such as the EWMA control chart.

Fig. 12 shows the centralized control chart for a WTG, with applications described in Section 5 (nacelle yaw; tower vibration; and gearbox temperature). For the case of this control chart, the control limits are set to 3 and 3, corresponding to three standard deviation limits. For the case of this particular WTG, the control charts detect a change in yaw in autumn 2010, while the behaviour of the other physical quantities remains in control.




Keywords:
Wind turbine monitoring Wind farm monitoring SCADA data
Fault detection Condition monitoring Performance evaluation
Abstract

The monitoring of wind turbines using SCADA data has received lately a growing interest from the fault diagnosis community because of the very low cost of these data, which are available in number without the need for any additional sensor. Yet, these data are highly variable due to the turbine constantly changing its operating conditions and to the rapid fluctuations of the environmental conditions (wind speed and direction, air density, turbulence, . . . ). This makes the occurrence of a fault difficult to detect. To address this problem, we propose a multi-level (turbine and farm level) strategy combining a mono- and a multi-turbine approach to create fault indicators insensitive to both operating and environmental conditions. At the turbine level, mono-turbine residuals (i.e. a difference between an actual monitored value and the predicted one) obtained with a normal behavior model expressing the causal relations between variables from the same single turbine and learnt during a normal condition period are calculated for each turbine, so as to get rid of the influence of the operating conditions. At the farm level, the residuals are then compared to a wind farm reference in a multi-turbine approach to obtain fault indicators insensitive to environmental conditions. Indicators for the objective performance evaluation are also proposed to compare wind turbine fault detection methods, which aim at evaluating the cost/benefit of the methods from a production manager＊s point of view. The performance of the proposed combined mono- and multi-turbine method is evaluated and compared to more classical methods proposed in the literature on a large real data set made of SCADA data recorded on a French wind farm during four years
: it is shown than it can improve the fault detection performance when compared to a residual analysis
limited at the turbine level only.

Introduction

Renewable energy has been a growing sector for several years, because of the necessity to reduce CO2 emission in the near fu- ture. The electrical power produced by wind turbines has been multiplied by 10 in the last 10 years. A significant amount of the electricity consumed in the world now relies on the electric power produced by wind farms, which have to be operational all along the year. Failures may cause important production losses, mainly due to the damages they cause and the time it takes to repair, which are no longer acceptable. This calls for a drastic change  in maintenance solutions, which must switch from periodic and corrective to condition-based. One of the motivations, and a per- spective, for the work presented in this paper is to develop a fault detection procedure that can deliver information on a developing fault early enough so that it can be used for condition-based or predictive maintenance decision making: indeed an in-advance detection allows the maintenance decision-maker to better plan the maintenance operations.
From the sensor technology point of view, 3 technologies are possible to monitor a wind turbine, [1]:
Using high rate data from a Condition Monitoring System (CMS) monitoring e.g. vibration: the use of these data  for fault detection usually relies on signal processing methods aiming at identifying the signature of a fault, e.g. in the signal spectrum;
Medium rate 1-second SCADA data: these data can be used for fault detection using for example model-based fault de- tection and isolation (FDI) methods, [2];
Slow rate 10-minutes averaged SCADA data: in this case, the measured quantities are averaged on 10-minutes length windows.
Several condition monitoring systems (CMS) are available on the market [1,3每5]. They are based on vibration analysis for the most part. They require additional sensors to be placed on each nacelle to measure the vibrations of several components of the drive train. The highly sampled acceleration signals they produce must be analyzed by an expert using signal processing methods, so as to detect faults in progress. Indeed, the automatic decision systems developed still generate a high number of false alarms. These make them a costly solution to implement [6,7].
On the opposite, SCADA systems have been integrated in wind farms since the emergence of wind energy. They generate loads of cheap data containing useful information on the turbines state but the data low sampling rate, one average measure every 10 min, is not adapted to an accurate in-depth monitoring of wind turbines. However, their availability for free and the breadth of SCADA data scope have encouraged researchers to propose solutions to create SCADA based fault indicators [1,8]. The work presented in this paper is developed for 10-minutes averaged SCADA, classically recorded for production supervision purposes.
SCADA data monitoring relies on the numerous variables mea- sured mainly for production supervision purposes. Fault moni- toring approaches based on SCADA data differ in the way the data can be merged to synthesize a fault indicator residual, and can be categorized in ＆＆mono-turbine methods＊＊ or ＆＆multi-turbine methods＊＊, as proposed in [9]. Mono-turbine methods combine variables recorded on a unique wind turbine while multi-turbine methods combine variables coming from all the turbines within a wind farm or subset of a wind farm. In this work, it is proposed to use 10-minutes SCADA data in an innovative way, combining a mono-turbine (or turbine-level) and a multi-turbine (or farm- level) information processing.
Wind turbines SCADA variables are highly non-stationary be- cause of the frequent changes in operational conditions and of the variations in external conditions. Following a classical diagnosis approach [2], mono-turbine methods aim at explaining a SCADA variable evolution as a function of other SCADA variables recorded on the same turbine and thus generate residuals (unexplained part) for diagnosis purposes. Variables may be linked by causal relations
- a change in some variables induces a change in other variables-  or by similarity relations two variables evolve in the same way because they are submitted to the same excitations, [9]. The most common causal model in wind energy is the power curve [1], which expresses the link between the wind speed and the active power produced by the turbine. It can be used as a visualization tool by comparing the power curve built with data gathered during the current period with the reference curve provided by the construc- tor [10]. It can also be used to create fault indicators by measuring the difference between the produced power and its value predicted by a model using the wind speed. In order to reduce the dispersion of the power data, additional input variables may be added such as the air density, the wind direction [8,11,12], the rotational speed or the pitch angle [13]. The model may be a simple polynomial approximation, a Gaussian process model or an artificial neural network [14]. The fault indicators may be the difference at each sampling time between the measured power and the expected power or it can be a distance between a reference power curve and an on line curve built with the most current measures [8]. The limitation of this approach comes mainly from (i) the length of the training period, which has to be large in order to cover  all the wind speed and temperature ranges, and (ii) the difficulty to localize the fault once it is detected with the power curve. To address the localization issue, one solution is to split the wind turbine into smaller independent systems, such as the rotor, the gearbox, the generator, the transformer and the convertor and to build models of these reduced systems, [15,16]. One popular variable to be modeled is the temperature of some components, whose variations can be explained by changes in the operational conditions or in the outside temperature. Models explaining the temperature variations use at least the produced power, the na- celle temperature and the train rotation speed as input variables. They are learnt on data measured when the turbine operates in normal conditions. The fault indicator is defined as the difference between the actual measured temperature and the expected tem- perature, named residual. Models may be static, i.e. they use data measured at one sampling time [17,18] or dynamic, such as ARX models [19,20]. They can be simple linear regression models or more complex artificial neural networks [21每25]. Let note at this point that several published works follow a classical model-based fault detection and isolation approach (model-based FDI) to build fault indicators sensitive to faults, but robust to disturbances, for wind turbine monitoring, [26每30]. In these works, the residuals are built using state observers or parity equations, [2]. The proposed methods are usually implemented on wind turbine benchmarks, emulated by differential equations models where different faults can be simulated. They use high frequency SCADA data, recorded every second. Though the results obtained are very interesting, the problem addressed in these works is very different in nature from the one addressed in the present paper where 10-minutes SCADA data are used and the methods are validated on real data.
Another approach is to model the temperature of one com- ponent as a function of the temperature of some other reference components that should evolve in a similar way, such as the tem- peratures of two bearings but also the bearing temperature and the stator temperature [18] or the temperature of the hydraulic break and the bearing temperature [31]. Such models using similarity may be simpler than models using causal relations but they rely on two temperatures evolving in the same way.
Mono-turbine approaches merge variables from the same wind turbine to generate fault indicator residuals that are insensitive to changes in its operational conditions. However, these residuals remain sensitive to the variations in the external environment such as the wind direction, air humidity and so on. On the op- posite, multi-turbines approaches merge variables recorded from different turbines of the same wind farm in order to reduce the influence of the environmental conditions. Indeed, turbines of the same farm are submitted to the same weather conditions so variables should evolve in a similar way, somehow. [32] compares the behavior of different turbines in the same farm using curves displaying the temperature of a drive train bearing as a function of the produced power. The temperatures and powers are measured during a period of time and the curves from all the turbines are plot on the same graph. Faults can be identified visually when one of the curves deviates from the others. [10] measures the difference between the power curve given by the manufacturer and an actual power curve built using data gathered during a current period of time. The difference between the two curves can exhibit a loss in performance. The differences measured for all the turbines in the farm are compared one with each other to detect a turbine with a larger loss of performance, which can be due to a fault. [33] compares the evolution in time of the temperature of a component normalized by the external temperature for turbines from the same farm. The deviation of the temperature of one turbine from the others can be the symptom of a fault. [17] builds residuals from the differences in the normalized temperatures and concludes that changes in operational conditions can create themselves fluctua- tions that are too large to allow for reliable fault detection.
The literature review shows that most authors proposed meth- ods to build fault indicators using mono-turbine approaches and so get rid of the influence of the operating conditions on the fault indicators. Few authors adopt a multi-turbine approach that allows getting rid of the influence of the external conditions, and mostly as a visualization tool. A method to synthesize a fault indicator for each turbine in a farm by comparing the temperature measured on a turbine to a farm reference (average or median of temper- atures measured on all the turbines within the farm) has been proposed in [34] and used in [35] on other types of measured SCADA variables. It has been shown that such an indicator remains sensitive to the operating conditions, which can be different from one turbine to another. Hence, no solution able to get rid of the influence of both the operating conditions and external environ- ment has been proposed thus far. Moreover, as stated by [36],  in their extensive review on wind turbines condition monitoring using SCADA data, there is a lack of published performance metrics to properly evaluate the advantage of one method from the others in terms of false alarm, true failure prediction and normal behavior prediction. To address these issues and fill this gap, we propose in this paper a hybrid multi-level synthesis method to take benefits of both approaches 每 mono- and multi-turbine 每 and to build fault indicators combining the two approaches. At the turbine level, residuals obtained with a mono-turbine model learnt during a normal condition period are first calculated for each turbine. At the farm level, these residuals are compared to a wind farm residual reference, in a multi-turbine approach. The use of mono- turbine residuals enables the influence of the operating conditions to be reduced while the use of a wind farm reference enables the changes in the environmental conditions to be accounted for. The performance of the method proposed is evaluated and compared to methods proposed in the literature on a large data set made of SCADA data recorded on a French wind farm during four years. Objective performance evaluation metrics are also proposed to compare the methods, which aim at evaluating the cost/benefit of the methods from a production manager＊s point of view. Fault indicators should warn of a progressing fault early enough for a maintenance operation to be scheduled. However, they should not generate false alarms with the extra cost of sending a maintenance operator team on a remote site for no reason.
The contribution of the work presented in this paper is then
twofold:
The first and main contribution of the hybrid multi-level approach proposed in this paper lies in the online real-time comparison of the prediction residual computed on one tur- bine (i.e. at the turbine level, using a turbine normal behavior model) to a farm-level reference prediction residual, com- puted using the prediction residuals from the neighbor wind turbines. This approach is said ＆＆hybrid multi-level＊＊ because

Second, the other contributions of the work presented in this paper are (i) the definition of objective performance metrics for the fault detection consistent with the applicative requirements of a wind farm manager and (ii) using these performance metrics, the evaluation of the proposed fault detection approach on a 4 years real data base from wind farm with 6 turbines, which shows that it has better performance than a residual analysis limited at the turbine level only.
The outline of this paper is as follows. Section 2 first presents different possible methods for the generation of fault indicators and focuses on the proposed combined mono- and multi-turbine approach developed in this work. The objective performance met- rics considered for the evaluation of the fault detection perfor- mance are also detailed in this section. The data set used to analyze the performances is described in Section 3 followed by a presenta- tion and a discussion of the results obtained.

2. Method
This section presents the proposed two-levels methodology to synthesize fault indicators by combining both a mono-turbine and a multi-turbine approach. Each of the two levels is described: synthesis of the fleet reference at the farm-level, and elaboration of a relevant turbine-level variable for comparison to the fleet reference. Finally, the performance evaluation metrics, which are used latter in Section 3 to compare the obtained fault detection results with different fault indicators, are introduced.
2.1. Fault indicators synthesis

2.1.1. Farm-level: comparison of the turbine variables to a fleet refer- ence
In wind farms, turbines are part of a fleet: they are of the same make and are subject to the same environmental conditions (wind speed, external temperatures, . . . ). Thus, the SCADA variables monitored on turbines from the same farm should evolve in a similar way when the turbines operate under normal conditions. The rationale for the method proposed in this paper relies on this assumption. SCADA variables from different turbines are compared on line. A deviation between a variable computed from a turbine and a farm reference is an indicator of an abnormal behavior. Its general concept is presented in Fig. 1.
 SCADA variables or model residuals computed from SCADA variables are recorded on line, averaged over a given time period and compared to a fleet reference. The distance between the vari- able monitored on a wind turbine and the fleet reference serves as a fault indicator for the turbine.
Let V be a variable measured on a wind turbine or a model residual synthesized on a wind turbine. V is assumed to carry in- formation on the turbine deterioration, and several ways to choose this variable V are considered in Section 2.1.2. Vj(k) is the value of the variable V from turbine j at time k. Let NT be the number of turbines in the wind farm. Let Win be an analysis window of size W . For each turbine j, with j varying from 1 to NT , a fault indicator Fj(k) is built at time k, as follows:
1. For each turbine j in the farm, an averaged value of the turbine-level variable Vj is calculated over the NWj samples present in the analysis window Win:
k

it combines a mono-turbine (or turbine-level) step with a multi-turbine (or farm-level) step. To the best of the authors＊ knowledge, no other existing and published method proposes to compare online in real-time the prediction residual gener- ated on a given turbine to a farm reference synthesized from the prediction residuals generated on the neighbor turbines.

Fig. 1. Multi-turbine approach: generic principle for the synthesis of fault indicator Fj for wind turbine j. The choice of the variable V as a mono-turbine residual corresponds to the combined mono- and multi-turbine method proposed in this paper.




the number of samples NWj is smaller than NWL, V j(k) is not computed as it would not be representative enough.
2. The fleet reference V fleet (k) is calculated using the averaged values V j(k) at the turbine level, as follows:

The fleet reference is calculated using the median and not the mean, so as to make it insensitive to abnormal values generated by faulty turbines. For the practical implemen- tation, the fleet reference is computed if the number of averaged values V j(k) computed at time k is higher than a limit number NTL (i.e. information is available from enough turbines) and the reference should be representative of the farm normal conditions as long as more than half of the turbines used to calculate the reference operate in normal conditions.
3. A fault indicator Fj(k) is then calculated for each turbine j as the distance between the monitored variable Vj(k) and the

In practice, NWL  is set to  W  + 1, which means that the variable  V j(k) should be available during at least half the analysis window W . NTL is set to NT  1 , which means that V j(k) should be available for more than half the turbines of the farm for the fleet reference   to be computed.
2.1.2. Synthesis of a turbine-level variable for comparison to the fleet reference
At the turbine level, the variable V used to create the fault indicator may be of two kinds: it can be either a residual generated as the difference between a measured value of SCADA variable and its prediction by a normal behavior model, or in a more straight- forward way, it can be directly a measured SCADA variable, Fig. 1.
The variable V may be a residual, computed as the difference between a SCADA variable and its prediction by a normal behavior model. In this paper, in the same logic as using a component temperature as a fault indicator, the difference between the measured temperature of a component and its temperature predicted by a physics-related model is used. In the generator, the active power produced by the turbine, the rotation speed of the drive shaft, and the nacelle temper- ature can account for a change in a component temperature.

So a linear model relating these variables to the component temperature is proposed, Eq. (4)

with Tj(k) the component predicted temperature, Pj(k) the active power produced, ?j(k) the shaft rotation speed, Tnacellej
(k) the nacelle temperature, recorded by the SCADA system
and a, b, c and d constant parameters. To built a normal behavior model, the parameters a, b, c and d of the model are regression coefficients learnt using a classical least squares algorithm [2], on data gathered during periods when the turbine is operating under normal conditions. The variable V is then taken equal to the temperature residual Rs taken as the difference between the actual measured temperature and its predicted value, i.e. for turbine j at time k:

The normal behavior model (4) expresses normal tempera- ture variations due to the turbine producing electric power. Thus, the model is valid only when the turbine is in operat- ing conditions and, for the practical implementation, all the samples gathered when the power produced by the turbine is below a production threshold Thp , i.e. when the turbine is stopped, are removed from the analysis window Win.
This hybrid ＆＆mono-multi-turbine＊＊ approach consisting in generating a fault indicator by comparing the residual gen- erated for each (＆＆mono＊＊) turbine to a (＆＆multi-turbine＊＊) farm reference is the original fault indicator synthesis proposed and defended in this paper for its good performance and lower sensitivity to both internal (operating conditions) and external (environmental variations) influences. The main steps for the implementation of this fault indicator synthesis are sketched in Algorithm 1.
In a more direct way, the variable V may be a SCADA variable, relevant in itself as a fault indicator, such as a component temperature. As seen in the literature -e.g. [18,31]-, compo- nent temperatures are commonly used as fault indicators. An overheating component can be the sign of a mechanical prob- lem or of a cooling system failure. In this case, the variable V is taken as the monitored temperature T of a considered component, i.e. for turbine j at time k:

Reading or sensors errors may affect SCADA measures. Thus, in the practical implementation of this fault indicator, any temperature value outside an acceptable range is removed from the analysis window Win.
This approach based on the direct comparison of SCADA data of each individual turbine to a farm reference has already been proposed and assessed in [34] for temperature mea- surements, and extended to other kinds of measurements in [35]: it has been proved too sensitive to the influence of possibly different operating conditions between each indi- vidual turbine. It is considered in this paper for comparison purposes with the proposed combined mono- and multi- turbine method.


Algorihm 1: General algorithm for the combined mono- and multi-turbine fault indicator generation


Phase 1 - Offline learning phase

Phase 2 - Online implementation phase 

2.1.3. Fault detection based on the generated fault indicators
The occurrence of a fault on wind turbine j leads to a change in the corresponding fault indicator Fj. Ideally, a fault indicator should be sensitive to a fault occurrence, but insensitive to the vari- ations of the farm environment and of the wind turbine operating conditions: building such a fault indicator was the motivation to propose a combined ＆＆mono-multi-turbine＊＊ approach. Under these assumptions, the statistical properties, in particular the expected value, of the fault indicator Fj changes with a fault occurrence, and the fault detection can be performed using statistical hypothesis testing tools (e.g. testing  hypothesis H0	E(Fj)	?0j vs. H1 E(Fj)	?0j), and can be implemented by setting a threshold on Fj. In theory, in this classical setting, when no information is available on the properties of the fault indicator Fj in presence of a fault, the value of this threshold can be determined using the estimated or assumed statistical properties of Fj when no fault is present in order
to guarantee, for example, a false alarm rate. In our approach, we want to avoid any additional hypothesis on the distribution of Fj under the ＆＆no fault ＊＊ assumption, which could be different for each turbine and could possibly lead to a different threshold for each turbine. Consequently, in the following, our approach is to set the detection threshold for the whole wind farm, on the basis of the whole available historical 4-years database for the farm in order to get a given false alarm level.

2.2. Performance evaluation indicators

Fault indicators are to be used in wind farms to assist mainte- nance operators. They should allow the detection of faults occur- ring on a wind turbine without generating too many false alarms. However, in the literature on wind turbine fault indicators, no paper addresses the issue of the performance evaluation of the indicators proposed. Most of them merely report the ability of their indicator to detect a particular fault. In this paper, we chose to evaluate the fault indicators proposed from a wind farm produc- tion managers point of view. Fault indicators should definitely be able to detect faults on wind turbines, to avoid a major degradation and then a costly repair. However, they should be able to do it early enough for a maintenance operation to be scheduled. Another major issue is the cost of false alarms. A false alarm results in a useless maintenance action: a maintenance team has to be sent to the production site to inspect the supposedly faulty turbine. Because wind turbines are usually located in remote sites, the inspection takes time and consequently costs a lot of money. So, in this paper, we propose 3 performance evaluation indicators, able to make a fair and objective assessment of the indicator usefulness: the detection time before failure, the useless maintenance action number and the indicator persistence, which gives to the operator some indications about the relevance and performance of the fault detection process based on the proposed indicator.
Performance evaluation indicators are computed from a data base gathered on a wind farm during a period of length D, where faults occurred. The data base is previously split into normal con- ditions periods and faulty periods. A normal conditions period is a period of time where no fault occurred on a wind turbine. A faulty period is a period of time where a fault was continuously present on a turbine. It ends when the failure occurs, i.e. when the turbine stops functioning.
Let Th be a detection threshold, set on the fault indicators Fj(k). When the indicator exceeds Th, an alarm is raised. Th is set for the whole wind farm, i.e. the same value is set for all the turbines fault indicators, Fj(k) .

2.2.1. Detection time before failure
The detection time before failure measures the time separating the first time the fault indicator exceeded the detection threshold Th during a faulty period from the failure time, i.e. the end of the faulty period. It gives an estimation of the time left to the maintenance team to repair the fault before the failure.

2.2.2. Number of useless maintenance actions
A false alarm occurs when the fault indicator exceeds the de- tection threshold Th during a normal conditions period. The false alarm rate is converted into an equivalent ＆＆number of useless maintenance actions＊＊ so as to better consider its cost. A ＆＆false alarm day＊＊ is a day when at least one false alarm occurred on at least one of the turbines. A ＆＆false alarm period＊＊ is a period made of consecutive false alarm days. The shorter possible length for a false alarm period is one day. During a false alarm period, a maintenance team has to be sent once to the wind farm location. So each false alarm period results in a useless maintenance action. The useless maintenance action number is the number of times a maintenance team has to be sent to the wind farm during the period D; it is equal to the number of false alarm periods.

2.2.3. Indicator persistence
The persistence measures the percentage of time during which the fault indicator remains above the detection threshold Th during a faulty period. A persisting indicator makes the maintenance operator more confident in the occurrence of the fault than an indicator constantly being set on and off.

3. Results and discussion

3.1. Data base

The methods presented in Section 2 are applied on a set of real data, gathered on a French wind farm located in the south of France. The 6 wind turbines forming the farm are identical. They are of the same make, conceived to produce 2 MW, with a horizontal axis. SCADA data were recorded every 10 min during 4 years, from November 2009 till December 2013.
During this 4 years, 6 single faults affecting the generator oc- curred on different turbines at different times, some of them gen- erating a major failure and, consequently, the machine shutdown for several weeks, see Fig. 2:
Fault on two bearings: 2 bearings broke down on 2 genera- tors, because of a lack of lubrification. One of the generators had to be replaced. These 2 episodes are named ＆＆faulty bear- ing WT6 (FB_WT6)＊＊ and ＆＆faulty bearing WT9 (FB_WT9)＊＊, for faulty bearing on wind turbines 6 and 9.
? Faults on two stator windings: two generators were stopped

The residual Rsj(k) is generated using Eq. (5), with Tj the predicted bearing or stator temperature using the normal behavior model in Eq. (4). The temperature variations due to changes in the power production are accounted for by the model. The model is learnt for each turbine, during a normal conditions period of two months at the beginning of the 4 years recording. Whenever a fault occurs on a turbine, the turbine model is learnt just after the necessary repair is made, on a fixed period of 2 months.

This model residual Rsj(k) is then averaged to obtained Rsj(k), using Eqs. (1) with Vj Rsj ;
The average residual Rsj(k) is finally compared to the fleet reference Rsfleet (k) computed using Eq. (2) with the averaged residuals from all the turbines, to obtain the fault indicator RESmultij (k).
3.2.2. Implementation of classical fault detection indicators for com- parison
   For each turbine j, three other fault indicators are considered: two of them are generated using a mono-turbine approach, the third one is generated by a simple multi-turbine comparison of the raw measurements (no residual generation at the turbine level).
The two indicators Tmonoj (directly based on raw measure- ments) and RESmonoj (based on residuals) are implemented following a classical mono-turbine approach, i.e. using SCADA data recorded from the turbine j alone:


an over-heating period which damaged the winding insu- lating material. The generators had to be replaced. These episodes are named ＆＆faulty winding WT9 (FW_WT9)＊＊ and ＆＆faulty winding WT11 (FW_WT11)＊＊.
? Faults on two cooling systems: a fault occurred on the cooling

either the bearing temperature or the stator tempera- ture.
In this case, the SCADA temperature is averaged over
24 h and thresholded, in a way very similar to the traditional SCADA detection system.
loose. These episodes are named ＆＆faulty cooling system WT7 (FCS_WT7)＊＊ and ＆＆faulty cooling system WT10 (FCS_WT10)＊＊.
The SCADA data used to implement and compute the fault indicators are the bearing temperatures, the stator temperatures, the nacelle temperatures, the active power produced and the shaft rotational speed on the 6 turbines.

3.2. Implementation of fault indicators
Four fault indicators are implemented and compared using the performance evaluation indicators presented in Section 2.2 calculated on the 4 years of data: the first considered fault indicator corresponds to the proposed combined mono- and multi-turbine approach, the three others are more classical and are implemented for comparison purposes. Each fault indicator is calculated every 10 min, using a sliding window of size W 144 samples, i.e.
24 h. NWL  is thus set to 72 and NTL  to 4. When relevant, the    limit production threshold Thp, above which the temperature can be predicted, is set to 50 kW, mainly because production levels below 50 kW correspond to starting or shutting down transient behaviors. In the following, depending on the considered fault to monitor, Tj can be either the bearing temperature or the stator temperature.

3.2.1. Implementation of the proposed combined mono- and multi- turbine approach	     	     
The indicator Rsfleet (k) is computed for each turbine j, following the proposed combined mono- and multi- turbine approach, see Algorithm 1:

Tj(k) Tj(k) where Tj can be either the bearing temper- ature or the stator temperature.
In this case, the residual generated using Eq. (5) and the model in Eq. (4) is averaged and thresholded. The temperature variations due to changes in the power production are accounted for by the model. The model is learnt for each turbine, during a normal conditions period of two months at the beginning of the 4 years recording. Whenever a fault occurs on a turbine, the turbine model is learnt just after the necessary repair is made, on a fixed period of 2 months.
An indicator Tmultij (directly based on measurements) is im- plemented using a multi-turbine approach, using a fleet ref- erence

	
每 Tmultij (k)   T j(k)   T fleet (k), using Eqs. (1) and (2) with Vj Tj, where Tj is the bearing temperature or the stator temperature. The component temperature is averaged and compared to the fleet averaged temperature refer- ence.

3.3. Performance results

Fig. 3 shows the evolution of the 4 fault indicators during the four years when the SCADA data were recorded on the 6 turbines of the wind farm. The upper figures display the evolution of the mono-turbine indicators (temperatures on the left, residuals on the right), the lower figures the multi-turbine indicators (temper- atures on the left, residuals on the right). The detection thresholds corresponding to a useless maintenance action number of 10, 20 and 30 (i.e. a total of 10, 20 or 30 useless maintenance actions

Fig. 2. Occurrence of failures and ＆＆failure periods＊＊ on the 6 wind turbines of the considered farm during the 4 years recorded database - Bearing failures in red, cooling system failures in green and windings failures in blue. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Fig. 3. Evolution of the 4 indicators for the 6 wind turbines over the 4-years period. The colored shaded areas correspond to the failure periods shown in Fig. 2. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article. will be generated on the whole farm during 4 years) are also displayed. The faulty periods are delimited by vertical lines. The five faulty periods correspond to the following faults, in chronolog- ical order: FB_WT6, FB_WT9, FCS_WT7, FW_WT11 and FCS_WT10 and FW_WT9 during the same faulty period. One can see that the seasonal variations inducing a change in the external temperatures are visible on the mono-turbine fault indicators. Both the tempera- tures and residuals are higher in summer and lower in winter. The fault indicators monitoring the 6 turbines follow the same pattern. During faulty periods, abnormal increases in temperatures can be seen on the faulty turbines. However, the temperature increases are somehow buried in the variations observed in the temper- atures during normal conditions periods. The faulty indicator to normal indicator contrast ratio (defined in a similar way as a signal to noise ratio: the energy of the indicator during a faulty period on the energy of the indicator during a normal condition period) is rather low, which makes the detection of the fault with a minimum amount of false alarms using Tmonoj quite difficult. The evolution of RESmonoj is rather similar. Since it is a residual, its evolution varies around 0. Its standard deviation is reduced compared to Tmonoj since the variations in temperature due to the change in power production are accounted for by the model and thus removed but its mean remains affected by seasonal temperature variations. Abnormal changes due to faults are more visible on the indicators but the fault detection using a fixed detection threshold seems still difficult to do.
The use of a multi-turbine approach makes it possible to remove the external temperature variations (seasonal effects) affecting the mono turbine indicators since the temperature variations are also present in the fleet reference and  consequently  subtracted  from the residuals. The faulty indicator to normal indicator contrast ratio seems to be increased and it is all the more so when RESmultij  is used, since the residual is rid of variations due to external temper- ature changes and variations due to changes in power production. Whenever a fault occurs on a turbine, it is preceded by an increase in the corresponding turbine residual, which clearly stands out from the other residuals. A fault detection strategy using a fixed detection threshold seems much more feasible.

Fig. 4. Performance characteristics curves: advance detection time as a function of the number of useless maintenance interventions for 6 different faults: FB_WT6, FB_WT9, FCS_WT7, FW_WT11, FCS_WT10, FW_WT9.

To compare the four indicators performances, two performance curves plotting the persistence or the detection time before failure as a function of the useless maintenance action numbers are built for each of the 6 faults. The curves are similar to traditional ROC curves, [37]. For a given value of the detection threshold, the detec- tion time before failure (advance detection time), the persistence and the useless maintenance action number are calculated and form a point in the performance curves.
The two performances curves are displayed in Figs. 4 and 5. The performance curves of multi-turbine indicators based detection are displayed in plain lines and mono-turbine ones in dotted lines. One can see that for the faults FB_WT6, FB_WT9, FW_WT11 and FW_WT9, the detection implemented with multi-turbine indica- tors outperforms the detection based on the mono-turbine ones, as could be expected from Fig. 3. Using these multi-turbine indicators allows detecting the faults earlier with a reduced number of useless maintenance actions. If the detection threshold is set at its highest value, i.e. set to obtain zero useless maintenance actions, with RESmultij  it is still possible to detect all the faults at least 500 min
ahead of time i.e. about 4 days. If the threshold is decreased to allow
3 useless maintenance actions, the faults can be detected as early as 1200 min ahead of time i.e. about 8 days, which is sufficient to plan a maintenance intervention. When mono-turbine indicators are used, FB_WT6 and FW_WT11, two major faults, cannot be detected. The detection time before failure remains close to zero whatever the value of the detection threshold. FB_WT9 and FW_WT9 can be detected two weeks ahead of time, with RESmono or Tmono, but at the very high cost of more than 15 useless maintenance actions. The two faults where the mono-turbine indicators performances equal the multi-turbine indicators are the faults on the cooling systems, FCS_WT7 and FCS_WT10, which occurred suddenly and generated a sudden and large increase in the recorded temperature.
The detection time before failure as a function of the useless maintenance action number curves show that RESmultij performs slightly better than Tmultij . The performances are comparable on

FB_WT6, FW_WT9, FCS_WT11, FCS_WT7 and FCS_WT10 but improved with RESmultij on FB_WT9. However, the persistence as a function of the useless maintenance action numbers curves shows
the superiority of RESmultij . Indeed, even if the detection time be- fore failure is equivalent, the persistence is globally higher when RESmultij is used. This is an important criterion because the higher the persistence is, the higher the confidence the operator may have in the indicator. Based on this criterion, RESmultij clearly outper- forms all the other fault indicators on the considered real data set, especially the mono-turbine ones. This is illustrated in Fig. 6 where a zoom in time is made on the period preceding FB_WT9. Indicators are presented in the same order as in Fig. 3. The colored lines are the detection thresholds which set the number of useless maintenance actions at 10, 20 or 30. Their value is adapted to each indicator. One can see that, though a slight increase in Tmonoj can be visually observed during the period preceding the fault, this increase cannot be detected with a fixed threshold. RESmonoj can detect the fault but only because it exceeds the detection threshold for a very short period of time. This could be figured out from the persistence curve whose value remained close to zero. On the contrary, the multi-turbines indicators show an obvious increase, which can be easily detected. The fault symptom appears earlier in RESmultij and the indicator remains more often above the detection threshold, which results in a higher persistence.

4. Conclusion

In this paper, we propose a new hybrid multi-level strategy to build fault indicators to monitor turbines within a wind farm, following a combined mono- and multi-turbine approach. The approach proposed reduces the effect of changes in the operational and environmental conditions on the fault indicators. Residuals computed from a mono-turbine model are compared to a fleet reference residual to form a multi-turbine fault indicator. The mono-turbine residuals are less sensitive to the internal variations because the SCADA temperatures are first processed using a model explaining the variations of the temperature in function of the operational conditions. Rises in the temperature due to increases in the power produced or in the rotational speed are accounted for by the model and removed from the residuals. The mono-turbine residuals are further post processed in a multi-turbine approach, which enables the impact of the variations in the environmental conditions to be reduced. Indeed, turbines in a same wind farm are part of a fleet. They are of the same make and model, and their variables evolve in a similar way since they are submitted to the same environmental conditions. New performance evaluation criteria were proposed to analyze the indicator performances for fault detection purpose, keeping in mind the cost of any useless maintenance action. The performance of the combined mono- and multi-turbine fault indicators were compared to more tradi- tional mono-turbine indicators on a data base made of SCADA data recorded every ten minutes from a real wind farm on a four year period. As far as we know, this is the first time a paper presents such an extensive analysis of fault indicators performance for fault detection, on real data with a wind farm production manager point of view. The results clearly showed that the effects of changes in both environmental and operational conditions on the combined mono- and multi-turbine residuals were reduced during the fault- free periods. This results in the combined mono- and multi-turbine fault indicators being able to detect all the faults occurring on the farm with a limited number of useless maintenance interventions and with an increased persistence.

Optimal Time Interval Between Periodic Inspections for a Two-Component Cold Standby Multistate System
Abstract¡ªThe establishment of the optimal time interval be- tween inspections for multistate redundant systems considering availability and costs related to maintenance and production losses is a challenging issue. This paper extends previous research for redundant multistate systems where the time-to-repair cannot be neglected. Discrete-time Markov chains are used to define transition probabilities between the different system states and the costs related to each transition. To optimize the time interval between inspections, the total cost is minimized utilizing the Markov chains properties followed by a numerical search technique. Two models are analyzed and numerical examples are presented. System I is a binary system with cold standby redundancy and component repair, while system II is a multistate system with cold standby redundancy and component repair. The main contribution of the method presented in this paper is the establishment of the optimal time interval between inspections for cold standby systems comprised of components that have different levels of degradation and where the component state can be determined only through periodic inspections. Systems with these characteristics are widely applied in industry, but are still not fully modeled in the literature.
I.INTRODUCTION
    Although time-based preventive maintenance is a strategy widely applied in industry, it is known that this maintenance approach leads to repair and replacement of components before the end of its useful life, increasing maintenance costs. This fact contributes to foster the interest of organizations and research in the development of models that allows for the use of preventive maintenance based on condition, where different states of system degradation can be observed and the best state for repair and replacement can be established. These systems are called multistate systems (MSSs).
    MSSs with different degradation states allow for more accurate component aging prediction, being more realistic than approaches that consider binary states (operating or failed) components. MSS can have several degradation levels, from perfect operation to complete failure. Some examples of MSS are hydraulic bombs in petroleum refineries, software systems, power generation systems, and train engine bearings. MSS may consist of a single component or many components. As the number of states and number of components increases, reliability analyses become much more complex [1].
    There are situations where MSSs have one or more components in a cold standby position. In these cases, if continuous monitoring is not possible, periodic inspections are recommended. These inspections verify the state of each component at predetermined times and initiate component repair if a high level of degradation is detected. The time between inspections should be optimized to maximize availability and to minimize costs [2]. Frequent inspections can serve to improve the availability, but involve high costs of preventive maintenance. On the other hand, lengthy times between inspections reduce total costs of inspection, but increase the costs of corrective maintenance and downtime, since there can be a long period when the system is unavailable [3], [4]. The establishment of the optimal time between inspections is essential to ensure the required availability with the lowest cost possible.
    Despite recent growth in the number of papers addressing this topic, redundant MSSs have been studied since the 1970s. In one of the first works about this theme, El-Neveihi et al. [5] developed a basic theory for the study of systems with finite number of states. Even though Karpinski [6] established the optimal time interval between periodic inspections for an MSS of a single component using Laplace transforms, most research about MSS consider continuous monitoring, as can be seen in Zhang et al. [7], Hsieh and Chiu [8], and Sheu and Zhang [9]. Also, these approaches study two main problems: 1) the optimal redundancy allocation for system reliability [10], [11], and 2) the optimal time interval for component replacement [9], [12], [13].
    Some authors analyzed the reliability of systems with different levels of degradation, but do not classify them specifically as multistate. These authors based their studies on Markov chains methods [14], [15]. However, it was only after the study presented by Levitin and Lisnianski [10], [16] that many studies on redundant MSS emerged utilizing the universal generating function (UGF) method.
    At the first moment, these authors developed a method for the joint optimization of redundancies and replacement intervals for MSS using UGF and genetic algorithms [10]. Later, the authors improve the method considering imperfect maintenance [16]. This method has been used along with Markov processes in recent studies concerned with MSS [2], [9], [11], [12], [17], [18].
    In addition to Markov process and UGF, some recent studies are applying recursive methods and Lz-transforms method to analyze the reliability and optimize parameters of MSS [19], [9], [20], [21]. The focus of Sheu and Zhang [9] and Lisnianski et al. [21] is optimization of age replacement time and Sheu et al. [19] focus in optimization of preventive maintenance schedule. Most of the studies on MSS analyze complex systems subject to continuous monitoring assuming that repair starts immediately after the failure occurrence. These papers aim to optimize the acquisition, allocation, and redundancy level along with the establishment of the optimal time interval to replacement. Just a few studies consider MSS subject to periodic inspections where the components and system state are verified only at inspections. However, this is a situation observed in many industrial scenarios, especially when the analysis of some component requires local inspection and the access is difficult. In these cases, the components are only monitored periodically.
    Le and Tan [22] optimized sequential inspections and continuous monitoring for an MSS assuming instantaneous repair. In this system, inspections are performed after a warning given by a continuous monitoring system, and repair or replacements are performed for the whole system and not for individual components. Lu et al. [20] utilized non-homogeneous continuous-time Markov chains, z transform, and particle swarm optimization to optimize the maintenance threshold and the inspection intervals of an MSS with five maintenance effects. Ruiz-Castro [23] analyzed the performance of an MSS with and without preventive maintenance using Markov process and reward processes algorithmically. This author considers a system subject to internal failures and external shocks with random inspections. Using semi-Markov process and Monte Carlo simulation, Koutras et al. [24] established inspection time and maintenance policies that optimize the dependability and/or performance of an MSS with deterioration.
    Considering a binary redundant system with instantaneous repair and constant failure rate, Alebrant Mendes et al. [25] established the optimal time interval between periodic inspections using Markov chain and search technique.
    As shown in the previous paragraphs, the interest for studies that address MSS subject to inspections has increased in the last few years. Nevertheless, none of them address the optimization of the time interval between periodic inspections of a system in which the components are only monitored periodically. Also, the application of discrete-time Markov chains to solve such problems is a challenging novelty.
    The main contribution of the method proposed in this paper is the analysis of redundant MSS comprised of components that have different levels of degradation that can be determined only through periodic inspections. This configuration is common in systems that cannot be continuously monitored and are difficult to access, such as, internal components of heavy equipment where substantial disassembly effort is necessary to access and verify components. The establishment of the optimal time interval among inspections, considering inspection, failure, and unavailability costs is important to ensure the desired reliability with the lowest cost possible.
    The complexity of problems involving redundancies increase when the time-to-repair is considered as a random variable distributed according to some defined parametric distribution. Usually, the authors suppose that repair times are exponentially distributed or fit a phase-type distribution, making the problem solution easier using Markov processes, as seen in Tian et al. [17], Mine and Kawai [26], Liu et al. [11], Lu et al. [20], and Montoro-Cazorla and Perez-Ocon [27].
    The study of systems having different levels of degradation using Markov processes also becomes complex as more components and degradation states are added. Soro et al. [28] and Sheu and Zhang [9] took into consideration only the degradation levels of the whole system, reducing the number of variables in the model and allowing the inclusion of many degradation states. In those studies that focused on the degradation states at the component level, as seen in Mokaddis et al. [15], Montoro-Cazorla and Perez-Ocon [27], Jia et al. [29], and Guilani et al. [30], only a few states and components were considered, since increasing the number of states and components substantially increases the number of equations needed to describe the problem.
    This paper presents a model for systems with two components, in which one of them is in the cold standby position, and having three levels of degradation besides the failure state. The addition of components into the system increases the number of equations and the space required to describe them, but the model can be generalized and utilized to determine the optimal time interval between inspections of redundant MSS with n components.
    The maintenance of a redundant MSS is a stochastic process since it has n components and each component can be in any level of degradation or failure at any time. Each combination of component states represents a likelihood in the state space of a Markov chain process. When periodic inspections are performed, the system state (different levels of degradation or failure) is verified at discrete times (only during the inspections). This scenario justifies the application of discrete-time Markov chains to study such problems.
    This paper is organized as follows. Section II describes the system, lists the assumptions and notation used, and ex- plains the methodology applied for modeling the problem. In Section III, the problem is solved defining the transition prob- abilities and modeling the costs of the system. In Section IV, numerical examples are presented and analyzed. Section V summarizes the paper and includes concluding remarks.
II.METHODOLOGY
    This section describes the system modeled and the assumptions accepted.
A. System Description
    This paper builds upon and extends the method developed by Alebrant Mendes et al. [25] to establish the optimal time interval between periodic inspections. Since the time-to-repair cannot be neglected in many situations, this study first extends the proposed method to incorporate this aspect. Through the inclusion of the time-to-repair in the model, its effect in the system reliability and costs can be analyzed. Next, an additional aspect was modeled. Given that many redundant systems suffer degradation during operation, and that, in many cases, the degradation level can be verified through inspections, the method was extended to accommodate MSS and inspection. The systems analyzed in this study are presented next.
    These systems reflect real systems installed in a petrochemical company that operates in southern Brazil. This is a large company comprising several operational systems, many of them 1) in a difficult-to-inspect position (demanding some disassembling for inspection) and 2) protected by redundancy. The operating condition of some of these difficult-to-inspect systems cannot be properly evaluated and, as far as detection is concerned, they change directly from operational to failed state (as system I described next). The operating condition of other difficult-to- inspect systems, however, may be evaluated and a maintenance decision may be taken based on that evaluation (as system II de- scribed next). Regarding the numerical example (presented in Section IV), in order to protect confidential information, the real numbers concerning costs, mean time between failures (MTBF), and mean time-to-failure (MTTF) were modified, but the aver- age proportion [between MTBF/mean time-to-repair (MTTR) and among costs] observed in the petrochemical company was preserved.
    a) System I ? Binary system with cold standby redundancy and component repair
    In this system, one component is active and the other is in a cold standby position. When the active component fails, the cold standby component is instantaneously activated. During the periodic inspection, the binary state (operational, failed) of each component is verified and component repairs are per- formed when necessary. Time-to-repair is a random variable and the system operates unprotected (without redundancy) until the repair is completed. The repaired components return to the system in the cold standby position. The system fails when both components fail between two consecutive inspections or if one component fails while the other is under repair.
    b) System II ? Multistate system with cold standby redundancy and component repair
    In this system, one component is also active and the other is in a cold standby position. Each component has, besides the failure state, three different operational states related to its degradation level:
    1) excellent (e) ? component is operating in a perfect state;
    2) good (g) ? component is operating in a good state, but it is possible to verify some level of degradation; and
    3) poor (p) ? component is still operating, but in an advanced state of degradation, being recommended performing a repair to avoid failure.
    Components move from excellent to good and from good to poor, and if not repaired, from poor to failure state. Components never fail before passing through all levels of degradation. However, the system can pass through more than one state and fail between two consecutive inspections. When the active component fails, the cold standby component is instantaneously activated and experiences the same degradation levels of the previous component. A component is sent to repair if a poor or failure state is detected during inspection.
C. Assumptions
    This study has the following assumptions for system I:
    1) components have only two states (operating or failed);
    2) perfect and instantaneous switching;
    3) times-to-failure follow an exponential probability distribution;
    4) components have different parameters for the probability distribution of times-to-failure and equal parameters for the probability distribution of times-to-repair;
    5) components in cold standby position do not degrade or fail;
    6) each inspection is perfectly and instantaneously performed; and
    7) perfect repair.
    For System II, the assumptions are:
    1) components degrade incrementally, experiencing all degradation states before fail;
    2) perfect and instantaneous switching;
    3) times-to-transition between degradation states and times-to-repair fit an exponential probability distribution;
    4) components have different parameters for the probability distribution of times-to-transition and equal parameters for the probability distribution of times-to-repair;
    5) components in cold standby position do not degrade or fail;
    6) each inspection is perfectly and instantaneously performed; and
    7) perfect repair.
    This study analyzes systems where failures are neither detected nor repaired until an inspection is performed. If the system failure occurs before the inspection, the downtime results in an increase of costs. These costs increase until the next inspection. When the system is still working and a component failure is detected, the component is repaired (with an associated cost and time) to preserve the reliability level. In system II, if inspection detects a poor condition, the component is also repaired (with an associated cost and time) to prevent system failure.
    Markov chains are an attractive model for this class of problem because complex behavior can be represented efficiently, and an appropriate mathematical model developed and used to optimize design or maintenance. However, it should also be clearly stated that use of Markov chains requires a series of assumptions that are mostly appropriate and reasonably accurate. There may be cases where the assumptions simply are not valid and the models proposed in this paper should not be applied. For example, when a repair is ongoing and an inspection occurs, it is assumed that the random repair time begins again.
    Exponential probability distributions were used to facilitate the use of Markov chains. The exponential distribution is appropriate and convenient to model times-to-failure, times-to-repair, and times-to-state transition for many components. However, if it is not the case, other approaches, such as Monte Carlo simulation, should be utilized.
III. RELIABILITY AND COST ANALYSIS OF REDUNDANT SYSTEMS SUBJECT TO PERIODIC INSPECTIONS
    In this section, the transition probabilities for the two systems are developed using Markov chains. Next, the costs related to system maintenance are determined and a cost function is developed and minimized to find the optimal time interval between inspections.
A. Definition of Transition Probabilities Using Discrete-Time Markov Chains
    A Markov chain is a process consisting of a finite number of states and known transition probabilities pij , where pij is the probability of transitioning from state i to state j. The probabilities pij depend only on states i and j, not depending on the time or previous number of transitions n or the previous states visited. The set of states in a Markov chain process is called state space [31].
    Considering that each different configuration possible to be found during an inspection is a state and that transition times from one to another state are random variables, it is possible to define a state space diagram for both systems being studied, as presented next in this paper.
    Every time a periodic inspection is performed, the system state is verified. Once in a failure state, the system does not change to another state anymore. Each state of system failure is an absorbing state of the Markov chain. The state space diagram and the transition matrix of each system are presented as follows. 
    The model corresponds to two-component systems. In theory, a corresponding Markov chain could be developed for larger systems with more components, but practically, it would become inefficient and cumbersome. Therefore, for larger systems, alternative models might be more practical. Stochastic Petri nets and Monte Carlo simulation have demonstrated much promise for problems of this type
    a) System I ? Binary system with cold standby redundancy and component repair: The state space diagram for system I is presented in Fig. 1. The circles Sn represent each state from the state space. Each letter inside the circles represents the com- ponent state: operating (O), standby (S), repair (R), or failure (F). Each number associated with each letter inside the circles represents one component (1 and 2). For example: the state S0 has the component 1 in operating state O1 and the component 2 in cold standby state S2. The states S6, S7, and S8 represent system failure and are absorbing states for the purpose of this study.
    The matrix P1, in Appendix B, shows the transition prob- abilities for system I. For example, the probability p30 is the probability of transitioning from state 3 to state 0. This prob- ability is the probability of component 2 being repaired and component 1 being operative during the time interval between inspections.
    Since the time-to-failure (TTF) of the active component, the time when the standby component starts to operate, and the time- to-repair are random variables, standard probability theory was used to determine these probabilities. For example, the probability p30 of transitioning from state 3 to state 0 can be determined as the probability of the time-to-repair of component 2 being smaller than the time interval between inspections combined with the probability of the TTF of component 1 being higher than the time interval between inspections: Pr R2 < ¦Ó T1 > ¦Ó  . These probabilities are expressed by 15 equations. Equations (1)?(7) are presented next. The other equations can be checked in Appendix A.
    1) Probability of component 1 remaining operational during the time interval between inspections:
    2) Probability of component 1 having failed and component 2 starting to operate but not failing during the time interval between inspections:
    3) Probability of components 1 and 2 failing during the time interval between inspections:
    4) Probability of component 2 remaining operational during the time interval between inspections:
    5) Probability of component 2 having failed and component 1 starting to operate but not failing during the time interval between inspections:
    6) Probability of component 1 being repaired and component 2 remaining operational during the time interval between inspections:
    7) Probability of component 1 being repaired before component 2 fails, given that the component 2 fails and component 1 starts to operate during the time interval between inspections:
    Considering (1)?(7) presented here and (32)?(39) presented in Appendix A, P1 can be rewritten. The matrix P1 can be checked in Appendix B.
    b) System II ? Multistate system with cold standby redundancy and component repair: The state space diagram for system II is presented in Fig. 2. The circles Sn represent each state from the state space. Each letter inside the circles represents the component state: operating in an excellent state (Oe), operating in a good state (Og), operating in a poor state (Op), cold standby (S), repair (R), or failure (F). Each number associated with each letter inside the circles represents one component (1 and 2). For example, the state S0 has component 1 operating in excellent state O1e and component 2 in cold standby state S2.
    Fig. 2 shows that there are 21 possible states (S0 to S20) in the state space of system II. The state of the components of each system state is presented in Fig. 3.
    The system starts with component 1 operating in an excellent state and component 2 in a cold standby position. The system can stay in the same state or move to another state. Repair of failed components starts just after the next periodic inspection. Components in a poor state of operation are also sent to repair after the periodic inspection, if the other component is available to operate. If the other component is being repaired, no action is taken during the periodic inspection. However, if the other component is in a failure state, the failed component is sent to repair during the periodic inspection. In both cases, the system stays operating with the component in a poor state.
    Times-to-repair of each component vary according to their probability distribution. The system fails when all components fail during the time interval between inspections or if a component fails while the other component is being repaired. After failure, the system is not repaired. Thus, states S18, S19, and S20 are absorbing states.
    Matrix P2 (see Appendix B) presents the transition probabilities of the system. The probabilities in matrix P2 were solved using probability theory. For example, the probability p4,15 is the probability of component 1 being repaired before component 2 having failed and, in the sequence, component 2 failing and component 1 degrading until the poor state of operation, during the time interval between inspections. This probability can be calculated through the combination of three probabilities:
    1) probability of the time-to-repair of component 1 being smaller than the sum of the times-to-degradation and failure of component 2;
    2) probability of the sum of times-to-degradation and failure of component 2, plus the time-to-degradation of component 1 until the good state of operation, being smaller than the time interval between inspections; and
    3) probability of the sum of times-to-degradation and failure of component 2, plus the times to component 1 degrading until the poor state of operation, being higher than the time interval between inspections:
    The calculation of these probabilities follows the same methodology used to solve the probabilities of system I. Some probabilities and their solution in terms of integral equations are presented in the following. The probabilities were solved using the software MATLAB 7.9 R2009b.
    The system has a total of 79 equations that represent the states transition probabilities. Two examples of these probabilities and their solutions are presented next:
    1) Probability of component 2 degrading and failing after component 1 has been repaired and component 1 starting to operate and degrading from the excellent state to the poor state of operation, during the time interval between inspections:
    2) Probability of component 2 degrading from the excellent state to the poor state of operation and component 1 remaining under repair, during the time interval between inspections:
B. Cost Models
    Optimizing the measure of performance per unit of time is equivalent to optimizing the measure of performance over a long period. Thus, the cost models in this paper are based in the establishment of the maintenance total cost per cycle. In this application, a cycle is the period of time elapsed from the beginning of the system operation until its failure:
    Four elements were considered to determine the costs associated with the redundant system subject to periodic inspections:
    1) cost of periodic inspection (Ci);
    2) cost of component repair (Cr);
    3) cost of downtime per time (Cp); and
    4) cost of system repair (Cs).
    The cost of periodic inspections comprises costs of manpower, tools and materials required to perform the inspection, even if there are no components in the failure state. The cost of component repair includes costs of manpower, tools, replacement of parts, and materials utilized to repair failed component. The cost of system repair is related to costs incurred to reactivate the system to its completely operational condition after a system failure. Finally, the cost of downtime refers to the production losses during the time that the system is down. This cost includes the loss of sales opportunity and monetary fees for delivery delay.
    The costs of periodic inspection, component repair, and system repair are assumed to be constants and must be determined for each system individually. On the other hand, the cost of downtime is a function of time, because losses increase along with the length of system unavailability. To establish this cost, it is necessary to determine the expected downtime.
    For a binary system (system I) comprised of two components (1 and 2) in parallel, in which one component is in cold standby position and both components have times-to-failure that follow an exponential probability distribution, the mean time to system failure (MTTFij ), given that the system is in state i and goes to state j that is a failure state, is calculated as:
    For an MSS (system II) comprised of two components (1 and 2), in which one component is in cold standby position and both components have times-to-transition that follow an exponential probability distribution, the mean time to system failure (MTTFij ), given that the system is in state i and goes to state j that is a failure state, is calculated as following. The letters associated with the parameter ¦Ë represent the component operation state:
    Consider a system having only one component with times-to-transition following an exponential distribution. Supposing that this component does fail during the time interval between inspections (¦Ó) and that this interval is relatively small when compared with the expected TTF, the expected downtime would be:
    Consider a system with two components where times-to- transition follow an exponential distribution. Supposing that these components both fail during the time interval between inspections (¦Ó) and that this interval is relatively small when compared with the expected TTF, the approximated expected downtime would be:
    Equations (25) and (26) are approximations that are appropriate when the time interval between inspections is smaller than the expected time to system failure, as usual in industry. Exact relations could be derived based on a truncated conditional distribution. However, this approximation is useful for the purpose of this study, given the complexity of the cost rate model.
    The approximations in (25) and (26) are based on three observations:
    1) all transitions related to downtime are associated with a system failure, so the expected downtime is conditional on the occurrence of a system failure (all components in operation in the beginning of the interval fail),
    2) the occurrence of failures in a cold standby system is modeled as a homogeneous Poisson process with rate ¦Ëi until the last component fails, and
    3) the probability of more than one failure for a Poisson process with rate ¦Ëi is small for an interval shorter than the expected time to system failure.
    Combining 2) and 3) it is possible to approximate the occurrence of exactly n failures during the interval, where n is the number of components working in the beginning of the time interval between inspections. For a Poisson process, the expected occurrence of a particular number of failures is distributed uniformly in a fixed interval. So, for n = 1, the expected TTF is ¦Ó /2 and downtime is estimated as ¦Ó /2 + MTTR. For n = 2, the expected times-to-failure are ¦Ó /3 and 2¦Ó /3, where the second fail represents the system fail and downtime is estimated as ¦Ó /3+ MTTR. When the time intervals between inspections become longer than the expected TTF, observation 3) does not hold and this approximation is not appropriate.
    As the time interval between inspections increases, the expected downtime becomes longer than the system MTTF and the approximations used previously are no longer appropriate. For ¦Ó >MTTFij, the expected downtime can be approximated using (¦Ó MTTFij) + MTTR. As a result, the expected downtime for all cases can be approximated by:
    where n is the number of components that are in an operational condition at the beginning of the interval between inspections.
    When the inspection interval becomes much larger or much smaller than the expected failure time, the approximation in (27) converges to the exact solution, and for other cases it provides a reasonable approximation that is practical and efficient. If a decision maker desires an exact solution, a more rigorous probability model could potentially be developed [31]. However, for most applications, there is unlikely to be a detectable difference in the optimal inspection intervals selected.
    The cost matrix of each system is presented next. The costs showed in the matrices represent the costs incurred for each transition between two states in the respective system state space. For both systems, for each inspection, cost of inspection is computed. Whenever a component fails, cost of repair is added to the model. Whenever the whole system fails, the cost of inspection, cost of system repair, and cost of downtime are summed. The matrices C1 and C2 show the costs for systems I and II.
    After determining the costs involved in each redundant system subject to periodic inspections, it is necessary to combine these costs to establish the total cycle cost. This total cost can be calculated utilizing the discrete-time Markov chain properties. Accordingly to these properties, the expected number of times that the process passes in the transient state j, given that it started in the transient state i, is given by the matrix N [see (28) at the bottom of the page], where Q is the transient part of matrix P [31].
    Since the sum of each line in N reveals the expected number of discrete time steps before absorption, given that the chain began in the ith non-absorbing state, and using the probabilities and costs for each transition, the expected cost in a cycle can be calculated by:
    The length of the cycle depends on how many times the system goes from state 0 to the other states and how long the time between inspections is. Then, it is possible to calculate the expected length of the cycle by:
    Based on (29) and (30), the total cost as a function of ¦Ó can be rewritten as:
    The total cost in (31) can be minimized and the optimal time interval between inspections can be established setting values for the costs of inspection, component repair, downtime, and system repair. Since the time interval between inspections is the only variable in the equations, it is possible to determine the minimum total cost using numerical search techniques.
    Genetic algorithms as other meta-heuristic search algorithms could be utilized to obtain solutions for this problem, but this approach was not necessary. This problem has a relatively complex objective function of costs, but its optimization is performed for only one decision variable: time interval between inspections. Consequently, a simple one-dimensional numerical search is sufficient. As this model is extended, there will be more advanced multivariable problems requiring methods such as genetic algorithms.
NUMERICAL EXAMPLES
    Numerical examples for each system studied are presented and analyzed next. For comparison purposes, identical parameters of costs and time-to-repair were utilized in both systems. Equivalent parameters of MTTF were considered. The MTTF of each component in system I (1¦Ë1 and 1¦Ë2), which is a binary system, is equivalent to the sum of mean times-to-transition between states excellent to good, good to poor, and poor to failure of each component in system II (1¦Ë1e + 1¦Ë1g + 1¦Ë1p and 1¦Ë2e + 1¦Ë2g + 1¦Ë2p), assuming that the system is not inspected and repaired.
    The parameters values were determined to represent real systems scenarios. Despite being similar, components often have failure rates that are different. The cost parameters utilized are selected based on real scenarios where the cost of inspection and component repair is smaller than the cost of system repair and downtime. Figs. 4 and 5 show the total cost per cycle per month and the optimal time interval between inspections in days for systems I and II, respectively. The parameters utilized were:
    1) System I?Different parameters of exponential distribution for times-to-failure for each component: ¦Ë1= 0.005, ¦Ë2=0.006 (these parameters represent MTTFs of 200 and 166.67 days, respectively).
    2) System II ? Different parameters of exponential distribution for times-to-transition between different states of degradation for each component: ¦Ë1e=0.02, ¦Ë1g=0.01, ¦Ë1p=0.021, ¦Ë2e=0.025, ¦Ë2g=0.015, and ¦Ë2p=0.026 (these parameters represent MTTFs of 200 and 166.67 days).
    3) Equal parameters of exponential distribution for times-to- repair: ¦Á1 and ¦Á2=0.05.
    4) Ci=10 000 and Cr=30 000: cost of component repair is higher than cost of inspection.
    5) Cs=100 000 and Cp=100 000/units per day: downtime cost is higher than the other costs. It means that unavailability incurs higher costs than those associated with inspection and repair. See the equation shown at the top of the next page.
    Fig. 4 shows that the time interval between inspections that minimize the total cost for system I is 14 days. In the same way, Fig. 5 shows that the optimal time interval between inspections for system II is 28 days. Even with equivalent MTTFs, the optimal value for the time interval between inspections is quite different between the two systems presented. The binary system has a higher minimum total cost and requires a time interval be- tween inspections smaller than the MSS. This happens because in system II it is possible to identify an advanced level of degradation and perform the repair before the failure has happened. This reduces the costs related to system failure and downtime.
    Moreover, results show that the model developed for the optimization of binary systems is not effective for MSSs. The analysis of MSSs assuming that they have a similar behavior to binary systems can lead to an unnecessary reduction of times interval between inspection and a consequently increase of the total costs.
    A sensitivity analysis was conducted to analyze the effect of different parameters (¦Ë1e, ¦Ë1g, ¦Ë1p, ¦Ë2e, ¦Ë2g, ¦Ë2p, ¦Á1, ¦Á2, Ci, Cr, Cs, and Cp) in the total cost per cycle of system II. Fig. 6 presents six graphs. Fig. 6(a) and (b) shows the effect of ¦Á in system with different parameters of ¦Ë1e, ¦Ë1g, ¦Ë1p, ¦Ë2e, ¦Ë2g, ¦Ë2p. Fig. 6(c), (d), and (f) shows the effect of Ci, Cr, Cs, and Cp, on the total cost and the optimal time interval between inspections.
    Accordingly to Fig. 6(a), higher repair rates (¦Á) increase the minimum CT while the optimal times interval between inspections remain approximately the same. When Fig. 6(b) is com- pared with Fig. 6(a), it shows that smaller rates of degradation and failure reduce the system total cost. Fig. 6(c) shows that higher costs of inspection (Ci) cause higher minimum CTs and times interval between inspections. Fig. 6(d) demonstrates that costs of repair (Cr) have a small effect on CT and that this effect does not change over the range of ¦Ó. Fig. 6(e) demonstrates that costs of system repair (Cs) also have a small effect on CT. Finally, Fig. 6(f) shows the effect of costs of downtime on the minimum CT. Costs of downtime have a higher effect on minimum CT and this effect increases when ¦Ó increases.
    There are two ways to generalize the solution of the problem presented in this paper to consider systems consisting of a larger number of components:
    1) the exact solution can be obtained via Markov chains by implementing in software the same reasoning presented in this paper for the two-component systems. Software support would enable to cope with systems constituted by a larger number of components, since the software could handle the hundreds or thousands of transition states that would be established in cases involving a larger number of components.
    2) An approximate solution could be obtained using Monte Carlo simulation. The Monte Carlo simulation would have an important advantage related to the possibility of assuming any probability distribution for the variables involved. In any case, it is worth mentioning that safety systems usually have a backup (as the problem presented in this paper), but a relatively small number of backups, usually two, three, or four components in parallel. This makes it feasible, though laborious, to develop a software or to structure a Monte Carlo simulation.
    Aiming to solve the generalization problem by simulation, discrete event simulation approach could be used to develop the simulation framework along with Monte Carlo simulation to generate the random numbers that represent the probabilities that are characteristic of stochastic processes. The distributions parameters and the costs variables would be the simulation input parameters. As output, we would have the systems TTF, the numbers of inspections and repairs, and the total cost per cycle. The variables of the simulation would be:
    1) the time variable t (referring to the amount of time that has elapsed until the end of the simulation) would be represented by the system TTF;
    2) the counter variables would be the number of inspections, number of repairs, and time of operation; and
    3) system state variable (that describes the system state at t) would be all the system states possible to be found during a periodic inspection.
    The periodic inspections would be the events when the values of the variables would be changed or updated and relevant out- put would be collected. The random variables generated through Monte Carlo simulation would be used to represent the probabilities of the distributions of times-to-transition and times-to- repair. Having the probability, it would be possible to determine the distribution matching time that represents a random time-to- transition or time-to-repair. Every time, when we run the simulation, we would get as output an independent random variable that would have always the same probability distribution. The average of these values would be then used as an approximator of our variable of interest. In order to have a quality approximator and an acceptable error, it would be necessary to run the simulation many times and collect an appropriate number of random variables. The logic that would be needed to build the simulation is similar to the logic developed to solve the problem using Markov Chains. However, the equations that would be needed to be solved are much simpler, because it would not have integral or differential calculations.
V. CONCLUSION
    This paper presented a model to establish the optimal time interval between periodic inspections for MSS with redundancy in cold standby position utilizing discrete-time Markov chains.
    MSS in which different states of degradation are identified captures in detail the component aging, representing in a better perspective many of real situations than other approaches that consider systems as binary (operating or failure state). When continuous monitoring is not possible, periodic inspections are required. During these inspections, component states are verified and repair is started when a failure state or an advanced state of degradation has been detected. The time interval between inspections must be optimized to minimize costs. Frequent inspections improve the system availability, but require higher costs of preventive maintenance. On the other hand, long periods be- tween inspections reduce the costs associated with inspections, but also increase the costs of corrective maintenance, downtime, and costs related to safety accidents. The establishment of an optimal time interval between inspections is important to ensure satisfactory system availability along with the lowest cost possible.
    The method presented in this paper uses discrete-time Markov chains to define both the transition probabilities between system states and the costs related to each transition. To optimize the time interval between inspections, the total cost of maintenance was modeled and minimized using the Markov chains properties and a numerical search technique. The minimal cost and the optimal time interval between inspections were obtained taking into consideration the costs of unavailability (downtime) and maintenance (inspection, component repair, and system repair).
    Two redundant systems in cold standby position were modeled and numerical examples for systems comprised of two components were presented: 1) System I ? Binary system with cold standby redundancy and component repair; 2) System II?MSS with cold standby redundancy and component repair. Results reveal that the binary system has a minimum total cost higher and requires a time interval between inspections smaller than the MSS. This happens because in MSS it is possible to identify advanced levels of degradation and perform the repair before system failure. This reduces the dominant costs related to downtime. Results also show that the model developed for the optimization of binary systems is not effective for MSS. The analysis of MSS assuming they have the same behavior of binary systems conducts to an unnecessary reduction of time intervals between inspection and a consequently increase of the total costs.
    This model pertained to a system with cold standby redundancy. An analogous model could also be developed for active redundancy, or in an extended research effort, a system using both active and cold standby together.
    For future research directions, one promising option would be the establishment of the optimal number of redundant components for an MSS comprised of identical components, aiming to achieve predetermined reliability requirements. Also, a model with imperfect maintenance or immediate detection of failures would be interesting and practical for some applications. Furthermore, an interesting improvement in the model developed could be the assumption of the cost of repair as a function of time. This would represent the inclusion of another random variable into the model.
    
Reliability optimization of series-parallel systems with K-mixed redundancy strategy
    This paper revisits the redundancy allocation problem, a well-known problem in reliability optimization, and implements a novel redundancy strategy, called K-mixed, to improve system reliability. The K-mixed strategy is a general form of a mixed strategy. The mixed strategy is a combination of active and standby redundancy strategies which was introduced in 2014. Initially, the mathematical formulation for calculating the reliability of the K-mixed strategy is investigated and then its power and efficiency are evaluated against different test problems and a famous benchmark problem. In order to solve the proposed benchmark problem, an efficient genetic algorithm is developed and the results are compared with those reported elsewhere. It is found that the proposed K-mixed strategy results in higher reliability in most situations than would otherwise be achieved by its mixed counterpart. It is concluded that by using the K-mixed strategy in an optimization model, system designers have the opportunity to select the best strategy for each subsystem from among all the different strategies available to design systems with maximum reliability.
1. Introduction
    High technology systems are mostly complex and expensive, but also need to be highly reliable because they are required to have important roles in modern industry. Reliability and mean time to failure (MTTF) are two fundamental measures of such systems. Reliability is de?ned as the probability that a system (or an entity) sufficiently performs its specific functions over a specific period of time under specific environmental conditions [1]. In order to improve the reliability of a specific system, one can use highly reliable components and/or a well- designed configuration to meet functional requirements and performance specifications. Finding the best configuration for a system by adding parallel components is called the redundancy allocation problem (RAP). RAP has been proved to belong to the NP-hard class of optimization problems [1]. In RAP, there are discrete component choices with known characteristics such as reliability, cost, and weight, where the objective is to find the optimal number of redundant components in each subsystem to maximize the overall system reliability subject to some constraints.
    The redundancy strategy is used to determine how the redundant components are used in the system (or its subsystems). Traditionally, there are two redundancy strategies: active and standby. The active redundancy strategy is based on the assumption that all redundant components start operation simultaneously from time zero although only one is required for the whole system to operate. The standby strategy comes in three variants of cold, warm, and hot [2]. In the cold- standby strategy, the redundant components are protected from the operational stress associated with system operation so that no component fails before its activation. Components in the warm-standby strategy are somehow affected by the operational stresses. Finally, in the hot-standby strategy, component failure does not depend on whether the components are idle or in operation. The mathematical formulation for the hot-standby strategy is the same as that for the active one [3]. In all three variants of the standby strategy, redundant components are used sequentially based on a predetermined order at failure times of operating components by using a switching system. Recently, a new redundancy strategy, called mixed strategy, has been introduced which combines the active and standby strategies and is shown to outperform both [2]. In other words, the mixed strategy is the general form while the active and standby ones are its special forms.
    Either of two scenarios may be envisioned for a switching system. In the first scenario (S1), the failure detection system/mechanism continually monitors system performance to detect a failure in which a redundant component is then activated. In addition to component failure, a switching system malfunction may occur at any time. In this scenario, switch reliability is a non-increasing function of time (老(t)). In the second scenario (S2), the switching system is used only at the occurrence of failure of a component. Failure of the switching system, therefore, happens only when it is used, and the probability of switch failure is considered as a constant value (老) [2?4].
    Over the past decade, many optimization methods have been proposed for solving the RAP with active strategy [5?9], standby strategy [4,10?14], and a combination of active and standby strategies [9,15,16]. Underlying all these studies is the assumption that the redundancy strategy for a specific subsystem can only be one of the active or standby strategies. In most cases, the redundancy strategy is considered to be a predetermined decision while it is considered as a decision variable in some recent studies [3,14?16]. Recently, a new redundancy strategy, called the ※mixed redundancy strategy§, has been introduced [2]. It potentially uses both active and standby strategies in a subsystem simultaneously. Therefore, the number of active and standby components in each subsystem is a decision variable and must be determined by a mathematical model. Ardakan et al. [17] developed the mixed strategy for a bi-objective RAP problem and demonstrated its advantages over the active and standby counterparts. More specifically, results have shown that the mixed strategy leads to more than 10 structures with superior reliability to those obtained in all previous cases.
    Feizabadi and Jahromi [18] developed the mixed strategy for a RAP problem with non-homogeneous components. Put more clearly, they considered the case in which standby components could be different from active ones but that all active and standby components had to be identical. In another study, they extended their model to a bi-objective RAP [19]. Recently, Gholinezhad and Hamadani [20] developed a mixed strategy for situations in which all the active and standby components were different. They developed a mathematical model for this non-homogeneous RAP problem and used a genetic algorithm (GA) to solve it. Ardakan et al. [21] investigated the capability of the mixed strategy applied to a reliability-redundancy allocation problem (RRAP) which is more complicated than the RAP. Results demonstrated that the mixed strategy led to a higher reliability in all the benchmark problems than did either active or standby strategies. The above mentioned studies con?rm the ability of the newly introduced mixed strategy to create highly reliable systems. In a recent study, Peiravi et al. [22] proposed a new redundancy strategy called K-mixed strategy which is a general form for the mixed strategy. Because of the complex nature of K-mixed strategy, they implemented the strategy only on one specific subsystem with 4 components.
    In this paper, a famous series-parallel system with 14 subsystems is investigated when all four different redundancy strategies, i.e., active, standby, mixed and K-mixed can be used in any subsystem. For this purpose, a new mathematical model is developed where the redundancy strategy and the number of components in each subsystem are considered as the decision variables. The proposed mathematical model must select the best strategy with the best redundancy level to maximize the total system reliability. The results of implementing the K-mixed strategy to the benchmark problem are compared with those reported elsewhere to evaluate its applicability and efficiency. By solving this new mathematical model, the capabilities and advantages of the K-mixed strategy will be revealed, especially in comparison with the mixed one.
    The rest of the paper is organized as follows. In Section 2, the mixed and K-mixed redundancy strategies are described in detail. Also, the mathematical formulation for calculating the reliability of these strategies are investigated in this section. In Section 3, the modeling of the problem is presented and in Section 4, a GA is developed for solving the proposed model. A well-known benchmark problem is adopted and solved using the new strategy in Section 5 and different problems are created and the results are compared to demonstrate the superiority and advantages of the new K-mixed strategy over the previously developed active, standby, and mixed strategies. Conclusions are finally presented in the end of the paper.
2. Mixed and K-mixed redundancy strategies
2.1. Operating mechanism
    In both mixed and K-mixed strategies, the numbers of active and standby components in each subsystem are considered as decision variables which must be optimally determined. This is a common feature of these strategies. The differences between the K-mixed strategy and the mixed one may be illustrated by considering a subsystem with four components. For this specific subsystem, all the different redundancy strategies (i.e., active, standby, mixed, and K-mixed) as outlined in Table 1 below are implemented.
    In order to disclose the distinctive features of these two strategies, they are applied to the proposed subsystem with four components. As seen in Table 1, the proposed subsystem might have two different structures depending on whether the mixed or the K-mixed strategy is applied. These two structures are: I) Three components in the active mode and one in the standby; II) Two active and two standby components.
    Fig. 1 illustrates these two different structures within the framework of a mixed strategy. Clearly, the subsystem starts its operation with 3 active components in structure I or with 2 in structure II. When the first active component fails, nothing happens for either structure and the subsystem continues working with the rest of its active components. When the second component fails in structure I, the subsystem continues operation with the last active component. In structure II, how- ever, the first standby component is activated by the switching system to replace the failed component because the second failing component is the last active one. From this point onwards, only one component is active in both structures and one standby component is replaced in a predetermined order upon the failure of this only component. This procedure continues as long as there is at least one standby component. When the last standby component is activated, the subsystem fails when this component also fails. It is worth noting that the standby components begin to operate in the mixed strategy when the last active component fails; this procedure (i.e., activating a redundant standby component to replace a failed active one) is the same as that in the standby strategy. Numerical results have shown that, for a switch re- liability equal to 0.99, the mixed strategy yields a higher system re- liability than those achieved under either the active or standby strategy [2,17,21].
    Both the mixed and standby strategies require a switching mechanism/system to substitute the redundant component into the sub- system, and the switching system can fail itself. If the switching system fails at the time of component replacement, the subsystem (and thereby the entire system) fails while there may still exist redundant standby components that have not yet been used. Accordingly, the reliability of the switching system is considered to be a weakness of the standby and mixed strategies, as it has an important and decisive role in their performance.
    The main contribution that the K-mixed strategy intends to make is to introduce a new strategy that is less sensitive to switch reliability. This strategy can then be expected to improve system reliability. In the proposed K-mixed strategy as a general form of the mixed strategy, all the active components start operating at time zero. When the first active component fails, the switching system immediately replaces the failing component with the first standby one available. This is where the mixed and the K-mixed strategies show their main difference. While the mixed strategy in this same situation (i.e., when the first active component fails) activates no standby component and the system continues working using the remaining active components, the K-mixed strategy replaces the failed component with the first standby one available. When the next active component fails, it is replaced with the next standby component available, and so on. The procedure continues until all the redundant components are activated and added to the subsystem [2]. In other words, the K-mixed strategy strives to keep a specific number of components in the active mode (i.e., the initial number of active components) as long as possible. From this point onward, the subsystem works with the remaining active components. Subsequently, the subsystem fails when the last active component fails.
    Fig. 2 illustrates the proposed subsystem with the K-mixed redundancy strategy in both structures I and II (compared to Fig. 1). It is seen that the number of active components is considered in the K-mixed strategy to be the same as that in the mixed one. It should be noted that only one component in this structure is required at any given time for the subsystem to operate.
    In structure II, the system starts its operation with two active and two standby components. When the first active component fails as detected by the switching system, the failed active component is replaced with the first standby component. This results in the subsystem still operating with two active components. When the second failure occurs, the failing component is replaced with the last standby component and the subsystem continues again with two active components. When the third failure arrives, no standby component is available anymore and the subsystem must, therefore, operate with only one active component. The subsystem then fails with the next component failure.
    In redundancy strategies with standby components (i.e., standby, mixed, and K-mixed strategies), a switching mechanism or failure detection system is required to detect the failure of a component and to replace it with a standby one. However, the switching mechanism is itself subject to failure. It is, therefore, more realistic and reasonable to consider the reliability of system regarding the switching failure. Coit [4] described two distinct scenarios for an imperfect switching system. In scenario I, the failure detection is continually monitored to detect a failing component and to replace it with a redundant one. In this scenario, the switch can fail at any time and switch reliability is a non- increasing function of time (老(t)) and switch reliability does not depend on the number of required switches. In scenario II, the switching system can only occur in response to a failure. Failure of the switching system occurs with a constant probability (老) when the switch is required.
    There is a significant difference between the K-mixed strategy and the mixed strategy with respect to the switching system. In the mixed strategy, the switching system is activated when the last active component fails and switching failure, therefore, results in subsystem/ system failure. In the K-mixed strategy, however, the switching system is used to substitute a redundant component when the first active one fails. In all the possible combinations, it is assumed that the switching system follows the second scenario and that it is triggered only in response to component failure. If the switch fails upon triggering, then it does not switch on the standby component even if it is repaired until the next failure triggers the switch back into operation. This procedure has two advantages. First, the subsystem/system is still operating when the switching system fails. Second, there is enough time to repair or replace the switching system. This is while the repair time or the time for replacing the switching system is considered to be short. Regarding the new proposed strategy, there is a decrease in the probability that the system fails due to switching failure. This study is an attempt to minimize the required number of switching operations which may result in direct subsystem failure.
2.2. Reliability model
    As explained in Section 2.1, there are 2 different forms of the K-mixed strategy for a subsystem with four components; they include: I) three components in the active mode and one in the standby mode; and II) two components in the active and two in the standby mode. Regarding the complexity of the K-mixed strategy formulation, this specific subsystem is considered and the formulation is described in more detail in order to provide a better demonstration of the formulation. The K-mixed formulations for the proposed subsystem with imperfect switching in structures I and II are defined by Eqs. (1) and (2), respectively. The times-to-failure of all the components follow an exponential distribution. Detailed procedures of obtaining these equations are presented in Peiravi et al. [22].
    In the K-mixed strategy, the reliability formulation for each subsystem depends entirely on the number of active and standby components. The following formula describes a subsystem of structure I with four components (3 active and 1 standby).
    Formula for a subsystem with 4components
    The reliability of a subsystem of structure II with four (2 active and 2 standby) components is given by Eq. (7):
    where, 老is the failure-detection/switching reliability for scenario 2 which is a constant value, (1 ? 老) is the probability that the failure-detection/switching does not work, and ri(t) is the reliability of the component used in subsystem i at time t. Since the component time-to-failure follows an exponential distribution, ri(t) is calculated as follows:
    pdf for the minimum failure time of active components which is calculated as follow:
    In Eq. (4), f(t)is the probability density function of time-to-failure for the components and F(t)is the cumulative distribution function. In Eqs. (1) and (2), subsystem reliability is the sum of probabilities associated with mutually exclusive events that lead to successful subsystem operation during the mission time t.
    Changes in the number of components or the combination of active and standby components lead to changes in the formulation of the K-mixed strategy. It is clear from the example that when the number of active components is changed from three to two, the reliability formulation changes correspondingly and the number of probabilities increases from six to eight segments. Therefore, there exists no fixed number of terms for the reliability formulation in each subsystem so that the formula must be developed and extended for each subsystem based on its own redundancy level.
    Active components have different starting times in some cases; this is important for calculating f(u) that shows the minimum failure time of the active components. In these conditions, if the component time-to-failure follows an exponential distribution, it can be assumed that all the components start working at the same time because of the memoryless property of this distribution; hence, Eq. (4) is justified. For other distributions, calculation of f(u) is more complicated.
    The formulation of mixed strategy in the second switching scenario is given by Eq. (5) [2]:
    where, f(t) is the pdf for the maximum failure times of number of failures of a component in the subsystem and is calculated as follows:
    RAP belongs to the NP-Hard class of problems, so it is hard to obtain an exact solution [1]. It is even more difficult to solve the problem using the proposed mathematical model for the K-mixed strategy. A GA as an efficient meta-heuristic algorithm is, therefore, developed in this paper for the optimization of the problems.
3. Mathematical model for RAP
    Significant research has been directed toward well-known bench- mark problems, including series system, series-parallel system, parallel- series system, and complex system ones. In this paper, a series-parallel system made up of 14 subsystems is considered from the literature. In such a system, each subsystem, i, can have its own unique number of active and standby redundancies considered as a decision variable. There are multiple choices of components available in each subsystem, but only one type of component with an unlimited supply is allowed to be used. Each type has its own levels of such parameters as reliability, weight, and cost. The objective is to maximize system reliability by determining the best strategy and the number of active and standby components in each subsystem based on weight and cost limitations.
    The assumptions of the mathematical model are as follows:
    Components are non-repairable.
    The components＊ time-to-failure follow an exponential distribution.
    The components and the system are two-state (good or bad) ones. 
    Each component's failure is independent of others and does not cause damage in them.
    All the components in each subsystem are identical. 
    The subsystem has an imperfect switching system.
    The switching system is replaceable or repairable and the switch works as perfectly as a new one after each replacement or repair. 
    The second scenario of switching (detection and switching only at the time of failure) is considered for the switching system.
    The mathematical formulation for the proposed problem runs as follows:
    Eq. (7) specifies the objective function which contains the component type, redundancy level, and the best redundancy strategy for each subsystem to achieve maximum system reliability. Constraints on cost and weight are given by Eqs. (8) and (9), respectively. System reliability is calculated using Eq. (11) in which the second scenario of switching (i.e., switch activation only in response to a failure) is considered.
    where, RMixed and RK?Mixed are equations for calculating the reliability of the subsystems with mixed and K-mixed strategies.
4. Genetic algorithm
    GA belongs to the family of meta-heuristic algorithms successfully employed for the optimization of combinatorial problems due to its simplicity and practicality. In this paper, a powerful GA with an efficient encoding procedure is applied for solving the proposed model when all kinds of strategies with different type and number of components can be selected for individual subsystem. GA has different characteristics such as solution encoding, objective function, crossover and mutation which are described in the following subsections.
4.1. Solution encoding
    A new efficient chromosome is chosen to represent the possible solutions which consist of selected component and redundancy level for each subsystem. This solution encoding is represented in a 2 ℅ s matrix. The first and second rows represent the type and number of selected components, respectively. An example of encoding solution for this problem with s = 14 is shown in Fig. 3.
    The main point about this encoding is that all possible strategies which can be applied to the subsystem by considering the redundancy level are considered. Then, the reliability of subsystems by using different strategies is calculated. The strategy which leads to a higher reliability for the subsystem is selected. For example, in subsystem number five, 4 parallel components are considered. All redundancy strategies include active, standby, mixed and K-mixed can be applied to this subsystem. Therefore, the best strategy with the higher reliability must be determined by the algorithm. In previous studies, the redundancy strategy has been considered in the solution encoding and it makes the solution space more complicated.
4.2. Objective function
    After the solutions are generated, their fitness functions are calculated by considering the penalty for constraint violations. In order to transform an infeasible solution to a feasible one in the next iterations of the algorithm, a dynamic penalty function is implemented. In this method, infeasible solutions are penalized by reducing their fitness value regarding to their violation level. Also this penalty function encourages the population to seek the feasible region and near the border of feasible region. The penalty function used here is based on research in [2].
4.3. Crossover and mutation
    In this section, double-point crossover and a modified version of max?min crossover proposed by [16] is applied. The mutation operator is used to increase diversity and prevent premature convergence into a local optimization solution. In this paper, besides the simple mutation, a max-min mutation operator is performed [2,16].
5. Numerical results
    In order to demonstrate the efficiency of the proposed K-mixed strategy, it is initially applied to three different subsystems and then to a well-known benchmark problem. The three different subsystems are considered to have 2, 3, and 4 components and the reliability values for the components and the switching system are assumed to vary over a wide range. The benchmark problem is a series-parallel system with 14 subsystems originally due to Fyffe et al. [23] which was later modified by Coit [4] and subsequently used by many researchers [2?4,16?20,24].
5.1. Three different subsystems
    The reliability of a specific system depends on different parameters as component reliability, switch reliability, redundancy level, and redundancy strategy. The present section investigates the effects of these parameters on the reliability of a subsystem. In these analyses, three different subsystems with 2, 3, and 4 components are considered and the reliability of different strategies in each subsystem is calculated based on different combinations of component and switch reliabilities. The objective is to derive the relationships among the different para- meters of the system and the best strategy in different situations. The following ranges are considered for component and switch reliabilities: 
    Component reliability: [0.95, 0.93, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.50, 0.40, 0.30]
    Switch reliability: [0.99, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.50, 0.40]
    In previous studies of RAP in which this benchmark problem was considered, the optimal structure always used 2, 3, or 4 components in each subsystem and component reliability ranged over [0.80, 0.95]. In this paper, these three different levels of components are, therefore, considered as three different subsystems and the efficiency of each strategy is evaluated. The results are, finally, compared with those obtained from implementing the K-mixed strategy on the benchmark problem.
    All the combinations of component and switch reliabilities are considered and the reliability of each of the four strategies is calculated for each combination and compared. In each combination, the reliability values of different strategies are calculated and sorted in a descending order. The problem is solved with 11 different values for switch reliability and 12 different values for component reliability. This yields 132 test problems. All the different combinations are presented in three 12 ℅ 11 tables, in which the rows represent switch reliability and columns represent component reliability. The results obtained for the subsystem with 4, 3, and 2 components are presented in Figs. 4, 5, and 6, respectively. Each cell in these tables represents the order of different strategies from the best to the worst one. The abbreviations used in these tables are presented in Table 2.
    For more clarification, Figs. 4?6 are presented in different colors, with green representing the range in which standby strategy leads to better results, blue denoting the combinations in which the mixed strategy leads to better result, yellow representing the ranges in which the new strategy (K-mixed) results work better than others, and finally brown indicating the active strategy as the best one.
    As shown in Figs. 4 and 5, the active and standby strategies exhibit better performances at the corners of the matrixes (i.e., highly reliable components with a weak switch, or a highly reliable switch with weak components). In most of the combinations, however, the proposed K-mixed strategy outperforms others. It is worth mentioning that the mixed and K-mixed strategies can be de?ned for subsystems with more than 2 components while in a subsystem with two components, only the active and standby strategies can be de?ned.
    Figs. 7?9 present the effects of the switching system on the different strategies for all the three proposed subsystems. These figures also demonstrate the reliability values of different strategies for a switch reliability reduced from 0.99 to 0.40 and a component reliability fixed at 0.65. Clearly, the standby strategy is the one most sensitive to switch reliability followed by the mixed strategy (in structure II). The active strategy is not sensitive to switch reliability because it does not use any switching system. It is interesting to note that the K-mixed strategy in its first structure is somehow unaffected by switch reliability and may be considered as a reliable strategy. Therefore, if the switching system is not reliable, then the proposed K-mixed strategy is beneficial for the system.
5.2. Series-parallel system
    In this subsection, the famous series?parallel system in RAP is investigated and the K-mixed strategy is implemented on this benchmark problem. The series?parallel system consists of 14 subsystems in each of which three or four component choices with predetermined cost, weight, and reliability are available. The input data for this benchmark is presented in Table 3. The purpose is to maximize system reliability at a given 100 units of time by considering constraints on system cost (C = 130) and system weight (W = 170). It is assumed that the failure detection and switching hardware activate the redundant component only in response to a failure.
    In order to analyze the efficiency of different strategies, particularly the proposed K-mixed one, considering changes in switch reliability, the benchmark problem is solved for the different switch reliability values of 0.99, 0.95, 0.90, and 0.85. The main goal is to investigate the capability of the new strategy to cope with different situations. A second goal is to evaluate the performance of the different strategies studied by changing the switch reliability value. More specifically, it is interesting to observe how the best strategy for a subsystem might change with changes in switch reliability values.
    In order to make a clear comparison of the proposed and the conventional strategies, for each value of switch reliability, the problem is solved in two situations:
    Situation I: The subsystems can use only the conventional redundancy strategies; i.e., standby, active, and mixed,
    Situation II: In addition to the conventional strategies, the subsystems are allowed to use the K-mixed strategy.
    The proposed GA is used to solve the benchmark problem in each situation. The problem is solved then in five trials and the best solution is taken as the final solution. The best solution with maximum reliability is subsequently chosen for comparisons between the two different situations. Four different values are considered for switch reliability. The results obtained for each value are presented below.
    We begin with a switch reliability of 0.99 that was also used in all previous studies of the benchmark problem in question [2?4,16?20,24]. The problem is solved in the two situations (i.e., with and without the K-mixed strategy) and the results are reported in Table 4.
    Ardakan and Hamadani [2] considered the same benchmark problem and employed a mixed strategy to find an optimal solution with a reliability of 0.992328. They assumed that the component time-to- failure (TTF) followed an Erlang distribution. For the K-mixed strategy, we assume that all the component TTFs follow an exponential distribution. Therefore, in this paper, the mixed strategy is also considered with an exponential TTF so that exhaustive comparisons are possible to make. For other problems, the same procedure is applied and the best solutions are obtained. The benchmark problem is formulated and solved by considering the mixed strategy with an exponential distribution. The structure of the best solution obtained in this paper for the benchmark problem with an exponential TTF is the same as that obtained by Ardakan and Hamadani [2] and the reliability of the best solution with an exponential TTF is equal to 0.98194689. Thus, the same best structure is obtained for this problem with either an Erlang or an exponential TTF.
    The benchmark problem is then solved with the proposed K-mixed strategy applied to the system. The optimal structure is observed to be almost the same as the previous one and the three conventional strategies, but the K-mixed one, are used in all the subsystems. This result was predictable from Figs. 4?6. However, the GA developed in this paper finds a solution with a better reliability (i.e., 0.982022) compared to the best solution obtained in previous studies [2]. When component TTF in this new solution is changed to an Erlang distribution, the system reliability is equal to 0.992335, which is greater than the one (0.992328) obtained by Ardakan and Hamadani [2]. Table 4 shows the results of the comparisons between the two situations when a switch reliability of 0.99 is adopted.
    According to Table 4, the mixed strategy outperforms the K-mixed one while this strategy is also used in all the subsystems with more than 3 components when a switching system with reliability of 0.99 is employed. Moreover, the proposed GA is found to be superior to that presented in [2] as it was able to find a better solution. Table 5 reports the results obtained for the proposed benchmark with and without the K-mixed strategy when the switch reliability is equal to 0.95. Because none of the previous studies considered this value for switch reliability, the results of both situations are presented here only with an exponential distribution.
    As seen in Table 5, the final solution uses four subsystems with the K-mixed strategy when it is applied to the benchmark problem. This solution is distinctly different from the best structure with a switch reliability of 0.99. Comparison of the results for Situations I and II reveals that the K-mixed strategy improved the overall reliability of the system. These results are in complete agreement with those shown in Figs. 4?6. For example, the second subsystem uses two components with the active strategy while it is shown in Table 6 that the best strategy for this value of switch reliability (0.95) and two components is the active one.
    These results are also in complete agreement with our findings reported in Figs. 4?6 where it is seen that the best redundancy strategy for each subsystem depends on component reliability when the switch reliability is fixed. For example, in the first subsystem, three components of type 3 are chosen with a component reliability equal to 0.91. It is clear from Fig. 5 that the best strategy in this situation is KM1 (i.e., K-Mixed redundancy strategy in structure I). For other subsystems, the results can be compared with those in Figs. 4?6. Table 6 reveals the differences between the solutions obtained for the two Situations I and II. 
    Tables 7 and 9 report the results obtained for the benchmark problem with switch reliabilities of 0.90 and 0.85, respectively. Comparisons of the two Situations (i.e., using conventional or the K-mixed strategies) are reported in Tables 8 and 10 for each value of switch reliability. The interesting point in these two final test problems is that, as also shown in the previous section, different strategies exhibit different degrees of sensitivity to switch reliability. It was shown that the K-mixed strategy with the structure I had the minimum sensitivity to switch reliability. As a result of this fact, the final solutions in Tables 7 and 9 employ more subsystems with the K-mixed strategy in Structure I (i.e., KM1).
    Fig. 10 presents the reliability values for the different subsystems in the two situations I and II. Fig. 11 presents a complete comparison of Situations I and II for different values of switch reliability. Clearly, the proposed K-mixed strategy helps the system remain highly reliable despite the reduced reliability of the switching system. When switch reliability is lower, the overall system reliability in both Situations I and II get closer because the system prefers to use an active strategy which is not sensitive to the switching system. Fig. 12 presents the structure of the series-parallel system considering different switch reliability values. It shows how each subsystem uses the best redundancy strategy to gain the highest reliability.
6. Conclusion
    In this paper, a recently introduced redundancy strategy called the K-mixed strategy was investigated as a general form of the mixed strategy. This strategy can be used in all systems with redundant components. The mathematical formulation of this strategy was initially presented and a well-known series-parallel system was considered and a novel mathematical model was developed to evaluate the efficiency of the proposed strategy. The problem was formulated as a non-linear integer programming model subject to a number of constraints. The reliability of the system was then calculated in two different situations: with and without the K-mixed strategy used in the subsystems. Moreover, different values of switch reliability were considered and the problem was solved. Finally, the efficiency of the proposed strategy was investigated against each of these values. Numerical results revealed that the proposed K-Mixed strategy outperformed the previously used mixed, standby, and active strategies for most combinations of components and switch reliability. For future studies, the proposed strategy is suggested to be implemented in other reliability problems such as RRAPs.
    
The evolution of system reliability optimization
    System reliability optimization is a living problem, with solutions methodologies that have evolved with the advancements of mathematics, development of new engineering technology, and changes in management perspectives. In this paper, we consider the different types of system reliability optimization problems, including as examples, the redundancy allocation problem (RAP), the reliability allocation problem and the reliability-redundancy allocation problem (RRAP), and provide a flow of discussion and analysis on the evolution of the approaches for their solutions. We consider the development and advancement in the fields of operations research and optimization theory, which have allowed the formalization and continuous improvement of the methods and techniques to address reliability design problems of even very complex systems in different technological domains. Technological advances have naturally brought changes of perspectives in response to the needs, interests and priorities of the practical engineering world. The flow is organized in a structure of successive ¡°Eras of Evolution,¡± namely the Era of Mathematical Programming, the Era of Pragmatism, the Era of Active Reliability Improvement. Insights, challenges and opportunities are highlighted.
1. Introduction
    ¡°Success is walking from failure to failure with no loss of enthusiasm¡± ? Winston Churchill
    Reliability engineering is a formal engineering discipline, founded on mathematical principles, particularly of probability theory and statistics, for systematically and rigorously analyzing functional problems in components and systems with the aim to produce a reliable design. As an engineering discipline, reliability aims at analyzing and evaluating the ability of products and services to perform the functions that they are intended to provide by design.
    While technology improves and advances, the complexity of modern engineered systems also increases. At the same time, consumers¡¯ expectations for high functionality, high performance and high reliability increase, leading to challenges and opportunities. Then, although system reliability optimization problems have been studied, analyzed, dissected and reanalyzed, the continuous rise of new challenging problems demonstrates that this general research area will never be devoid of interesting problems to be solved.
    On one side, the development and advancement in the fields of operations research and optimization theory have allowed the formalization and continuous improvement of the methods and techniques to address reliability design problems of even very complex systems in different technological domains. On another side, the evolution of technology, the advancement of research ideas and theories, have naturally brought changes of perspectives, in response to the needs, interests and priorities of the developing practical engineering world. So, the development and application of formal optimization methods to the practical goal of achieving maximum reliability under various physical and economic constraints, has remained an ongoing topic of scientific development.
    In formal terms, the task of optimization involves formally conceptualizing the decision variables, the constraints and the single or multiple objective functions that describe the performance of the engineering design problem, and searching for the combination of values of the decision variables that achieve the desired goals with respect to the objective functions. Whether expressed explicitly in mathematical terms or not, every engineering design problem has design objectives which should be maximized or minimized or designed to achieve some acceptable requirement. When there is a single predominant objective that can be expressed with a series of explicit mathematical equations, then the problem can potentially be solved using mathematical programming methods or useful heuristics. Researchers working within the broader mathematical programming community are continually developing new methods and algorithms to solve broader classes of problems, larger and more difficult problems, and to do so more efficiently than before.
1.1 Eras of research evolution
    The research in complex system reliability optimization has evolved as a continuum of ideas and techniques. This evolution can be loosely and chronologically classified into the following three eras:
    Era of Mathematical Programming
    Era of Pragmatism
    Era of Active Reliability Improvement
    The era of mathematical programming is preceded by the original development of innovative groundbreaking methods, such as dynamic programming and the simplex algorithm (or linear programming), for which the reliability optimization problem has served as a very interesting and practical vehicle to demonstrate the methods and apply them. Yet, practicing reliability analysts recognized the limitations of these methods in practical applications, mostly because only problems that could be formulated to strictly meet the assumptions required by the methods could be solved, and this is rarely the case in practice. Furthermore, only small and/or unrealistic problems could be solved, because of computation limitations at the time.
    Driven by the desire to apply reliability optimization in practice, the era of pragmatism evolved and became increasingly important. New problems were solved and new methods developed in response to the pressing needs to consider and integrate into the problem some critical issues that could not be readily accommodated by the rigorous mathematical methods. For example, actual engineering systems problems could fall outside the assumptions required by the methods. Analysts were interested in complex forms of redundancy perhaps mixing functionally equivalent, but different components, whose failure behavior may not be simply described as a transition from one functioning state to a failure state, but rather as a process of transition across multiple states. To address a broader range of problems, compromises could be accepted, thus expanding the practical usefulness and applicability of the optimization methods.
    In these previous eras, reliability optimization had been mostly considered as singular, static analyses to be conducted and implemented to yield the desired design reliability of the system. Under this view, common assumptions were made on the existence of populations of homogeneous components and systems, sharing the same failure behavior but with failure occurrences being independent from one another. Furthermore, the implied assumption was that the conditions defined or considered when conducting the analysis are static and remain unchanged throughout the horizon of the analysis (the often-called mission time).
    When changes occur during the system lifetime, then the results of the analysis are simply no longer valid and applicable. We are currently experiencing another industrial revolution, particularly driven by the increase in information sharing, data availability and computational capabilities. In particular, with the proliferation of sensors, environ- mental stresses, usage stresses, failure data, etc., can be collected and processed at regular time intervals, and the advancements in information knowledge that these can bring on the states of the systems, offer new opportunities of development for the analysis and assessment of reliability. The era of active reliability improvement is, then, ongoing and it is visionary in recognizing that components and system conditions change throughout their lives, and system reliability optimization methods, to be practically useful, need to dynamically respond to these changes.
    Within each of the three macro-eras discussed above, further sub- classification can be introduced, in a specific and unique way. In the era of mathematical programming, further sub-classification can be done based on the specific mathematical algorithms developed to solve the reliability design problem. For the era of pragmatism, sub-classification can refer to which practical consideration in the design problem, which previously could not be analyzed, was now readily being addressed by the available optimization models. Within the era of active reliability improvement, sub-classification can be based with reference to the available data and new models for real reliability improvement.
2. Reliability optimization problems
    System reliability design problems have multiple, and often competing objectives. However, there are some universal ones, including reliability (to be maximized) and cost (to be minimized). Often, the approach taken is to follow a prioritization of the objectives by the decision-makers, select the most important objective as the objective function and constrain the other objectives within acceptable limits.
    For each formulation to be studied or solved, system reliability optimization problems must have three elements: decision variables, constraints and an objective function or functions. The decision variables are those variables that can be changed or decisions that can be made to improve performance, with respect to the objective function or functions. Examples of decision variables include component type (with its intrinsic characteristic of failure behavior and reliability), redundancy configuration in the system and others. Constraints are mathematical expressions of practical limitations, such as monetary budget or acceptable reliability, which limit the choice of decision variables in relation to their feasibility of respecting the constraints.  The objective function measures the performance of the system for given values of the decision variables, and thus, enables the decision on the optimal combination of variables values for the optimal solution. The objective function can often be the system reliability to be maximized, or the system cost to be minimized.
    Different forms exist of the system reliability optimization problem. Three typical ones are the redundancy allocation problem (RAP), the reliability allocation problem and the reliability-redundancy allocation problem (RRAP). The solution methods for each problem are obviously different, because of the assumptions and structure of the problem. A thorough review of system reliability optimization is given by Kuo et al. [1?3].
2.1. Redundancy allocation problem
    The most widely studied reliability optimization problem is the RAP. For many systems composed of discrete component types, with fixed cost, reliability and weight, in mathematical terms system design becomes a combinatorial optimization problem. For providing the demanded system functions, there may often be alternative component types available, at different cost, reliability, weight and other proper- ties. The practical problem is to select the optimal combination of components types (decision variables) to collectively meet reliability, weight, etc. (constraints) at a minimum cost (objective function), or alternatively, to maximize reliability (objective function) while achieving given minimum (or maximum) values of other system properties (constraints).
    For the mathematical formulation of this problem, we can consider that there are mi discrete component type choices available for each subsystem (i = 1, ÿ, s), which the system is formed of. Fig. 1 shows a typical example of a system with a number s of k-out-of-n logic sub- systems. If k is equal to 1 for each subsystem, then this is a simple series-parallel system. For each subsystem, ki components must be selected from the mi available choices (e.g., assuming an unlimited amount available for each of the components). The optimal decision is driven by the possibility of placing additional components in parallel in any of the subsystems and/or adding levels of redundancy (> ki) of lower reliability components as an alternative to using more reliable (and expensive) ones. In other words, there is a large number of possible solutions to test, even for relatively small problems (small number of subsystems, small number of components, small number of components types, etc.).
    The RAP for a series-parallel system, as the one shown in Fig. 1, can be formulated as to maximize reliability or minimize cost, under various constraints. Often the RAP is solved for series-parallel systems, but other system structures have been considered, as well as more advanced forms of series-parallel systems, including those with several failure modes, phase mission types, systems with uncovered failures and systems with imperfect fault coverage.
    RAP has proven to be quite difficult to solve. Chern [4] showed that the problem is NP-hard and many different optimization approaches have been used to determine optimal or ¡°very good¡± (i.e., near-optimal) solutions, including (1) dynamic programming, (2) integer programming, (3) mixed integer and nonlinear programming, or (4) evolutionary algorithms.
2.2. Reliability allocation problem
    The reliability allocation problem and RAP are fundamentally different problems. For the reliability allocation problem, the system structure is fixed and the component reliability values are continuous decision variables. For this problem, there is no general restriction on the system structure. An example of a general system is depicted in Fig. 2. Component cost and other parameters are defined as mathematical functions of the component reliability. Increasing the component reliability (and thus, the system reliability) increases the cost, weight and other factors, which may be included as part of the constraints or the objective function. Here, also, the goal of the optimization is typically to maximize system reliability or minimize system cost, and since the decision variables are continuous, different forms of non- linear programming can be used to determine the optimal solutions. To assure that the constraints are satisfied, Lagrangian multipliers are often introduced as part of the objective function.
2.3. Reliability-redundancy allocation problem (RRAP)
    The reliability-redundancy allocation problem is the most general problem formulation. The system is composed of one or more ¡°sub- systems,¡± i.e., collections of logically connected sets of components. Each subsystem has xi components with reliability of ri as decision variables. The problem is then to optimally allocate redundancy and reliability to the components of each subsystem with an objective to maximize the overall system reliability. The system and subsystems can be in any arrangement, as depicted in Fig. 2. Again, typically the objective of the optimization is to maximize system reliability or minimize system cost.
    RAP is often considered for series-parallel systems, but other system structures can be considered as well. The reliability allocation problem and RRAP has been applied to many different system structures, including common structures (series, parallel, etc.), but also consecutively connected systems, sliding window systems, flow transmission systems, common bus systems and others.
2.4. Component assignment and sequencing problems
    Two other related system reliability problems are assignment or sequencing of components within a system. For assignment problems, there is typically a defined system structure, but the available components are assigned to specific locations with the system. Sequencing is particularly interesting and important for systems with standby redundancy, where the components are activated as needed in accordance with a defined sequence. Sequencing problems are solved for optimization for systems with consecutively connected systems and/or standby components with cold or warm redundancy required a defined activation sequence.
    The problem of assignment of components to positions within a system to maximize reliability was originally presented by Derman et al. [5,6]. An important early research effort [7] defined optimal assignments for different system structures. The related problem involves sequencing of redundant components. More recently, sequencing of standby redundant components has been considered by researchers to maximize system reliability [8,9].
2.5. Other optimization problems
    There have been many other related system reliability optimization problems as well. There have been extensions of the original problems, as well as related problems including spares provisioning, optimization of series-parallel topology, optimal load distribution, optimal mission abort policy, test plan allocation, preventive maintenance optimization and others [2].
3. Era of mathematical programming
    Methods initially developed to solve complex system reliability optimization problems can be referred to as belonging to the era of mathematical programming. The emphasis was on applying advanced mathematics to obtain mathematically provable optimal solutions. The priority was on solving problems to optimality, but in doing so, often the structure and size of the problems were limited to be able to apply the rigorous mathematical model. The problems being solved in the end were rarely realistic or indicative of actual design problems. Assumptions were often introduced for mathematical convenience, and problems that did not meet those conditions were avoided. However, these original methods are very important and influential, and they have served as the foundation for much of the research and development work in quantitative reliability engineering that followed.
    Solution of complex system optimization problems was possible because of advancements in operations research theory and the development of new algorithms. A key mathematical challenge was to find an efficient way to at least approximate solutions to problems otherwise unsolvable with classic analytical methods. Numerical and mathematical approaches were introduced to successfully solve such problems, with the turning point having been the realization of the first computer, which provided the possibility to perform sets of operations for hand- ling large numbers of data in a time much shorter than previously possible.
    The newborn field of computer science sparked the mind of several mathematicians that tried to formulate methods to use computers to help solving practical problems involving high computational efforts. The field of mathematical optimization was then born. Several pioneers from the first half of the 20th century contributed to develop formulations and algorithms to be implemented in computing machines to solve difficult optimization problems, including from the fathers of  linear programming, Leonid Kantorovich and George Dantzig, Richard Bellman, originator of dynamic programming, to the founders of evolutionary algorithms, such as Lawrence Fogel, John Holland, Ingo Rechenberg and Hans-Paul Schwefel. Their ideas were the milestones from which other variants of the methods developed until today.
    When mathematical programming methods associated with the field of operations research were being developed and formalized, but still in their infancy, researchers were searching for interesting applications. Maximization of system reliability was considered an attractive application. Indeed, it is a difficult and challenging problem, yet practical and useful to demonstrate the utility of new mathematical programming algorithms. Typical formulations of the problem are challenging, with a highly nonlinear objective function and often integer decision variables.
    Dynamic programming was applied to the system reliability problem as one of the very first applications explored. Considering the initial formulations of RAP, dynamic programming could almost be directly applied to efficiently obtain optimal solutions.  The problem was that it was difficult to extend it to solve more realistic or actual design problems. Linear programming, or the simplex algorithm, is a very powerful advancement, which allowed for very large linear problems to be solved. However, system reliability is a highly nonlinear objective function, so researchers had to be creative to exploit the power of the simplex algorithm to solve reliability problems. RRAP is nonlinear with both continuous and integer decision variables creating another very challenging problem, that was often solved using some variation of nonlinear programming.
3.1. Dynamic programming
    Dynamic programming was originated in 1954 by Richard Bellman [10], and system reliability optimization was among the first problems studied. The aim was to treat mathematical problems arising from the study of multi-stage decision processes. The key advancement, as compared to previous methods, relies on the fact that when analyzing these problems, not all the possible sequences of the present and following stages are needed, i.e., it is possible, instead, to state general conditions to determine for each stage the most suitable decision ac- cording to the current state only, whereas classical approaches gathered information about all the sequences, making the calculation cumber- some and unpractical [10].
    A problem solvable by dynamic programming can be performed as a system, described by a set of quantities, the state parameters, that undergo a state variation caused at a certain time t by a decision made by the user. The solution aims at taking an initial decision for guiding the future ones so that it is possible to maximize a given objective function of the state parameters. In many cases the number of parameters considered to make the decision is very large, especially when considering stochastic processes in which decisions determine a distribution of outcome states. In these cases, the approach allows reducing the dimension of the problem by focusing on the current time. To perform the dynamic programming optimization, the state parameters and the sequence of decisions to analyze, i.e., a policy, are needed. The optimal policy is, then, the one that determines the decision required at each time with respect to the current state of the system.
    Bellman [11] and Bellman and Dreyfuss [12,13] demonstrated that an optimal solution to the RAP could be found using dynamic programming. In their problem, there was only one component choice for each subsystem, and the objective was to maximize reliability with a single cost constraint. For each subsystem, the problem was to identify the optimal levels of redundancy. A well-known disadvantage of dynamic programming formulations is the difficulty of efficiently solving problems with multiple constraints.
    Fyffe et al. [14] used a dynamic programming approach to solve a more difficult design problem. Their problem involved a system with 14 subsystems and cost and weight constraints. For each subsystem in the Fyffe formulation, there are three or four different component choices each with different reliability, cost and weight. However, several of these component choices are dominated by other competing choices. Similar to Bellman, only 1-out-of-n redundancy was considered. To accommodate multiple constraints within a dynamic programming formulation, they used a Lagrangian multiplier for the weight constraint within the objective function.
    Instead of using Lagrangian multipliers, Nakagawa and Miyazaki [15] used a surrogate constraint combining the cost and weight constraints into one. They then solved a series of problem iterations with different surrogate multipliers, with a heuristic to successively update the surrogate multipliers. Stopping criteria was provided to identify cases when their algorithm would not lead to an optimal solution. The algorithm was demonstrated by solving 33 variations of the Fyffe problem with different weight constraints. Of the 33 problems, they found optimal solutions to 30 of the problems. Otherwise, the final solution was not feasible (although there are feasible solutions to the problem).
3.2. Linear programming/integer programming
    Linear programming (LP) and Integer Programming (IP) are powerful methods to find the maximum or minimum of a linear function describing the performance under assessment, which is called the objective function. A standard mathematical definition is the following: max{cx; Ax b,0}or min{cx; Ax	b, x	0}
    For this formulation y = cx is the objective function to be maximized or minimized, x is the vector of non-negative variables to be found, b and c are vectors of known coefficients, and A is a matrix of known coefficients that when multiplied by x have to satisfy the constraints expressed by the vector of coefficients b. The problem is defined within a convex polyhedron-shaped feasible region, intersection of finitely half-spaces represented by linear equalities/inequalities.
    This optimization framework was initially used by the Soviet economist Leonid Kantorovich who was trying to organize the actions of soldiers to decrease expedition costs and increase enemy losses. At the same time another economist, T.C. Koopmans was working on the applicability of linear programming to solve classical problems. Their work was recognized by the Nobel prize in economics in 1975. Following that, mathematician George Dantzig developed an LP methodology to solve optimization problems, providing a formal proof of the solution [16]. One of the most important achievements was the reduction of the possible solutions, and therefore, the advantage of the method in terms of computing power needed.
    If the objective is to maximize reliability or minimize cost given multiple nonlinear but separable constraints, many variations of the problem can be transformed into an equivalent integer programming problem using 0?1 decision variables. This was originally demonstrated by Ghare and Taylor [17] who used a branch-and-bound approach to solve many randomly generated RAPs with 30 subsystems with 15 constraints, and 99 subsystems with 10 constraints. Ghare and Taylor assumed that there was only one component choice for each subsystem and redundancy was active always 1-out-of-n redundancy.
    Bulfin and Liu [18] also used an IP approach to solve the RAP. They developed one heuristic and two exact algorithms to be applied de- pending on the problem structure. They formulated the problem as a knapsack problem and used a surrogate constraints approach, similar to Nakagawa and Miyazaki [15]. The surrogate multipliers were approximated as the optimal Lagrangian multipliers as found by sub- gradient optimization. Bulfin and Liu formulated the Fyffe problem and its variations as integer programs and solved the 33 problems previously investigated by Nakagawa and Miyazaki, and other examples as well. They also considered only subsystems with 1-out-of-n redundancy. Coit and Liu [19] extended their approach to k-out-of-n redundancy subsystems if no mixing of component types is allowed within the subsystem parallel structure.
    Other examples of IP solutions to the redundancy allocation problem were presented by Misra and Sharma [20], Gen et al. [21], and Gen et al. [22]. Misra and Sharma presented a very fast and useful algorithm to solve integer programming problems formulated like those of Ghare and Taylor [17]. Gen, Ida, Tsujimura and Kim and Gen, Ida and Lee formulated the problem as a multi-objective decision-making problem with distinct goals for reliability, cost and weight.
    For other system reliability applications, LP has been proved useful in the context of structural system reliability by Corotis and Nafday [23]. They used LP to identify the most critical failure mode for a structural system. More recent papers [24] demonstrate that LP is particularly useful in structural system reliability analysis. The LP bounds can be applied for any type of system and for different component probabilities. These bounds are the narrowest possible bounds that one can be obtained for a system, for any specified information for the component failure probabilities.
    The main drawback of using LP or IP is that the size of the problem to be solved, and its computational cost, increases exponentially with the number of components, questioning its efficiency when it comes to realistic, complex systems. An approach has been proposed to overcome this issue and extend the applicability of LP. Decomposing the entire system into subsystems based on failure modes can be applied to identify component state probabilities and joint probabilities of the states of a small number of components. It can also provide bounds for the failure probability of large systems. This is particularly useful when other methods are not applicable. This approach has been presented by Chang and Mori [25]. The idea is the development of a relaxed LP (RLP) bounds method to reduce the number of design variables using the universal generating function (UGF) [26].
    RLP bounds method can be applied to a single series or parallel system, but it is not applicable to a general system that consists of both series subsystems and parallel subsystems. For this reason, an additional assumption can be made to obtain the Strategic Relaxed Linear Programming (SRLP). After decomposing the system according to different failure modes, each critical failure mode is also considered as a system (or subsystem) itself. The bounds on the system failure prob- ability can be computed by the RLP bounds method if it is a series or parallel system, and the bounds on its joint failure probability can also be computed by the RLP bounds method. The bounds estimated by the RLP bounds method are, then, used as constraints in solving the LP problem for estimating the failure probability of the entire system.
3.3. Nonlinear programming
    Nonlinear programming (NLP) refers to a collection of optimization methods defined by the same main principles of linear programming, with the difference that the objective function and/or its constraints, and therefore the feasible region of the problem, are defined with at least one nonlinear equation. The addition of nonlinear equations makes the optimization problem much more difficult to be solved, for example:
    In a nonlinear function it is hard to assess whether a maximum is local or global, and unlike linear functions where a max/min location is restricted to the borders of the feasible region, for a nonlinear function it can be in the interior of the feasible region.
    If the objective or any constraints are non-convex, the problem may have multiple disconnected feasible regions and multiple locally optimal points within such regions.
    The numerical method chosen to get to the solution may cause two different starting points to lead to two different solutions.
    It is difficult to ensure that the constraints applied to the problem meet the requirements of the feasible region.
    A tolerance region for the solution has to be considered with a proper uncertainty.
    NLP solvers generally attempt to solve the problem by computing gradient values at various trial solutions, and moving in the direction of the negative gradient (when minimizing, positive gradient when maximizing). They usually also exploit second derivative information to follow the curvature as well as the direction of the problem functions. To solve constrained problems, NLP solvers must take into account feasibility and the direction and curvature of the constraints as well as the objective. A review of nonlinear programming optimization methods is provided by Floudas [27].
    Mixed integer and nonlinear programming have been effectively used to solve the redundancy allocation problem. Considering reliability optimization, important research contributions were provided by Tillman et al. [28,29]. In these problems, component reliability is a continuous decision variable and component cost is expressed as a function of reliability and other parameters.
3.4. Evolutionary algorithms
    Evolutionary algorithms (EA) are a group of optimization methods that perform their task with a built-in ability to evolve. EA have the three following common features:
    1. Population-based, i.e., they handle a group of solutions, the population, manipulated in different ways to optimize the problem;
    2. Fitness-oriented, meaning that EAs favor individuals (a solution belonging to a population) which are fitter than others according to established criteria. Each individual has a gene representation which is its code together with a performance evaluation, i.e., its fitness value. Choosing fitter individuals drives the optimization and the convergence of the algorithm;
    3.Variation-driven: In order to simulate evolution steps, individuals are subject to random variations, necessary to explore the problem's solution space.
    The basic intent of EAs is to implement the Darwinian concept of survival of the fittest, applying it to functions to optimize. Through each generation, the solutions considered weak in terms of the specific criteria adopted for the optimization face extinction, whereas the best ones combine to produce new individuals that potentially can improve the convergence to an optimal solution.
    The first attempts to mimic evolution by simulating genetic processes date back to Fraser [30] and Bremermann [31]. The main contributor, however, is John Holland, who in 1975 published ¡°Adaptation in Natural and Artificial Systems [32]¡± in which he introduced the main fundamental concepts of genetic algorithms (GA). In GA, each in- dividual of the population has two characteristics: a chromosome and a fitness value representing its quality. The chromosome is composed of genes; in the original formulation each gene was considered as a bit, therefore either 1 or 0, and the chromosome was a string of zeros and ones. In the following years, several researchers developed new forms of GAs.
    A chromosome can be viewed as a sorted string or vector. The evolutionary process starts when all fitness values of the initial population have been assigned. Afterwards, the selection process begins, in which some individuals are selected in order to be included in the mating pool. The fittest individuals are more likely to be selected and spread their properties to the offsprings: individuals in the mating pool are combined to produce new hybrids whose finesses are evaluated to decide whether or not to pass onto the next population, replacing other individuals. It is common practice to keep a constant number of individuals inside a population at each stage.
    GA have not been applied practically for system reliability problems until the 1990s, when researchers such as Coit and Smith [33,34] implemented it in a combinatorial reliability design problem. The evolutionary optimization proved very efficient in terms of cost-effectiveness of the selection of the parts and allocation redundancies for system reliability. Several authors then tackled optimization problems by GA. For example, Painton and Campbell [35] presented a model based on such methods, highlighting again their robustness and capability of finding the optimum over a high dimensional nonlinear space in a considerably shorter time than the required one for enumeration. In order to improve the reliability of a personal computer, they identified the main components and their failure modes in order to determine some possible improvement levels.
    With regards to applications, reliability allocation to minimize total operating costs, subject to an overall plant safety goal, was presented by Yang et al. [36]. System optimization was used to enhance the design, operation and safety of new and/or existing nuclear power plants. They determined the reliability characteristics of reactor systems, sub- systems, major components and plant procedures in accordance with a set of top-level performance goals. The cost for improving and/or de- grading the reliability of the system was also included in the reliability allocation process as a multi-objective problem formulation. GA was demonstrated to determine effective solutions for a typical pressurized water reactor.
    Konak et al. [37] presented general guidelines regarding the implementation of GA for multi-objective reliability optimization, pro- posing a list of techniques and highlighting the advantages and difficulties of each of them. The reliable network design problem has been studied using multi-objective GA. Kumar et al. [38] presented a multi- objective GA to optimize telecommunication networks while simultaneously improving network performance and design costs given a system reliability constraint. Kim and Gen [39] studied bicriteria spanning tree networks considering the objectives of cost and reliability, while Marseguerra et al. [40] determined optimal surveillance test intervals using a multi-objective GA to improve reliability and availability.
    Problems studied by Martorell et al. [41,42] involved the selection of technical specifications and maintenance activities at nuclear power plants to increase reliability, availability and maintainability for safety- related equipment. They also considered the optimal allocation of more reliable equipment, testing and maintenance activities to assure high reliability, availability and maintainability levels for safety-related systems. Additional limited resources (e.g., budget and workforce) were required, to form another a multi-objective problem. Solutions were obtained by using both single-objective GA and multi-objective GA, to solve the problem of testing and maintenance optimization with the objective functions of unavailability and cost.
    Various other meta-heuristics have been used for reliability optimization. For example, Ant Colony Optimization (ACO) is a population- based, general search technique for the solution of difficult combinatorial problems [43]. The method is inspired by the pheromone trail laying behavior of real ant colonies. In ACO, artificial ants probabilistically build solutions by taking into account pheromone trails, which change dynamically at run-time, to reflect the agents acquired search experience and heuristic information on the problem instance. ACO algorithms have been applied for the reliability optimization of series- parallel systems [44], also including quantity discounts on the redundant components [45], and network optimization by embedding a Cellular Automata approach combined with Monte Carlo simulation for network availability assessment [46], within a multi-objective ACO search engine [47]. ACO has also been applied in hybrid form with Simulated Annealing (SA), called ACO SA, for the design of communication networks [48], where the design problem is to find the optimal network topology for which the total cost is a minimum and the all- terminal reliability is not less than a given level of reliability.
    SA is another type of meta-heuristics introduced by Kirkpatrick et al. [49] and Cerny [50] as a general probabilistic method for solving combinatorial optimization problems. SA searches the global optimal solution avoiding entrapment in poor local optima by allowing a (probabilistically) occasional uphill move to worse solutions. A SA algorithm for communication network reliability optimization has been proposed [51], which selects the optimal set of links that maximizes the overall reliability of the network subject to a cost constraint, given the allowable node-link incidences, the link costs and the link reliabilities. The algorithm employs a variation of the SA approach coupled with a hierarchical strategy to achieve the global optimum. SA has also been applied to search the optimal solution of system reliability-redundancy allocation problems [52] also considering nonlinear resource constraints [53]. Different SA strategies have been applied to solve multi- objective system reliability optimization problems [54].
    Particle Swarm Optimization (PSO) is another algorithm conceptually based on the social behavior of biological organisms that move in groups, such as birds and fishes [55]. The basic element of PSO is a particle, which can fly throughout the search space toward an optimum by using its own information and that provided by other particles within its neighborhood. As in GA, the performance of a particle is determined by its fitness that is assessed by calculating the objective functions of the problem to be solved. Then, PSO has certainly some similarities to evolutionary algorithms such as GAs, but it also in- corporates a cooperative approach. Indeed, all individuals (particles) which are allowed to survive change their positions over time and one particle's successful adaptation is shared and reflected in the performance of its neighbors. Originally developed for the optimization of continuous unconstrained functions, PSO did not attract much attention from the reliability community, because most reliability optimization problems are of discrete nature and have constraints. However, it has been shown that properly adapted PSO can be an effective tool for solving some discrete constrained reliability optimization problems [56]. PSO has, then, been applied to solve reliability optimization and RAP of complex systems [57].
    Several optimization meta-heuristics have been designed for various optimization applications in reliability engineering, with varying degrees of success. As no meta-heuristic is so versatile to always outperform the other meta-heuristics in all kinds of reliability optimization problems, developing new, good optimization approaches can be very helpful in some specific applications and benefit practitioners providing more options. Overall, some preferences in practice is given to the use of GAs, as they are able to solve both integer reliability problems and mixed-integer reliability problems. Furthermore, their applicability is not limited to series-parallel systems. In many reliability optimization problems, the optimal solutions found by GAs have turned out to be superior to those of the other meta-heuristic methods for both integer reliability problems (in which component reliabilities are given and redundancy allocation is to be decided) and mixed integer reliability problems (in which both the component reliabilities and redundancy allocation are to be decided simultaneously). Therefore, GAs are very competitive and attractive meta-heuristic methods, especially appropriate for design of nonstandard series-parallel systems. In addition, the multiple solutions found by the GA sometimes vary significantly in the
    component reliabilities and/or redundancy allocation for systems. This offers the design engineer a variety of options from which to choose with only small differences in the system reliability.
4. Era of pragmatism
    After exhausting much of the inventory of reliability optimization problems that could be solved to optimality by mathematically rigorous methods, researchers entered into an era of pragmatism. The driver for this was the need to expand the types of problems to treat, considering more complex systems and more realistic reliability behaviors of the components, without necessarily being able to mathematically prove the optimality of the solutions (although this remains highly desirable). 
    Original problem formulations that were solved to optimality often adhered to some common assumptions, although not always, including (i) active redundancy, (ii) perfect switching of redundant components, (iii) limitations on mixing functionally equivalent components within a parallel structure, (iv) binary behavior of components and systems, and others. These assumptions simplified the problems and optimal solutions could be found, but artificially constraining the problem spaces far from real conditions. Therefore, the usefulness of these methods was limited, and there was a need to analyze systems with more realistic behaviors, including multi-state systems, uncertain systems, realistic forms of redundancy, etc.
    For the more realistic and complex problems, the assumptions or model forms required for mathematical programming algorithms could generally not be satisfied. At the same time, more advanced computers and computer processing provided for exhaustive forms of heuristic search. GA and other forms of meta-heuristics were now used pre- dominantly.
4.1. Multi-state systems
    For components and systems used in practice, often a binary state description (functioning or failed) may not be a proper representation of the reliability behavior, because the component and system reliability performance has a range of different levels (Barlow and Wu [58], Hudson and Kapur [59]). However, evaluation of multi-state system (MSS) reliability is more difficult, and potentially mathematically cumbersome.
    Levitin et al. [60], and Levitin and Lisnianski [61] presented pioneering and influential research models to optimize system design for multi-state systems. Levitin et al. [60] determined an optimal system structure, and Levitin and Lisnianski [61] optimized imperfect preventive maintenance intervals. They used a universal generating function (UGF) approach to evaluate multi-state reliability and a GA to search the solution space to determine the best solution, although not guaranteed to be the optimal solution. UGF is a convenient function based on a z-transform that is useful to systematically and efficiently compute multi-state reliability.
    Levitin and his colleagues continued to extend their innovative work to analyze additional applications of multi-state systems. The first formulation of joint structure and maintenance optimization problem for multi-state systems was presented in Levitin and Lisnianski [62], and the optimization approach was extended to systems with common cause failures by Levitin [63]. Later, Levitin and Xing [64] analyzed systems with propagating failures. Each of these research efforts re- presented fundamental advancements. System reliability optimization could, then, be applied to an entirely new class of systems design problems.
    In recent years multi-state models for system reliability assessment have become increasingly popular. In particular, significant research efforts have been devoted to the solution of RAPs for series-parallel multi-state systems (MSSPS) [3,65?67], which was first introduced in [68]. Series-parallel structures are typically considered because they are quite common in practice. Due to the difficulty of the problem, meta-heuristics are often used to solve MSSPS RAP, even though they can become time-consuming, especially on large systems.
    On the other hand, theoretical analysis of meta-heuristics for MSSPS RAP has been generally lacking. Exact/approximated algorithms or guidance for meta-heuristics design have not yet been proposed in the MSSPS RAP literature, while it is important because the application of RAP to multi-state models often requires exhaustive computational resources. Indeed, the difficulty of solving MSSPS RAP is not only due to the well-known problems of MSS reliability evaluation, but also to the discrete, probabilistic and nonlinear nature of RAP problems.
    Another form of a system where the components exhibit multiple states is when component failure time distributions or state prob- abilities and replaced with a stochastic degradation process. This problem can be particularly challenging when the individual component degradation processes are dependent or have interactions. Song et al. [69] determined optimal replacement intervals and inspection intervals for systems with dependent failure processes. Bian and Gebraeel [70] developed a prognostics model for a multi-component system with degradation interactions.
4.2. Uncertainty
    The optimization of system reliability relies on a model to provide a representation of the system failure behavior. The model is built on a number of hypotheses on the types of distributions which the stochastic failure processes of the components obey. The values of the parameters of these distributions need to be estimated, and there is always some level of estimation uncertainty. There is intrinsic uncertainty and in- complete knowledge of the system behavior. Uncertainty can be model or structural uncertainty, which exists on the hypotheses of the model, or parameter uncertainty, which exists on the values of its parameters. 
    In the literature, a number of aspects, factors and causes of un- certainty have been identified, as summarized by Armacost and PetEdwards [71], Zimmermann [72]:
    Lack of information or knowledge: Lack of information, knowledge and/or data is the main source of uncertainty. This type of uncertainty can be reduced by collecting more information and data.
    Approximation: Any model involves some degree of approximation, which is necessary when there is insufficient information to describe exhaustively the phenomenon of interest or when it is desirable to simplify the analysis due to computational constraints or other reasons.
    Abundance of information or knowledge: People are incapable of assimilating many pieces of data and information simultaneously. The analyst usually focuses on those parameters and those pieces of data and information that are considered to be more important, while neglecting the others. This type of uncertainty is related to biases in subjective probability assignments (see Kahneman and Tversky [73] and Aven [74]).
    Conflicting nature of pieces of information/data: When there is conflicting data, increasing the amount of available information and data would not decrease the uncertainty. More data may just increase the conflict among different pieces of information and data. Some information are affected by errors creating the conflict, although the analyst cannot identify them, or otherwise, the model used by the analyst is poor.
    Measurement errors: The measurement of a physical quantity, such as temperature, weight, length, is always affected by the precision of the measurement capability.
    Linguistic ambiguity: An expert may express that something is big, but the meaning of ¡°big¡± is ambiguous, and can be interpreted in different ways.
    Subjectivity of analyst judgments: There can be different interpretations of the same information and data, depending on cultural background and competence of the analyst.
    Uncertainty analysis involves identifying and studying the sources of uncertainty and propagating the effects onto the output of the model. Uncertainty analysis determines the uncertainty in the model output that results from uncertainty in the model inputs (Helton et al. [75]). In the practice of reliability engineering and quantitative risk analysis, it is common to distinguish between aleatory and epistemic uncertainty (Apostolakis [76], Helton and Oberkampf [77]). Aleatory uncertainty refers to phenomena occurring randomly, so probabilistic modeling is appropriate to describe such occurrences. Epistemic uncertainty involves quantifying the degree of belief of the analysts on how well it represents the actual system. It is typically expressed as subjective probabilities of the parameters of the probability models. It can be reduced by gathering information and data to improve the knowledge on the system behavior.
    For system reliability optimization, uncertainty must be properly accounted for. It is often important to consider the uncertainty in the system reliability estimation so that risky solutions with unsatisfactorily high reliability estimation uncertainty can be avoided. System de- signers and users are generally risk-averse. Decision makers would generally prefer the design of a system whose reliability is estimated with large confidence, as assured by the low uncertainty of its estimation. Thus, maximization of the system reliability and minimization of its estimation uncertainty is an important formulation, that should be emphasized.
    System reliability optimization research originally did not consider the uncertainty in the reliability estimation, although Rubinstein et al. [78] is an early example of a model to maximize the expectation of a series-parallel system reliability estimate with component uncertainty. However, maximization of the expectation of the reliability estimate may not be adequate, if it is important to avoid system designs with unacceptably high uncertainty. It is therefore desirable to use a multiple-objective optimization algorithm, which explicitly considers the component uncertainty.
    In Marseguerra et al. [79], a multi-objective GA is developed to select optimal network designs that balance the dual objectives of high system reliability and low uncertainty in its estimation. Monte Carlo simulation is used to evaluate the objective function and Pareto optimality is introduced to handle the multiple preference criteria. The decision variables are the numbers of components, xij, of a given type j to be allocated in the various sections (node pairs & links) i of a network system, i = 1, 2,ÿ, s, and j = 1, 2, ÿ, mi. The network is designed to maximize the expectation of the network reliability and minimize the variance of the estimate (by maximizing the negative variance). Introducing cost and weight constraints, the multi-objective optimization problem may be formulated as follows:
    This is an appropriate formulation for a risk-averse decision maker, as opposed to most optimization algorithms that require or assume risk- neutrality. Many decision makers may prefer a risk-averse solution, with a marginally lower expected value of reliability compared to a solution with a higher expected value, but with unacceptable un- certainty. Epistemic uncertainty has also been accounted for using interval and fuzzy multi-state models [67,80?82].
4.3. Different types of redundancy
    The original formulations of the system reliability optimization problems assumed that all redundancy was active redundancy. This is a convenient assumption because the failure time of a parallel subsystem of components is the maximum of individual component failure times, and the reliability, or probability of survival for some mission time, can be expressed using standard probability principles that are independent of any failure time distribution assumptions. However, many actual subsystem design problems, use a variety of active, cold, warm or hot standby, often within the same design, and therefore the original formulations and solution methods were not practical or applicable for many actual problems.
    System designs with active redundancy have fully activated components that can continue to provide needed design functions in the advent of failure of a primary component, until all redundant components have failed as well. Cold standby redundancy involves the use of non-activated components that can be switched-on in response to failure of a primary component. Often systems are designed with both types of redundancy within different parts of the system, and there are examples where the redundancy type is also a design variable. Cold standby redundancy requires switches to detect failure and activate redundant units. However, the switches can also fail and must be considered in the optimization model. It is assumed that components in cold-standby do not fail, while components in warm standby can fail, but at a lower rate those comparable active or hot standby components. Components in hot standby still require a switching mechanism, but fail at the same rate as active components.
    A solution methodology was developed to determine optimal design configurations for nonrepairable series-parallel systems with cold- standby redundancy by Coit [83], who considered a component with non-constant component hazard functions and imperfect switching. There were multiple component choices available for each subsystem and component failure times are distributed according to an Erlang distribution. Optimal solutions are determined using IP with 0?1 decision variables.
    There are other engineering system design projects the choice of redundancy type becomes an additional design variable. System design optimization was demonstrated in Coit [84] to maximize reliability when either active or cold-standby redundancy can be selectively chosen for individual subsystems. Formulation of the problem allowing a choice of redundancy strategies is more realistic and practical. Optimal solutions to the problem are found using IP considering imperfect switching of standby redundant components [84]. The optimal system design is distinctly different from the corresponding design obtained with only active or only cold standby redundancy. The same problem was later solved using a GA [85]. Most recent research on systems with imperfect switches has been done by Kim [8] and Levitin et al. [86].
    The problem with a choice of redundancy strategies has been ex- tended in several original ways including a mixed strategy [87] combining both active and cold-standby components within the same sub- system. Other recent meaningful system reliability optimization research considering mixed component redundancy have provided important models for more varied and practical applications [88?90]. There have also been multiple objective formulations to the problem with different redundancy types [91,92].
    More recently, other different redundancy strategies or types of problems have been considering including the standby element sequencing problem and the combined sequencing and inspection/ checkpointing/allocation policy optimization for different types of standby (hot, cold, warm, mixed). Some research efforts combining system structure optimization with optimal standby component sequencing are included in [8,9,93,94].
5. Era of active reliability improvement
    There are currently some very promising on-going research activities that can be considered collectively as an era of active reliability improvement. System reliability optimization is not a static model, but it is being conducted continuously in response to new data being collected on failures, component and system degradation, environmental stresses and usage histories. As an integral part of the optimization process, system performance can be optimized and improved dynamically as new data is collected and analyzed to provide a better under- standing of usage conditions and failure behavior, or to compensate for changing conditions. Modern sensor and communications technologies facilitate the collection and transmission of data so that the optimal system design and maintenance plans can be continually enhanced.
    Standard assumptions for most system reliability optimization models have been that component failure times form a homogeneous population, and that the failure time distributions, or reliability for a fixed time, are static or stationary. In practice, both assumptions are at best approximations of actual conditions. Although a population of components may form a homogeneous population, their corresponding failure times are influenced by specific environmental stresses and user requirements/stresses that can vary appreciably for specific sub-populations of users or applications. Also, there can be systemic trends in stresses, that can result in shifting failure time distributions over time. In these cases, there is not actually a homogeneous population of identically distributed failure times, and therefore, most optimization models cannot accommodate these realities
    The era of mathematical programming resulted in entirely new in- sights on optimizing system design, and demonstrated how advanced mathematics can be used to solve this problem. The era of pragmatism extended the more theoretical models or developed new ones to address actual conditions of fielded systems. However, in both of these eras, the optimization results were a final result. The solution to these difficult optimization problems was intended to be performed once, perhaps with associated sensitivity analyses. Of course, as new data was collected, the analysis could be repeated, but the optimization process did not directly integrate changing conditions. In the era of active reliability improvement, the changing conditions and data analyses are an integral part of the model.
5.1. Dynamic system reliability models responding to new data
    Dynamic optimization of system reliability has the potential to achieve responsive system designs, which are highly reliable with changing or diverse conditions. To achieve the highest level of reliability and minimum cost, engineering designs and maintenance plans must address changing conditions or new data that provides better estimates of model coefficients or parameters. To accomplish this, the optimization must be dynamic. The model is solved over-and-over or continually as part of the optimization in response to new data/conditions/etc. as the system is operated.
    Yildirim et al. [95,96] present two comprehensive models that involve integrating sensor-based degradation modeling and remaining life distributions with classical mathematical programming, specifically mixed integer programs. The resulting model optimizes predictive maintenance decisions for a complex system to minimize cost. The application being solved is the unit-commitment problem, a well-studied optimization problem, pertaining to power generation and trans- mission. Yildirim et al. [97] considers opportunistic maintenance scheduling again within an integrated framework that combines mixed integer programming and sensor-based degradation models.
    Hao et al. [98] addresses dynamic optimization of workload assignment to actively control the degradation and failure time for multiple-units system. Components are degrading and failing, but the rate of degradation is a function of workload assignment. Multiple units are arranged in parallel, and several identical machines may need to operate together to simultaneously produce products to meet the high production demand. This parallel configuration is designed with redundancy to compensate for unexpected events. As data is collected, there is Bayesian updating of degradation model parameters and re- optimization of workloads.
    Recent developments by Li et al. [99] have proposed providing industrial assets with a degree of agency, in order to enable real time prognostics and optimization of the asset's operation conditions. They consider the feasibility of improving system-level performance in industrial systems by integrating social networks into the IoT (Internet of Things) concept.
    Bascifti et al. [100] considers a complex system of buses, generators and transmission lines. The model considers the scenarios where un- expected failures happen based on the updated remaining life distributions. The modeling framework in this case is a stochastic optimization model with chance constraints that leverages sensor-based remaining life predictions.
5.2. System reliability optimization customized for specific subsets of users
    Data analytics can also be exploited such that the optimal system design can reflect differences within a population. There can be regional differences or fundamental differences within the user population, and by observing and quantifying specific usage conditions and failure patterns, an optimal design can simultaneously correspond to a collection of diverse users or conditions. A failure time distribution can be considered as a function of usage and environmental stresses, and specific reliability values can then naturally vary to reflect these differences.
    Ramanan et al. [101] studies an advanced distributed optimization problem. There are several interesting aspects and challenges to this problem. The first relates to the computational challenges associated with large scale decentralized optimization and the second relates to the underlying high-performance computing architecture that is would be suitable for such decentralized systems. Advanced data processing of large data sets within subnetworks (local utility companies) is required. 
    Bei et al. [102] presents a model to fully investigate the integrated redundancy allocation and maintenance planning problem with the presence of uncertain future usage stresses. Component failure time distributions are expressed as a function of environmental and usage stresses. A component system design, with component choices and redundancy levels is selected by the optimization model, but specific preventive maintenance intervals are selected for different usage and environmental stress vectors. The problem is formulated as a two-stage stochastic optimization problem with discrete scenarios defined for different usage and environmental conditions. Zhu et al. [103] extends this model by considering uncertain aperiodic changing stresses.
6. Future challenges in system reliability optimization
    The safe and productive performance of industrial systems depends on optimal designs that use equipment reliably, and on testing and maintenance activities that assure the required high level of reliability, availability and maintainability of the equipment. This is done through the efficient assignment of resources that are usually limited. A number of challenges arise in relation to the modern complex systems reliability optimization:
    Integration and response to continual streams of data proving new and updated information.
    Accounting for both aleatory and epistemic uncertainties within the decision-making framework of system reliability optimization 
    Cooperative optimization of multi-agent systems, with individual objectives to be optimized within an overall system optimization 
    Integrated optimization of reliability design, maintenance, spare parts inventory and logistics management
    Dynamic optimization of evolving systems under changing conditions
7. Conclusions
    In this paper, we have provided an organized discussion and review on the evolution of the subject of complex system reliability optimization, which is at the heart of reliability engineering. We have presented how the development of solutions to such problem, and their application, have evolved as a continuum of ideas and techniques, which we have chronologically organized into the three eras of Mathematical Programming, Pragmatism, and Active Reliability Improvement.
    In this flow of development and advancement, we have highlighted the joint pull force coming from the fields of operations research and optimization theory, and from the evolution of technology. Their combination has led to the advancement of research ideas and theories, brought new perspectives from the engineering world, and resulted in the development and continuous improvement of methods and techniques to address reliability optimization problems of increasingly complex systems.
    The underlying message that emerges from this is that system re- liability optimization is an ongoing topic of scientific development and will always be so. The research is actively pulled by the advancements in mathematics, operations research and optimization theory, and in response, researchers will continually develop new methods and algorithms to solve, more efficiently than before, broader classes of problems, and larger and more difficult problems. At the same time, the research is pushed by the changes in technology, and in the engineering and social worlds, practitioners will continually demand for new developments to cope with the practical challenges encountered on the field.
    In conclusion, today we are treating problems that involve more complex systems and more realistic reliability behaviors of the components, including multi-state, uncertain behaviors, etc. We are beginning to address them dynamically, as new data is collected and analyzed to provide a better understanding of usage conditions and failure behavior, or to compensate for changing conditions, so that the optimal system design and maintenance plans can be continually enhanced. This is possible due to the collection and transmission of data by modern sensor and communications technologies. Yet, new opportunities and challenges are always arising, and it will always be necessary to find efficient ways to solve new problems or problems previously unsolvable.
    
    

Keywords:
Fault detection Wind turbine SCADA data
Non-singleton fuzzy inference system Expanded linguistic terms and rules

Abstract

Wind power generation e?ciency has been negatively a?ected by wind turbine (WT) faults, which makes fault detection a very important task in WT maintenance. In fault detection studies, fuzzy inference is a commonly- used method. However, it can hardly detect early faults or measure fault severities due to the singleton input and the limited linguistic terms and rules. To solve this problem, this paper proposes a WT fault detection method based on expanded linguistic terms and rules using non-singleton fuzzy logic. Firstly, a generation method of non-singleton fuzzy input is proposed. Using the generated fuzzy inputs, non-singleton fuzzy inference system (FIS) can be applied in WT fault detection. Secondly, a mechanism of expanding linguistic terms and rules is presented, so that the expanded terms and rules can provide more fault information and help to detect early faults. Thirdly, the consequent of FIS is designed by the expanded consequent terms. The defuzzi?ed result, which is de?ned as the fault factor, can measure fault severities. Finally, four groups of experiments were conducted using the real WT data collected from a wind farm in northern China. Experiment results show that the proposed method is e?ective in detecting WT faults.


1. Introduction

The rapid development of wind energy has boosted the installations of wind turbines (WTs) [1]. Meanwhile, an increasing demand for higher power generation e?ciency has put more pressure on the op- eration and maintenance (O&M) of WTs [2]. One of the essential tasks of O&M is to deal with WT faults, which have negatively a?ected power generation e?ciency and caused a heavy economic loss [3]. If WT faults are detected in time, it would greatly reduce O&M costs [4]. Therefore, due to its signi?cant role in the O&M of WTs, there have been increasing studies on WT fault detection, such as the detection methods based on signal processing [5], image processing [6], machine learning [7], deep learning [8], etc.


Fault detection based on condition monitoring (CM) is one of  the most commonly-used methods. The main principle of the method is to examine whether the collected on-line data are within  the  normal range. If the data are out of the normal range, there might be  an anomaly or a fault. Furthermore, WT faults can be detected and diag- nosed according to di?erent anomaly data. Fault detection based on CM is e?ective and easy to operate. In [9], an unsupervised anomaly de- tection approach for WT condition monitoring was proposed. In [10], a temperature-based real-time aging monitoring method was  presented for power converter modules. In [11,12], CM-based fault detection methods were put forward to detect the WT faults in gearbox and generator respectively.
In fault detection methods based on CM, many types of data can be used, including Supervisory Control and Data Acquisition (SCADA) data [13], vibration data [14], strain data [15], acoustic data [16], and lu- brication oil data [17]. However, the acquisition of the vibration data and other types of data requires additional sensors, which increases the maintenance cost [18]. Fortunately, modern wind farms (WFs) usually install SCADA system, which collect hundreds types of on-line WT data at a certain interval. SCADA data are cost e?ective, as no additional sensors are needed [19]. A lot of research based on SCADA data has been put forward to realize the fault diagnosis and fault prognosis. In [20], machine learning models based on domain knowledge were proposed to realize fault detection. In [21], deep neural network (DNN)-based framework was put forward to detect WT gearbox faults. In [22], many data-mining approaches for wind power prediction were evaluated, which can be used for fault detection. In [23], a fault de- tection method based on multiclass support vector machine algorithms was proposed. Meantime, fault prognosis has gained a rapid develop- ment in recent years [24]. Based on SCADA data, an on-line fault and help to detect early faults. Moreover, the defuzzi?ed  fault  factor can tell fault severities.

The rest of this paper is organized as follows. Section 2 introduces the related work and the problem description. Section 3 describes the proposed method in detail. Experiments and comparisons are listed in Section 4. Section 5 is the conclusion of the present work.

2. Related work and problem description

2.1. Review of the FIS method in WT FAULT detection

FIS method of WT fault detection based on SCADA data generally consists of the following three steps [30,31].
Step 1: Data prediction. First, the original normal SCADA data
Dorg (x) (consisting of n variables) are collected, as shown in Eq. (1).


prognosis method [25] and a prior knowledge-based prognosis method of WT pitch faults were proposed.
Among the CM and fault detection approaches, one of the com- monly-used method is the fuzzy inference system (FIS). FIS evaluates inputs with ¡°if-then¡± rules based on fuzzy logic. There are two parts in  the rule: ¡°if¡± part gives the evaluation of the input and ¡°then¡± part returns a fuzzy output according to the rule [27]. Based on FIS, many  fault detection methods have been proposed. In [28], FIS was estab- lished   with  rare  association   rule  to  predict   the  spatiotemporal   dis-
tribution  of  energy  security  weaknesses  in  transmission  networks.   

In where, Di (x) is the ith variable in Dorg (x). The SCADA data usually consist of various variables [34,35], including wind speed (m/s), active power (kw), generator temperature (¡ãC), and bearing temperature (¡ãC), etc.
Then, fault-related variables of SCADA data are chosen. These variables are predicted by their relevant data (including current data, historical data and current data of other WTs [31]), as indicated in Eq.  (2).


[29], an adaptive neuro-fuzzy inference system and hybrid models were developed. In [30], a system  for WT condition  monitoring using where, Dk is the fault-related variable and Dd (j) is the jth relevant data adaptive neuro-fuzzy interference systems was proposed, and it has achieved good experiment results. In [31], an e?ective generalized model for WT anomaly identi?cation method based on fuzzy synthetic
evaluation was put forward. In [32], a development of a fault diagnosis of Dprd. fpredict is the prediction method, which mainly includes time series method [36], neural network method [37], etc.
Step 2: Anomaly detection by the prediction error. With the collected data Dk (x) and its predicted data Dk (x), the prediction error scheme based on identi?ed fuzzy models was presented. In [33], a  monitoring strategy of short-circuit fault between turns of the stator can be calculated by Eq. (3).

windings and open stator phases by fuzzy logic technique was proposed.

However, several problems remain in the WT fault detection based on FIS: (1) Early faults cannot be detected using conventional FIS methods due to the averaged singleton input. The widely-applied methods usually use daily averaged data as the input to reduce false alarms of WTs. However, the details of the data are omitted. (2) Fault severities cannot be measured by conventional FIS methods due to the limited number of FIS consequent terms (for example, only ¡°normal¡± and ¡°fault¡±).
In order to tackle the above problems, this paper proposes a WT   fault detection method with SCADA data based on expanded linguistic terms and rules using non-singleton fuzzy logic. First, a method of generating non-singleton input is proposed. The non-singleton  inputs can be obtained by transforming probability density functions (PDFs) of the grouped prediction errors, which enables the application of non- single FIS in WT faut detection. Second, more antecedent and con- sequent terms are expanded by the proposed method. The rules of fault detection are also expanded accordingly. Consequently, the  fuzzy system can detect the fault at an early stage and provide more in- formation about fault severities. Third, the FIS consequent  is designed by the expanded consequent terms, and the defuzzi?ed output is de- ?ned as the fault factor, which can tell the severity of the fault.
The contributions of this paper include:

(1) A generation method of non-singleton fuzzy input is proposed, and non-singleton FIS is applied to detecting WT faults. To the authors¡¯ knowledge, it is the ?rst time that non-singleton FIS has been ap- plied to detecting WT faults.
(2) A method of expanding linguistic terms and rules in FIS is proposed. The expanded terms and rules can provide more fault information,

Then, probability distribution function (PDF) of the prediction errors can be obtained. Also, the upper bound Bup and the lower bound Bdn of the PDF are calculated as follows: (1) A con?dence interval is set, and
(2) the value of the left endpoint of the con?dence interval is de?ned as the lower bound and that of the right endpoint as the upper bound. For example, if   is set as 99%, the value corresponding to 0.5% in the PDF    is the lower bound and the value corresponding to 99.5% in the PDF is  the upper bound. Therefore, if the collected on-line prediction error dr   is greater than Bup, it is marked as ¡°high¡±, whereas if dr  is less than Bdn,  it is marked as ¡°low¡± (in some researches, more bounds are introduced, such as ¡°very high¡± and ¡°very low¡± [30]).
Step 3: Fault detection based on FIS. In order to detect a certain fault, prior knowledge is extracted as rules to detect WT faults [38]. According to certain combinations of data anomalies, WT faults can be diagnosed. Fig. 1 shows an example of the WT fault detection with the conventional FIS method. Two variables of WT SCADA data (Data 1 and Data 2) are collected and predicted. Then, PDFs of the two prediction errors are calculated and the lower bounds and the upper bounds are obtained. At a certain moment, Data 1 is marked as ¡°high¡± and Data 2 is marked as ¡°low¡± (as illustrated by red points in Fig. 1). Then, it is de- termined as Fault X according to Rule 001.

2.2. Problem description

There are several problems that prevent the conventional FIS method from detecting the early fault. The ?rst problem is the limita- tion of singleton input in FIS. In order to reduce false alarms, daily averaged data, rather than the 10-min data, are often used by FIS methods, which would result in the lack of data details. Fig. 2 shows an example. In this case, the gearbox oil temperature is monitored at an interval of 10 min. The blue dot region is a time window T, in which the data points are averaged as one value. It can be found that although the averaged value in T is lower than the upper bound, many data points in T are higher than the upper bound. Such case can be regarded as a potential anomaly, which cannot be detected by the conventional sin- gleton-based FIS.
The second problem is the limited number of linguistic terms of a fault. Linguistic terms in WT fault detection are extracted from the prior knowledge and experts¡¯ experience. The number of linguistic terms is small due to the limited prior knowledge and experts¡¯ experience. In practice, the consequent of rules usually include only two terms of ¡°Normal¡± and ¡°Fault¡±. Consequently, it prevents FIS from detecting  early faults and telling fault severities.

3. Method

3.1. Architecture of the proposed method

The architecture of the proposed method is designed based on the process of conventional FIS, as shown in Fig. 3. There are six phases in this architecture (The red dotted parts are the originalities of the pro- posed method).
Phase 1: Data prediction. As discussed in [30], according to the commonly-used FIS methods, the original data are collected and pre- dicted. In this study, the current data and the historical data are se- lected to predict the target data. Many approaches can be used to predict data [39,40]. In this paper, the widely-applied neural network (NN) method is used to realize the prediction. The prediction error can be obtained by Eqs. (1)¨C(3).
   
Phase 2: Non-singleton input transformation. Di?erent from the conventional FIS method, the proposed method introduces non-sin- gleton FIS input in WT fault detection. The prediction errors are divided into groups. The PDF of each group is calculated. Then each PDF is transformed to a non-singleton input. With the non-singleton input, non-singleton FIS can be applied in WT fault detection. Details are discussed in Section 3.2.
Phase 3: Linguistic terms and rules acquisition. Similar to the con- ventional FIS method [41], linguistic terms and rules are extracted from prior knowledge and experts¡¯ experience. First, rules for detecting WT faults, such as ¡°IF A is High AND B is Low THEN Fault 01 (Fault)¡±, are acquired. Second, linguistic variables and terms are extracted from the rules, such as ¡°A¡± is extracted as a linguistic variable, whereas ¡°High¡±, ¡°Normal¡±, ¡°Low¡± are extracted as the linguistic terms of ¡°A¡±. Conse- quently, the original membership functions (MFs) can be calculated by fuzzy statistics method [42]. There are many types of MFs, such as Gaussian MF [43] and interval type-2 MF [44]. In this paper, the tri- angular/trapezoid MFs are adopted. These MFs can be easily obtained by data-driven method, and many similar methods [30,31] have achieved good results using this type of MFs in detecting WT faults.
Phase 4: Linguistic terms and rules expansion. Linguistic terms of antecedent are expanded, with the aim of enabling the FIS  to detect early faults and measuring fault severities. Then according to di?erent combinations of expanded antecedent terms, consequent terms at dif- ferent fault levels are generated. Accordingly, the expanded rules  are also obtained. Details are described in Section 3.3.
Phase 5: Fuzzy inference. In this phase, fuzzy inference engine [45]     is applied to processing fuzzy input sets into fuzzy output sets. First, ?ring levels are calculated by the non-singleton fuzzy inputs and antecedent MFs. Then, output sets are obtained by the calculation of

consequent MFs and levels.Phase 6: Fault factor. Normally, the conventional FIS method only
uses the output of the rule as the ?nal result of the fault detection. In this paper, benefitted from the expanded linguistic terms and rules, the
   
The designed process is shown in Fig. 4. First, each obtained pre- diction error Dk is regarded as a linguistic variable. Then, Dk is divided into groups by Eq. (4). fault detection can go a step further. The fuzzy output sets are de
 to a crisp output (fault factor), which is designed to measure fault severities. Details are described in Section 3.4.

3.2. Non-singleton input TRANSFORMATION

Compared with singleton FIS, fuzzy logic based on non-singleton  input proved to be a more e?ective method in theory [45]. However, in practice, it has not been applied in the fault detection ?eld. In  this  paper,  the fuzzy number  of non-singleton  input is correlated  to the PDF
of prediction  errors, which realizes  the application  of non-singleton  

where, t is the group number and P is the number of data points in each group (in this paper, P is set as 144). Next, each Dk is converted to a  fuzzy number. Di?erent from singleton FIS, which uses a single value as the input, non-singleton FIS uses fuzzy number as the basic input unit.
To qualify as a fuzzy number, a fuzzy set A must possess the following properties [41]: (1) A must be a normal fuzzy set; (2) A¦Á  must  be  a closed interval for every ¦Á ¡Ê (0, 1] and (3) the support of A must be bounded.
Normally,  the  distribution  of   is  considered  as  a  normal  distribution in this method. 

a closed interval for every ¦Á ¡Ê (0, 1]; (3) By setting an appropriate con?dence interval, the support of f k is bounded. This means fk
completely meets the requirements of a fuzzy number. Thus, it can be used in non-singleton FIS.
Therefore,   the   non-singleton   input   transformation   can   be   implemented. First, the mean ¦Ìk  and the standard deviation ¦Òk  of each Dk are calculated by Eq. (5) and Eq. (6) respectively.

Then, the fuzzy number of non-singleton input can be obtained by transforming each f k , as shown in Eq. (7).

where, t is the group number of Dk and P is the number of data points in each group. The fuzzy number of non-singleton input is brie?y marked as ¦Ìk (x) in Eq. (8).

MFs before the expansion. (b) Terms and their MFs after the expansion.

distribution of the input data set. The non-singleton input makes the calculated membership degree more accurately reflect the actual WT

condition, which could help to detect early faults that are undetected by

Therefore, the input data set ¦Ì? (x1, x2¡­xq) of the lth Rule Rl (with q
linguistic terms) can be calculated by Eq. (9).

3.3.  EXPANDING  linguistic  terms  AND  rules

where, ¡ï is the minimum t-norm operator [46].

   
In this section, a method of expanding linguistic terms and rules is proposed.
The limited linguistic terms is a problem in FIS. In order to solve this problem, Jerry M. Mendel ?rstly proposed a basic theory on linguistic

terms expansion [47]. The main idea is to extract new linguistic terms from the original ones according to their features. Inspired by this

where, ¦ÌFl is the antecedent MF. Then, ?ring levels Fll are calculated with ¦Ì? (x1, x2¡­xq) and ¦ÌFl (x1, x2¡­xq) by Eq. (11).

theory, a mechanism of expanding linguistic terms and rules for WT fault detection is designed in this paper.
PART 1: The expansion of antecedent linguistic terms. Theoretically,

the expanded terms should be generated from the existing ones.


Therefore, in this paper, every two existing adjacent terms are used to generate a new term.



where, X is the domain of the input sets.
Fig. 5 shows an illustration of the computing process of non-sin- gleton membership degrees. It can be found that the membership de- gree is no longer determined by a single input value, but by the

statistical method. Then, the crossing points of MFs and their x-values are marked (PSL and PSH in Fig. 6(b)). The crossing point is the fuzziest point (take PSL for example, it is neither ¡°Low¡± nor ¡°Normal¡±). Thus, its membership degree in the new term is de?ned as 1. Finally, the vertical


Fig. 5. Illustration of the computing process of non-singleton membership degrees. (a): The non-singleton fuzzy input and MFs of the antecedent. (b) and (c): The calculated membership degree (the red dot) of the antecedent terms. (d) Calculation results.



line through the crossing points is made and the original critical points are connected. The obtained triangles (pink dotted lines in Fig. 6(b)) are the MFs of the expanded terms.
Operator  is de?ned as the above generation process. Thus, a new
linguistic term can be obtained by Eq. (12).

where, Tmnew is the expanded new term. Tmleft and Tmright are its two adjacent terms.
PART  2:  Expanding  rules  and  consequent  linguistic  terms.  With  the expansion   of   antecedent   linguistic   terms,   rules   are  accordingly   ex- panded. Supposing that the original rule R of a fault has N antecedents, each term in the antecedent is expanded to more new terms according to the expansion method described in PART 1. Thus, each antecedent in R has two types of terms, the original term Tmorg  and the expanded term Tmexp. Through di?erent combinations of Tmorg  and Tmexp, new rules are generated. If the new rule has m original terms (accordingly, there will be  N ? m  expanded  terms),  the  consequent  of  the  rule  is  de?ned  as Error  Level  m.  Consequently,  there  is  only  one  error  result  before  the expansion,  whereas  N + 1  Error  Levels  after  the  expansion,  with  the most serious fault Error Level N and the least serious fault Error Level 0. Next, consequent linguistic terms are expanded. Each Error Level is de?ned  as  a  new  consequent  linguistic  term.  Thus,  there  would  be N + 1 consequent linguistic terms of the fault. The linguistic terms and
rules expansion algorithm is summarized in Algorithm (1).
Algorithm 1. Linguistic Terms and Rules Expansion

Fig. 7. The designed consequent and the process of calculating the fault factor.
(a) The designed consequent and the calculated ?ring levels. (b) The gravity center of the intercepted consequent.

Then, the triangular MF, which has been e?ectively used in con- sequent de?nition, is applied in all consequent terms, as shown in   Fig. 7(a).
To get the output data set of the lth rule, the lth consequent is calculated by the lth ?ring level obtained by Eq. (11), as shown in Eq. (14).

its linguistic variable), 

3.4.  Design  of  the  consequent  AND  FAULT  FACTOR

In the conventional FIS method, the process of WT fault detection ends once the fuzzy consequent (for example, ¡°fault¡± or ¡°normal¡±) is obtained. However, this result could neither help to ?nd early faults nor tell fault severities due to its limited consequent terms. In this section, defuzzi?cation is applied and the fault factor (used to measure fault severities) is designed. Fig. 7 is an illustration of the designed con- sequent and the process of calculating the fault factor.
First, original consequent terms Tmcsq and the expanded consequent terms Tmexp?csq are combined, as shown in Eq. (13).
   
In summary, the technologies of the proposed method are described as follows: NN-based data predictions and non-singleton transformation (Section 3.2) are used to obtain non-singleton inputs. Expanded lin- guistic terms and rules are obtained by prior knowledge and the ex- panding method (3.3). Then, the non-singleton FIS is applied in fault detection. The defuzzi?ed fault factor (3.4) can tell the severity of the fault. Moreover, multiple types of faults can be detected by one FIS of the proposed method with su?cient rules.

4. Experiments and discussion

4.1. Experiment settings

In order to verify the e?ectiveness of the proposed method, four groups of experiments are conducted using real WT SCADA data collected in a wind farm (WF) in northern China. There are altogether 22 WTs in this WF. All the WTs are of the same type. The capacity of each

WT is 2 MW. The SCADA data collected in two years (with a time in- terval  of  10  min)  are  used  in  the  study.  The  commonly-used back
propagation  neural  network  (BPNN)  method  is  adopted  to  make  the prediction. Moreover, similar to the previous studies [30,31,33], the number of data points in a group is set too large, the condition would be monitored at a long time interval. (2) If the number is set too small, the statistical characteristics of the data would be weakened. Therefore, the number of data points in each group is set as 144 in this paper. In the following experiments, the faults used for evaluation are all real faults, and they undergo the development from early faults to serious faults. The earlier these faults are detected, the more conducive it is for WT maintenance.
Experiments are designed as follows: the proposed non-singleton input is veri?ed to detect the early anomaly in Section 4.2. The e?ec- tiveness of the proposed method for detecting early faults is tested in Section 4.3 (with multivariate input) and Section 4.4 (with one-variable input), respectively. Finally, the robustness of the method is veri?ed in experiments in Section 4.5.


4.2. Experiment 1: electiveness of the non-singleton FIS

This experiment is conducted to verify the e?ectiveness of the proposed non-singleton input in WT fault detection.
In June 2015, the gearbox oil temperature of WT 12 started in- creasing due to the aging of the oil. The high temperature of gearbox oil is a sign of a potential fault. In order to detect this anomaly, the data of gearbox oil temperature is used in this experiment. First, BPNN method is used to predict data and prediction errors are obtained. Fig.  8(a)  shows the prediction errors of the historical data and Fig. 8(b) shows    the PDF of the prediction errors. Then, the upper bound (3.71 ¡ãC) and the lower bound (?3.28 ¡ãC) are calculated.
Then, the corresponding MFs are obtained by the fuzzy statistics method [42]. The mathematical description of the MFs are listed as follows:

where, ¦Ìlow (x), ¦Ìnml (x) and ¦Ìhigh (x) are the MFs of ¡°low¡±, ¡°normal¡± and
¡°high¡±, respectively.
 In this experiment, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method. In order to make a better comparison, the setting of the pro- posed method in this experiment is the same as that of the conventional FIS method except for the fuzzy input part. The conventional FIS method uses singleton input, while the proposed method uses the non- singleton input.
Fig. 9 shows the experiment results. Fig. 9(a) and (c) show the averaged prediction errors in one day (APE-D) and in every 10 min (APE-M) respectively. (1) Using the conventional FIS method, APE-D exceeds the upper bound on 2015-06-24, triggering an alert, as shown in Fig. 9(b). However, from 2015-06-19 to 2015-06-23, although APE-D does not exceed the upper bound due to the high data variance, many APE-M exceed the upper bound, which could be considered as a po- tential anomaly. As can be seen, the conventional FIS method cannot capture these early anomalies. (2) Using the proposed non-singleton FIS method, the anomaly detection is improved, as shown in 9(d). It can be noted that the anomaly is detected on 2015-06-19, ?ve days earlier than the conventional FIS method.
Some details of the anomaly detections are shown in Fig. 10. On 2015-06-15, both the data calculated by the conventional FIS method (DTM) (the blue point in Fig. 10), and the data calculated by the pro- posed method (DPM) (the red point in Fig. 10) are below the upper bound, which indicates that the WT is in normal condition. On 2015- 06-19, DTM is below the upper bound while DPM is above the upper bound. It can be found that due to the large data variance, DPM in- creases. As a result, the anomaly can be detected in advance by the proposed method. It is not until 2015-06-24 that DTM  exceeds  the  upper bound, which is ?ve days later. From this experiment, it can be concluded that the proposed non-singleton FIS method can e?ectively detect early WT anomalies.

4.3. Experiment 2: COMPARATIVE experiment with MULTIVARIATE input
In this section, two groups of experiments are carried out to verify the e?ectiveness of the proposed method in WT fault detection with multivariate FIS input. Similar to Experiment 1, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method.


Fig. 8. The prediction errors and their PDF of gearbox oil temperature (WT 12).
(a) Prediction errors of the historical data. (b) PDF of the prediction errors.
4.3.1. 
Experiment 2.1: detecting cooling system FAULT
In early 2015, the cooling systems of WTs were improved to en- hance their heat dissipation capability. However, several WTs were poorly updated. The converter fan was improperly installed, leading to  an increase of converter temperature. As there are no sensors that can directly measure the converter temperature,the converter choke coil temperature (CCCT) and converter controller top  temperature  (CTT)  are monitored instead. The rule of the fault from the prior knowledge is summarised as: IF (CCCT is high) AND (CTT is high)  THEN  (the  con-  verter temperature is high and there are faults in WT¡¯s cooling system), which is listed as the ?rst rule in Table 1.
First, the prediction errors and their PDFs of CCCT and CTT are calculated. Also, the corresponding MFs are obtained by the fuzzy statistics method. Second, the original MFs and rules are expanded. (1)  As has been described in Section 3, the antecedent terms are expanded
Fig. 9. The anomaly of gearbox oil temperature (WT 12) and the detection results. (a) Averaged prediction errors in one day. (b) Anomaly detection result using the conventional FIS method. (c) Averaged prediction errors in every 10 min. (d) Anomaly detection result using the improved non-singleton input.
Fig. 10. The fault detection based on non-sin- gleton fuzzy input. (a), (b), (c) and (d) are four cases in the fault detection. (a): 2015-06-15. (b): 2015-06-19. (c): 2015-06-24. (d): 2015-06-30.
The blue point indicates the data calculated by the conventional FIS method and the red point indicates the data calculated by the proposed non-singleton FIS method.







Fig. 11. The consequent of converter high-temperature fault.

from three (low, normal and high) to ?ve (low, sub-low, normal, sub- high and high). (2) Accordingly, the original rules are expanded from one to four, as shown in Table 1. As can be seen, the expanded MFs and rules enrich the outputs.
 Then, the FIS consequent  is designed according to the descriptions in Section III, as shown in Fig. 11. The designed consequent has four terms: Normal, Warning (2 sub-highs), Minor error (1 sub-high and 1 high) and Major error (2 highs). It can be found that  the  original method has a consequent of two terms whereas the proposed method has four.
 Fig. 12 shows the monitored data during the fault and the detection results using the conventional FIS method and the proposed method. It can be found that during this period, both the prediction errors of CCCT and CTT increases.
In the conventional FIS method, on 2015-05-01, CCCT exceeds its upper bound, but CTT remains normal. On 2015-05-03 CTT also ex- ceeds its upper bound, triggering an alarm (as shown in Fig. 12(a), (b) and (c)). However, it can be found that from late April to early May, there are tendencies of increasing temperatures in both the prediction errors of CCCT and CTT. The conventional FIS method fails to detect such early faults.
Di?erent from the conventional FIS method, the proposed method
uses non-singleton input, expanded terms and rules in WT fault detec- tion. Moreover, the conventional FIS method depends on upper and lower bounds, while the proposed method uses fuzzy areas to determine membership degrees. From the experiment results, it can be found that:
(1) The fault is detected 5 days earlier by the proposed method. (2) On 2015-04-28, the fault factor rises above zero and a warning is triggered. From 2015-04-28 to 2015-05-10, the fault factor keeps increasing, in- dicating that the fault is getting worse.
 From the results of the experiment, it can be concluded that com- pared with the conventional FIS method: (1) the proposed method can detect faults at an early stage, and (2) it can tell the severity of the fault.

4.3.2. Experiment 2.2: detecting BLADE ANGLE sensor FAULT
In August 2014, the output active power of WT 16 decreased. After a shutdown inspection, a fault of blade pitch angle sensor was found. The measured angle is inconsistent with the actual angle, leading to a misjudgment of the control system. Consequently, the output active power is lower than it should be. According to this fault type, a rule can be summarized: ¡°IF (wind speed is normal) AND (output active power is low) AND (pitch angle is normal) THEN (possible  fault:  pitch  angle  sensor fault)¡±. Then, (1) the conventional method (such as the method  in [33]) and (2) the proposed method are used to detect this fault.
Similar to Experiment 2.1, ?rst, the PDFs of the prediction errors of
wind speed, output active power, and pitch angle are estimated. Then, the upper bounds and the lower bounds of these PDFs are obtained, and MFs are established by the fuzzy statistics method. The prediction er- rors of wind speed, active output power and pitch angle are shown in Fig.  13(a),  Fig.  13(b),  and  Fig.  13(c)  respectively.  The  experiment

Fig. 12. The monitored data and detection results of Experiment 2. (a) The averaged prediction errors of CCCT in one day. (b) The averaged prediction errors of CTT  in one day. (c) Detection result of the conventional FIS method. (d) The averaged prediction errors of CCCT in every 10 min. (e) The averaged prediction errors of CTT in every 10 min. (f) Detection result of the proposed method.

Fig. 13. The monitored data and detection results of Experiment 2.2. (a) Prediction errors of wind speed. (b) Prediction errors of active output power. (c) Prediction errors of pitch angle. (d) Detection result of the conventional FIS method. (e) Detection result of the proposed method.
results are shown in Fig. 13(d) and Fig. 13(e).
It can be found that the proposed method detects the fault (on 2014- 08-15) two days earlier than the conventional method (on 2014-08-17). From the experiment results, it can be concluded that the proposed method is e?ective in detecting WT faults with multivariate inputs.

4.4. Experiment 3: COMPARATIVE experiment with single-input

 This experiment is designed to show the e?ectiveness of the pro- posed method in WT fault detection with one-variable FIS input. Similar to Experiment 1, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method.
In July, 2015, due to the aging of the generator front bearing of WT 06, its temperature rose rapidly within a few days. On July 20th, WT 06 had a sudden breakdown. In this experiment, the monitored generator front bearing temperature (GFBT) is used. The same as that in the above experiments, GFBT is predicted and the prediction errors are obtained. The  upper  bound  and  the  lower  bound  of  the  predicted  errors  are

 As can be seen in Fig. 14(a) and (b), the prediction errors of GFBT increases rapidly from 2015-07-17 to 2015-07-19. The fault detection results of the conventional FIS method and the proposed method are shown in Fig. 14(c) and (d). It can  be found that: (1)  The conventional FIS method does not detect the fault until 2015-07-19, only one day before the breakdown. (2) Using the proposed method, the fault is detected on 2015-07-17, three days before the breakdown, giving more time for maintenances. (3) From 2015-07-17 to 2015-07-19, the fault factor is increasing, which indicates that the fault is getting worse.
It can be concluded that: (1) the proposed method is e?ective in
detecting early WT faults with one-variable input. (2)  The  proposed  fault factor can also tell the severity of the fault.
 Furthermore, in order to make a good comparison of the proposed method with more di?erent FIS methods. Another group of experiments is  conducted.  The experiment  setting is  the same  as that in Experiment
3. The following methods are compared with the proposed method: (1) the FIS method with a single prediction model (such as the method in [30]), (2) the FIS method with multiple prediction models (such as the method  in [31]), (3) the proposed  method  without  non-singleton input,
(4) the proposed method without rule expansion.
 Table 2 shows the experiment results. It can be found that con- ventional FIS methods cannot e?ectively detect early faults, either with

a single prediction model or with multi-prediction models. The pro- posed method can detect the fault at an early stage. Therefore, it can be concluded that: (1) The proposed method can e?ectively detect the early fault. (2) Both the proposed non-singleton input and the proposed expansion of terms and rules are e?ective in WT fault detection.



Robustness has always been an important factor for evaluating the methods in industrial applications [48,49]. In this section, experiments are conducted to verify the robustness of the proposed method.
The experiment settings are the same as those in Experiment 3. The normal data  of 300  days  are added  in the experiments. All  the data of 300 days are used to test the false alarm rate of the proposed method. In order  to  further  verify  the  robustness  of  the  proposed  method,  noises are  added  to  the  experiment  data.  First,  through  the  measurement,  it can  be  found  that  the  normal  data  themselves  have  a  signal-to-noise ratio (SNR) of 40 dB. Then, di?erent Gaussian white noises are added to the  data  to  test  the  robustness  of  the  method.  As  a  result,  the  experi- ment data have the ADDITIONAL SNR from 45 dB to 25 dB. Fig. 15 shows some examples of the experiment  data (has a SNR of 40 dB itself)  and the data with the added noise.
Table 3 shows the result of the experiments. It can be found that: (1) When the experiment data of 300 days (before adding noise) are used, there is no false alarm. Therefore, it can be concluded that the proposed method has a low false alarm rate in WT fault detection. (2) Moreover, the proposed method has no false alarm when the noise (no less  than  35 dB) is added. The experiment result shows that the proposed method has certain anti-noise capability. (3) In reality, in most cases, the noise which a?ects the data is not as intense as that in the experiments. Thus, it can be concluded that the method could keep robust and maintain a low false alarm rate in practice. (4) The missing detection rate is zero in all the experiments. Therefore, it can be concluded that the proposed method is robust in detecting WT faults.

4.6. Discussions

 Four groups of experiments have been conducted in this section, and the experiment results show that: (1) The real faults in experiments have been successfully detected at an early stage and their severities are told by the proposed method. (2) No false alarms occur using the normal data of 300 days. It can be concluded that the detection results

Fig. 14. The monitored data and detection results of Experiment 3. (a) The averaged prediction errors of GFBT in one day. (b) The averaged prediction errors of GFBT in every 10 min. (c)¨C(f): Detection results. (c) The conventional FIS method. (d) The proposed method. (e) The proposed method without non-singleton input. (f) The proposed method without terms & rules expansion.


Table 2
Experiment results of di?erent types of FIS.

Table 3
The Results of the Robustness Experiments.

Fig. 15. The experiment data (has a SNR of 40 dB itself) and the noise-added data of generator phase 3 temperature (one of the data used for predicting GFBT). (a) 40 dB. (b) 35 dB. (c) 30 dB. (d) 25 dB. are correct and the proposed method can e?ectively detect early WT faults and provide more information on fault severities.
 Similar to other FIS methods, linguistic variables and terms are used in the proposed method. In fault detection, each linguistic term is in- volved in calculating the fuzzy output according to the rules, and the detection result is obtained by combining all the fuzzy outputs. The fuzzy process makes the fault detection more ?exible.
 Moreover, the proposed method has the advantage of detecting multi-class faults. In the FIS of the proposed method, there could be many rules (for multi-class fault) in the rule base. Therefore, if more  rules are added to the rule base, more fault types could be detected. In the experiments of the paper, four di?erent types of WT faults are de- tected by only one FIS of the proposed method.
In conventional methods, if the upper bound is adjusted as a very low value, or the lower bound is adjusted as a very high value, the early fault can also be detected. However, in such case, many ¡°Normal¡± samples can be misjudged as ¡°Abnormal¡±, which could result in higher false alarm rate. In comparison, using the proposed method, early faults can be e?ectively detected without adjusting the upper bound and the lower bound.

5. Conclusion
This paper presents a WT fault detection method based on expanded linguistic terms and rules using non-singleton FIS. Di?erent from the conventional FIS methods, the proposed method is improved to detect early WT faults and to tell fault severities. There are two main con- tributions of this paper. First, this paper proposes an e?ective WT fault detection method based on FIS. For the ?rst time, a fuzzy number transformation method is introduced to convert the PDFs of the pre- diction errors of the WT SCADA data into fuzzy numbers, so that the non-singleton FIS can be applied to detecting WT faults. Second, this paper presents a method of expanding linguistic terms and rules gen- erated from the original ones. With the expansion, FIS could detect WT faults at an early stage and fault severities could be told with the de- fuzzi?ed fault factor. Four groups of experiments are conducted, using the SCADA data collected in a real wind farm. The experiment results show that the proposed method can e?ectively detect early WT faults and provide more information on fault severities.

