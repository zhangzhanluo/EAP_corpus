Library-Based LAMMPS Implementation of High-Dimensional Neural Network Potentials
ABSTRACT: Neural networks and other machine learning approaches have been successfully used to accurately represent atomic interaction potentials derived from computationally demanding electronic structure calculations. Due to their low computational cost, such representations open the possibility for large scale reactive molecular dynamics simulations of processes with bonding situations that cannot be described accurately with traditional empirical force fields. Here, we present a library of functions developed for the implementation of neural network potentials. Written in C++, this library incorporates several strategies resulting in a very high efficiency of neural network potential-energy and force evaluations. Based on this library, we have developed an implementation of the neural network potential within the molecular dynamics package LAMMPS and demonstrate its performance using liquid water as a test system.
1. INTRODUCTION
Over the past decades, molecular dynamics (MD) simulations have become a central tool for understanding how the properties of materials result from the interactions and dynamics of atoms. Ideally, the forces needed in MD simulations to integrate the equations of motion are determined by solving the electronic Schrödinger equation of the electrons in the felid of the nuclei. Such an ab initio approach, however, is computationally very costly, and consequently forces are often computed from simple empirical potentials postulated based on physical considerations with parameters tuned to reproduce key properties of the material under study. While empirical potentials are available for many types of systems and can dramatically extend the time and length scales accessible to MD simulations, they may introduce systematic inaccuracies as a result of the approximate functional form. Complex chemical environments or the dynamic formation and breaking of covalent bonds are in particular difficult to model within this framework. In recent years, machine learning potentials have become a new promising tool to bridge the gap between accurate and flexible but computationally expensive electronic structure methods and approximate but computationally convenient empirical force fields. Specifically, machine learning potentials have been used to predict physical properties of inorganic solids or complex molecular systems with ab initio accuracy and to model chemical dynamics, such as proton transfer or dissociative chemisorption The general idea of this approach is to use statistical learning techniques to devise potentials with a very flexible functional form with many parameters and to train them using ab initio reference data. While some recent machine learning potentials such as the Gaussian approximation potential (GAP), Coulomb-matrix based method or the spectral neighbor analysis potential (SNAP) are based on kernel methods, artificial neural networks have been used for a longer time, as first applications date back to 1995 (see refs 13 and 14 for a comprehensive list of applications). In an approach proposed in 2007 by Behler and Parrinello, the potential energy surface is represented using high-dimensional neural networks, which are trained to reproduce complex ab initio reference potential energy surfaces and allowed to perform simulations with large numbers of atoms. While during the training process the expensive calculation of energies and forces from the reference method is required, the final neural network potential (NNP) can predict the training data points and interpolate in between them at a fraction of the computational cost. This ability can be used to drive a fast MD simulation with ab initio accuracy.
From a practical point of view, the NNP approach requires software to accomplish two central tasks. First, a training program is needed that adjusts the neural network parameters in such a way that the reference data is correctly reproduced. Once the training is done and the parametrization is ready, a computer code is needed to execute the neural networks and calculate the forces needed in the MD simulation. While in principle these two pieces of code can be developed independently from each other, this would possibly result in many lines of duplicated code and additional maintenance work. Thus, it is beneficial to pursue a library-based programming paradigm such that common functionalities and data structures can be collected and combined as needed by different applications. Following this approach, we have developed a C++ library for high-dimensional neural network potentials (HDNNPs). The present work focuses on the basic building blocks of the library and how they are implemented in order to optimize performance. In addition to the NNP core library we present its first application, an implementation of the method in the popular MD package LAMMPS, and provide benchmark results for several systems simulated on a modern HPC cluster. Although similar implementations of the method by Behler and Parrinello written in Fortran, Python/Fortran, C++, r as an extension to Tensorflow have been developed recently, a clear illustration of the expected performance when applying the method is generally not provided. However, for a prospective user it is of optimization and parallelization, how fast MD simulations can be run in practice. Unfortunately, a comparison of all available software is currently unfeasible as the implementations often differ in available features and technical details.
The C++ library and the LAMMPS interface are designed to work in conjunction with existing parametrizations created with the RuNNer software. Further applications based on the presented library, such as an HDNNP training program, are also available and will be described in an upcoming publication. The NNP library and the LAMMPS implementation are available freely online as part of the n2p2 neural network potential package at http://github.com/CompPhysVienna/n2p2.
The remaining article is organized as follows: in Section 2 the neural network potential method is shortly outlined Section 3 focuses on the implementation and features of the NNP library as well as the LAMMPS interface. Finally, we present benchmarks for multiple test cases in Section 4 before we offer some conclusions in Section 5.
2. The High-Dimensional Neural Network Potential Method
This section is intended as a concise summary of the HDNNP method and does not provide a detailed introduction, for which we refer the reader to refs (41and42).
2.1. Atomic Neural Networks
The high-dimensional neural network potential method introduced by Behler and Parrinello is based on the assumption that the total potential energy E pot can be written as the sum of individual atomic contributions
where Nis the number of atoms. The atomic energies Ej, which depend on the local environment of the atoms, are calculated from feed-forward neural networks as depicted in Figure 1.
Figure 1. A simple feed-forward neural network with two hidden layers. Information about each atom’s surroundings is entered in the input layer neurons on the very left. The neural network processes the data by successively computing the neurons in the hidden layers and presents the resulting atomic energy contribution in the single output layer neuron. Note that here we omitted the atom index j of the input and output layer label.
These networks consist of multiple layers of nodes, also called neurons, which are interconnected by weights representing the fitting parameters of the neural network. The neurons of the input layer of the neural networks are populated with sets of real numbers Gj = {Gj,1,...,Gj,Njs}, called symmetry functions, that characterize the local environment of atom j (see Section 2.2). The neural network then propagates the data to neurons in the hidden layers with the simple rule
where yik denotes the value of neuron i in layer k, bik is the associated bias, ajilk is the weight connecting yik with yjl, and nl is the number of neurons in layer l = k–1. Thus, neuron values are computed from a weighted sum of all incoming connected neurons in the previous layer. In addition the bias is added to allow for a shift independent from input neurons. The activation function fa is usually a nonlinear function (e.g., a hyperbolic tangent or the logistic function), which enables the neural network as a whole to reproduce complex nonlinear potential energy surfaces after weights and biases have been correctly adapted in a training stage. After processing the input data through the hidden layers, the recursion of eq 2 will ultimately result in the atomic energy contribution presented as the value of the single output neuron. Note that separate neural networks and typically also separate symmetry function setups are used to account for different chemical elements. For a given chemical element the symmetry function parameters, the weight parameter values, and the neural network architecture are constrained to be identical. This construction ensures permutation symmetry because atoms of the same chemical element are treated equally.
To obtain analytical expressions for the atomic forces, one simply applies the chain rule to the total potential energy expressed in eq1
Where is the gradient with respect to the Cartesian coordinates of atom i. Note that the individual atomic energy contributions Ej do not have any physical meaning attached to them as they are not observables but merely a vehicle of the method to construct the total potential energy. In particular, only Epot is typically known from a reference method and can therefore be used during training. However, splitting up the total energy as shown in eq 1 comes with a major benefit: scalability. A neural network potential trained for small system sizes is capable of  predicting the potential energy surface for larger configurations by simply adding new summands in eq 1, as long as the atomic environments are properly included in the training set. Training a neural network potential, i.e., optimizing the weight and bias parameters, requires a data set of structures and corresponding potential energies, as well as optionally the forces, computed using the reference method of choice (for instance density functional theory). The procedure to generate appropriate training sets and the optimization algorithm itself are out of scope of this work and have been described elsewhere.
2.2. Symmetry Functions
To provide the atomic neural networks with information about each atom’s surroundings, sets of atom-centered symmetry functions (ACSFs) are computed from the Cartesian coordinates of the atoms. Here, the term “symmetry” refers to the invariance of these functions with respect to global translation, rotation, and index permutation. The use of such symmetry functions ensures that by construction also the total energy is invariant with respect to these operations as required. A variety of symmetry functions have been proposed in the past for the prediction of potential energy surfaces  or structure detection. Note that symmetry functions are only one set of possible descriptors of the local atomic environment used in machine learning approaches.
Symmetry functions can be organized in two groups: Radial symmetry functions are computed only from distances to neighbor atoms, whereas angular symmetry functions also involve angles between triplets of atoms (one central and two neighboring atoms). However, we note that in general symmetry functions for high-dimensional NNPs are many-body functions depending simultaneously on a large number of degrees of freedom in the atomic environments. Here, we recapitulate three frequently used basic types of symmetry functions, one radial variant (Gradial) and two angular variants (Gang.n. and Gang.w.) proposed by Behler in 2011.
The calculation of the radial symmetry function G radial for a given atom iinvolves summing over all neighboring atoms
where rij = |r⃗i – r⃗j| is the distance between atoms i and j, fc(r) is a cutoff function, and η and rs are free parameters determining the spatial shape of the symmetry function. The functional form of angular symmetry functions is similar but requires a double sum over all neighbors with corresponding exponential and cutoff function terms, and the angle θijk = ∠(j,i,k) enters as central term to distinguish different angular environments. The angular symmetry function comes in two flavors, the “narrow” version is defined as
whereas the “wide” variant
omits the factor e–ηrjk2fc(rjk) and thus effectively reduces the damping for large angles and neighbor atoms far from each other but not from the central atom. Both angular symmetry functions depend on the free parameters η, ζ, and λ.
As apparent in eq 3, calculating atomic forces from the neural network potential energy surface requires computation of the gradient of symmetry functions with respect to the Cartesian coordinates. Analytic expressions of symmetry function gradients are presented in the Supporting Information. The cutoff functions fc mentioned above ensure a smooth decay of symmetry functions and their derivatives when neighbor atoms reach the cutoff radius rc. Different functional forms have been proposed in the literature, e.g. involving the cosine
or the hyperbolic tangent
See Section 3.3 for additional choices based on polynomials.
3. Implementation
In general, extending an existing MD software with a new interaction type can be accomplished following two different strategies. First, one could implement all new functionality within the given framework, i.e. add new functions or classes to the code which calculate energies and forces for this particular interaction. In this way no computational overhead and no new dependencies will be added to the MD software. For interactions represented by simple functional forms with a small number of tunable parameters (e.g., pairwise potentials) this is the obvious design choice. For technically more complex interactions a library approach seems preferable. Here, all functions, data types, and variables related to the interaction are gathered in an external library, which provides an interface for the MD program to pass on required information, e.g. atomic positions or neighbor lists. Energies and forces are then computed within the library and returned back to the calling MD code. While this scheme introduces some overhead, it also comes with major advantages. The functionality introduced by the library is not restricted to a single MD software anymore but may be interfaced by multiple codes. The library as common code basis then ensures that changes made to internal routines (e.g., bug fixes, performance improvements, etc.) are automatically carried over to all interfacing programs. With numerous required computational steps (e.g., symmetry function calculation, neural network execution) and parameters (e.g., symmetry function types and parameters, neural network weights and topology) the neural network potential method clearly qualifies as a complex interaction, and a library strategy is suitable. The following sections introduce our NNP library implementation and present the LAMMPS interface.
3.1. NNP Library
The neural network potential library (libnnp) constitutes the foundation of our implementation. It is written in C++ adhering to the C++98 standard and utilizes features from the Standard Template Library (STL). Also, there are no dependencies to third-party libraries which simplifies its portability to different environments. The library provides classes supporting documentation generation from source code via doxygen.(47) They can be easily extended, or new classes can be derived to introduce additional functionality in the future. Although all classes are bundled together in the NNP library, it is still helpful to imagine them as part of two groups with different purpose.
First, some classes are intended for storing the data of the atomic configurations:
Structure class: Holds information for one configuration, i.e., contains the geometry of the periodic box and the positions of all atoms (a vector of type Atom) as well as member functions to compute neighbor lists.
Atom class: Stores the coordinates of an individual atom, its neighbor list (a vector of type Atom::Neighbor), and temporary results from symmetry function calculation.
Atom::Neighbor class: Within the scope of the Atom class this nested class contains the relative location of an atom’s neighbor, temporary symmetry function results, and the cutoff function cache (see Section 3.3).
The other group of classes organizes the neural network potential calculation depending on the large variety of possible user settings. The general layout of these classes is schematically shown in Figure 2.
Mode class: The top-level class of the NNP library containing the complete information required to calculate energies and forces for configurations of type Structure. Existing and future applications should use its interface or inherit from here.
Element class: Since the HDNNP method allows different neural network topologies and symmetry function setups for each element, all per-element components are grouped within this class. The Mode class contains as many instances of Element as there are elements in the system.
NeuralNetwork class: A simple implementation of a feed-forward neural network with selectable number of hidden layers and nodes per hidden layer. Linear, hyperbolic tangent, logistic, and softplus activation functions are available.
SymmetryFunction class: The base class for all symmetry functions providing common member variables and functions. Actual symmetry function implementations are inherited from here. New symmetry functions can be easily added by copying, renaming, and modifying existing child classes.
SymmetryFunctionGroup class: Similar to SymmetryFunction this is the base class for all symmetry function groups (see Section 3.2).
CutoffFunction class: A collection of different cutoff functions.
SymmetryFunctionStatistics class: This class collects statistics of symmetry functions during runtime.
Settings class: A map collecting all user-provided settings in the form of keyword-string pairs.
Log class: All NNP library output intended for the computer screen is passed on to this simple logging class. From there it is possible to redirect all information to multiple targets (screen, C-style file pointers, or C++ file streams) or even completely silence the output.
ElementMap class: This class provides a convenient mapping from atomic numbers to element strings and vice versa for all used atom types. Often required to construct other classes.
Figure 2. Top-level class layout of the neural network potential library libnnp: The Mode class contains all parts required to calculate the HDNNP energy and forces for a given configuration. Neural networks and symmetry functions (boxes labeled with Gi) are gathered per element. Symmetry functions with similar parameters are collected in symmetry function groups.
The NNP library features some basic thread-level parallelization via OpenMP in the Mode and Structure classes. Their respective member functions for neighbor list, symmetry function, and force calculation automatically distribute the workload when running on a multicore processor. Although OpenMP support is only provided for a few functions in the library, these cover the most time-consuming computations. Hence, applications will benefit immediately from this simple form of parallelization.
3.2. Symmetry Function Groups
The procedure to compute the HDNNP energy for a given atomic configuration is easily outlined: the outermost loop is running over all atoms in the structure, as required by eq 1. For each atom, the symmetry function set is calculated and subsequently used as an input for the corresponding neural network. The atomic energy contributions delivered by the neural networks are then summed up to yield the total potential energy. Computing analytic forces complicates this procedure slightly, but the computation of symmetry functions and the evaluation of the neural network remain as the two main computational building blocks. It turns out that the calculation of symmetry functions for a given atom is usually by far more time-consuming than the evaluation of the attached neural network. In particular, HDNNP setups with numerous angular symmetry functions are computationally dominated by the double loop structure in eqs 5 and 6. Naturally, in our effort to create an efficient HDNNP implementation we focused our optimizations on the corresponding parts of the code. Taking into consideration the symmetry function setup of existing HDNNPs,(3,7,33,36,38,48−50) possible strategies to restructure naive loop implementations and utilize temporary memory for intermediate results, we decided for a simple but effective method to reduce the computational cost via symmetry function groups. Here, we will now outline their construction and explain how performance benefits from their usage.
The basic idea is to avoid unnecessary loops over the same neighbor lists for symmetry functions with different parameters. As an example, consider a partial list (Table 1) of angular symmetry functions for oxygen atoms used in a NNP developed for water and ice.(3)
It enumerates different neighbor atom combinations and parameters η, λ, ζ, and rc. With the functional form in eq 5 in mind, two observations can be exploited for performance gains. First, all symmetry functions share the same cutoff rc and thus the same list of neighboring atoms. It would be inefficient to calculate the cutoff function values fc(rij), fc(rik), and fc(rjk) as well as their derivatives repeatedly for each symmetry function. Instead, it is beneficial to reorder the lines in Table 1 and group entries together according to their neighbor species signature and cutoff value. Each of the (e1,e2,rc)-combinations now forms a symmetry function group whose members correspond to the remaining parameter variations (here η, λ, and ζ). Table 2 shows the arrangement of the example symmetry functions in three groups.
The procedure to compute HDNNP energies and forces described at the beginning of this section is now modified as follows: while the outermost loop is still running over all atoms, the symmetry function loop is replaced by a loop over symmetry function groups. For each group, the correct neighbors are extracted from the neighbor list, and the corresponding cutoff function terms are calculated. Only then, in the innermost loop, the distinct parts of individual symmetry functions, i.e., the terms (1 + λ cos θijk)ζ and e–η(rij2+rik2+rjk2), are computed. At second glance another optimization step for angular symmetry functions is possible since it is common that multiple symmetry functions with equal η but different λ and ζ parameters are included. Thus, once calculated for a given η, the term e–η(rij2+rik2+rjk2) can be reused for multiple symmetry functions.
Grouping of symmetry functions works similarly for eqs 4 and 6. The NNP library implements all three basic symmetry function groups and uses them per default. Note that the concept behind the optimizations described in this section is likely to be applicable also to future symmetry functions or other descriptors of local atomic environment since common features will usually imply more efficient computation pathways than the sequential calculation.
3.3. Cutoff Functions and Caching
As described in Section 3.2, in a naive implementation of an NNP cutoff functions for identical distances are repeatedly calculated for each symmetry function. The introduction of symmetry function groups reduces the number of cutoff function calls significantly since temporary results are reused to compute multiple symmetry functions in a row. Still, different groups with equal cutoff function radius rc may loop over similar neighbor combinations and thus require the re-evaluation of cutoff functions. This computational overhead is avoided by the NNP library because the Atom::Neighbor class is equipped with additional storage for cutoff function values and their derivatives. During the symmetry function calculation the first symmetry function group encountering a particular ‘‘central to neighbor atom situation’’ will calculate fc and  and store the results. Other groups will then later retrieve the values from the cache instead of recalculating them.
The NNP library implements several cutoff functions, most of which (except the hyperbolic tangent in eq 11) also support the definition of an inner cutoff radius rci := αrc via a parameter α ∈ [0,1). The cutoff function returns 1 up to the inner cutoff and then falls off to 0 up to the actual cutoff rc with the desired functional form
3.4. LAMMPS Interface
After having presented the structure and capabilities of our NNP library in the preceding sections, we will now move on with the details of the LAMMPS implementation. LAMMPS is a widely used molecular dynamics software, which supports numerous potentials and simulation techniques. The documentation is comprehensive, it uses a spatial decomposition algorithm together with message passing interface (MPI) for parallelization, and the modular design allows for an easy extension with new features. For these reasons, LAMMPS is an ideal framework for the implementation of the neural network potential method.
The LAMMPS developer guide(17) provides a starting point for any extension and describes the code design and top-level classes. The authors state that nonbonded interactions are implemented as subclasses of the Pair base class. The name does not imply a restriction to pairwise interactions; also many-body potentials are included. Thus, we introduce the derived class PairNNP which makes the neural network functionality available in the LAMMPS scripting language via the keywords pair style nnp. As mentioned in the developer guide, during runtime at each time step the PairNNP compute method is called, and lists of atoms and their neighbors are provided. If the simulation is run in parallel, each MPI process is handling only a group of “local” atoms for which the contributions to potential energy and atomic forces are calculated. They are accompanied by surrounding “ghost” atoms which are required for the interaction computation but are local to other processes. At each time step the information about per-process available atomic configurations needs to be rearranged into data structures compatible with the NNP library. The atomic energy and force computation is then performed internally by the NNP library, and the results are returned back to the LAMMPS PairNNP class. Technically this scheme is implemented as follows: the class Interface Lammps is derived from the NNP top-level class Mode described in Section 3.1, and an instance is added as a member of PairNNP. In contrast to its base class, Interface Lammps contains an additional private Structure member and provides public functions to transfer atomic configurations and neighbor information from LAMMPS to this internal data storage. Once the data has been transferred, symmetry functions and atomic networks are computed via methods inherited from the Mode class. Finally the atomic energies and forces from local atoms are reported back to LAMMPS. See Figure 4 for a schematic representation of the class inheritance.
Special care has to be taken to correctly update the LAMMPS force arrays. Contributions of the form  (see eq 3), where i is a ghost atom and j is a local atom, must not be communicated and summed up during the interaction computation. Instead they are stored in force arrays of ghost atoms and later processed in a separate LAMMPS communication step. This ensures that the virial is correctly calculated by LAMMPS.
4. Benchmarks
In the preceding sections we described the building blocks of the NNP library and explained how they are combined in order to implement the method in the molecular dynamics package LAMMPS. Here, we proceed with an overview of the efficiency obtained for our NNP implementation in a modern high-performance computing (HPC) environment. To provide a useful performance measurement we decided not to benchmark individual parts of the library but rather collect timing results for real-world application of different NN potentials in MD simulations with LAMMPS (version 16Mar18).
All calculations discussed below were performed on the Vienna Scientific Cluster (VSC), an HPC system equipped with dual-processor nodes (2x Intel Xeon E5-2650v2, 2.6 GHz, 8 cores per processor) interconnected via the Intel QDR-80 InfiniBand fabric. All source code was compiled with the Intel C++ compiler and linked to the Intel MPI library, both part of the Intel Parallel Studio XE 2017 Update 7. Compilation was performed with a small set of optimization flags (-O3 -xHost -ipo).
4.1. Neural Network Parametrizations
Two neural network potentials were used for benchmarking, both modeling water and ice but with different number of atomic species. The first one is a model for liquid water and multiple ice phases reproducing the potential energy surface of density functional theory with the RPBE functional including van der Waals corrections according to the D3 method. This NNP has been used to investigate the importance of van der Waals corrections to explain anomalous properties of water and for the determination of the line of maximum density at negative pressures. This model was trained with the RuNNer software and will be abbreviated as “nnp-H2O” in the remaining work. The other neural network potential (“nnp-mW”) was developed purely for benchmarking purposes as it is not based on a computationally expensive ab initio method but rather on the coarse-grained monatomic model for water developed by Molinero and Moore, also known as mW water. In this approach, water molecules are represented by single particles favoring tetrahedral coordination structures via the Stillinger-Weber interaction, which includes terms depending on the angle between triplets of atoms. This system is ideal for our testing purposes, because it requires the use of angular symmetry functions to properly describe the atomic environments just like in the case of complex ab initio potential energy surfaces. Moreover, the mW model is implemented in LAMMPS, which makes it very easy to generate reference data for the training procedure. Note that naturally the applicability of this neural network potential is limited to benchmarking since the performance of the mW implementation exceeds that of the neural network representation by far due to its simpler functional form. However, as an effective one-component system the mW model is suitable for our purpose here, which is to demonstrate the achievable performance for single-component systems (in contrast to the nnp-H2O model with a more complex symmetry function setup for two chemical elements).
General settings for both neural network potentials are compared in Table 3, and symmetry function parameters for nnp-H2O, previously reported in ref (3), and resulting symmetry function group definitions are provided as Supporting Information. Note that since in the mW model water molecules are represented by single particles, only one corresponding “atomic” neural network for this atom type is required.
For the mW model we chose a mixture of all three symmetry function types from Section 2.2 with parameters chosen along the guidelines given in ref (42) and listed in Table 4. Note that there are many choices of symmetry function parameters equally suited to describe the local atomic environment, e.g. systematic approaches recently presented in refs. The performance is of course mostly determined by the number of symmetry functions (and their type) and not by the selected parameters. The corresponding symmetry function groups are listed in the Supporting Information. The training data set consists of 1991 configurations, where initially 1300 structures, each containing 128 mW particles, were taken directly from MD simulations of water and ice at different temperatures and pressures. The remaining configurations were obtained via artificial modifications (shearing and scaling of the simulation box, random displacements, and deletions of individual atoms) of the initial configurations. Training of the nnp-mW potential was performed with a training application based on the NNP library, which implements parallel Kalman filter based neural network training. We excluded roughly 10% of the entire data set from training. The configurations in this “test” set are used to detect overfitting and assess the overall quality of the fit. Only if the root-mean-square errors (RMSEs) of both the training and the test set are sufficiently low and do not deviate much from each other is the neural network capable of reproducing the potential energy surface at and in between the sample points in the training data set. Excellent training results were achieved for the mW model: the RMSE of energies in the training/test set converged to 0.24/0.27 meV/atom, and the RMSE of training/test forces converged to 15.5/15.7 meV/Å.
4.2. Comparison of Performance Optimization Strategies
In order to compare the performance gains achieved through the performance optimization strategies described in Sections 3.2 and 3.3, we set up MD simulation runs under realistic conditions on the VSC. Starting configurations of liquid water with almost cubic box geometry were prepared for both models. For nnp-H2O a snapshot containing 2880 water molecules from an equilibration run at 300 K and 1 bar was used. For the nnp-mW model, two configurations, one with 128 and another one with 8192 mW particles, were prepared at 298 K and zero pressure. These three test systems were propagated in time for 1000 time steps with the velocity Verlet algorithm, which yields trajectories consistent with the microcanonical ensemble. The time steps were 0.5 and 10 fs for nnp-H2O and nnp-mW, respectively.
Four LAMMPS binaries were created with different NNP-library optimizations enabled at compile time: the basic version without any optimization (“Basic”), one with the cutoff function cache enabled (“CFC”), another one making use of symmetry function groups (“SFG”), and finally one with both optimizations activated (‘‘SFG+CFC’’). For each binary the benchmark simulations were carried out on different numbers of VSC nodes starting from serial runs on a single node up to runs with 512 MPI tasks on 32 nodes. LAMMPS binaries automatically collected timing results, which were parsed from the output files.
Figure 5 shows the performance in units of time steps per second for all three test cases and each binary as a function of the number of MPI tasks. As is to be expected, in general the performance increases with the number of cores, but the relative speedup decreases for each additional core added due to parallelization overhead. Clearly, the naive implementation without any optimization yields the worst timing results. Caching cutoff function values gives a small speed gain, while the introduction of symmetry function groups significantly boosts the performance. Combining both optimization methods is only marginally better than the SFG-only version, yet a consistent improvement over all simulations is clearly visible. As it turns out, the relative performance gain over the basic version is comparable for different numbers of MPI tasks. Thus, it is practical to specify average values for each optimization method which are provided in Table 5.
Summarizing the results from Figure 5 and Table 5, we find that the LAMMPS implementation based on our NNP library performs best when both the cutoff function caching and symmetry function groups are enabled as on average the performance is increased by approximately a factor of 3. Naturally, the default compile options for the NNP library are set accordingly. Depending on neural network parametrization, system size, and number of neighbors within the cutoff sphere, the efficiency of the MPI parallelization may vary. Figure 6 shows the speedup and the parallel efficiency of the SFG+CFC version for all three test cases considered here.
4.3. MPI/OpenMP Hybrid Parallelization Performance
The parallel performance results obtained so far were solely based on the MPI parallelization strategy of LAMMPS. However, the NNP library offers an additional layer of thread-level parallelism via OpenMP (see Section 3.1) which can be used to run hybrid MPI/OpenMP simulations with LAMMPS. This approach allows for the combining of the capabilities of MPI (internode communication) and OpenMP (intranode shared memory access) to reflect the hierarchical hardware setups found on many cluster systems (interconnected nodes with multicore processors). Typically, in such a hybrid simulation run each MPI process dynamically spawns multiple OpenMP threads and distributes the local workload to cores within each node. The total number of cores used is therefore the product of MPI processes and OpenMP threads. In comparison to a pure MPI run with the same number of cores, the potential benefit is a reduction of MPI communication overhead and a better scaling behavior. However, the actual performance depends strongly on the hardware setup and the implementation details.
In Figure 7 we present benchmark results for the nnp-H2O model and a system size of 2880 molecules. A binary with all optimizations (SFG+CFC) and additional OpenMP support was used, and timing information was collected for hybrid MPI/OpenMP runs with 1, 2, and 4 OpenMP threads per MPI process. While the communication overhead drops as expected with multiple threads, an overall speedup is unfortunately not observed. Instead, up to about 128 total cores used there is even a clear decrease in performance of about 5% and 15% for 2 and 4 threads, respectively. Nevertheless, the situation changes above 256 cores as the performance approaches that of the pure MPI run. Ultimately, for the maximum number of cores (1024) there is even a small speedup of about 7%. A potential explanation for the observed behavior is the incomplete parallelization at the OpenMP level competing against the communication overhead. Due to technical reasons the LAMMPS interface parallelizes only the symmetry function calculation via OpenMP while additional energy and force computation parts remain serially executed by each MPI process. Thus, for small numbers of total cores this disadvantage dominates. Only with many MPI processes involved, the communication reduction benefit outweighs the parallel performance loss due to serial code parts.
The results demonstrate that pure MPI simulations using the LAMMPS interface already scale well to many cores. However, some additional performance gain can be achieved using a MPI/OpenMP hybrid parallelization strategy if many MPI processes are used and the communication time would otherwise be substantial. With some additional programming efforts in the future, the NNP library may be tuned to reduce the amount of serial computations and further improve the hybrid parallelization performance. For the remainder of this Article we discuss only results obtained with pure MPI and without OpenMP parallelization.
4.4. Dependence of Performance on Neural Network Potential Settings
In the previous section we showed that both optimization strategies presented in Section 3 significantly increase the performance when compared to a naive implementation. Here, we address two more detailed questions on how the performance is affected by NN parametrization specifics:
1. Is it possible to further increase the efficiency of the NNP implementation speed by making use of different cutoff functions, i.e. switching from a transcendental functional form to cutoff functions based on computationally less demanding polynomials?
2. How big is the performance loss if for a given NN parametrization additional symmetry functions are introduced? This question usually arises in the creation of a new neural network potential when training results indicate that the spatial resolution is not sufficient and additional symmetry functions should be used. More radial symmetry functions will usually not influence performance, but the number of angular symmetry functions will require a balance between accuracy and speed.
In order to answer these questions we set up three additional NN parametrizations based on the nnp-mW model and ran benchmarks with the 8192 mW particle system. Regarding the first question, the modification ‘‘nnp-mW-fc poly2’’ uses the same NN settings as the original model with only one exception: the cosine cutoff function f cos of all symmetry functions is replaced by the polynomial function f poly2 (leaving α = 0 unchanged) as described in Section 3.3. To tackle the second question, two variants with additional angular symmetry functions were constructed from the original nnp-mW model. First, the variant “nnp-mW-η” adds new symmetry functions of type Gang.n. and Gang.w. with η = 0.001. The remaining parameters are set following the scheme from the original model, i.e. λ = ±1, ζ = 1,3 and rc = 12.0 bohr, see Table 6 for a list of new symmetry functions. Similarly, the variation “nnp-mW-ζ” extends the original model by new symmetry functions with ζ = 9, see Table 7 for a list of additional symmetry function entries.
The three variants of the nnp-mW model were trained with the same data set as described in Section 4.1, and a comparable fit quality was achieved. Benchmark MD simulations were run with the 8192 mW particle system under the same conditions as in the previous section with all four LAMMPS binaries. Finally, the timing data were compared to those of the unmodified nnp-mW model, and the performance gain or loss was averaged over runs with different number of MPI tasks.
Table 8 shows the results for all variants and binaries in percent with respect to the performance of the nnp-mW model. With the results in the first line of the table, we are able to answer our first question posed. While the basic implementation would still benefit from the use of a computationally less expensive cutoff function, the advantage almost disappears when all NNP library optimizations are enabled. This is because the cutoff function calls are drastically reduced by caching and by the use of symmetry function groups. Regarding the second question about the performance loss caused by additional symmetry functions we find that the usage of symmetry function groups (binaries SFG and SFG+CFC) is clearly favorable. If symmetry function calls are executed sequentially without any reuse of intermediate results (binaries Basic and CFC), the expected maximum decrease in performance corresponds to the number of additional symmetry functions since the calculation of radial symmetry functions and the execution of other parts of the code can be neglected. In the case of 8 additional entries for the nnp-mW-η model, this amounts to 25%. Likewise for nnp-mW-ζ a maximum drop of 37.5% is to be expected. Indeed the observations presented in Table 8 closely match these expectations. On the other hand, the performance drop experienced with symmetry function groups enabled (columns SFG and SFG+CFC) is less pronounced as the corresponding numbers in Table 8 are consistently lower. This behavior is easily explained with the construction of symmetry function groups (see Section 3.2) in mind: With the given choice of parameters the additional symmetry functions can be appended to the list of symmetry function group members of the original nnp-mW model (see the Supporting Information) to form the symmetry function groups of the nnp-mW-η or nnp-mW-ζ model. Therefore, the additionally required computation steps involve only those that are not already part of calculations common to all group members, which effectively reduces the cost of the extra symmetry functions.
The three variants of the nnp-mW model were trained with the same data set as described in Section 4.1, and a comparable fit quality was achieved. Benchmark MD simulations were run with the 8192 mW particle system under the same conditions as in the previous section with all four LAMMPS binaries. Finally, the timing data were compared to those of the unmodified nnp-mW model, and the performance gain or loss was averaged over runs with different number of MPI tasks.
Table 8 shows the results for all variants and binaries in percent with respect to the performance of the nnp-mW model. With the results in the first line of the table, we are able to answer our first question posed. While the basic implementation would still benefit from the use of a computationally less expensive cutoff function, the advantage almost disappears when all NNP library optimizations are enabled. This is because the cutoff function calls are drastically reduced by caching and by the use of symmetry function groups. Regarding the second question about the performance loss caused by additional symmetry functions we find that the usage of symmetry function groups (binaries SFG and SFG+CFC) is clearly favorable. If symmetry function calls are executed sequentially without any reuse of intermediate results (binaries Basic and CFC), the expected maximum decrease in performance corresponds to the number of additional symmetry functions since the calculation of radial symmetry functions and the execution of other parts of the code can be neglected. In the case of 8 additional entries for the nnp-mW-η model, this amounts to 25%. Likewise for nnp-mW-ζ a maximum drop of 37.5% is to be expected. Indeed the observations presented in Table 8 closely match these expectations. On the other hand, the performance drop experienced with symmetry function groups enabled (columns SFG and SFG+CFC) is less pronounced as the corresponding numbers in Table 8 are consistently lower. This behavior is easily explained with the construction of symmetry function groups (see Section 3.2) in mind: With the given choice of parameters the additional symmetry functions can be appended to the list of symmetry function group members of the original nnp-mW model (see the Supporting Information) to form the symmetry function groups of the nnp-mW-η or nnp-mW-ζ model. Therefore, the additionally required computation steps involve only those that are not already part of calculations common to all group members, which effectively reduces the cost of the extra symmetry functions.
5. Conclusion
In this work, we have presented a neural network potential library that serves as an efficient foundation for the application of Behler-Parrinello type neural network potentials. With its data types and class structure, the library provides everything necessary to develop new standalone programs as well as interfaces to external software. As an example, we have integrated our neural network potential implementation into the molecular dynamics program LAMMPS, making the diverse molecular dynamics techniques included in this powerful package available for NNP simulations. Due to the library approach, the advantages of code optimizations within the library are automatically passed on to the interfacing software. We presented how the LAMMPS interface benefits from two optimization strategies: cutoff function caching and symmetry function groups. With both of these optimization options enabled we find that a speedup by a factor of 3 can be achieved on a modern HPC system in highly parallelized simulations.
With our LAMMPS implementation the possibility of the neural network potential method to simulate large systems over long time periods with the accuracy of the underlying ab initio potential energy surface can be fully exploited. As we demonstrated, it is feasible to run massively parallelized MD simulations of 2880 water molecules with a DFT neural network potential parametrization at rates of about 100 time steps per second. As the NNP performance is independent of the reference electronic structure method and the number of training data points, the same performance could be reached for high-level reference data from quantum chemical methods. With an appropriate time step this amounts to around 4 ns per day using 512 cores on a current HPC system.
 
Realistic Atomistic Structure of Amorphous Silicon from Machine-Learning-Driven Molecular Dynamics
Abstract: Amorphous silicon (a-Si) is a widely studied noncrystalline material, and yet the subtle details of its atomistic structure are still unclear. Here, we show that accurate structural models of a-Si can be obtained using a machine-learning-based interatomic potential. Our best a-Si network is obtained by simulated cooling from the melt at a rate of 1011 K/s (that is, on the 10 ns time scale), contains less than 2% defects, and agrees with experiments regarding excess energies, diffraction data, and 29Si NMR chemical shifts. We show that this level of quality is impossible to achieve with faster quench simulations. We then generate a 4096-atom system that correctly reproduces the magnitude of the first sharp diffraction peak (FSDP) in the structure factor, achieving the closest agreement with experiments to date. Our study demonstrates the broader impact of machine-learning potentials for elucidating structures and properties of technologically important amorphous materials.
Amorphous silicon (a-Si) is a fundamental and widely studied noncrystalline material, with applications ranging from photovoltaics and thin-film transistors to electrodes in batteries. Its atomic-scale structure is traditionally approximated in a Zachariasen-like picture with all atoms in locally “crystal-like”, tetrahedral environments, but without long-range order.  However, the real material contains a nonzero amount of coordination defects, colloquially referred to as “dangling bonds” (under-coordinated sites) and “floating bonds” (overcoordinated sites). Knowing the properties and abundance of such defects is important, as they can control electronic and other macroscopic properties. We note at the outset that, although defect sites in a-Si may be passivated by hydrogenation (to give “a-Si:H”) in some synthetic conditions, we here focus on the archetypical, hydrogen-free material as made in ion-implantation or sputter-deposition experiments.
Even the most advanced experimental approaches do not directly allow the observation of the bulk atomic structure in amorphous materials. Despite significant advances, including in situ NMR techniques and “inverse” approaches such as Reverse Monte Carlo (RMC) modeling of diffraction data, only indirect knowledge can be gained about the local atomic environments, and that only in a statistical sense. For almost three decades, molecular-dynamics (MD) simulations have therefore played a crucial and complementary role, with a-Si being a prominent example These simulations either use density-functional theory (DFT) or classical force fields. DFT-MD describes a system with quantum-mechanical accuracy and can largely correctly capture the structural and bonding subtleties of liquid and amorphous matter. However, it is computationally expensive, and therefore allows only limited system sizes (a few hundred atoms at most) and time scales to be simulated. Indeed, the cooling rates in previous DFT simulations of a-Si (≈ 1014 K/s) are orders of magnitude faster than those in experiments. Classical force fields require much less computational effort, giving access to nanometer-scale (“device-size”) structural models, both for a-Si and for multicomponent systems derived from it (see ref (25) for but one example). However, they are rarely accurate enough to fully describe the structural variations present in the amorphous state.
Capitalizing on today’s “big-data” revolution, machine-learning (ML) algorithms are increasingly used to generate interatomic potentials for atomistic simulations. By “learning from” (or rather, fitting to) quantum-mechanically computed reference data for energies and forces, ML-based interatomic potentials can enable simulations with an accuracy that is largely comparable to DFT, but with a computational cost that is orders of magnitude lower, and with linear (order-N) scaling behavior. Comparison to experimental observables is thereby the ultimate benchmark and means of validation for the quality of any ML-based interatomic potential, as we stress that no experimental but only DFT-computed data enter the “learning” process.
We believe that such ML potentials are particularly promising for disordered and amorphous materials, which must be represented by nanometer-scale structural models containing several hundreds or thousands of atoms. A landmark example has been the development of an artificial neural-network potential for the phase-change material GeTe, enabling simulation of the crystallization properties including entire nanowires. We recently introduced a ML potential for amorphous carbon, based on the Gaussian approximation potential (GAP) framework and the Smooth Overlap of Atomic Positions (SOAP) atomic similarity kernel, which captures the intricate structural, mechanical, and surface properties of the material and, more recently, has enabled accurate large-scale simulations of the growth mechanism. Very recent work using neural-network potentials allowed for the atomistic modeling of amorphous LixSi phases relevant in battery applications. Finally, such potentials were used in seminal studies to describe the complex phase transitions between polymorphs of crystalline Si.
In this Letter, we show how realistic atomistic modeling of a-Si can be enabled by a ML-based interatomic potential, again using SOAP and GAP. We first report on melt–quench simulations with cooling rates much slower (that is, better) than what can be achieved in quantum-mechanical-based simulations, and we show how this leads to a higher-quality and lower-energy structure of a-Si. Our structural models show excellent agreement with experiments probing local structure, including 29Si NMR shifts and diffraction data for high-quality samples, and open the door for future combined modeling and experimental studies on disordered and amorphous materials.
Simulated quenching from the melt is a widely used technique for generating amorphous model networks. In this, one starts with a liquid and progressively lowers the temperature, “freezing in” an amorphous structure. However, for silicon, this approach is not trivial, due to the change in local environments between the high-coordination metallic liquid and the tetrahedral-like amorphous state. We decided to perform a set of variable-volume and constant-pressure (NPT) quench simulations, in which we varied the quench rate, and thus the run-time, by several orders of magnitude. These were carried out using LAMMPS; details are in the Supporting Information. For the moment, we focus on a system size of 512 atoms in the cell and perform a single simulation at each quench rate. This system size is significantly larger than what has so far been accessible to DFT (64–216 atoms), but smaller than what is possible for empirical potentials; this will be addressed directly later on. Our fastest quench rate (1014 K/s) corresponds to early, seminal DFT studies, whereas our simulations at 1012 K/s mirror the limit of what is presently possible for DFT-quality MD. By contrast, we here use a recently developed GAP model which allows us to increase the simulation time 10-fold beyond that, namely, decreasing the quench rate to 1011 K/s, while retaining similar accuracy.
While an increase in simulation time by 1 order of magnitude may seem incremental at first sight, the full power of ML potentials becomes apparent when looking at the overall computational effort required (Figure 1a). For demonstration, we performed a brief DFT-MD simulation on a 512-atom a-Si network and use the timing information for a rough extrapolation (Supporting Information). Quenching with a rate of 1011 K/s would thus require around 16 million core hours, or current nominal costs of $185 000 on the UK national supercomputer. In contrast, the same quench rate in GAP-MD required below 40 000 core hours, equivalent to nominal costs below $500. Using GAP, it would hence be possible to decrease the quench rate even further, but given the results obtained at 1011 K/s, we subsequently chose to increase the system size instead (see below).
The slow quench rate of 1011 K/s, “unlocked” here using GAP, is indeed required to generate reliable structural models of a-Si. This is seen in Figure 1b: we took structural snapshots at various increments of the simulations, optimized them into local minima, and plotted their energy (relative to the thermodynamically stable form, diamond-type c-Si) as a function of how far the quench has progressed in time from the liquid to the final a-Si structure. The right-hand side shows the experimental sample stability with increasing annealing and thus ordering, based on calorimetry (as is common, we approximate ΔE ≅ ΔH when comparing theory and experiment). Intermediate quench rates lead to a-Si networks that are as stable as freshly deposited or partially annealed samples (ΔE ≈ 0.17–0.20 eV/atom). By contrast, our slowest quench at 1011 K/s yields a structure whose stability matches the experimental result for a well-annealed sample from ref (12) (ΔE ≈ 0.14 eV/atom). The GAP-computed bulk moduli for these a-Si networks range from 62–69 GPa and increase with slower quenching (the material becoming “harder”); the computed Young’s moduli increase from 73 to 98 GPa; see Supporting Information.
The benefit of slow quenching is further seen in two of the most common structural indicators used for amorphous solids. In a-Si, the bond angles are distributed around the ideal tetrahedral value (109.5°; Figure 1c). Fitting Gaussian distributions to these data allows us to determine the full width at half-maximum (fwhm), which decreases gradually from 30° to 22° with increasingly slower quenching. The experimental value for the bond-angle deviation of ≈11° (ref (10)) is consistent with the half width at half-maximum (HWHM) for our slowest quench. Moreover, the medium-range structural order is important in covalent amorphous networks, and we quantify it here using shortest-path ring statistics. In diamond-type c-Si, all atoms are in six-membered ring (cyclohexane-like; m = 6) configurations, whereas a-Si also contains a large number of five- and seven-membered rings, and a lesser amount of smaller and larger ones. All rings with m ≠ 6 depart from the reference crystalline state, and as such are a measure of disorder, but we here distinguish them further as follows. Five- and seven-membered rings are still expected to be energetically viable (supported by their abundance in a-Si), whereas, for example, four-membered rings will be clearly under strain. We therefore label rings with m < 5 as “small-ring defects”, and rings with m > 7 as “large-ring defects” (Figure 1d).
In Figures 2a–b, we show computed structure factors, S(Q), which can be compared to diffraction experiments. The third peak (at ≈5–7 Å–1) gradually splits into two well-defined subpeaks when moving from the 1014 K/s (yellow) to the 1011 K/s data (purple). This is qualitatively consistent with experimental observations: as-deposited samples show a fairly featureless third peak, whereas annealed ones (and also our 1011 K/s result) exhibit a clear splitting into subpeaks. Even better agreement with the experimental structure factor can be achieved for a larger structural model containing 4096 atoms, which we will show below.
We furthermore computed solid-state 29Si NMR chemical shifts, δ, for all atoms in the unit cells, thereby characterizing each atomic environment individually. We use established DFT-based algorithms and reference all δ values to tetramethylsilane (TMS), analogous to experiments. The results for the different GAP structures are shown in Figure 2c (histograms). Furthermore, due to the broad distribution of δ values in the amorphous state, we fit Gaussian profile functions to these data (lines), as detailed in the Supporting Information. We compare the output of these computations to experiments for pure a-Si prepared by sputter deposition.  The latter samples were analyzed via secondary-ion mass spectrometry (SIMS), showing no measurable oxygen contamination and ≈0.2 atom % hydrogen in the samples. This low level of impurity is thought to have little or no impact on the 29Si NMR results, enabling direct comparison to our simulations. In addition to the numerical values reported in ref (44), we fit a Gaussian profile to the experimental data for the sample annealed at 520 °C (before the onset of crystallization at higher temperature). We perform this fit using the same procedure as for our DFT data (Table 1). This yields numerical quality criteria that can be used to assess any given structural model.
Clearly, simulations using the two fastest quench rates (yellow and orange) lead to structures with very large scatter in the computed NMR shifts, as a direct consequence of their distorted atomic environments (and thus large fwhm for the Gaussian fits). The network generated at a slower quench rate, 1012 K/s (red), agrees more appreciably with experiment with regard to both the broadness and the center of mass for the Gaussian fit (δDFT = −37 ppm); the latter can be compared to δexp = −38.3 ppm for as-deposited a-Si, and δexp = −42.9 ppm for a sample annealed at 580 °C. Hence, there is a progressive shift to lower frequency in the experimental data with increasing structural ordering, and this is reproduced by our quenched structure at 1011 K/s (δDFT = −51 ppm), both qualitatively and quantitatively (to within a few ppm). The results for the 1011 K/s quench also compare well with those for a DFT-optimized Wooten–Winer–Weaire (WWW) network of a-Si (δDFT = −53 ppm; structure taken from ref (48)), while those for the faster quenches do not (Table 1).
We now place our melt–quench simulations into a wider context, as there are several different ways of modeling a-Si. First, we survey results of RMC modeling, which is an established means of extracting structural information from diffraction data. Recent work by some of us showed that reasonable restraints can improve the RMC modeling of a-Si. In particular, the SOAP similarity measure, initially developed to encode atomic structure in ML potentials, proved useful for this purpose. SOAP-RMC output, subsequently relaxed using DFT, has thus been shown to provide a high-quality structural model of a-Si. We now take the same structures but anneal them further using GAP: heating to 1100 K, holding, and cooling back to 300 K, for a total simulation time of 50 ps. This relatively short annealing is thought to be appropriate, as a recent DFT-MD study showed that annealing a quenched structure at 10 ps versus 20 ps had no appreciable effect on the outcome. We also performed the same annealing procedure for the DFT-optimized WWW model from ref (48); a somewhat similar strategy has been followed before, based on a tight-binding model and a system size of 216 atoms.  Finally, we include a state-of-the-art 216-atom structure that was carefully generated in a recent work, by slow quenching using the empirical Tersoff potential and subsequent multistep optimization using DFT (here labeled “Tsf+DFT”). 
We compare these structures in Figure 3 using three types of quality indicators. First, we report the number of coordination defects (Figure 3a), counting 3- and 5-fold bonded atoms with a bond-length cutoff of 2.85 Å. We then measure the distortion from ideal tetrahedral coordination environments: by fitting Gaussians to the angle distributions and determining their fwhm, and by using a numerical order parameter that was employed earlier for tetrahedral environments in liquid water and chalcogenide glasses  (Figure 3b). Beyond nearest-neighbor environments, we quantify the medium-range order using shortest-path ring statistics, as above, again considering as “defects” any rings with fewer than five or more than seven members (Figure 3c). In all cases, the GAP-quenched structure with the slowest quench rate (1011 K/s) exhibits very good figures of merit. Interestingly, the count of five-membered rings (m = 5) decreases continuously in progressively more ordered GAP-quenched structures, but that of seven-membered rings (m = 7) increases instead, as shown on the far right of Figure 3c.
Finally, we prepared a larger a-Si structural model containing 4096 atoms (Figure 4a), using GAP-MD and a variable quench rate between 1011 and 1013 K/s, as detailed in the Supporting Information. This system size is in reach for ML-based interatomic potentials, as they scale linearly with system size due to their finite cutoff radius (cf. Figure 1a). Having access to ab initio quality structural models on the 4 nm length scale allows us to study the medium-range order more closely. This fundamental question has been discussed in recent work on nearly hyper-uniform networks, in particular, by quantifying the inverse height (H–1) of the first sharp diffraction peak in the structure factor at around 2 Å–1. This quantity is taken as a measure for the degree of structural ordering.
We compare our structure with the current state of the art, viz., a-Si systems containing 100 000 atoms, in Figure 4b. Although the latter were generated with an improved WWW algorithm, not by slow quenching, they allow us to place our work in the context of existing ultralarge structural models. Surprisingly, the latter system size alone does not seem to be needed if the structural modeling itself is sufficiently accurate. Indeed, looking at H–1, our GAP approach outperforms the previous simulation results in much larger cells, and leads, again, to almost quantitative agreement with experiment (H–1 = 0.58 with GAP, H–1 = 0.57 in experiment; Figure 4b). By comparison, an a-Si structure of the same size (4,096 atoms) but generated using empirical potentials gave a much larger H–1 = 0.81 (ref (54)). Moreover, our slowest-quenched GAP-based system, even smaller with 512 atoms/cell, yields H–1 = 0.66, remarkably still outperforming the 100 000-atom structure from ref (14) (H–1 = 0.68). Beyond the first sharp diffraction peak alone, Figure 4b also shows that the agreement in the structure factor between the 4096-atom GAP system and experimental data at larger Q is excellent, and significantly better than for the VBSB 100 000-atom system.
In conclusion, we have shown that machine-learning-based interatomic potentials can lead to an unprecedented level of quality in the structural modeling of amorphous materials. We used a Gaussian approximation potential (GAP) to generate high-quality atomistic models of amorphous silicon, quenching from the liquid at a rate of 1011 K/s, hitherto inaccessible to DFT-quality simulations. These structural models agree convincingly with calorimetry, 29Si NMR experiments, and X-ray structure factors, including the height of the first sharp diffraction peak. We note that ML potentials are critically dependent on the quality of the quantum-mechanical input data, and as of today require significant effort to be developed in the first place; in the present case, our GAP has “seen” diverse liquid and amorphous configurations and interpolates between these. These findings will have implications for future research on disordered and amorphous materials, opening the door for quantitatively accurate atomistic modeling with direct links to experiments, for a-Si and beyond.
 
Designing exceptional gas-separation polymer membranes using machine learning
Abstract
The field of polymer membrane design is primarily based on empirical observation, which limits discovery of new materials optimized for separating a given gas pair. Instead of relying on exhaustive experimental investigations, we trained a machine learning (ML) algorithm, using a topological, path-based hash of the polymer repeating unit. We used a limited set of experimental gas permeability data for six different gases in ~700 polymeric constructs that have been measured to date to predict the gas-separation behavior of over 11,000 homopolymers not previously tested for these properties. To test the algorithm’s accuracy, we synthesized two of the most promising polymer membranes predicted by this approach and found that they exceeded the upper bound for CO2/CH4 separation performance. This ML technique, which is trained using a relatively small body of experimental data (and no simulation data), evidently represents an innovative means of exploring the vast phase space available for polymer membrane design.
INTRODUCTION
Polymer membranes are used to effect a variety of gas separations such as the removal of carbon dioxide from natural gas, oxygen from air, hydrogen recovery, and more recently in carbon capture. Separation performance is typically characterized by the membrane’s permeability (Pi), i.e., the throughput of gas type i, and selectivity (α), the purity of the output stream. Pi is defined from Fick’s law of diffusion, ∣∣Ji∣=PiΔpℓ, where Ji is the flux of gas i and ∆p is the pressure drop across a membrane of thickness ℓ. Pi is further decomposed into the product of a thermodynamic solubility constant and a diffusion constant, Pi = Di × Si. The ideal selectivity, α, between two gases is the ratio of their permeabilities: αA/B=PAPB=DADB×SASB, where DADB and SASB are the diffusivity and solubility selectivities, respectively. While there has been an increased emphasis on the use of permeabilities and selectivities when gas mixtures (rather than pure gases) are used, the data on these systems are sparse, and hence, for the purposes of this work, we discuss pure gases.
While an optimal polymer membrane for a given gas pair should have both high permeability and high selectivity, these quantities are typically observed to be negatively correlated. This concept is demonstrated in a “Robeson plot” for a variety of polymers and gas pairs. The Robeson plot for CO2/CH4 separations is shown in Fig. 1; a multitude of other Robeson plots exist for different gas separations. These plots illustrate the empirically determined current best performance for a given separation as defined by the upper bound correlation (lines in Fig. 1  Note that the upper bound evolves with time as scientists invent new materials so that while the slope of this line is apparently unchanged, the intercept increases with time. Thus, we use designations such as the 1991 upper bound or the 2008 upper bound  to designate the temporal evolution of this observed trade-off relation. The challenge, therefore, in synthesizing next-generation polymer membranes is in designing materials that cross the current upper bound. These ideas have motivated the discovery of new classes of polymeric materials, e.g., thermally rearranged (TR) polymers and polymers of intrinsic microporosity with improved performance over conventional polymers.
Synthesizing and testing the vast number of possible polymer constructs and their potential chemical modifications with our currently available chemistry toolbox is an expensive and time-consuming proposition. Instead, several theoretical methods and models have been developed as a means to understand diffusion and solubility in polymeric materials, with the goal of permitting a more rational design of next-generation materials.
On the most basic level, gas permeability can be empirically predicted using group contribution methods, where polymer repeat units are decomposed into subunits and the estimated gas permeability contribution of each of these moieties is added together. This approach is only sensitive to the presence of various atoms/functional groups in a polymer backbone but does not necessarily take their connectivity into account. Further, these methods do not systematically evolve as newer classes of polymers are synthesized and measurement tools improve. Group contribution methods therefore represent a first step for predicting the gas transport properties of these polymeric materials. A more theoretically underpinned concept is that permeability, in the framework of the solution-diffusion model, can be predicted with knowledge of polymer free volume. This follows by relating the diffusion of a gas molecule of a known size with the amount of volume in the polymer that facilitates its motion. This idea has been developed to relate the “slope” of the upper bound line to the relative sizes of the gas molecules involved in a given separation (i.e., for a given Robeson plot) However, this correlation is imperfect, and there is an incomplete understanding of the underpinning free volume concept. These free volume models have also been developed for estimating gas solubility in polymers. This concept is important for glassy polymers, which are known to swell and plasticize in the presence of CO2, thereby markedly altering their gas solubility. While gas solubility in polymers can be elegantly derived from well-understood models such as Sanchez-Lacombe theory extended to the nonequilibrium polymer glass state (such as the non-equilibrium lattice fluid model), these results often contain a number of unknown parameters to describe polymer-gas interactions and the extent of glassiness. This complicates a full, predictive understanding of the underlying phenomena. Other models, such as the dual-mode sorption, have also been found to qualitatively explain trends observed for gas solubility in glassy materials, but they are often limited to specific families of polymers. It is safe to say therefore that while there is good qualitative understanding of gas transport in glassy polymers, there is hardly any scientifically grounded, predictive models in this context. This concept is underpinned experimentally by the enormous scatter in the data shown on a Robeson plot, which represents not only our empirical understanding of gas transport but also the lack of design cues that can guide the synthesis of new materials. A means of rationally designing advanced membrane materials, without resorting to empirical experimentation, thus remains an open challenge.
Here we propose a different approach, which could eventually lead to the understanding of the underlying molecular processes, i.e., machine learning (ML) In its current form, ML represents a class of statistical models that make predictions on properties based on a set of data, but without a detailed understanding of the underlying physics in these situations. These models are greatly dependent on the availability and accuracy of large sets of applicable data. Thus, when ML has been used for polymer property prediction, researchers have primarily focused on large sets of theoretically generated data (“Materials Project”). Other ML methods in the past have typically been applied to experimental datasets with less than 100 data points for any given property, which tends to limit the accuracy of the predictions of this exercise.
Our approach uses all the gas permeation data that we could find in the literature, i.e., typically 500 to 1000 polymers for each gas, to develop an ML model as outlined schematically in Fig. 2. While we have not chosen to curate these datasets to prevent user bias, this larger dataset appears to allow us to develop more reliable models. We train the ML algorithm using a training dataset (which is part of the available dataset) and test its predictions on the remaining polymers for which gas permeation data exist. This validated model can then predict the gas permeation behavior of a large body of polymers that have been synthesized to date (~11,000), but which have not been experimentally characterized in this context. Our ideas have some parallels to the group contribution methods discussed above, but with the advantage that we do not define the chemical building blocks ahead of time. Instead, we explore the polymers whose permeabilities have been measured by using a topological, path-based, fingerprinting method to describe the polymer backbone structure so that materials with previously unexplored chemistries can be easily added to the dataset as synthetic advances are made (23). Once the predictions on the 11,000 polymers have been made, we focus specifically on the polymers that are predicted to lie well above the upper bound, i.e., polymers particularly well suited for that separation but ones that have not been tested to date. We then experimentally validate the predicted values of PCO2 and PCO2/PCH4 for these previously unexplored polymers. Thus, ML appears to be a powerful method to predict (and hence design) materials that are optimal for a given application, particularly with limited sets of experimental data.
RESULTS
We compiled a literature-based database of the diffusivities, solubilities, and permeabilities for six gases—methane (CH4), carbon dioxide (CO2), helium (He), hydrogen (H2), nitrogen (N2), and oxygen (O2)—in a variety of polymers. The number of data points for each gas varied somewhat due to what was available in the literature, with a majority of the datasets having at least 500 polymers for each gas, as shown in Table 1; this represents a sizable portion of the polymers that are typically included in the most up-to-date Robeson plots. We then randomly split this dataset into one of two categories for each gas; one is used for training the ML model, while the other is initially withheld during training. The training datasets were ≈75% of our total database for each gas, which represented at least 250 polymers for each gas. We then apply the trained model to the remaining 25% of the polymers (test set) and use these data as verification of the model’s accuracy. We found that the prediction of the ML model on these test datasets typically had an R2value of 0.8 or larger, although this correlation improves as the training dataset is made larger (see Table 1).
One challenge when creating ML models for evaluating physical properties is choosing appropriate descriptors to describe the materials being studied. Our first approach only included the number of each atom type in a repeat unit. However, this was found to be an ineffective means to properly model the experimental permeation data. Instead, we choose to use a fingerprinting method where the chemical connectivity in a polymer’s repeating unit is represented numerically. Fingerprinting has a distinct advantage over traditional group contribution methods, where all of the possible building blocks must be defined a priori and remain static; fingerprinting methods are an inherently more dynamic representation because they can evolve to include materials as they are synthesized. Further, they take into account the chemical connectivity between the different units. We transformed each polymer into a binary “fingerprint” using the Daylight-like fingerprinting algorithm as implemented in RDKit. This topological-based approach analyzes the various fragments of a molecule containing a certain number of bonds and then hashes each fragment to produce a binary fingerprint that computationally represents the molecule; this is shown schematically in Fig. 2. After a polymer’s repeat unit was read into memory via a molfile, it was broken down into fragments containing between 1 and 7 units (represented for n = 1 to n = 4 in Fig. 2), and the structure was hashed into a fingerprint with 2048 bits of information to encode all of the possible connectivity pathways of the monomer. This process is repeated for each group in the molecule to generate the full fingerprint. Each bit was treated as a single feature in our model, which allows us to study the effects of various functional groups and their linkages on gas transport. Each monomer was connected to at least nine other identical repeat units to properly account for longer paths along the polymer backbone. This fingerprinting technique is the simplest representation of the polymer chemistry and structure that is sufficient to capture trends observed in the experimental data.
After training our model [which uses the Gaussian process regression (GPR) method] on each gas’s permeability dataset (see Materials and Methods), we used both cross-validation in the training set and a hold-out test set to evaluate model performance. While Table 1 includes data from relatively large train set sizes, we have systematically varied the size of this initial training set—we find that the mean squared errors only begin to decrease for train sizes larger than ~400 and that the mean square error of the model (see Supplementary Materials) decreased monotonically as this size is increased. This explains why previous efforts, which typically used 100 polymers in their ML studies, were less insightful. Our choice of large train sizes thus reflects our goal to have a more generally applicable ML-derived model. Despite the varying amount of test data for each gas, each model performed similarly well with mean squared errors on the order of 2 to 4 Barrer (1 Barrer = 1 × 10−10 cm3[STP] cm2/cm3 s cmHg; correlation curves for each gas are provided in the Supplementary Materials). Overall, we were satisfied with the test set performance and retrained the models on the full dataset to be used in predictions on new polymers never before tested. We then downloaded 11,325 molfiles from the National Institute for Materials Science (NIMS) Materials database (which represents a large repository of previously synthesized polymers) and apply the ML model to these polymers to predict their gas transport performance Only a few structures (≈1.5%) in this prediction dataset were also in our full training set, meaning that the vast majority of polymers in the NIMS database that we predict represent new gas transport data, with no known experimental data.
One of the challenges in using ML modeling for property prediction is associating these predictions with physically meaningful quantities. This is the focus of much current research. Our model, which uses a fingerprinting method, makes it difficult to point to a specific set of physical quantities that are important in the prediction of gas permeabilities, such as free volume descriptors of the polymer chain. However, by examining the higher-performing materials—those which are above the upper bound—and their common characteristics, we are able to gain insight into what physical quantities are important for enhancing gas permeability and selectivity. We can also analyze the chemical structure of these high-performance materials to discern design motifs that are expected to give the best performance. Figure 3 (A and B) shows the learned gas transport data of the polymers in the NIMS database for O2/H2 and CO2/CH4, plotted in the Robeson plot format. Representative data used for model training are also shown.
Almost all predicted selectivities/permeabilities remain just below the Robeson 2008 upper bound line for the O2/N2 and CO2/CH4 gas pairs. However, more than 100 polymers are significantly above the 2008 upper bound for the CO2/CH4 gas pair. The polymers that are above this bound have several common characteristics. Of the 11,325 polymers in the dataset, polysulfides accounted for only 7.00%; however, they made up most (53.00%) of the polymers that crossed the CO2/CH4 2008 upper bound. In addition, the percentage of polysulfones (5.30% total, 18.00% above the upper bound) and polyimides (17.65% total, 35.00% above the upper bound) have a larger share in the upper bound–breaking group. Aromatic polyethers consisted of 30.78% of the total prediction dataset, but only 21.00% of the upper bound–breaking group; similarly, polyvinyls consisted of 13.7% of the total dataset but only 1% were above the upper bound. This implies that these functional groups are typically linked to suboptimal membrane performance (additional statistical analysis of the polymer classes in the CO2/CH4 Robeson plot is shown in the Supplementary Materials). The upper bound–breaking polymers were further analyzed by creating a two-dimensional histogram for group pairs. It was found that 18.00% belonged to both the polysulfone and polyimide classes, and 17.00% belonged to both the polysulfone and polyether classes. Thus, it was observed that materials containing a sulfur group, an oxygen along the backbone, and/or nitrogen rings performed the best in this context. Thus, our models for this gas pair seem to point to physically meaningful chemistries that can be used to enhance gas separations and may be further used in the future to identify strategies that have not been experimentally studied.
We focused our attention on two polymers predicted to lie well above the upper bound for CO2/CH4 separations (SDs are from the GPR). These two polymers are identified in the NIMS database as poly[(1,3-dioxoisoindoline-2,5-diyl)sulfonyl(1,3-dioxoisoindoline-5,2-diyl)-1,4-phenyleneoxy-1,4-phenylene] (ID: P432092) and poly[(1,3-dioxoisoindoline-2,5-diyl)sulfonyl(1,3-dioxoisoindoline-5,2-diyl)-1,4-phenylenemethylene-1,4-phenylene] (ID: P432095). Their locations on the CO2/CH4 Robeson plot, as well as the structure of their repeat units, are shown in Fig. 4. Both of these polymers are polyimides containing sulfone groups; in addition, P432092 contains an aromatic ether linkage; each of these groups is highlighted during our analysis of the ML data as being related to high CO2/CH4 selectivities.
Although similar sulfur-containing polyimides have been tested for gas separations in general, CO2/CH4 selectivity has not been tested with these specific polymers. We synthesized both polymers and tested their CO2/CH4 transport performance to experimentally verify the ML data. The synthesized polymers were cast from solution into thin (≈30 μm) films via doctor-blading and tested using the well-known constant volume/variable pressure experimental technique with an upstream experimental pressure of ≈2 atm. The experimental results are plotted in relation to their predicted values in Fig. 4; the polymers exceed the 2008 Robeson upper bound for this gas pair as predicted by the ML model, and both P432092 and P432095 exhibit selectivities ~7 and 5.5 times, respectively, that of the upper bound at the same permeability value. Further, we find that the experimental and predicted data points are in relatively good agreement with each other (within the error of the prediction), indicating that the ML model may be used as a predictive tool in identifying previously unexplored polymers for gas separations.
DISCUSSION
The ML algorithm–based approach derives permeability predictions by using a detailed knowledge of the monomer structure and chemistry. We began with an approach that looked at only atoms, but found it to be insufficient; a description that includes connectivity within a monomer is found to be sufficient in terms of predicting permeability. This approach ignores all higher-order polymer descriptors such as stereoregularity, polarity, and chain length. While we find that these variables are not required to gain reasonably accurate predictions of polymer properties, more sophisticated means of representing the polymer chain that can include these nuances may further increase the accuracy of the ML model and allow us to properly hone in on these more complex design cues. However, with current fingerprinting methods, there is no logical means by which appropriate descriptors can be defined for predicting an arbitrary property. How this choice should be made remains a topic of research. We also observe that other fingerprinting tools with similar complexity may have similar accuracy as the Daylight-like fingerprinting method we used here. An open question in the ML field is choosing the proper descriptor for a set application.
Our ML approach is designed with the specific goal of quickly characterizing gas permeabilities for an extremely large set of polymers and then a posteriori correlating high-performance materials with common functional groups and bond linkages; this allows us to determine which chemistries and structures are worth experimental observation. We emphasize that we do not relate these results to a molecular understanding of a polymer property as viewed through one of the many theoretical models available, e.g., for gas transport. In a similar vein, a number of past experimental work have focused on the effect of various polymer backbone properties on either solubility or diffusion, e.g., the effect of sulfur groups on CO2 solubility and more polymer backbone stiffness on gas diffusion constants. Our approach focused specifically on predicting polymer permeabilities, as the available literature data that decomposes permeabilities into solubility and diffusion are less plentiful. The specific dependence of solubility and diffusivity on polymer structure can be potentially probed using this approach in the future provided a more complete database is available—this might allow us to probe the factors affecting solubility and diffusivity separately.
Our ML algorithm, as currently used, only tests against already synthesized polymers. A superior approach would be to include out-of-the-box polymer architectures in the algorithm and then imposing a “synthesizability” constraint as a means of selecting polymers for further study. However, practical implementation of this approach has not been determined, and it remains a topic of debate.
The approach presented above is easily amenable to an inverse design approach. Namely, we can design polymers with a desired combination of permeability and selectivity for a gas pair by using, e.g., a genetic algorithm to construct the optimal fingerprint vectors. This is ongoing work in our laboratory.
Dynamic Stall in Pitching
Airfoils: Aerodynamic Damping
and Compressibility Effects
Keywords
stall hysteresis, unsteady aerodynamics, stall flutter
Abstract
Dynamic stall is an incredibly rich fluid dynamics problem that manifests
itself on an airfoil during rapid, transient motion in which the angle of incidence
surpasses the static stall limit. It is an important element of many
manmade and natural flyers, including helicopters and supermaneuverable
aircraft, and low–Reynolds number flapping-wing birds and insects. The
fluid dynamic attributes that accompany dynamic stall include an eruption of
vorticity that organizes into a well-defined dynamic stall vortex and massive
excursions in aerodynamic loads that can couple with the airfoil structural
dynamics. The dynamic stall process is highly sensitive to surface roughness
that can influence turbulent transition and to local compressibility effects
that occur at free-stream Mach numbers that are otherwise incompressible.
Under some conditions, dynamic stall can result in negative aerodynamic
damping that leads to limit-cycle growth of structural vibrations and rapid
mechanical failure. The mechanisms leading to negative damping have been
a principal interest of recent experiments and analysis. Computational fluid
dynamic simulations and low-order models have not been good predictors
so far. Large-eddy simulation could be a viable approach although it remains
computationally intensive. The topic is technologically important owing to
the desire to develop next-generation rotorcraft that employ adaptive rotor
dynamic stall control.
1. BACKGROUND
Largely because of the helicopter application, dynamic stall has been an active research topic
in fluid dynamics for more than 60 years. However, its foundation dates back nearly 90 years,
with the origins of classical unsteady aerodynamics that include Wagner (1925), Glauert & Holl
(1929), and Theodorsen (1935), who developed analytic solutions to incompressible flow around
thin airfoils undergoing time-dependent motions. Other early contributions were from K¨ ussner
(1936), von K´arm´an&Sears (1938), Sears (1941), and Lomax (1953), who analyzed unsteady aerodynamics
problems associated with oscillating wings, sinusoidal vertical gusts, and compressible
flow, respectively. Kramer (1932) was one of the first to experimentally document the augmented
lift associated with dynamic stall. Bailey & Gustafson (1939) and Gustafson & Myers (1946) used
tufts and a blade-mounted camera on an autogyro rotor blade to document dynamic stall, which
they found to occur on the retreating blade during forward flight.
Figure 1 illustrates the dynamic stall process that occurs on an airfoil oscillating about its
quarter-chord location. Figure 1a shows the aerodynamic loads and pitch moment during the
pitching cycle, and Figure 1b shows the flow field structure during the pitching cycle that was
visualized with particle streak lines introduced at an upstream location.
The conditions in Figure 1 exemplify deep dynamic stall, which occurs when the maximum
angle of attack, α0 + α1, of the pitching motion exceeds the steady airfoil stall angle of attack, αss.
Here α0 denotes the mean angle of attack, and α1 denotes the pitch amplitude. This results in a
fully developed dynamic stall vortex-shedding phenomenon (McCroskey 1981) that produces large
peak aerodynamic loads and severe cycle hysteresis. The nomenclature is intended to differentiate
this condition from that of light dynamic stall, which occurs for lower peak angles of attack and
for which the peaks in the aerodynamic forces and pitch moment are less severe.
At the start of the pitch-up portion of the cycle (stage 1 in Figure 1), the boundary layer on
the suction surface of the airfoil (upper surface in the flow visualization) is attached, and the lift
force increases linearly with the pitch angle. This continues to the point at which the pitch angle
reaches the steady airfoil stall angle of attack, αss. The lift then continues to increase beyond
αss as a result of two mechanisms: (a) a delay in the boundary layer separation owing to the
pitching motion and (b) the formation of a closed separation bubble near the leading edge of the
airfoil.
The delay in the boundary layer separation is attributed to two effects. One is an increase in
the effective camber that is predicted from quasi-steady thin airfoil theory when α˙ > 0 (Leishman
2000). The other is the acceleration of the boundary layer due to the Magnus effect produced by
the motion of the leading edge (Ericsson & Reding 1984, 1988). Carta (1971) showed analytically
that the adverse pressure gradient over the suction side of a pitching airfoil was less than that of
a steady airfoil. The adverse pressure gradient is further attenuated as the pitch rate increases.
Walker et al. (1985) added that increasing the pitch rate promoted a stronger static pressure
suction peak at the leading edge that ultimately led to a more energetic dynamic stall vortex.
Stage 2 of the flow development corresponds to the first appearance and subsequent growth
of the dynamic stall vortex. This involves the spontaneous generation and ejection of vorticity
from the boundary layer into the inviscid outer flow in a process detailed by van Dommelen &
Shen (1980). The growth of the dynamic stall vortex results in additional aerodynamic loading
that exceeds that of the steady airfoil.
The onset of dynamic stall on a pitching airfoil can involve a number of mechanisms that depend
on Reynolds and Mach numbers. These include (a) the bursting or breakdown of the separation
bubble, (b) an abrupt breakdown of the boundary layer reverse flow within the separation bubble,
and (c) boundary layer–shock interaction that causes flow separation.

(a) Illustration of dynamic stall events based on air loads and pitch moment cycle. Characteristic stages of the flow development in the
pitching cycle are denoted by the numbers next to specific flow visualization images and aerodynamic load and moment cycle maps.
(b) Visualized flow about a pitching airfoil undergoing deep dynamic stall. Panel b adapted with permission from Corke et al. (2011).
Certain stall situations, particularly on airfoils that coincide with thin airfoil theory, exhibit a
sudden breakdown of the laminar separation bubble owing to turbulent transition ( Johnson &
Ham 1972). McCroskey et al. (1976, 1981) extensively studied this process at lower Mach numbers
at which shock waves were not in play. In these experiments, the authors documented two alternate
scenarios for the onset of dynamic stall. The first involved trailing-edge stall in which separated
flow originating at the trailing edge gradually moved forward as the airfoil pitched upward. In
this case, the dynamic stall vortex formed aft of the maximum thickness point of the airfoil, never
reaching the leading edge.
The second scenario also originated at the trailing edge. It was manifest by a boundary layer
flow reversal that moved forward over the suction side of the airfoil. The occurrence of reverse
flow in the near-wall region of the boundary layer without the occurrence of large-scale separation
emphasizes the importance of not using the criterion of zero-skin friction as a separation indicator
in the pitching airfoil flow field (Reynolds&Carr 1985). The flow reversal was documented to have
little effect on the turbulent boundary layer or aerodynamic behavior until the forward motion
reached the maximum thickness point of the airfoil. Immediately thereafter, if not concurrently, the
turbulent boundary layer separated and abruptly moved upstream and downstream. This initiated
a dynamic stall vortex near the leading edge. McAlister & Carr (1979) and Lee & Gerontakos
(2004) observed a similar scenario.
Doligalski et al. (1994) comprehensively reviewed work focused on both the process leading to
the development of the dynamic stall vortex and its subsequent detachment from the airfoil surface.
The authors noted the proclivity of laminar boundary layers to develop a sharply focused eruption
of vortical fluid from the surface in the presence of an adverse pressure gradient. In pitching
airfoils at sufficiently high Reynolds numbers, several studies suggested that the initiation of the
dynamic stall vortex appears to be associated with a similar viscous/inviscid interaction in the form
of the sudden eruption of a thin plume of vortical fluid from the surface that subsequently deflects
downstream and undergoes roll-up. Reynolds & Carr (1985) discussed the buildup of localized
boundary layer vorticity preceding dynamic stall in terms of a balance between the self-induced
upstream propagation of boundary layer vorticity and its downstream advection by the external
flow. The strong favorable pressure gradient near the airfoil leading edge of a pitching airfoil
introduces large amounts of vorticity into the boundary layer on the forward portion of the airfoil
concurrent with a decreasing external velocity over the aft portion. This sets the stage for a critical
localized accumulation of vorticity just aft of the suction peak.
At higher free-stream Mach numbers, the locally supercritical levels over the pitching airfoils
become the trigger for dynamic stall. This was evident from the experiments of McCroskey et al.
(1981), who found that a group of airfoils that exhibited one of the two trailing-edge dynamic
stall scenarios at lower free-stream Mach numbers showed leading-edge dynamic stall at a higher
free-stream Mach number in which supersonic flow occurred over the suction surface (Carr et al.
1982; McAlister et al. 1982; McCroskey et al. 1981, 1982). Chandrasekhara et al. (1998a) found
that complex interactions between the separation bubble and weak shock waves in the supersonic
region initiated the dynamic stall. At higher Mach numbers, M∞ > 0.45, the extent of the
supersonic region over the pitching airfoil was found to considerably increase. Strong normal
shock waves were observed to form. These led to an abrupt thickening and subsequent separation
of the boundary layer flow. Chandrasekhara et al. (1998a) observed that an oblong vortical structure
formed after the shock. Thus, the shock-induced flow separation was the origin of the dynamic
stall vortex. The laminar separation bubble that formed near the leading edge was observed to be
present but played no role in the development of the dynamic stall vortex.
In stage 3 of the flow development shown in Figure 1, the fully formed dynamic stall vortex
convects over the suction side of the pitching airfoil. This motion of the low-pressure vortex core
shifts the airfoil center of pressure toward the trailing edge, resulting in an acute nose-down pitch
moment, referred to as moment stall. This occurs shortly after the dynamic stall vortex forms
(Harris et al. 1970). The moment stall precedes the lift stall. The time lag between the moment
stall and lift stall correlates with the time needed for the dynamic stall vortex to convect past the
trailing edge (Bousman 1998).
The low-pressure core of the dynamic stall vortex also augments the aerodynamic suction
on the airfoil, increasing the lift compared to a steady airfoil. The amount of lift enhancement,
however, is decreased at compressible Mach numbers. The added lift drops sharply when the
dynamic stall vortex convects off the airfoil and the flow is fully separated. This signifies stage 4
of the flow development.
In stage 5 of the flow development, the pitch-down motion of the airfoil causes the flow to
begin to reattach. This begins at the leading edge and gradually proceeds to the trailing edge. As
the flow attaches, there is a shift in the aerodynamic load distribution toward the leading edge,
which restores the positive pitch moment.
Recovery from dynamic stall is somewhat of a stochastic process in which significant differences
in aerodynamic loading can occur from one pitch cycle to the next (Green & Galbraith 1995,
Wernert et al. 1996, Young 1981). Liiva & Davenport (1969) proposed that these differences
resulted from random features of the separated shear layer. This was later supported through
particle image velocimetry measurements by Shih et al. (1992) and Wernert et al. (1996), who
observed that the separated shear layer contained nonrepeated patterns of vortical structures from
one cycle to the next. Werner et al. further remarked that the nonreproducible nature depended
on the size of the separated shear region and the reduced frequency of the pitching motion. The
cycle-to-cycle variations could be smoothed out through ensemble averaging correlated with the
pitching motion. McAlister et al. (1982) observed that 50 pitch cycles were sufficient to yield
converged statistics.
Early rotor dynamic stall investigations focused on aeroelastic issues such as stall flutter (Brooks
& Baker 1958, Ham 1966, Zvara & Ham 1960). Ham & Garelick (1968) were the first to realize
the vortical nature of rotor blade dynamic stall. The signature of “very concentrated positive
vorticity” was captured in surface pressure measurements that revealed a dynamic low-pressure
region that formed near the leading edge and convected toward the trailing edge. Integrating
the pressure distribution around the rotor section to obtain the aerodynamic forces revealed a
severe hysteresis in the pitch moment that could couple with the pitching motion and result in
negative aerodynamic damping. Ham (1968) later linked the dynamic stall phenomenon with the
occurrence of stall flutter.
Following Ham (1967), investigators sought to identify the underlying mechanisms for stall
onset delay of a pitching airfoil that could lead to analytic or empirical models for predicting
the aerodynamic loads. At the same time, theories for unsteady flow separation developed that
highlighted fundamental differences between steady and unsteady boundary layers (Haller 2004,
Sears & Telionis 1975, Telionis 1970, van Dommelen & Shen 1980).
The early dynamic stall research almost exclusively focused on incompressible Mach numbers.
However, McCroskey et al. (1981, 1982) showed that the local Mach number near the leading
edge of pitching airfoils can be three to five times higher than that in the free stream. As a
result, incompressible free-stream Mach numbers as low as 0.18 could result in supercritical Mach
numbers over the airfoil. Carr & Chandrasekhara (1992) have particularly focused on the effect
of compressibility on dynamic stall.
1.1. Dynamic Stall Regimes
As mentioned above, the conditions shown in Figure 1 correspond to deep dynamic stall. This
follows the nomenclature set by McCroskey (1981), who divided dynamic stall into four categories:
no stall, stall onset, light stall, and deep stall. Although the majority of dynamic stall research has
focused on deep stall, it is avoided in normal rotorcraft operation owing to the severe forces
and vibrations that occur in this condition (Carr & Chandrasekhara 1996). McCroskey (1981)
delineated the stall regimes based on the maximum angle of attack, αmax = α0 + α1.
1.1.1. No stall. In the no-stall regime, the airfoil trajectory remains below the static stall angle
of attack, αss. Under these conditions, the aerodynamic loads are predicted well by quasi-steady
aerodynamic theory.
1.1.2. Stall onset. Stall onset is the regime in which the airfoil trajectory reaches the static stall
angle of attack. This condition produces the maximum useful lift without excessive drag or pitch
moment. Again, under these conditions, the aerodynamic loads are predicted well by quasi-steady
aerodynamic theory.
1.1.3. Light stall. The light-stall regime marks the first development of a dynamic stall vortex.
The onset, growth, and convection of the vortex are sensitive to the airfoil chord Reynolds number
and free-stream Mach number, as well as the unsteady parameters, including the pitching reduced
frequency, k, and α0 and α1. Lift and moment stall peaks are less severe in this regime. The flow
separation region is on the order of the airfoil thickness (McCroskey 1981). The boundary between
stall onset and light stall is abrupt and can be identified by the first appearance of moment stall.
1.1.4. Deep stall. In the deep-stall regime, the dynamic stall vortex is strongly developed. The
aerodynamic loads fluctuate dramatically through the pitching cycle with large peak forces and
strong hysteresis. The aerodynamic loads exhibit little sensitivity to Reynolds number, airfoil
geometry, or pitching motion. The flow separation region in this case is on the order of the airfoil
chord length (McCroskey et al. 1981).
1.2. Compressibility Effects
Compressibility affects the dynamic stall process of a pitching airfoil in a number of ways. This has
been the topic of many research efforts, including those by Carr & Chandrasekhara (1991, 1992,
1996), Carr et al. (1994), Chandrasekhara et al. (1993, 1998a), Dyken et al. (1996), and Bowles
(2012). Below we summarize the major effects compressibility has on the different elements of the
dynamic stall process.
1.2.1. Stall delay. Typically at M∞ > 0.3, but possibly as low as M∞ = 0.18, the locally
supersonic flow over the pitching airfoil reduces the flow acceleration around the leading edge,
thereby reducing the flow separation delay and the peak lift and pitch moment coefficients. This
is, however, a steady effect and is not related to the unsteady motion of the airfoil. For the same
pitching motion reduced frequency, the stall penetration, αds − αss, remains nearly constant with
increasing Mach number (Bowles 2012). Here αds denotes the dynamic lift stall angle.
1.2.2. Aerodynamic damping. At higher Mach numbers, the attached flow significantly lags
behind the pitching motion, so the hysteresis in the aerodynamic loads between pitch-up and pitchdown
increases withM∞. As discussed in more detail below, this directly impacts the aerodynamic
damping. In this way, compressibility acts in a similar manner as the pitching reduced frequency.
1.2.3. Shock-induced stall. Local compressible Mach numbers over the pitching airfoil initially
promote leading-edge dynamic stall through incipient separation bubble bursting. At higher freestream
Mach numbers, shock-induced dynamic stall becomes the overriding mechanism.
1.2.4. Stall vortex strength. Compressibility effects weaken the strength of the dynamic stall
vortex. This reduces the vortex-induced suction pressures on the airfoil. This is closely related to
Helicopter rotor disk in forward flight (a) and rotor-disk locations of dynamic stall events during in-flight maneuvers of a UH-60A
helicopter (b). Figure adapted with permission from Bousman (1998).
the decrease in the gestation period and residence time of the dynamic stall vortex. Although the
magnitude of the induced pressures decreases with increasing Mach number, the dynamic range
of pressure coefficients on the airfoil, Cp/Cpmin , increases with increasing Mach number. This
generally has a marginal impact on the lift and drag forces. However, it has a significant effect
on the pitch moment, with the outcome being an increase in the cycle-averaged aerodynamic
damping with increasing Mach number (Bowles 2012).
1.3. Helicopter Rotor Aerodynamics
The helicopter rotor flow field features complex, time-varying blade angles of attack, out-of-plane
mechanical and elastic rotor blade motion, unsteady boundary layers, massive flow separation,
vortex blade interaction, and varying inflow velocity. Each of these affects the rotor aerodynamic
performance and may contribute to excessive loading, mechanical vibration, and aerodynamic
noise generation (Conlisk 2001). To compound the flow complexity, the rotor blade may experience
instantaneous local Mach numbers, Ml , that are incompressible, Ml < 0.2, transonic,
0.7 ≤ Ml ≤ 1, or supersonic, Ml > 1, within a single rotor rotation (Leishman 2000, Liiva &
Davenport 1969, McCroskey et al. 1976).
Figure 2a illustrates the rotor disk for a helicopter in forward flight with velocity, Uff. The
rotor blade is considered to be rotating at a constant angular velocity of ω. The angular position of
the rotor is denoted by ψ. For 0◦ ≤ ψ ≤ 180◦, the rotor blade is moving opposite to the direction
of the relative airflow caused by forward flight and therefore is experiencing local velocities that
are larger than Uff . This is referred to as the advancing blade. For the other half of the rotation,
180◦ ≤ ψ ≤ 0◦, the motion is in the direction of the relative airflow caused by forward flight;
therefore, the blade experiences local velocities that are less than Uff. This is referred to as the
retreating blade. The lift generated on the advancing blade must be the same as on the retreating
blade; therefore, the angle of attack on the retreating blade must be larger than that on the
advancing blade. As a result, the rotor blade oscillates in pitch as it rotates from the advancing
to retreating sides of the rotor disk. This provides one major motivation for the pitching airfoil
research discussed above.
The main parameter governing the unsteady nature of the pitching rotor blade is the reduced
frequency, k = π f c /Uff, where f is the physical pitching frequency, and c is the rotor blade chord
length. The reduced frequency of the pitching motion represents the ratio of timescales associated
with the unsteady pitch motion to that of the free-stream convection (Leishman 2000). For k = 0,
the flow is steady. For 0 < k < 0.05, the unsteady effects are generally small and can be neglected,
provided the pitch amplitude is not too large. For k ≥ 0.05, the flow field is considered to be
unsteady. If k ≥ 0.2, the flow field is considered to be highly unsteady. The reduced frequency
can also be thought of as a phase lag parameter in which the fluid reaction lags behind the pitching
motion owing to the inertial effects. Larger reduced frequencies correspond to an increased
phase lag between the pitching and fluid motions. In general, the reduced frequency is constantly
changing on the helicopter rotor because of the local inflow dependence on Uff. As a result, it is a
somewhat ambiguous parameter with regard to the unsteadiness associated with helicopter rotor
aerodynamics (Leishman 2000). However, in wind-tunnel experiments, the reduced frequency is
a constant value for a fixed combination of pitching frequency and free-stream speed.
Dynamic stall most commonly occurs on the retreating side of the rotor disk (Harris et al. 1970,
Tarzanin 1972, Young 1981), although there is some evidence of dynamic stall on the advancing
rotor (Bousman 1998). Bousman (1998) found that stall events occurred at multiple sites during
flight tests of a UH-60A helicopter. Figure 2b shows a map of these events in the rotor disk plane.
Although the majority of the rotor stall did occur on the retreating blade side, some stall events
occurred in the first quadrant (0◦ ≤ ψ ≤ 90◦) of the advancing blade.
The potential for dynamic stall depends on the rotor blade loading, which is the ratio of the rotor
thrust coefficient to the rotor solidity,CT /σ , and the flight speed. Increased blade loading results in
increased dynamic stall. However, at lower blade loading, increased forward flight speed promotes
an increase in dynamic stall. Helicopter performance boundaries are defined by the rotor dynamic
stall. The dependent variables are the blade loading, CT /σ , and the advance ratio, μ = Uff/R,
which is the ratio of the flight speed to the rotor blade tip speed. At high advance ratios, dynamic
stall can occur on the advancing blade owing to shock formation (Martin et al. 2008).
The practical outcome of dynamic stall on the rotor is an increase in unsteady loads on the
rotor that are subsequently transmitted to the rotor hub and control system. In some cases, the
conditions of dynamic stall can lead to negative damping that results in a limit-cycle growth of
rotor displacements. This phenomenon is referred to as stall flutter (Ham & Young 1966), which
can lead to catastrophic mechanical failure of the rotor.
2. AERODYNAMIC DAMPING AND STALL FLUTTER
Stall flutter is a single-degree-of-freedom aeroelastic motion that results from negative system
(combined aerodynamic and structural) damping. On a helicopter, it appears as a limit-cycle
growth in amplitude of rotor torsional oscillations at the torsional natural frequency (Fung 2008).
This can drive other modes of vibration of the rotor as well. For a helicopter, limit-cycle oscillations
triggered on the retreating blade side are often damped as the flow reattaches on the advancing
blade side (Ham & Young 1966). However, if the aerodynamic damping on the advancing side is
near zero, successive rotations of the rotor through negatively damped retreating blade regions
can lead to unchecked, additive limit-cycle growth to excessive amplitudes (Bowles 2012).
The stability of the air load system has been traditionally quantified through the aerodynamic
damping coefficient, 	cycle, derived by Carta&Niebanck (1969) and Oates (1989). The derivation
considers a single-degree-of-freedom structure subject to harmonic forcing in a uniform airstream.
The cycle aerodynamic damping is given as
	cycle = −CW /(πα2
1 ) = − 1
πα2
1

Cmc /4dα, (1)
where CW is the normalized energy transfer between the airstream and airfoil, and Cmc /4 is the
moment coefficient about the quarter chord.
Equation 1 can be experimentally computed from the area enclosed in the cycle variation in
the pitch moment coefficient, Cmc /4 (α), namely,
	cycle = 1
πα2
1
 αmax
αmin
(CD
mc /4
− CU
mc /4 )dα, (2)
where the superscripts U and D denote the upward and downward angle of attack motion of the
airfoil, respectively.
For Theodorsen’s model of a thin airfoil (flat plate) undergoing pure pitch oscillations about its
quarter-chord location in a potential flow field, the aerodynamic damping is (Carta & Niebanck
1969)
	cycle = πk/2. (3)
When the maximum angle of attack in the pitching cycle is below the static stall angle of attack, αss,
the aerodynamic loads and moments are predicted well by Theodorsen’s (1935) method. Under
these conditions, the aerodynamic damping coefficient is always positive and linearly dependent
on the reduced frequency, k, as given in Equation 3.
If the airfoil pitching range is such that its maximum angle of attack exceeds αss by a few degrees,
then light dynamic stall ensues (McCroskey 1982). Figure 3a depicts the moment coefficient
during a pitch cycle for light stall. McCroskey et al. (1981) found that light dynamic stall had
the largest tendency to produce unstable aerodynamic loading on helicopter rotor sections. As
indicated in Figure 3, clockwise loops in the moment coefficient trajectory are associated with
negative damping. Counterclockwise loops lead to positive damping.
Deep stall occurs when the maximum angle of attack during the pitching cycle far exceeds αss
(McCroskey 1982). Figure 3b illustrates the moment coefficient during a pitch cycle undergoing
deep stall. Based on the cycle-averaged damping, deep stall is generally more aerodynamically
stable than light dynamic stall. This is apparent in Figure 3 in which the clockwise moment
coefficient loop for light stall is much larger than that for deep stall.
Bowles (2012) and Bowles et al. (2012, 2014) developed a formulation for a time-resolved
aerodynamic damping coefficient. This involved the use of the discrete Hilbert transform of the
pitch moment and incidence time series. The integral Hilbert transform of a data series y(t),
denoted here as ˜y(t), is defined as
˜y(t) ≡ H[y(t)] = −1
π
P
 ∞
−∞
y(τ )
τ − t
dτ, (4)
where H is the Hilbert operator, and ℘ denotes the Cauchy principal value of the improper
integral.
The single-degree-of-freedom equation of motion (EOM) for a pitching airfoil semichord b
in a uniform airstream is
πρb4α¨ (t) + h∗(t)α˙ (t) + κ
∗(t)α(t) = M(t), (5)
where h∗(t) = hR(t) + ihI (t) = ¯h(t)eiγ1(t) is the viscous damping moment, κ
∗(t) = κR(t) + iκI (t) =
κ¯ (t)eiγ2(t) is the aerodynamic stiffness, and M(t) is the pitch moment. This EOM can account for
nonlinearities inherent to the dynamic stall problem, unlike formulations that rely on constant
amplitude, constant phase damping, or stiffness (Oates 1989). Both h∗(t) and κ
∗(t) contribute to
the total aerodynamic damping.
Substitution of the polar representations of h∗ and κ
∗ into Equation 5 yields
α¨ + 2h0(t)eiγ1(t)α˙ + ω2
0(t)eiγ2(t)α = 1
πρb4 M, (6)
where the damping coefficient, h0, is defined as
h0 ≡ ¯h /2πρb4, (7)
and the undamped natural frequency, ω0, is
ω0 ≡

κ¯ /πρb4. (8)
In this formulation, the instantaneous pitch angle, α, its derivatives, and the instantaneous pitch
moment, M, remain time dependent.
To employ the Hilbert transform analysis, one replaces each term in Equation 6 by its analytic
signal counterpart found through the Hilbert transform; this generates the new EOM
A¨ + 2h0(t)eiγ1(t)A˙ + ω2
0(t)eiγ2(t)A = 1
πρb4
M, (9)
where A = α + i ˜ α = α1eiωt and M = M + i ˜M = A(t)eiφ(t) (Bowles 2012; Bowles et al. 2012,
2014).
Substituting A,M, and the first and second derivatives of A into Equation 9, and equating the
imaginary parts, one obtains
As defined by Carta&Niebanck (1969), the left-hand side of Equation 10 is the total aerodynamic
damping, −ξ/πρb4. Therefore,
ξ (t) = − 1
α1
( ˜M cos ωt − M sin ωt)   
MI
= −A(t)
α1
sin(φ(t) −   ωt
ψ
),
(11)
where MI denotes the quadrature (out-of-phase) pitch moment component.
Nondimensionalizing Equation 11 provides the aerodynamic damping coefficient,
	(t) = ξ/qc 2 = −ACm (t)
α1
sinψ(t), (12)
where ACm (t) =
	
C2m
+ ˜C
2m
, and ψ(t) is the phase lead or lag between the pitch moment and the
time-dependent angle of attack. Equation 12 illustrates how the phase lead or lag between the
pitch moment and the prescribed angle of attack determines the sign of the aerodynamic damping;
its form is identical to previous derivations of the aerodynamic damping (Bisplinghoff et al. 1955,
Oates 1989, Rainey 1957). Furthermore, it remains that a phase lag, ψ(t) < 0, produces positive
damping, whereas ψ(t) > 0 indicates negative damping or a local exchange of energy from the
airstream to the airfoil.	reflects the stability of the pitch moment, which in turn is a manifestation
of the developing pressure field. An unstable pitch moment, 	 < 0, implies unstable pressure
loading and can be described as a necessary, aerodynamic condition to excite the stall flutter of an
elastic body.
The novelty of the approach, unlike previous investigations of aerodynamic damping that rely
on 	cycle, is that Equation 12 allows one to calculate the intracycle aerodynamic damping. The
average damping, 	avg, is then 	avg = 1
T

 T
0 	(t)dt, where T is the period of a single oscillation.
2.1. Attached Flow
The cycle-integrated damping, 	cycle, for attached flow has been documented to increase with
increasing Mach number (Lorber et al. 1992). In fact, this increase can be modeled by typical
compressibility corrections, for example, the Prandtl-Glauert rule (Bowles 2012), up to the formation
of shocks on the airfoil upper surface. When shocks form, the cycle-integrated damping
rapidly increases. This cycle-averaged statistic, however, masks the complete physics that includes
negatively damped portions of the pitch cycle that could promote unstable flutter.
As an illustration of this behavior, Figure 4 depicts the Hilbert transform–based damping
analysis for two experimental runs of nearly equal stall penetration (αmax − αss ≈ −1.1◦) but at
low– and high–subsonic Mach number conditions. Figure 4a,d shows the pitch moment variation
with incidence. In each of these panels, Cm denotes the data series obtained by spatial integration
of the chordwise static pressures. These are ensemble-averaged quantities (≈100 cycles) chosen
in lieu of the instantaneous pitching moment to remove the cycle-to-cycle variations common
to the dynamic stall problem (Wernert et al. 1997). The Hilbert transform (˜Cm), in-phase (CmR ),
and quadrature (CmI ) pitch moments are also displayed. At M∞ = 0.2, the elliptic, counterclockwise
nature of the pitching moment is clearly seen. Figure 4b,e presents the polar representation
of the intracycle damping coefficient. In this representation, the radial component indicates the
magnitude of the damping, ACm (t)/α1, and the vector angle is the phase difference between the
aerodynamic pitch moment and the pitch motion, ψ(t) = φ(t)−ωt. Recall from Equation 12 that
	(t) ∝ −sinψ(t); therefore, when ψ(t) is in the third and fourth polar quadrants, the damping is
Time-resolved damping analysis demonstrating unstable damping for cases in which αmax < αss. (a,d ) Data series, discrete Hilbert
transform, and in-phase and quadrature moments. Arrows indicate the direction of increasing time. (b,e) Polar representation of the
time-resolved damping coefficient. (c,f ) Transient damping coefficient over a single pitch cycle. The formation of normal shocks on the
airfoil upper surface at M∞ = 0.6 (d–f ) results in unstable damping. (a–c) M∞ = 0.2, k = 0.052, and α = 5.2◦ − 5.1◦ cos ωt.
(d–f ) M∞ = 0.6, k = 0.046, and α = 3.3◦ − 5.4◦ cos ωt. Figure adapted with permission from Bowles et al. (2012, 2014).
positive, whereas in the first and second quadrants, the damping is negative. The polar representation
of the intracycle damping coefficient at M∞ = 0.2 shown in Figure 4b traces out only a
small region in the third quadrant, yielding the nearly constant (albeit noisy) positive intracycle
damping coefficient shown in Figure 4c.
At M∞ = 0.6, the pitch moment exhibits no crossovers, indicating no observable negative
damping, although the ellipse is clearly distorted. The distortion is a consequence of shock development
and the concomitant nose-up moment. This behavior causes the phase difference (Figure 4e)
to decrease and become positive at ωt = 0.83π (α = 7.5◦). This leads to a brief period of negative
aerodynamic damping. This negative damping is not observed in Figure 4d because of the much
larger magnitude of the positive damping during pitch-down over the same range of angle of
attack.
The destabilizing effect of shock formation is further illustrated in Figure 5, which shows
plots of the upper-surface pressures, pitch moment Cmc /4 , instantaneous angle of attack α, and
time-resolved damping 	(t) at M∞ = 0.6 and α = 4.0◦ − 4.4◦ cos ωt. For this case, as in the
M∞ = 0.6 case examined in Figure 4, localized, negative damping occurs during a portion of
the pitching cycle. The mechanism for the negative intracycle damping stems from the growth of
a λ shock during the pitch-up portion of the cycle. The position of the shock front was inferred
from the brief increase in the pressure captured by the onboard sensors. This shock wave was
Upper-surface pressures, α = 4.0◦−4.4◦ cos ωt. The zero-axis and nondimensional chord location of each
pressure sensor is indicated on the right. Each pressure time series is offset by CS
p
≈ −1.5 and color coded to
represent the local Mach number, Ml , on the airfoil surface. The position of the shock front is outlined.
Figure adapted with permission from Bowles et al. (2012, 2014).
also observed in high-speed schlieren images of the leading-edge flow field (Bowles 2012). The
destabilizing portion of the motion occurs after the streamwise propagation of the shock front
comes to a halt near the quarter-chord location. The destabilization results from a decrease in the
pressure upstream of the shock front that increases the leading-edge suction. This couples with
higher static pressure aft of the shock, which results in a nose-up pitch moment that is responsible
a) Effect of dynamic stall regime, set by the maximum incidence, on the cycle damping coefficient, k ≈ 0.05, α1 ≈ 5◦. The static stall
angles of attack for each Mach number are indicated by the colored arrows. (b) Measured pitch moment for light stall, αds − αss ≈ 2◦, at
M∞ = 0.2, 0.3, 0.4, 0.5, and 0.6; k ≈ 0.05; and α1 ≈ 5◦. The solid lines are for pitch-up and dashed lines for pitch-down. For clarity,
the moment plots at the five Mach numbers are offset by Cmc /4
= +0.1 and color coded in accordance with panel a. Figure adapted with
permission from Bowles et al. (2012).
for a decrease in the aerodynamic damping at that phase in the pitching cycle. A similar decrease
in damping occurs during pitch-down, just prior to the shock front retreating to the leading edge.
Based upon examination of the minimum intracycle aerodynamic damping coefficient,	min, we
can summarize the effect of reduced frequency and the Mach number on the aerodynamic damping
characteristics of attached pitching flow as follows. (a) Regardless of the Mach number, increased
reduced frequency has a stabilizing influence (increasing positive aerodynamic damping). (b) At
any fixed reduced frequency, the effect of increasing the Mach number on the damping is minimal,
except when shocks begin to form on the suction surface. (c) Shock waves can produce negative
damping that could trigger stall flutter even in an attached pitching condition!
2.2. Light and Deep Dynamic Stall
As described above, dynamic stall occurs when the pitching motion exceeds the static stall angle
of attack, αss. This leads to the eruption of surface vorticity and the formation and downstream
convection of a dynamic stall vortex. The process results in large peak aerodynamic loads and
pitch moments and strong cycle hysteresis that directly affects the aerodynamic damping.
Figure 6a highlights the typical behavior of the cycle-averaged damping coefficient, 	cycle,
under light- and deep-stall conditions for different free-stream Mach numbers. Figure 6b singles
out five pitch moment cycles from the data set that have similar dynamic stall penetration at their
respective Mach numbers.
One can observe that the minimum damping in these cases occurred when the maximum angle
of attack, αmax = α0+α1, was approximately 2–3◦ past αss. This level of stall penetration classically
falls into the light dynamic stall regime described by McCroskey (1981). As the maximum incidence
increases to enter the deep-stall regime, the cycle damping becomes more positive, which is a result
of the strongly developed dynamic stall vortex (Carta & Niebanck 1969, Fung 2008). Similar
damping and Mach trends are reported by Liiva (1969).
One can observe the trend of increased damping with increasing Mach number in the accompanying
pitch moment cycles. Beginning at M∞ = 0.2, the clockwise rotation of the pitch moment
path for −2◦ ≤ (αds − αss) ≤ 2◦ is emblematic of an unstable damping coefficient (see Figure 3).
At higher Mach numbers, the pitch moment path direction in the vicinity of αss is decidedly counterclockwise,
which signifies positive damping. Moreover, the size of the moment hysteresis loop
decreases with increasing Mach number, which is consistent with an increase in positive cycle
damping with increasing Mach number, observed in Figure 6a.
Based on this assessment of the cycle-averaged damping, one might conclude that there is less
tendency for stall flutter under light and deep dynamic stall at higher Mach numbers. However,
the cycle-averaged statistics can conceal important physics that can lead to localized, unstable
damping during the dynamic stall process.
To demonstrate this, investigators analyzed three of the pitch moment cycles at M∞ = 0.2,
0.3, and 0.6 in Figure 6 using the Hilbert transform formulation to examine intracycle damping
characteristics. Figure 7a,c,e shows the polar representation of the aerodynamic damping for
the pitch cycle. Figure 7b,d,f simultaneously indicates the time-resolved damping, the ensemble
pitching moment, Cmc /4 , and the position of the dynamic stall vortex based on tracking the local
minimum in the suction-side surface pressure (Lorber & Carta 1988).
At the lower Mach number in Figure 7a, M∞ = 0.2, the phase ψ(t) at the start of pitch-up
(αmin) indicates that the aerodynamic damping is near zero or neutrally stable. As the airfoil pitches
up, ψ(t) moves into the second quadrant, indicating negative aerodynamic damping. During this
portion of the pitch-up cycle, the pitch moment amplitude, ACm (t), increases such that the polar
representation of the damping sweeps out a large area in the second quadrant corresponding to
negative damping.
The phase lead indicated by ψ(t) during pitch-up is a consequence of the induced stall delay
produced by a positive pitch rate (α˙ > 0) and an accompanying alleviation of the suction-surface
adverse pressure gradient. This allows the aerodynamic pitch moment to remain nearly constant as
the airfoil pitches up. The result is a steady increase in the negative damping throughout pitch-up,
as documented in Figure 7b.
The rate of negative damping is observed to accelerate at ωt = 3π/4. This portion of pitch-up
coincides with the initial formation of the dynamic stall vortex near the leading edge at x = 0.006.
The damping continues to become more negative up to the point at which the dynamic stall vortex
arrives at the quarter-chord (pitching) location (x = 0.25). This corresponds to ωt = 0.8π, where
(2	(t))/(πk) = −13.89.
As the dynamic stall vortex convects downstream past the quarter-chord location toward the
airfoil trailing edge, the aerodynamic damping begins to increase. The low (suction) pressure
that follows the position of the vortex loads the trailing edge and thereby produces a negative
(nose-down) pitch moment. For the M∞ = 0.2 case, the negative pitch-down moment induced
by the vortex persists into the pitch-down portion (α˙ < 0) of the cycle so that the aerodynamic
damping continues to be negative. Positive damping does not begin until ωt = 1.04π, where the
upper-surface flow field becomes completely separated.
To summarize these observations, we note the following: Large negative aerodynamic damping
that could spawn flutter occurs in a portion of the pitching cycle. Additionally, the source of
negative aerodynamic damping stems from the stall delay that results from the α˙ > 0 pitching
motion. Negative damping is also augmented by the formation of the dynamic stall vortex. Once
the dynamic stall vortex convects past the pitching location, its effect is stabilizing. After the
dynamic stall vortex convects past the trailing edge, the fully separated flow field initiates positive
aerodynamic damping. Finally, the positive damping reaches a maximum when the flow fully
reattaches.
Time-resolved damping analysis of light dynamic stall. (a,c,e) The polar representation of the transient aerodynamic damping. The
light blue arrows indicate increasing time. (b,d,f ) The transient damping, pitch moment, and position of the dynamic stall vortex.
(a,b) M∞ = 0.2, k = 0.051, and α = 9.8◦ − 4.9◦ cos ωt. (c,d ) M∞ = 0.3, k = 0.051, and α = 10.7◦ − 4.9◦ cos ωt. (e,f ) M∞ = 0.6, k =
0.046, and α = 5.4◦ − 5.1◦ cos ωt. Figure adapted with permission from Bowles et al. (2012).
As evident in Figure 6, the cycle-averaged damping coefficient for the pitching airfoil at
M∞ = 0.2 is negative. Therefore, it may not be surprising to find the large negative damping
peak in the time-resolved damping coefficient. However, in this case, the ratio 	min/	cycle is 7.73!
Thus, there exists a significant possibility for stall-flutter oscillations.
In contrast to the lower–Mach number case, at higher Mach numbers, the cycle-integrated
damping is always positive. Therefore, one would not expect these conditions to exhibit stall flutter.
However, the flight data shown in Figure 2b suggest otherwise. Figure 7c,d for M∞ = 0.3 and
Figure 7e,f for M∞ = 0.6 each reveal a negative peak in the intracycle damping that accompanies
the pitch-up motion of the airfoil. The intracycle damping for M∞ = 0.3 has all the same
characteristics as its M∞ = 0.2 counterpart. However, the magnitude of the negative damping
peak is not as large, which is attributed to a reduction in the time that the dynamic stall vortex
resides over the airfoil, as is evident by the x trajectory in Figure 7d.
At M∞ = 0.6, dynamic stall is triggered by shock waves on the suction surface of the airfoil.
The dynamic stall vortex in this case forms downstream of the shock front, at x = 0.134. This
is still upstream of the pitching location and therefore contributes a positive pitch moment that
results in a negative peak in the aerodynamic damping. The magnitude of the peak negative
damping coefficient is larger than that at M∞ = 0.3, which is a reversal of the previous trend with
increasing Mach number.
The x trajectory of the dynamic stall vortex in Figure 7f reveals a very gradual convection.
Chandrasekhara et al. (1994) found that the convection of the vortex in this instance is supplanted
by a near-constant-speed, low-pressure traveling gust that moves downstream. This gust fails to
induce the same positive aerodynamic damping that the dynamic stall vortex produced when it
traversed past the pitching location on the airfoil at the lower Mach numbers.
2.3. min versus cycle
For helicopter rotor operations, the magnitude of the minimum damping coefficient within the
pitch cycle, 	min, is potentially a more critical property than is the cycle-averaged damping coefficient,
	cycle. Local, negative intracycle aerodynamic damping, especially of the magnitude documented
by Bowles (2012) and Bowles et al. (2012, 2014), can spawn limit-cycle vibration growth
and possibly account for the stall-flutter-related divergence found during high-speed forward
flight or high g-force maneuvers (Bousman 1998). Bowles (2012) compared the cycle-averaged
and minimum intracycle aerodynamic damping coefficients for different dynamic stall regimes
over a range of Mach numbers from 0.2 to 0.6. For conditions in which the peak angle of attack
was less than the steady stall angle of attack, αmax < αss, the difference between 	min and 	cycle
was negligible across the whole Mach number regime. However, when αmax > αss, the minimum
intracycle damping was always significantly less than the cycle-averaged damping, and negative.
This was the case for the whole Mach number range.
2.4. Leading-Edge Turbulent Trip Effects
As discussed above, the pitch-up motion of the leading edge (α˙ > 0) results in an acceleration
of the boundary layer flow, which delays separation. In a similar manner, the flow acceleration
near the leading edge should make the boundary layer more stable to disturbances and therefore
suppress turbulent transition. McCroskey et al. (1981) noted that the nature of the boundary
layer separation that precedes vortex development strongly influences dynamic stall, and this is
particularly true in the light-stall regime. At least at Mach numbers below which dynamic stall is
influenced by shock waves, the transition state of the boundary layer (i.e., laminar versus turbulent)
should influence dynamic stall vortex inception and strength.
Effect of leading-edge trips on (a) cycle-averaged and (b) minimum intracycle damping coefficients for two Mach numbers. Roughness
consists of masking tape and 320- and 400-grit sandpaper strips. Data adapted from Bowles (2012).
With this in mind, there have been several studies on the effect of leading-edge turbulent
trips on dynamic stall (Bowles 2012, Chandrasekhara et al. 1996). Bowles (2012) examined the
effect of three turbulent trips consisting of strips of masking tape and strips of sandpaper of two
grit sizes. The turbulent trips were placed so that they overlaid the position of the leading-edge
flow separation bubble that formed just prior to stall on the static airfoil. This corresponded to
0.005 ≤ x ≤ 0.030. The surface visualization with the different trips showed that they eliminated
the separation bubble (Bowles 2012). Figure 8 summarizes the results, documenting the effect on
the cycle-averaged and intracycle damping coefficients.
Based on the conventional cycle-averaged damping coefficient, at the lower Mach number,
M∞ = 0.2, the leading-edge trips weakened the dynamic stall vortex (Bowles 2012), which resulted
in the cycle-integrated damping becoming positive. The distributed sand grain roughness was
found most effective. At M∞ = 0.6, dynamic stall vortex formation occurred downstream of the
shock front, which was at x = 0.134. The turbulent trips were therefore well upstream (x ≤ 0.03)
and, based on the cycle-averaged damping coefficient, had a minimal effect.
The minimum intracycle damping coefficients for the distributed roughness trips at M∞ = 0.2
support the cycle-averaged view, with a significant reduction in the negative damping peak that
is associated with the dynamic stall vortex formation. The edge roughness trip was found to be
much less effective. However, at M∞ = 0.6, at which the dynamic stall vortex forms downstream
of the shock, the distributed roughness trips result in significantly larger negative damping, which
is not indicated by the conventional cycle-averaged damping coefficient. For the two sandpaper
trips, for example, 	min ranges from −5 to −9, and 	cycle is nearly neutral. This reverse trend in
TRANSONIC GLOVE
Successful control of the detrimental effects associated with dynamic stall was achieved by Martin et al. (2008)
by use of a transonic leading-edge glove and passive vortex generator combination on a VR-7 airfoil. The vortex
generators were placed at x = 0.1. It is significant to note that neither the leading-edge glove nor the passive vortex
generators alone were successful at controlling dynamic stall. Their combination, however, reduced the local Mach
number, thereby enabling shock-free operation of the passive vortex generators. In the light-stall regime, this had
the effect of virtually eliminating the adverse pitching moment associated with dynamic stall for Mach numbers
up to 0.4. In the deep-stall regime, it significantly reduced adverse pitch moments at Mach numbers up to 0.375.
These results motivate the development of flush, surface-mounted active flow control devices that can mimic the
effect of passive vortex generators without incurring the adverse effects of shock formation. One recent approach
involves the plasma vortex generator demonstrated by Schatzman & Thomas (2010).
aerodynamic damping with turbulent trips at higher Mach numbers associated with the advancing
rotor merits watching as a possible trigger for stall flutter. In contrast, the edge roughness trip
had little effect on 	min at M∞ = 0.6.
3. DYNAMIC STALL CONTROL
The sensitivity of dynamic stall vortex formation and development to leading-edge turbulent trips
suggests that passive and active flow control could be an effective approach toward mitigating the
detrimental effect of dynamic stall. There have been several investigations of dynamic stall control
by both active and passive means, as described in the survey by Lorber et al. (2000). Candidate
control schemes include variable rotor geometry, leading-edge blowing (Greenblatt&Wygnanski
2001, Sun & Sheikh 1999, Weaver et al. 2004), leading-edge plasma actuation (Corke et al. 2011;
Lombardi 2011; Lombardi et al. 2013; Post & Corke 2006), vortex generators (Heine et al. 2011,
Martin et al. 2008, Traub et al. 2004), synthetic jets (Ekaterinaris 2002, Florea & Wake 2003,
Traub et al. 2004), and fixed-wing devices such as slots (Carr et al. 2001), leading-edge droop
(Chandrasekhara et al. 2004, Joo et al. 2006, Martin et al. 2008), pressure-side tabs (i.e., Gurney
flaps) (Chandrasekhara et al. 2004, Joo et al. 2006), and trailing-edge flaps (Feszty et al. 2004;
Gerontakos & Lee 2006, 2007, 2008).
One particularly notable experiment by Martin et al. (2008) examined passive streamwise vortex
generators for dynamic stall control at helicopter-relevant conditions. Figure 9 shows a photograph
of the transonic leading-edge glove and streamwise vortex generator (see the sidebar Transonic
Glove). Vane-type vortex generators were placed at x = 0.1 on a VR-7 airfoil that was also
fitted with a transonic leading-edge glove. These experiments were performed at Rec 	 2 × 106
and M∞ = 0.3−0.4. The experiments showed that vortex generators on the baseline airfoil were
not effective in dynamic stall control. Instead, they caused shock-induced dynamic stall in the
locally supersonic flow. Similarly, the transonic glove alone was also ineffective for dynamic stall
control. However, the combination of the transonic glove, which lowered the local Mach number,
and the vortex generators was found to be extremely effective at suppressing dynamic stall.
Measured pitch moment cycles exhibited a significant reduction or elimination of moment stall
with the transonic glove and streamwise vortex generator combination. However, the effectiveness
of the transonic glove and passive vortex generator combination still diminished at Mach
numbers above M∞ = 0.3−0.4, most likely as a result of shocks created by the passive vortex
generator.
Figure 9
Photograph of transonic leading-edge glove with streamwise vortex generator used by Martin et al. (2008)
for dynamic stall control. Figure courtesy of P.B. Martin.
At helicopter-relevant conditions, it would appear that any flow control approach that geometrically
extends above the airfoil surface will be a detriment, at least on the advancing blade,
where it can generate shock waves in the locally supersonic flow. To avoid this problem, Post
& Corke (2006) and Post (2004) investigated the use of surface-mounted plasma actuators for
dynamic stall control on pitching airfoils (see the sidebar Plasma Actuators). A general overview
of plasma actuators is given by Corke et al. (2010). The plasma actuator was located at the very
leading edge, where it was found to be most effective in controlling leading-edge flow separation
and extending stall on stationary airfoils. The actuator spanned the leading edge of the airfoil. It
was oriented so that when activated, it would produce a two-dimensional body force across the
span that would accelerate the boundary layer flow in the direction from the leading edge toward
the suction surface of the airfoil. In this manner, it was designed to augment the separation delay
PLASMA ACTUATORS
The term plasma actuator has now been a part of the fluid dynamics flow control vernacular for more than a
decade. A particular type of plasma actuator that has gained wide use is based on a single dielectric barrier discharge
mechanism that has desirable features for use in air over a range of static pressures. These plasma actuators most
generally consist of two electrodes, one uncoated and exposed to the air and the other encapsulated by a dielectric
material. The electrodes are typically arranged asymmetrically. The electrodes are supplied with an ac voltage that,
at high-enough levels, causes the air over the covered electrode to weakly ionize. The ionized air appears blue. For
these flow actuators, the mechanism of flow control is through a generated body force vector field that couples with
the momentum in the external flow. The body force can be derived from first principles, and the plasma actuator
effect can be easily incorporated into flow solvers so that their placement and operation can be optimized. They
have been used in a wide range of internal and external flow applications. Although initially considered to be useful
only at low speeds, they have been shown to be effective in a number of applications at high subsonic, transonic,
and supersonic Mach numbers. This has largely come from more optimized actuator designs that were developed
through better understanding and modeling of the actuator physics. For further information, readers are referred
to Corke et al. (2010).
mechanism produced by the pitching motion with α˙ > 0. Post & Corke investigated both steady
and unsteady open-loop forcing. In both cases, the actuator operated continuously throughout
the pitching cycle. In the unsteady operation, the plasma actuator was cycled on and off at a frequency,
f, that was found to be effective in controlling the separated shear layer. This utilized a
dimensionless frequency, F+ = f c /U∞ = 1, that was found to be most effective in maintaining
attached leading-edge flow on stationary airfoils (Post & Corke 2006). Assuming a convection
speed of vortices generated by the actuator pulsing of 0.5 U∞, F+ = 1, results in two vortices
spaced over the chord of the airfoil. This was verified by flow visualization (Post 2004, Post &
Corke 2006). For the pitching airfoil, the unsteady plasma actuator frequency was 20 times higher
than the pitching frequency. For conditions of deep dynamic stall, the steady actuation produced
a 4.7% improvement in the cycle-integrated lift. The unsteady actuation at F+ = 1 was slightly
better, with a 5.4% improvement in the cycle-integrated lift. Although the improvement in the
lift was small between the unsteady and steady actuation, the improvement in the pitch stability
was substantially better with the unsteady actuation.
Corke et al. (2011) proposed a method to use an unsteady plasma actuator, along with a highfrequency
response pressure sensor placed near the leading edge, to detect the onset of dynamic
stall, and thereby disrupt the development of the dynamic stall vortex. The method relied upon
the findings of Haddad et al. (2005), who showed that the receptivity of the boundary layer over
a parabolic leading edge to an unsteady disturbance grows on the order of 100 times prior to the
formation of a separation bubble. At pitch angles below the static stall angle, αss, the attached
boundary layer is not receptive to unsteady disturbances produced by a plasma actuator located
at the leading edge. As the airfoil pitch angle approaches αss, the separation bubble moves toward
the leading edge. The separation bubble is receptive to the low-amplitude unsteady disturbances
produced by the plasma actuator. These are transmitted to the pressure sensor when the separation
bubble extends from the leading-edge site of the disturbances to the pressure sensor location.
This typically occurs at an angle of attack just below αss. At this angle of attack, the unsteady
disturbances produced by the plasma actuator are easily detectable by the pressure sensor. The
unsteady disturbances remain detectable as long as the flow remains separated. However, they are
no longer detectable if the flow would naturally remain attached (Corke et al. 2011).
Based on this, Lombardi et al. (2013) devised a close-loop dynamic stall control approach that
introduced a low-amplitude periodic disturbance at F+ = 1 using a plasma actuator at the leading
edge that was operated in a low-power sense state. With this approach, the authors sought to detect
the periodic disturbance with a flush-mounted pressure sensor located on the airfoil that would
indicate the onset of leading-edge flow separation. If the disturbance was sensed, the plasma actuator
was changed to a higher-powered control state that would reattach the flow. Figure 10 shows
an example of the outcome of the closed-loop control for conditions that correspond to light dynamic
stall. Figure 10a illustrates the flow visualization and corresponding suction-side pressure
distribution at the maximum angle of attack during pitch-up, and at αmax +4◦ during pitch-down,
without and with closed-loop flow control. With the closed-loop control, the flow remains attached,
with a commensurate increase in the suction pressure peak at the leading edge. Figure 10b
shows the total effect of the closed-loop control on the lift and pitch moment coefficient cycles.
The closed-loop control logic for the determination of the onset of flow separation utilized a
threshold on the level of the periodic disturbance sensed by the pressure sensor. For the left pair
of plots, the threshold was such that the control was operating approximately 47% of the pitching
cycle, starting during pitch-up at 4.5◦ before αmax and ending during pitch-down at α = 8.0◦. This
is the same threshold condition for the flow visualization and pressure distributions in Figure 10a.
As evident by the lift and moment cycles, this resulted in a substantial reduction in the lift and
moment hysteresis. For the pair of plots on the right, the control operated over only 11.2% of
Flow visualization and corresponding suction-side pressure distribution for a pitching airfoil without and with closed-loop control cases
for light dynamic stall with α0 = 10◦, and α1 = 8◦ (a), and lift and moment coefficient cycles for two stall-detection thresholds (b). In
panel b, the dashed curves correspond to the cycle without flow control, and the solid curve to that with closed-loop control. With the
closed-loop control, the portion of the cycle over which the actuator was operating to control the flow separation is indicated by the red
circles that overlay the solid curve lift and moment cycle curves. Figure adapted with permission from Lombardi et al. (2013).
the pitching cycle starting 2.2◦ before αmax and ending just 0.1◦ past αmax during pitch-down. This
control was essentially only operating during the formation of the dynamic stall vortex. With even
that short duration, lift and moment cycles reveal the same fundamental improvement in the stall
hysteresis that occurred with the significantly longer actuator operation. Quantitatively, the cycleaveraged
lift increased by 12%, and 	cycle went from slightly negative (−0.007) to positive (0.007)
as a result of the closed-loop control. From an active flow control input energy perspective, this
targeted control provided an 88% improvement over the open-loop control by which the actuator
operated throughout the pitching cycle.
4. SUMMARY
This review highlights the rich flow physics that underlies dynamic stall on a pitching airfoil.
Although it is relevant to many manmade and natural flyers, we emphasize helicopter rotor aerodynamics
because dynamic stall ultimately limits their maneuverability and agility, as well as speed
and payload.
We place particular focus above on aerodynamic damping because of the tendency for dynamic
stall to lead to flutter, which is an aeroelastic coupling that can result in limit-cycle growth of vibration
amplitudes and eventual structural failure. The recently introduced intracycle aerodynamic
damping formulation of Bowles (2012) has provided new insight into this process that was previously
masked by the traditional cycle-averaged view. Particularly significant is the observation
that the intracycle damping can exhibit relatively large negative peaks over a wide range of Mach
numbers, even in cases with positive cycle-averaged damping. In this manner, successive rotations
of the rotor through negatively damped retreating blade regions can lead to additive limit-cycle
growth, even though the cycle-averaged damping remains positive.
The phase relation between the airfoil pitching motion and the aerodynamic pitch moment
determines the sign of the aerodynamic damping. The intracycle damping analysis makes it clear
that the initial formation and convection of the organized dynamic stall vortex prior to reaching the
maximum angle of attack are the sources of strong negative aerodynamic damping. This contrasts
with earlier interpretations (Liiva 1969) that attributed negative aerodynamic damping to the
sudden flow separation at stall and the combination of the nose-down pitch moment and α˙ < 0
at αmax. This has important implications for damping control schemes, which should properly be
focused on weakening the organized vorticity.
The dynamic stall process is highly sensitive to surface roughness that can influence turbulent
transition in the separating boundary layer and to local compressibility effects that occur at freestream
Mach numbers that are otherwise incompressible for the stationary airfoil. In lieu of shock
formation at higher Mach numbers, dynamic stall originates with a sudden eruption of vorticity
from the boundary layer near the leading edge, which leads to a strong viscous/inviscid interaction
with the outer flow. This gives rise to a well-defined dynamic stall vortex, which then convects aft
over the airfoil. It is the formation and subsequent evolution of the dynamic stall vortex that causes
massive excursions of the aerodynamic loads and pitch moment from linear thin airfoil theory.
Compressibility has been shown to alter the onset mechanism for dynamic stall, promoting
leading-edge stall through separation bubble bursting. In addition, compressibility effects generally
weaken the dynamic stall vortex strength. However, if the Mach number is high enough to
form shocks downstream of the leading edge, the dynamic stall vortex forms downstream of the
shock front, and its topology differs from that at lower Mach numbers. With such shock-induced
dynamic stall, any flow separation at the leading edge has little effect on the aerodynamic loads
and moments.
Numerous studies cited in this review have provided descriptions of the processes that characterize
both light and deep dynamic stall regimes, as well as the influence of parameters such as
airfoil geometry, reduced frequency, stall penetration angle, and Reynolds and Mach numbers.
However, a unified understanding of the physics surrounding the pitching airfoil flow field still remains
elusive. In fact, many aspects of dynamic stall inception in even incompressible flow remain
incomplete. Although compressibility effects have been well characterized, the physical mechanisms
associated with them are, in many cases, not fully understood. There remains a particular
need for better understanding regarding the physics of shock-induced dynamic stall that can occur
at higher subsonic Mach numbers.
At Mach numbers below which shocks occur, turbulent trips in the form of distributed roughness
near the leading edge are able to weaken the dynamic stall vortex and, as a result, positively
increase the aerodynamic damping. However, at higher Mach numbers associated with shockinduced
dynamic stall, the turbulent trips produced large negative intracycle damping. Because
helicopter rotor leading-edge surfaces are generally rough, this could be the source of unexpected
aeroelastic flutter on the advancing rotor. Ultimately, the sensitivity of dynamic stall to leadingedge
roughness bodes well for the possibility of dynamic stall control with minimal power input.
Above we cite several studies that have investigated dynamic stall control by both active
and passive means. Regardless of the approach, the focus is to (a) increase cycle-integrated lift,
(b) reduce impulsive blade pitching moments, and (c) assure a positive cycle-averaged aerodynamic
damping and minimize the intracycle negative damping that can occur during portions of the
pitching cycle. If the dynamic stall boundary can be extended via a suitable control scheme,
a revolutionary stall-free rotor could become a reality. However, despite the demonstrated
successes reported in marginalizing some detrimental effects of dynamic stall, few of the flow
control approaches have been demonstrated at the combination of Rec and M∞ that is applicable
to the full rotor environment. It is recommended that this be a focus of future dynamic stall flow
control studies. Of particular interest are smart active control schemes that can sense dynamic
stall onset and then exploit a shear layer receptivity mechanism in such a manner as to mitigate
the detrimental effect of dynamic stall with minimal power input. Particularly attractive are flush
surface actuator designs that can operate at high subsonic Mach numbers without giving rise to
the detrimental effects associated with shock formation.
DISCLOSURE STATEMENT
The authors are not aware of any biases that might be perceived as affecting the objectivity of this
review.
ACKNOWLEDGMENTS
The authors would like to acknowledge Dr. Patrick Bowles, Mr. Anthony Lombardi, and Mr.
Dustin Coleman at the University of Notre Dame; Dr.Mark Wasikowsi andMr. Thomas Wood
at Bell Helicopter; and Dr. Preston Martin at NASA Langley Research Center for insightful
discussions related to this article.






























Leading-Edge Vortices:
Mechanics and Modeling
Keywords
leading-edge vortex, translating, surging, rotating, pitching, LEV, vortex
model, data-driven, unsteady aerodynamics
Abstract
The leading-edge vortex (LEV) is known to produce transient high lift in
a wide variety of circumstances. The underlying physics of LEV formation,
growth, and shedding are explored for a set of canonical wing motions including
wing translation, rotation, and pitching. A review of the literature
reveals that, while there are many similarities in the LEV physics of these
motions, the resulting force histories can be dramatically different. In twodimensional
motions (translation and pitch), the LEV sheds soon after its
formation; lift drops as the LEV moves away from the wing. Wing rotation,
in contrast, incites a spanwise flow that, through Coriolis tilting, balances
the streamwise vorticity fluxes to produce an LEV that remains attached to
much of the wing and thus sustains high lift. The state of the art of vortexbased
modeling to capture both the flow field and corresponding forces of
these motions is reviewed, including closure conditions at the leading edge
and approaches for data-driven strategies.
1. INTRODUCTION
Understanding and achieving high lift is a recurring theme in aerodynamics. Figure 1 gives a
smattering of airfoil data for the maximum lift coefficient achieved over a wide range of Reynolds
numbers. At the relatively high Reynolds numbers typical of manned flight, high maximum lift
coefficients are easily attained as flow remains attached through reasonably severe pressure gradients.
The maximum lift coefficient falls off quickly in transitional flows but can be recovered by
installing turbulators to ensure a turbulent boundary layer and encourage attached flow. At lower
Reynolds numbers, however, boundary layers remain laminar and are more prone to separate in
adverse pressure gradients; airfoils stall at low angles of attack and low lift values. It is in this
regime that insects fly, inviting the question of how flight is possible if, in steady flow, their wings
produce only a quarter of the lift required. In a quest to explain the fundamentals of natural flight,
biologists performed numerous studies of live insects, mounted dried wings in wind tunnels, and
built mechanical models of the insect wing stroke (Dickinson et al. 1999, Ellington et al. 1996,
Usherwood & Ellington 2002, Willmott & Ellington 1997, Willmott et al. 1997). In doing this,
they discovered a flow structure that arose from unsteady and revolving motions of the wing but
failed to appear in conventional steady flow over the same wing—the leading-edge vortex, or, its
briefer sobriquet, the LEV.
Interest in the LEV grew quickly, due in part to efforts to develop agile small-scale vehicles
and biomimetic fliers and swimmers. The engineering approach to the high lift problem has been
Maximum lift coefficient, CLmax , as a function of Reynolds number in steady and unsteady flows. Figure
adapted with permission from Jones & Babinsky (2010).
to distill the complex insect wing stroke into canonical motions in which the LEV is a prominent
feature of the resulting flow. The current review focuses primarily on these canonical flows, largely
because they are simple enough to allow us to establish the underlying physics of the LEV. As we
discuss in detail below, the net effect of the LEV is to induce a transient fluid dynamic load on
the wing. This has benefits (e.g., for dynamic soaring, energy harvesting, or initiating aggressive
vehicle maneuvers), but can also lead to undesirable fatigue loading and vibrations, and in some
cases, can induce unwanted vehicle motions (e.g., in a gust encounter or air wake). The timescale
over which the LEV forms and sheds is fast—only a few convective times. This is within an insect
wing stroke, thus making natural flight possible at these scales. It is, however, also very fast with
respect to our ability to effect control, whether that be locally using flow control or more globally
at the vehicle level. The future development of more maneuverable and gust-tolerant vehicles thus
depends on our ability to better understand and control leading-edge separation and the resulting
vortex-dominated flow.
The significant enhancement of lift attributable to the LEV has motivated several questions
that have guided most investigations of this topic: What parameters control the growth of the LEV
and its circulation? What mechanisms govern its eventual shedding, and importantly, under what
circumstances might it remain attached to the wing? Can the behavior of the LEV—its evolution
and the force it exerts on the wing—be predicted with a simple mathematical model? Is there
hope for the control of LEVs, either to mitigate transients in gust encounters and other unsteady
flows or to enhance high lift on maneuvering wings? The current state of the answers to these
questions is reviewed here. In the next section, some fundamentals of LEV growth and shedding
are presented. Following that, a set of canonical problems and their governing parameters are
discussed, including a review of LEV formation during these motions. We then summarize the
state of mathematical modeling of these flows and the resulting unsteady loading on the wing.
The review concludes with a brief discussion of avenues for future work.
2. FUNDAMENTAL FEATURES OF THE LEADING-EDGE VORTEX
Many of the general features of incipient leading-edge flow separation—the formation and evolution
of the LEV and its resulting influence on force—can be observed in the fluid’s response
to the simplest of unsteady motions on the most basic of aerodynamic surfaces: the impulsive
rectilinear translation of a two-dimensional (2D) flat plate at large angle of attack. An example
of this flow is illustrated in Figure 2, which depicts the streamlines and vorticity field
of the flow developing over a flat plate of chord c and thickness 0.023c traveling at 35◦ incidence
and Reynolds number Uc/ν = 500, where U is the plate’s speed and ν the kinematic viscosity of
the fluid. Time is measured in convective units, t∗ = tU/c, with the motion initiated at t∗ = 0.
The results in Figure 2 were obtained using high-fidelity numerical simulation with the lattice
Green’s function/immersed boundary projection method (Liska&Colonius 2017). From the start
of the motion, the thin wing profile ensures a separation point that remains nearly pinned to the
leading edge. Vorticity generated at the edge forms a shear layer that quickly rolls up into the
LEV, clearly visible at t∗ = 0.5 (see the sidebar titled Vortex Identification). At early times, the
stagnation streamline forms a closed recirculation region on the upper side of the plate that occupies
only the forwardmost fraction of the chord. As the LEV grows, it induces a region of reversed
flow and secondary vorticity along the surface of the wing under the LEV, evident as early as
t∗ = 0.5. Together with the base of the feeding shear layer, this develops into a characteristic
lambda-shaped region of counter-rotating vorticity just upstream of the LEV. As plate motion
continues, the LEV is fed by the shear layer and grows in both size and strength. The stagnation
point behind the vortex moves downstream, reaching and eventually passing the trailing edge.
Vorticity field (color contours) and streamlines (solid gray lines) from a high-fidelity numerical simulation of flow about a two-dimensional
plate undergoing impulsive translation at angle of incidence α = 35◦ and Reynolds number Re = 500. The stagnation streamline is the
thick black line. Vorticity contours range from −20U/c to 20U/c, where U is the plate speed and c is the chord length. Time is
measured in convective units, t∗ = tU/c.
After three chord-lengths of travel, the LEV shown in Figure 2 breaks away from the plate and
convects into the wake. The progression of vortex development shown here is similar for other
wing sections at other Reynolds numbers, motion profiles, and angles of attack, so long as there
is large-scale flow separation at the leading edge.
The development and motion of the LEV has a large impact on the fluid dynamic force exerted
on the wing. The histories of the lift coefficient on the plate at various angles of incidence are
shown in Figure 3. Each case follows a similar trend: After the infinite force at t∗ = 0 due to
the inertial reaction of the fluid from the impulsive start, the lift resets to a finite value due to
the release of the starting vortex at the trailing edge, consistent with the Wagner effect (Wagner
1925). The lift increases from this initial value, quickly at first and then more gradually, to a peak
near t∗ = 2 that reaches as high as CL = 2.5, after which it drops rapidly. The rise in lift is stronger
and faster, and the subsequent drop is earlier, as the angle of incidence increases toward 45◦. All
of these behaviors are attributable to the dynamics of the LEV, illustrated in Figure 3b. Here,
the pressure coefficient along the suction side of the wing is shown for the same case of α = 35◦
presented in Figure 2. Throughout the evolution of the flow, it is clear that there is a region of
low pressure directly under the LEV. This region grows in both magnitude and size as the LEV
develops (for t∗
< 2.2) and then begins to attenuate as the vortex breaks off of the wing and moves
VORTEX IDENTIFICATION
In experiments and high-fidelity simulations, the boundaries of the LEV may be defined by various techniques,
including the 1 and 2 functions (Graftieaux et al. 2001), the Q criterion ( Jeong & Hussain 1995), and Lagrangian
coherent structure analysis (Haller 2001).
Figure 3
(a) Lift coefficient, CL, generated by a two-dimensional flat plate undergoing an impulsive translation at various angles of incidence α at
Reynolds number 500. Time is measured in convective units, t∗ = tU/c, where U is the plate speed and c is the chord length. The
purple circles correspond to the snapshots at 35◦ in panel b, showing the surface pressure coefficient, Cp (red line), and vorticity field
(color contours).
away, leading to the drop in lift observed at t∗ ≈ 2.5. In the physical world, where the flow is
not constrained to remain 2D, 3D instabilities generally cause the shed vortex structures to lose
their coherence; the flow proceeds toward a fully separated state with a concomitant loss of lift
and increase of drag. In summary, the LEV enhances the lift on the wing to values significantly
above what can be achieved in a fully developed separated flow at long times, but this enhancement
survives only as long as the LEV remains near the wing.
2.1. Control Volume Analysis
The discussion of LEV growth and evolution in this review is facilitated by definition of a control
volume suitable for accounting for the various fluxes into the vortex. The control volume depicted
in Figure 4 is constructed to serve multiple forms of wing geometries and kinematics in either
two or three dimensions; its contour C encloses the LEV and much of its feeding shear layer
at some instant. However, rather than evolving dynamically with the LEV, the control volume
will remain stationary relative to the wing and allow flux through any side. For analysis of the
spanwise variation of these fluxes, the control volume has infinitesimal width dy in that direction.
Conservation of mass in this volume is expressed, per unit spanwise length, as
−

C
n · v dC = ∂
∂y

S(y)
vy dS. 1.
In words, any net inflow of mass through the side (C) of the control volume must be balanced by
an increase in spanwise flow. In a strictly 2D flow, which has no such spanwise outlet, the influxes
must be balanced by an equal outflow. This outflow is initially equal to the rate of spatial growth
of the LEV as it exceeds the fixed boundary C. Later, when the vortex is no longer fed by the shear
layer, the outflow is attributable to the convection of the shed LEV.
The LEV circulation—positive in the configuration depicted in Figure 4—is defined by the
total spanwise vorticity enclosed by C:
Figure 4
Illustration of control volume used for analysis of the leading-edge vortex (LEV). (a) Side (or two-dimensional) view, with convective
(blue) and diffusive (magenta) fluxes into and out of the control volume. (b) Three-dimensional view on a finite–aspect ratio wing. The
unit-outward normal vector is n, and S(y) is the cross section of the control volume at spanwise location y, bounded by contour C.
(c) Schematic of the Coriolis tilting mechanism on a rotating wing. A rear view of the wing is shown, with the control volume shown as
the red box. Spanwise and vertical vorticity components are denoted by ωy and ωˆ z, respectively, and vy is the spanwise velocity.
In a 2D flow, this circulation remains invariant along the span by definition, but in three dimensions,
it is important to remember that vorticity is divergence-free, with the consequence
that
−

C
n ·ωdC = ∂(y, t)
∂y . 3.
That is, any spanwise variation of circulation necessitates the emergence of nonspanwise components
of vorticity to counter this variation, a familiar result from classical lifting-line theory with
important implications for the LEV.
To develop an evolution equation for LEV circulation, we find it useful to write the vorticity
transport equation in the reference frame of the wing—which, to serve the discussion later in this
review, may be rotating with angular velocity —in a form that will emphasize fluxes:
∂ωˆ
∂t
+ ∇ · (vωˆ ) = ∇ · (ωˆ v) + ν∇2ωˆ , 4.
where ωˆ = ω + 2 is the vorticity in an inertial reference frame and ω and v are the vorticity
and velocity observed in the frame corotating with the wing, respectively. The Coriolis term is
contained within the first term on the right-hand side, which also accounts for stretching and
tilting of the vorticity ω. Integrating the spanwise component of this equation over the control
volume and then applying the divergence theorem, we obtain an equation for (y, t):
∂(y, t)
∂t
= −

C
n · vωy dC +

C
ν
∂ωy
∂n
dC +

C
n ·ωˆ vy dC. 5.
This equation is similar to one used by Wojcik & Buchholz (2014b), but by using the fact that
vorticity is divergence-free, the form used here clarifies the effect of stretching and tilting on the
LEV circulation. In particular, the terms associated with net spanwise convective flux through
S(y) and the stretching of the spanwise vorticity in this direction exactly cancel each other and are
thus absent from this equation. Additionally, the variation in viscous flux in the spanwise direction
is likely negligible and has thus been omitted. The first two terms on the right-hand side describe
the convective and diffusive flux through the boundary C of the control volume, respectively, and
the third term describes the tilting of nonspanwise components of vorticity toward the direction
of the spanwise flow on C. In a strictly 2D flow, only the first two terms are nonzero. Furthermore,
the diffusive flux is only prominent along the part of C adjacent to the wing surface, denoted with
magenta arrows in Figure 4a, where it is likely small compared to the vorticity carried in via
the shear layer. Equation 5, together with the constraints given in Equations 1 and 3, provide a
framework in which to analyze LEV growth and stability; in particular, stability requires that the
right-hand side of Equation 5 is approximately zero.
2.2. Growth and Shedding in Two Dimensions
The early development of the LEV is characterized by monotonic growth in both its size, characterized
by a length, D(t), and its strength, quantified by its circulation, (t). As the control volume
analysis of the previous section shows, the rate at which the LEV grows spatially in a strictly 2D
setting is determined by the influx of mass from the shear layer at the leading edge and from the
flow induced between the vortex and the wing, as illustrated in Figure 4a. The first of these is
a function of the shear layer’s thickness and is thus entirely attributable to viscosity. The second
form of influx, in contrast, represents an entrainment of mass into the vortex core and is present
even if the vortex develops in a purely inviscid context, where the shear layer reduces to a spiraling
vortex sheet (Kaden 1931, Pullin & Wang 2004). Although this entrainment in a viscous flow
is somewhat mitigated by the induced boundary layer, it still represents the dominant path for
mass influx (Huang & Green 2015). As indicated by Equation 5, the growth of LEV circulation
is primarily determined by the convective fluxes of vorticity into the LEV. The vorticity in the
shear layer is the primary contributor to this growth, whereas the ingestion of secondary vorticity
in the boundary layer under the vortex—sometimes called “annihilation” (Wojcik & Buchholz
2014b)—tends to slow the growth.
The overall development of the LEV is a complex interplay of viscous and inviscid effects.
At early times, the vortex development is independent of the chord length and, with no imposed
length scale, is therefore necessarily self-similar. Xu & Nitsche (2014) have found that, for an
infinitely thin flat plate moving at normal incidence across a wide range of Reynolds numbers
(larger than 250), the circulation and centroid position of the vortex emerging from either edge
are well predicted at early times by the inviscid solution—the self-similar roll-up of a vortex sheet
from a semi-infinite plate (Pullin 1978). This inviscid solution is an extension of the classical result
by Kaden (1931) and leads to the conclusions that /(Uc) ∼ t∗1/3 and D/c ∼ t∗2/3. To adapt these
results to the LEV, the proportionality constant in these relations must depend on the angle of
incidence (Pullin & Wang 2004).
As the observations suggest, the dependence of LEV development on viscosity is second order
and difficult to succinctly describe in scaling arguments. Furthermore, in the latter stages of its
evolution, various influences appear to be in conflict with each other: At lower Reynolds number,
the shear layer will be thicker, increasing the flux of mass that this shear layer carries into the
LEV, but simultaneously, the induced flow ingested into the vortex will be relatively weaker.
Finally, with viscous influence comes a dependence on the geometry of the leading edge, since the
shape of this edge—most simply measured by a characteristic dimension, LLE—will determine the
thickness of the feeding shear layer. Since LEV development remains essentially independent of
chord length until late in the process, the viscous influence on vortex properties must be through
a leading-edge Reynolds number, ReLE = ULLE/ν. Through differences in this parameter, two
With a finite length of chord, the LEV cannot continue to grow indefinitely. Inevitably, the
supply of circulation from the shear layer is severed, the vortex becomes a topologically distinct
structure, and it is shed into the wake of the wing. This chord-limiting mechanism for shedding,
sometimes called “bluff-body shedding” (Rival et al. 2014, Widmann & Tropea 2015) is intuitive
and is clearly triggered when the extent of the vortex approaches that of the chord, D/c ≈ 1. It
is identifiable by the bursting of the closed recirculation region, as seen in the results depicted
in Figure 2. As the example in Figure 3 shows, this process is reflected in a dramatic change
in lift. Widmann & Tropea (2015) have postulated a second form of vortex shedding called
“boundary layer eruption”: The ingestion of secondary vorticity accumulates into the lambdashaped
structure and eventually chokes the feed of vorticity from the shear layer. The appearance
of this mechanism, which is independent of chord length, would necessarily depend on ReLE.
A typical vortex shedding event is likely to be a mixture of these mechanisms. Huang & Green
(2015) have illuminated the kinematics of the vortex shedding process on a pitching flat-plate wing
with Lagrangian coherent structure (LCS) analysis. They showed that the LCS saddle point at the
top of the lambda-shaped structure remains nearly stationary as long as the LEV continues to be
fed by the shear layer. This saddle, however, splits into two at the first break in the feeding shear
layer, and the subsequent motion of the first saddle point can be used to unambiguously identify
the instant at which the LEV sheds from the wing. More subtle changes in the vortex dynamics
(e.g., intermittent vorticity transfer from the shear layer) are indicated by the emergence and
rapid motion of additional saddle points. This process appears to be consistent with the boundary
layer eruption mechanism described by Widmann & Tropea (2015), but shedding occurs nearly
simultaneously with the reattachment point reaching the trailing edge.
3. KINEMATIC FAMILIES AND PARAMETERS
While biological flight has motivated some of the recent study of LEVs, the flows generated by
flapping wings cannot be easily disentangled to isolate the underlying LEV physics. As a result,
many investigators interested in these physics have decomposed flapping, as well as other complex
kinematics in which the LEV is a culprit, into a set of more basic wing motions illustrated in
Figure 5: translation, rotation, and pitch. It is important to stress that the use of the term “decompose”
does not suggest an underlying superposition principle; it is abundantly clear that no
such principle can exist in these large-amplitude motions with highly nonlinear fluid dynamics.
Rather, this decomposition provides a foundation on which further studies of combinations of
these canonical motions can build. In similar fashion, the geometry of the wing, though assuming
widely disparate forms across nature and technology, has generally been studied in a simple
symmetric form: a rigid, rectangular planform, with the profile of a flat plate or a thin NACA
(National Advisory Committee for Aeronautics) section. Of course, even basic motions can take
many forms if left to the whims of individual research groups. The canonical motions and geometries
established in recent years by two consortia of researchers—NATO AVT-202 (North
Atlantic Treaty Organization Applied Vehicle Technology) and the AIAA (American Institute
of Aeronautics and Astronautics) Massively Separated Flows Discussion Group (Ol & Babinsky
2016, Ol et al. 2010)—have aligned many of the LEV-focused investigations and are the target of
much of this review.
Wing translation, illustrated in subpanel iii of Figure 5a, is defined as motion in which the
wing is set at a fixed incidence and either driven in purely rectilinear motion at velocity U(t) or
subjected to a freestream flow with this velocity. (Note that the forces generated by these two
Figure 5
Overview of the canonical wing kinematics. (a) Schematics of the wing motions: (i) pitch and translation, (ii) pitch and rotation,
(iii) translation, and (iv) rotation. The vertical axis represents reduced pitch rate, K, and the horizontal axis is the inverse of the Rossby
number, Ro. (b) Position, velocity, and acceleration profiles for the translating and rotating wing. (c) Angle of incidence, pitch rate, and
angular acceleration for the pitching wing. U is the translational speed,  is the angular velocity of rotation, and α˙ 0 is the rate of
pitching.
Aspect ratio: the ratio
of wing span to chord
length, AR = b/c
different configurations will differ by an effective buoyant force, ρU˙ V, where V is the volume of
the wing.) The most basic form of U(t) is an impulsive start from rest to constant velocity, U.
A more practical form is obtained by a linear ramp to this constant velocity over some specified
distance, giving rise to the kinematics depicted in Figure 5b. Since this motion requires a sudden
change of acceleration at its start and finish, several recent studies have made use of a smoothed
(continuously differentiable) version of the ramp given by Eldredge et al. (2009). Each smooth
change in behavior from constant value to linear ramp is generated by the function
Ga (t) = 1
2a

log (2 cosh at) + at

, 6.
which approaches 0 as t → −∞and is approximately t as t → ∞. The parameter a controls
the degree of smoothing: Its inverse represents the blending time between behaviors. A smooth
increase in velocity from 0 to U between times t1 and t2, for example, is generated by
U
t2 − t1
[Ga (t − t1) − Ga (t − t2)] . 7.
In any case, U serves as the characteristic velocity, Uref , of the problem and, together with chord
length, c, leads to the dimensionless expression of time in convective units, t∗ = tU/c. Since this
definition equivalently expresses the distance traveled by the wing in units of chord lengths, it is
easily generalized for accelerating wings (Chen et al. 2010) to
t∗ = 1
c

t
0
U(τ) dτ. 8.
In pure translation of a wing of rectangular planform with span b, in a fluid of density ρ and
dynamic viscosity μ, the flow is characterized by only three dimensionless parameters: the angle
of incidence, α; the aspect ratio, AR = b/c; and the Reynolds number, Re = Uc/ν. It should
be noted that applications involving unsteady leading-edge separation are generally at low Mach
number, so the flow can be considered incompressible. The resulting forces on the wing are
nondimensionalized to form lift and drag coefficients, CL and CD, with the usual inertial scaling,
ρU2bc/2.
In wing rotation, illustrated in subpanel iv of Figure 5a, the wing adopts a fixed angle of
incidence, α, and rotates in a propeller-like motion. The axis of rotation passes through a point
that is either at or inboard of the wing root; the distance from the axis to the root is called the root
cutout, rc. Like translation, wing rotation may start impulsively, although it is usually initiated
with a linear ramp, often smoothed with Equation 6, toward a constant angular velocity, .
The rotating wing problem is nondimensionalized by some combination of the collection of
parameters , c, b, rc, α, and ν. However, in the interest of constructing a physically relevant
scale for convective and inertial effects, and perhaps facilitating a meaningful comparison with the
translating wing, a characteristic velocityUref must be formed from among these. Because the local
incident velocity varies linearly from the axis of rotation, every possibility for the characteristic
scale, Uref = R, requires a selection of some spanwise location R; the definitions for Reynolds
number, Re = Rc/ν, and force coefficients [e.g., CL = 2L/(ρ2R2bc )] follow. Within the
literature, various choices of R have been made, including midspan; three-quarter span; radius of
gyration, rg; and wing tip, rc+b, so that a comparison between any two rotating wing studies must
first reconcile their definitions of parameters.
In addition to Re and α, there are two ratios of length, b/c (i.e., the aspect ratio, AR) and rc/c,
that potentially influence the fluid dynamics of a rotating rectangular wing. These length ratios can
be varied independently, and it is useful to note that the translating wing represents the extreme
where rc/c → ∞. However, collectively, these two parameters have a physical role in assessing
the ratio of inertial force, ρU2
ref/c, to Coriolis effects, ρUref , measured by the Rossby number,
Ro = Uref/(c ). In fact, as the present review shows, the flow generated by a rotating wing is more
sensitive to this ratio than it is to the ratio of inertial to viscous effects (i.e., Reynolds number).
The definition of Ro requires the same arbitrary choice of reference location R, so that multiple
definitions of Rossby number have appeared in the literature, e.g., Ro = (rc + b)/c (Lentink &
Dickinson 2009) and Ro = rg/c (Wolfinger & Rockwell 2014). The first definition quantifies the
local inertia at the wing tip, while the second represents a spanwise-average influence. In cases
where rc = 0, the first definition is equivalent to the wing aspect ratio, AR, and the second differs
only by a constant factor. However, for wings with root cutout, the difference between these
definitions is more significant, and AR remains an independent parameter (Schlueter et al. 2014).
For either definition, a translating wing of finite aspect ratio represents the limit Ro→∞(with
the constraint that the product U = rc is kept finite in this limit). Thus, rotating wings with root
cutout fall on a spectrum of 1/Ro between translating and rotating, as depicted in Figure 5a.
Wing pitch, depicted in subpanel i of Figure 5a, is defined as a motion in which the wing
undergoes a steady increase in angle of incidence, α(t), from zero to some larger fixed value well
beyond its static stall angle. As in the other cases, most recent studies have used Equation 6 to
smooth the change in angle of incidence and avoid the infinite inertial forces due to impulsive
accelerations at the start and finish of this maneuver. For a wing of chord length c translating at
constant speedU and pitching at nominal rate α˙ 0, themotion is characterized by three parameters:
the Reynolds number, generally defined in the same manner as for pure translation motion; the
reduced pitch rate, K = α˙ 0c/(2U), which is generally explored in the range between 0.01 and 1;
and the pivot axis position, Xp/c, which typically lies between 0 (pivot about the leading edge) and
1 (pivot about the trailing edge).
The combination of rotation and pitch, illustrated in subpanel ii of Figure 5a, is less commonly
explored in the literature and is only briefly reviewed here. Furthermore, the flows generated by
other commonly studied kinematics, particularly those comprising oscillatory pitch (Lind&Jones
2016, Mulleners&Raffel 2012, Ohmi et al. 1991), surge (Granlund et al. 2014, 2016), or transverse
(Calderon et al. 2013b, Panah et al. 2015, Visbal 2011) motions; or canonical (Baik et al. 2012,
Choi et al. 2015, Fenercioglu&Cetiner 2012, Ol et al. 2009, Rival&Tropea 2010) or bio-inspired
(Bomphrey et al. 2009, Hubel & Tropea 2010, Jardin et al. 2012, Krishna et al. 2018, Ozen &
Rockwell 2010, Phillips et al. 2015) combinations of these often contain important LEV dynamics
but are omitted from this review for brevity.
4. THE PHYSICS OF LEADING-EDGE VORTICES
4.1. Rectilinear Translation
Arguably the simplest of the canonical kinematics is rectilinear translation, and this is the case
that was used to present the basic features of the LEV in Section 2. The flow topology, force
histories, and surface pressure distributions resulting from a 2D version of this motion are given
in Figures 2 and 3, respectively. From the start of the motion, the force produced by a wing
rapidly accelerated in translation grows as the LEV forms and falls off when the LEV sheds. The
mechanism of shedding may vary, but the LEV typically sheds within the first few chord lengths
of travel, and lift augmentation ceases at this point.
The fluid dynamic forces on the wing are intimately linked to the strength and position of the
LEV. Logically, varying the wing velocity will vary the rate at which vorticity is generated, and
thus the rate at which the LEV gathers circulation, and so will affect the LEV strength, shedding,
and force history of the wing. Faster wing motions (i.e., greater accelerations) have been found to
generate more vorticity and a tighter, more coherent LEV; slower accelerations produce weaker
vortices and lower forces (Chen et al. 2010, Mancini et al. 2015). Once the LEV moves off of
the wing, however, it follows a very similar trajectory for both cases (Manar et al. 2016, Mancini
et al. 2015). Despite this, the effect of the startup transient persists in the force history for many
convective times; in experiments, the fully developed wake and constant force is not established
until t∗
> 14 (Mancini et al. 2015).
In addition to effects from the LEV, force history is a direct function of acceleration due to
added mass, discussed further in Section 5 (Mancini et al. 2015, Pitt Ford&Babinsky 2013, Pullin
& Wang 2004). By removing the added mass component of the measured force, it can be shown
that the remaining circulatory force (i.e., that due to the LEV) is less sensitive to acceleration,
especially for t∗ ≤ 0.5 (Chen et al. 2010, Mancini et al. 2015). At this early stage, the trailingedge
vortex is still forming and all of the circulation in the flow is contained in the LEVs and
trailing-edge vortices (Pitt Ford & Babinsky 2013). Even for 1 ≤ t∗ ≤ 1.5, after the vortices shed,
the circulation within them is nearly equal and opposite. Bound circulation on the wing remains
small; the total lift force can be attributed to a combination of added mass and the LEV rather
than attached flow (Pitt Ford & Babinsky 2014, Pullin & Wang 2004).
On finite wings, tip vortex development affects both the formation and shedding of the LEV.
The severity of 3D effects decreases at higher aspect ratios, but all finite wings are susceptible to
LEV bending (resulting in arch vortices), stall cells, and wake instabilities. Numerical simulations
and flow visualization experiments have demonstrated that the flow topology remains the same at
early times for AR = 1, 2, and 4. From the start of the impulsive wing motion, flow separates at
the leading edge, trailing edge, and wing tips, forming a closed-loop vortex system, as shown in
Figure 6 (Taira & Colonius 2009, Freymuth et al. 1987). Past the initial transient, however, the
wake behind the wing becomes more dependent on aspect ratio as the tip vortices gain strength.
On a wing of aspect ratio 1, the tip vortices grow to affect the entire wing span and the vortex sheet
emanating from the leading edge remains attached to the wing. At higher aspect ratios, however,
Vortex structure on rectangular flat-plate wings with aspect ratios AR = 1 and 2 at angle of incidence α = 30◦ and Reynolds number
Re = 300. Isosurfaces of vorticity are shown in light grey; the vortex cores visualized with the Q criterion are highlighted in green.
Time is measured in convective units, t∗ = tU/c, where U is the plate speed and c is the chord length. Figure adapted with permission
from Taira & Colonius (2009).
the tip vortices are less dominant. At long times, the leading-edge shear layer separates from an
AR = 2 wing and forms an arch-shaped vortex that is not visible on an AR = 1 wing (Kim &
Gharib 2010, Taira & Colonius 2009).
Despite interactions, the LEVs and tip vortices remain distinct structures on rectangular wing
planforms; the sharp corners of the planform suppress vortex stability by impeding spanwise flow
and vorticity convection between the structures (Kim & Gharib 2010, Taira & Colonius 2009).
LEV shedding can be delayed on translating wings with rounder planforms that allow for vorticity
convection into the tip vortices, but vorticity transport in these 2D kinematics is not sufficient to
allow the LEV to remain attached for long times (Mancini et al. 2015, Taira & Colonius 2009).
4.2. Rotational Motion
The LEV on a rotating wing begins to form in a manner very similar to that of the translating
wing. Here, however, the spanwise velocity gradient that results from rotational motion produces
a spatially varying rate of vorticity generation, and thus the LEV that forms is conical, increasing
in both strength and size with radial distance from the axis of rotation. These radial increases
of the LEV are clearly visible in both the computations of Garmann & Visbal (2014), shown in
Figure 7, and in the plot of experimentally measured LEV circulation shown in Figure 8. As the
Figure 7
Surface pressure distribution (represented by pressure coefficient Cp ) on the suction side of a rotating wing at angle of incidence
α = 30◦ and different aspect ratios (AR), with overlaid isosurfaces of relative total pressure highlighting the core of the leading-edge
vortex. Arrows mark the expansion of the vortex core due to burst. ψ denotes the instantaneous angle of rotation. Figure adapted with
permission from Garmann & Visbal (2014).
Figure 8
Leading-edge vortex (LEV) circulation at spanwise locations y/b = 0.25, 0.50, and 0.60, where b is the span (a), and integrated lift
coefficient (b), versus angle of rotation ψ, for a rotating aspect ratio AR = 2 flat-plate wing at Reynolds number Re = 2,500 and angle
of incidence α = 45◦. As the wing accelerates through ψ = 30◦ (shaded region), both LEV circulation and lift increase rapidly. Figure
adapted with permission from Medina & Jones (2016).
wing accelerates through ψ = 30◦ (Figure 8b), both LEV circulation and lift increase rapidly.
Past this point, the wing continues to rotate at a constant velocity and the LEV continues to grow,
albeit more slowly. In the example of Figure 8, lift reaches a global maximum near ψ = 120◦,
which in this case corresponds to the point at which the outboard portion of the LEV has grown
to cover the entire wing chord (see Figure 9b). The LEV remains attached to the wing and
of a similar size and strength as the wing completes a full rotation, resulting in near-constant
lift.
Figure 9
Discretization of the flow field on an aspect ratio AR = 2 rotating wing at midspan. (a) Schematic of the wing and planar views. (b) Dye
flow visualization of the leading-edge vortex at rotation angle ψ = 120◦. (c) Chordwise planes of spanwise vorticity (top) and velocity
(bottom) at ψ = 115◦, centered at k = b/2 and separated by a spanwise increment, dz = 0.012b. Figure adapted with permission from
Medina & Jones (2016).
Due to the fundamental difference in LEV structure, the sustained lift values achieved on a
rotating wing are significantly greater than those on a translating wing (Lentink & Dickinson
2009, Manar et al. 2016, Usherwood 2008, Usherwood & Ellington 2002). The strength of the
LEV is highly (nearly linearly) dependent on angle of attack (Ozen & Rockwell 2012, Wojcik &
Buchholz 2014a), but neither velocity profile nor Reynolds number has a significant effect on the
nondimensional LEV strength (DeVoria & Ringuette 2012, Ozen & Rockwell 2012, Wojcik &
Buchholz 2014a) or the forces on the wing (Usherwood 2008). Measurements of the circulation of
the LEV on the rotating wing at a given spanwise location exceed the bound circulation predicted
by thin airfoil theory at that local velocity (Wojcik & Buchholz 2014a) but agree well with the
measured forces ( Jones & Babinsky 2010), supporting claims that lift on the wing is primarily
a result of the LEV and not bound circulation (Hemati et al. 2014; Pitt Ford & Babinsky 2013,
2014). Further evidence for this is given in Figure 7,where pressure contours are plotted under the
LEV, revealing a clear low-pressure region under this structure with higher pressure elsewhere.
The attached LEV is unique to the rotating wing case. Early hypotheses as to the source of
LEV stability focused on the convection of vorticity via spanwise flow (Ellington et al. 1996) but
were confounded by reports that blocking this flow did not result in vortex shedding (Birch &
Dickinson 2001). Later work demonstrated that, while spanwise flow absent rotational motion can
allow the LEV to remain attached for short times, it is insufficient for the attached LEV observed
on rotating wings at long times (Beem et al. 2011, Jardin & David 2014). On a rotating wing,
spanwise flow from root to tip, vy > 0, is generated by virtue of the linear variation of incident
flow along the span. It is typically of the same order of magnitude as the wing tip speed (Carr et al.
2013, Garmann & Visbal 2014, Harbig et al. 2013, Medina & Jones 2016, Wolfinger & Rockwell
2014) and is enhanced on the underside of the vortex core by the Coriolis term, which induces
an outboard-oriented “force” on any fluid element moving relatively slower than the incident
flow (Garmann & Visbal 2014, Jardin 2017). Although some have described this spanwise flow
as a mechanism for vorticity transport, there is no explicit representation of it in Equation 5; its
effect is canceled by spanwise vortex stretching. The third term of that equation does, however,
represent tilting of the nonspanwise components of vorticity toward the spanwise direction. This
tilting includes the influence usually described as the Coriolis effect ( Jardin 2017, Lentink &
Dickinson 2009). In canonical rotating motion of a wing at angular velocity  and angle of attack
α, for which z = cosα > 0, vorticity is generated in the fluid adjacent to the wing of strength
ωˆ z ≈ 2z > 0 (or ωz ≈ 0 in the corotating frame). On the underside of the control volume, where
the spanwise flowis strongest andwhere n·ωˆ ≈ −ωˆ z < 0, this flowtilts this component of vorticity
into a negative spanwise component, opposite that of the LEV. This process, which might be more
appropriately called Coriolis tilting, is illustrated schematically in Figure 4c. In addition to Coriolis
tilting, vorticity annihilation can be a significant contributor to the outflux of circulation (Medina&
Jones 2016, Wojcik&Buchholz 2014b). By careful accounting of the vorticity within the flow, as in
the control volume analysis of Section 2.1, researchers have shown that LEV stability—equivalent
to relatively little growth in LEV circulation and persistent attachment to the wing—must be
achieved through a balance of vorticity transport mechanisms including convection, stretching,
Coriolis tilting, and annihilation ( Jardin 2017, Wojcik & Buchholz 2014b). These mechanisms,
however, vary along the span in different manners, so vorticity equilibrium is only achievable along
some fraction of the wing. In fact, Jardin (2017) found that by artificially modifying the magnitude
of the Coriolis term and thus the Rossby number of the flow, the spanwise extent of this stability
can be altered.
At the point along the wingspan where the balance of vorticity becomes untenable, the LEV
bursts. Burst typically occurs near or beyond the midspan but moves inboard with increases in
Rossby number (Carr et al. 2013, Garmann & Visbal 2014, Harbig et al. 2013, Kolluru Venkata
& Jones 2013). It is visible near the midspan in the bottom row of Figure 7 and in Figure 9b. The
bottom row of Figure 9c provides measurements of the spanwise flow near the burst point of the
example from Figure 8. Spanwise flow at the k−1 plane is from the wing root to the wing tip but
reverses direction at k, and a region of tip-to-root flow is visible at k+1. The top row in Figure 9c
gives the vorticity field at each of the planes, where the process of negative vorticity entrainment
and LEV expansion is visible. During this process, the vortex line representing the core of the
LEV tilts aftward, the outboard portion of the vortex moves off of the leading edge, and the LEV
lifts off of the wing in a manner not dissimilar from that of the translating wing; measurements
beyond midspan at relatively high Re do not detect an attached LEV ( Jones & Babinsky 2010).
4.3. Translational Pitching
The LEV development on a wing that pitches while in translation is distinct from the previous
two motions and is governed primarily by the reduced pitch rate, K, the location of the axis of
rotation, Xp/c, and the wing’s aspect ratio, AR. Before discussing the physics of the flow’s response
to the pitching maneuver, it is useful to take note of the parameters that, to leading order, are
essentially unimportant in the flow response. As observed with other maneuvers, the Reynolds
number does not have a significant role; studies conducted over the range from O(102) to O(105)
share a common behavior (Ol et al. 2010). Furthermore, the response is not particularly affected by
the wing’s cross-sectional geometry. Most of the studies of the late 1980s were carried out with a
NACA 0015 section, generally pitched about its quarter chord, whereas many of the investigations
of the past decade focused on a flat plate of 2% thickness. Aside from differences in the angles at
which the milestone events occur, the studies in these respective cohorts reveal the same essential
physical processes and the same trends in the parameters.
LEV formation during the pitch-up maneuver, illuminated by experimental flow visualization
(Granlund et al. 2013, Shih et al. 1992, Stevens & Babinsky 2016, Walker & Chou 1987,
Yilmaz & Rockwell 2012), numerical simulation (Eldredge et al. 2009, Jantzen et al. 2014, Visbal
2017, Visbal & Shang 1989), and measurements of surface pressure (Acharya & Metwally 1992,
Strickland & Graham 1987, Visbal & Shang 1989) and shear stress (Schreck et al. 2002) follows a
characteristic sequence of events. At low angles, a separation point appears near the trailing edge,
and a recirculation region develops downstream of this point; as angle increases, the separation
point moves toward the leading edge. After this point reaches the leading edge, the flow departs
substantially from the quasi-steady behavior observed if the wing were held at any of these angles.
A closed separation bubble emerges near the leading edge, inside of which the thickening
boundary layer rolls up into the LEV. Subsequent behavior is similar to that observed in the other
maneuvers.
As pitch rate K increases, all of the same components of this process are observable, but vary in
two critical respects. First, and perhaps most obvious, is that the LEV is increasingly stronger due
to the faster flow speeds and larger pressure gradients brought about by more rapid rotation. The
second aspect, less obvious, is that many of the milestones—the formation of the recirculation
zone, the initiation of the LEV, and its eventual shedding—are increasingly delayed to larger
angles of incidence (Buchner et al. 2012, Granlund et al. 2013, Jumper et al. 1987, Schreck et al.
2002, Strickland & Graham 1987, Visbal & Shang 1989, Walker & Chou 1987).
These delays are also evident as the pitch axis is moved aftward (Visbal & Shang 1989, Yu &
Bernal 2016). It is not surprising that increases in K and Xp/c exhibit similar influences on the
delay. The wall-normal component of velocity at the leading edge, relative to that of the incident
flow, is U

sin α − 2KXp/c

; hence, increases in either parameter tend to reduce this component.
However, it is impossible to conflate their influences based solely on this argument, as increases
Figure 10
(a) Lift coefficient, CL, from pitch-up maneuvers about the leading edge of a flat plate from 0◦ to 45◦ at various pitch rates and
Reynolds number 1,000. Vorticity contours (in all cases, between ±30U/c) are depicted on the left for reduced pitch rate K = 1, 0.6,
and 0.2 (from top to bottom) when the plate is instantaneously at 30◦ incidence, corresponding to the circles on the respective lift
curves. (b) Lift coefficient versus instantaneous angle of incidence for K = 1 (top) and K = 0.2 (bottom), decomposed into the added
mass (dotted dashed line) and circulatory (dashed line) components; the quasi-steady result, 2πα (dashed gray line), is shown for reference.
Figure adapted with permission from Eldredge & Wang (2010).
in Xp/c do not intensify the LEV strength as K does. Visbal & Shang (1989) found that the
peak suction exerted by the LEV, a reasonable measure of its strength, increases steadily with K
but decreases with increasing Xp/c. The initial formation of the LEV cannot be predicted by a
kinematic parameter such as K or Xp/c. However, the leading-edge suction of the wing, which
measures the instantaneous state of the flow at the wing’s nose, appears to reach a critical value
across a variety of motions (Ramesh et al. 2014). This will be discussed further in Section 5.3.
The force generated by a pitching wing has several notable features that distinguish this maneuver
from those at fixed angle. Because the wing is simultaneously pitching and translating
throughout the maneuver, the force exerted on the wing is a mix of the influence from the vortices
developing at the edges and from the inertial reaction of the fluid (i.e., added mass). Examples of
this force are shown in Figure 10a, which depicts the histories of the lift generated at various pitch
rates, obtained from high-fidelity simulation of a flat plate pitched upward about its leading edge
to 45◦ at Reynolds number 1,000 (Eldredge & Wang 2010). Both the lift and time are displayed in
their conventional scalings with the translational velocity. Figure 10b shows the lift at two pitch
rates, K = 0.2 and 1, now plotted versus the instantaneous angle of incidence of the plate; in these
panels, the lift is decomposed into the added mass and circulatory contributions.
Each pitch-up begins with a smooth acceleration from α = 0◦ over a fixed time interval that
starts just before t∗ = 1 and finishes soon afterward; the short-lived peak centered at this instant
is due to the added mass from acceleration, as Figure 10b confirms. Since the final angle is
achieved earlier at higher pitch rate, the duration of the history is compressed as K increases.
This also explains why the added mass peak appears relatively wider at larger K in the plots versus
angle of incidence. The most significant feature of the lift response to this increase in K is its
dramatic increase in peak magnitude, from approximately 3 at K = 0.1 to nearly 16 at K = 1.
Though some of this increase in magnitude is attributable to added mass (proportional to K, as
Equation 11 confirms below), this part is relatively weak, as Figure 10b shows. Thus, most of the
lift enhancement is due to the circulatory part of the force, overcoming the negative lift due to the
decelerating rotation at the end of the maneuver. At angles below static stall, this circulatory lift
exceeds its quasi-steady value at all pitch rates larger than approximately 0.02 (Yu & Bernal 2016).
The excess lift is due to the rates of circulation accumulation in the trailing-edge vortex and the
LEV, which both increase with K; the increased strength of the LEV is apparent in the vorticity
depicted in Figure 10. Several researchers (Eldredge et al. 2009, Granlund et al. 2013, Strickland
& Graham 1987) have attempted to develop mathematical expressions for the lift coefficient that
account for pitch rate and pivot axis location, but these have been only moderately successful due
to the flow memory encoded in the vortex dynamics. Recent models for the force, discussed in
Section 5, have had more success by explicitly including these dynamics.
4.4. Comparisons and Combinations of the Canonical Motions
There are many similarities in LEV physics between the three canonical motions described above,
but there are also some key differences. Both of the inherently2Dkinematics (translation and pitch)
shed their LEV within a few chord-lengths of travel. They produce high lift at early times before
the LEV sheds, but lift drops off rapidly as the vortex moves off of the wing. The pitching wing
in particular produces extremely high lift due to a combination of the inertial reaction of the fluid
and the presence of the LEV. On a rotating wing, however, the spanwise flow that develops is
strong enough to contribute to LEV stability and so the fully developed flow is fundamentally
different—the LEV remains of a near-constant strength and in a near-constant position on the
wing.
Figure 11 gives a comparison of results from a number of research groups that have performed
experiments on rectangular flat-plate wings undergoing the four canonical motions given
in Figure 5. Note that in Figure 11, the x-axis is the convective time based on the constant
final velocity of the wing, U, so while acceleration (in translation, rotation, or pitch) is performed
over one chord-length of travel for all motions, the end of the unsteady portion of the maneuver
appears on the axis at t∗ = 1 for the pitching cases, where U is constant, and at t∗ = 2 for the
translating and rotating cases, where U varies with t. Note also that the lift coefficient given here
is the measured lift coefficient, which includes not only the direct effect of the LEV but also
any inertial effects that arise due to unsteady wing motion. Direct comparison of these results
highlights the similarities between the force curves of the translation and rotation cases and the
marked difference that is effected by adding the pitch-up motion in either case. Dynamic pitch
dramatically increases the buildup and magnitude of the lift transient during the unsteady part of
the maneuver; fixed-incidence translation and rotation result in much slower lift growth with less
overshoot. The primary difference between the lift histories of a pitching wing in rotational or
rectilinear motion is that the forces relax to a steady state much more quickly in rotation than in
translation.
In the results shown in Figure 11, the lift coefficient at long times approaches similar values
for both translational and rotational wing motions (except for the AR = 2 translating wing,
where the difference can be attributed to aspect ratio effects). However, several other studies
have reported that, due to the attached LEV, a rotating wing in fully developed flow produces
greater lift than does a translating one (Manar et al. 2016, Percin & van Oudheusden 2015). This
Figure 11
Lift histories from the four canonical wing motions given in Figure 5: translation, rotation, pure pitch, and pitch and rotation. Time is
measured in convective units, t∗ = tU/c, where U is the plate speed and c is the chord length. Figure adapted with permission from
Jones et al. (2016). Abbreviations: AFRL, Air Force Research Laboratory; AR, aspect ratio; TUD, Delft University of Technology;
UMD, University of Maryland.
Force model: only
captures the force (and
moment) response to
motion
Flow model: captures
the flow field response
as well as the force
apparent contradiction is attributable to the choice of lift normalization. Recall from Section 3
that the computation of CL on a rotating wing requires an arbitrary choice of reference velocity,
affecting the magnitude of the nondimensional lift force. The necessity of this arbitrary choice
highlights the need for flow models that capture the underlying physics of the LEV rather than
blindly producing force predictions.
5. LOW-ORDER MATHEMATICAL MODELING
There has been significant effort over the past few decades to distill the transient physics of leadingedge
flow separation into simple dynamical models. The objectives of this modeling effort are
multifold. First and foremost, one generally seeks a fundamental understanding of the process
of LEV evolution. Furthermore, by capturing the essential physics of the flow in a mathematical
model and, particularly, the response of the flow to various forms of external input (e.g., actuation,
gust encounter, or motion or deformation of the wing or control surface), we can better conceive
of ways to manage the flow to some desired end.
Modeling strategies for leading-edge separation can be categorized in different ways. One
categorization is based on whether the strategy leads to a force model or a flow model. Aflow model
should have far fewer degrees of freedom than, say, a fully resolved Navier–Stokes simulation,
since it is meant to be a distillation of the physics into some simplified form. Members of either
category can be roughly grouped into two approaches: models that build from first principles,
generally using vortex models from inviscid flow theory, and models that are reductions of the
full equations of motion. An example of this latter form is the work of Brunton et al. (2013),
in which they linearized the Navier–Stokes equations about a stable equilibrium at low angle of
incidence and then used the eigensystem realization algorithm to obtain a reduced-order model.
The model was then used to successfully predict the lift response to a modest 10◦ pitch-up/pitchdown
maneuver. Although such model-reduction approaches are a rich area of research, they are
generally limited in their ability to capture the full range of behaviors of the LEV because they are
based on linearization about some state or on a decomposition into modes that are only optimal
for a particular motion. Vortex element models, in contrast, are naturally adaptive to the changing
flow conditions and embody the essential nonlinearities of the flow through mutual interactions
between elements. The current review primarily focuses on these models and particularly their
most recent incarnations, in which they are informed by empirical data.
5.1. Computing the Force
A key to the success of unsteady aerodynamics models is to isolate the part of the force due to
inertial reaction of the fluid, i.e., the added mass terms, so that the model is dedicated to computing
the remaining circulatory force. Ultimately, a fluid exerts its influence on the wing through surface
traction: pressure and viscous stress. However, its influence can also be accounted for by the rate
of change of momentum—or, more specifically, the impulse—in the fluid. The great advantage
of the impulse perspective is that, whereas the surface pressure depends nonlinearly on the fluid
velocity field, the impulse depends on it linearly and can thus be decomposed into parts from
fluid vorticity, wing motion, and uniform flow. The vorticity, or circulatory, portion includes the
direct influence of the fluid vorticity, as well as an indirect influence from the wing due to the
no-penetration condition, that is, the impulse of the bound vorticity, which in a viscous setting is
the vorticity in the boundary layer. The parts of impulse resulting from the wing motion relative
to the fluid—the noncirculatory components—comprise the inertial reaction of the fluid and
give rise to the added mass. For example, for a 2D flat plate of length c and infinitesimal thickness,
the added mass component that gives rise to linear momentum due to translation normal to the
plate is ρπc2/4.
The focus in this section is on the application of an inviscid vortex model to flow about a flat
plate in which fluid vorticity is represented by discrete vortex elements, as depicted in Figure 12.
Suppose that the plate, instantaneously at angle α(t), is rotating at angular velocity α˙ about a
pivot point while translating at velocity (−U, ˙h ) relative to the ground frame. The pivot point is a
distance a = c/2−Xp forward of the plate centroid. The plate is surrounded by a set of Nv vortex
elements with positions xJ and strengths J = Jez, for J = 1, . . . ,Nv, each of which advects with
velocity uJ. Any point x in ground coordinates can also be expressed in plate coordinates, ˜x = ( ˜ x, ˜y),
via x = XC + ˜ xτ + ˜yn, where n = (sin α, cos α) is the plate’s normal vector, τ = (cos α,−sin α) is
its tangent vector, and XC is the position of the plate’s centroid. The velocity of this centroid, U,
has tangent and normal components
Uτ = −U cos α − ˙h sin α, 9.
Un = −U sin α + ˙h cos α − aα˙ . 10.
It can be shown, e.g., via conformal mapping from a unit circle (Milne-Thomson 1996), that the
rate of change of impulse leads to an expression for force on the plate–vortex system given by
f = −ρ
π
4
c2 ˙Unn − ρ
π
4
c2α˙Unτ + ρ
d
dt
Nv
J=1
J × (xJ − XC) + fv; 11.
the moment can be shown to have an analogous expression. The leading two terms comprise the
inertial reaction (i.e., the effect of added mass) and depend only on the plate kinematics, including
Figure 12
Schematic of a flat plate in general rigid body motion about a pivot point. The ground coordinate system is
denoted by (x, y) and the plate coordinates by (˜ x, ˜ y). The plate translates with velocity (−U, ˙h) (relative to
the ground) and rotates with angular velocity α about a pivot point at a distance a forward of the centroid.
The plate’s centroid and angle of incidence are instantaneously XC and α, respectively. The J-th vortex
element in the surrounding fluid is located at ˜xJ relative to the plate centroid.
˙U
n, the rate of change of the plate centroid’s normal velocity:
˙U
n = −˙U sin α + ¨h cos α − a ¨ α − U ˙ α cos α − ˙h α˙ sin α. 12.
All of the remaining terms are circulatory forces. The third term in Equation 11 constitutes the
direct effect of all fluid vortex elements on force via their motions and changes in circulation. In
steady flow at low angle of incidence, this term accounts for the classical Kutta–Joukowsky lift
through the advection of the starting vortex: The vortex moves away from the plate at the velocity
of the incident flow on the wing, −U, and its constant strength, 1, is equal and opposite to the
bound circulation on the plate, 0, so we have f = −ρU × 0.
The remaining circulatory contributions to force in Equation 11 are contained in the final term,
fv. It was noted above that the impulse of a vortex contains both its direct contribution as well
as the indirect contribution from its associated bound vorticity (most easily imagined as the effect
of the vortex’s potential flow image inside the plate); fv contains this indirect influence from
each vortex. This force is significant only when the centers of vorticity are near the edges of the
plate, where the bound vorticity has its greatest influence; it vanishes as these centers move away
from the edges. Rather than provide a complete derivation of this force here, the reader is referred
to Li & Wu (2016). For the purposes of developing intuition, it is reasonable to assume that
fv is zero, so that the circulatory force is well approximated by the third term in Equation 11.
It remains to describe the means of entry and subsequent evolution of vortex elements in the
fluid.
5.2. Vortex Modeling of the Separated Flow
For more than a century, inviscid flow theory has been our most effective language for describing
the aerodynamics of the attached flow past a wing, furnishing us with results that remain unsurpassed
in their brevity and reliability. Much of this success is attributable to the minimal incursion
of viscosity on the problem. In attached flows, the boundary layer remains thin and on the surface
of the wing, and thus it can be suitably ignored in favor of a bound vortex sheet. In those parts
of the problem where viscosity’s role is physically essential—in setting the flux of vorticity into
the wake—this role can be robustly replaced by a constraint, the Kutta condition, that prohibits
infinitely large velocities at the trailing edge of the wing.
A flow that separates before reaching the trailing edge poses obvious challenges to this strictly
inviscid description. The separated flow, instead of passing immediately into the wake, now has
many opportunities to interact again with the boundary layer of the wing, and even the point
(or line) of separation may be in question and determined by subtle viscous mechanisms. But for
flows that separate near the leading edge, as are the focus of this review, the mechanics remain
mostly inviscid, and we propose that these flows can also be described with models that are drawn
from the potential flow toolbox: advecting vortex elements that induce velocity upon each other
and are subject to the no-penetration condition of the wing. However, it should be recognized
that, in contrast to the trailing edge, where the vorticity flux is set by the Kutta condition that the
velocity remain finite, the condition at the leading edge is ambiguous. Indeed, the model might
be more effective if it is left slightly underdetermined in this respect, so that it can be augmented
by empirical measurements; this is discussed in the next two sections.
In previous work, researchers seeking to model the lift on supersonic delta wing aircraft
used inviscid flow theory to predict the influence of the LEV (Brown & Michael 1954,
Edwards 1954, Mangler & Smith 1959, Polhamus 1966), extending classical thin airfoil theory
with some representation of the separated flow. The models differ only in the manner in which
the LEV is represented: an analytical model of a spiral vortex sheet (Mangler & Smith 1959)
or a single, variable-strength point vortex representing the rolled-up core (Brown & Michael
1954, Edwards 1954). Vortex models of separating flows in agile flight or rotorcraft, notably
those developed by Sarpkaya (1975) and Katz (1981), increasingly relied on emerging computational
resources to provide greater fidelity than earlier analytical models. These 2D models
represent the separating flow and wake with discrete point vortices released in short time increments
from the respective edges of the wing. More recent models developed by Ansari et al.
(2006), Xia & Mohseni (2013), and Ramesh et al. (2014) have utilized similar approaches, although
with different leading-edge conditions, as discussed below. Vortex sheet models, such
as those by Jones (2003) and Shukla & Eldredge (2007), maintain connectivity between shed
vortex elements, enabling greater authority over the initiation of instabilities and their associated
length scales. Unfortunately, both the vortex sheet model and the discrete vortex element
model become progressively more expensive as time proceeds and the number of elements grows.
Furthermore, as the LEV convects along a wing at modest angle of incidence, these models
exhibit nonphysical behaviors in lieu of the essentially viscous interactions with the boundary
layer.
To avoid these issues, Wang&Eldredge (2013) took a simple approach similar to that of Brown
& Michael (1954) and used a single, variable-strength vortex element at each edge to capture the
formation and convection of the edge’s associated flow structure. Variable-strength vortices require
special care, however, because they necessarily produce a discontinuous pressure along a
curve between the vortex and the wing. Brown & Michael (1954) addressed this by modifying
the advection velocity of the vortex so that its altered force cancels the spurious force generated
by the pressure discontinuity. The velocity modification is proportional to the rate of transfer of
circulation, ˙ , from the wing to the vortex. Unfortunately, the Brown–Michael approach makes
it impossible to avoid a clumsy jump in force whenever the strength of a vortex is frozen. To
avoid this nonphysical result, Wang & Eldredge (2013) calculated the modified velocity to ensure
that the force on the wing is independent of ˙ through an approach they called “impulse
matching.”
As computational resources have become cheaper and more widely available, it has become
correspondingly faster to achieve high-fidelity simulations of dynamically separated flows. Vortex
models no longer have a clearly advantageous role as predictive tools. However, because of their
unique aptitude for capturing inherently nonlinear flow dynamics (vortex–vortex and vortex–body
interactions) even with a minimal number of elements, vortex models remain powerful for distilling
the full flow physics into a computationally tractable form.
5.3. Leading-Edge Conditions, Vorticity Flux, and the Suction Parameter
Classically, a mathematical model of wing aerodynamics based on 2D inviscid flow theory includes
three basic ingredients: a transport equation for the vortex elements in the fluid, such as those
discussed in the previous section; the Kutta condition, which ensures finite velocity and pressure
at the trailing edge; and Kelvin’s circulation theorem, a constraint that requires that any change
of circulation in the wake is balanced by an equal and opposite change to the bound circulation.
Traditionally, the Kutta condition is enforced—at every instant in an unsteady flow—by choosing
the bound circulation. Kelvin’s theorem, in turn, determines the strength of newly shed wake
vorticity. At low angles of incidence, the circumstances at the leading edge can be ignored, aside
from noting that the flow there will generally have large velocity and (suction) pressure. These
become infinite as the nose radius approaches zero, but the integrated pressure remains finite in
the limit and is called the leading-edge suction.
The three basic ingredients can be generalized to model a flow with expected leading-edge
separation. However, some condition must now be placed on the flow at the leading edge to
determine the strengths of vortex elements that enter the fluid at that point. A reasonable approach
is to express the rate at which circulation enters the LEV, which follows directly from the convective
flux of vorticity via the shear layer, as shown in Equation 5. This can alternatively be written in
terms of the speed of the flow, vLE, just outside the shear layer at the leading edge of the wing:
d
dt
= −1
2 v2
LE. 13.
Some researchers (e.g., Katz 1981, Sarpkaya 1975) have used this equation to determine the
strength δ of new vortex elements entering the flow near the leading edge in each time step.
However, it is considerably challenging to accurately calculate vLE in the vicinity of a discrete
set of singular vortex elements and a singular leading edge, and since the growth of the LEV
circulation depends strongly on this velocity, small errors can lead to significant miscalculations
of the growth.
For a suitable alternative, it is helpful to revisit the concept of edge suction. Enforcement of
the Kutta condition at an edge is equivalent to prescribing its suction force to be zero; thus, this
condition can be expressed as
σ(δ; S(t)) = 0, 14.
in which the strength δ of a nascent vortex element is determined by this condition on the scalar
function σ, which is called the edge’s suction parameter (Ramesh et al. 2014). In addition to δ, this
parameter depends on the current state, S(t), of the flow: the positions and strengths of all existing
vortex elements, as well as the current configuration and motion of the wing. The σ parameter has
two essential properties. First, it has units of velocity, and the edge’s suction force (per unit length)
is equal to ρσ2c/2. Second, the sign of σ describes the direction in which the flow is traveling
around the edge. In other words, σ is a measure of the average velocity circumnavigating the edge;
when the Kutta condition is enforced at that edge, this average is zero.
Both the leading and trailing edges of an infinitesimally thin representation of a wing have an
associated edge suction parameter, and each provides an opportunity for generalizing the constraint
imposed at that edge. It is usually sufficient to leave the suction parameter at the trailing
edge fixed at zero, as in classical aerodynamics. At the leading edge, however, it is reasonable
to relax this condition on its suction parameter, σL. Ramesh et al. (2014), for example, postulated
that the flow around the nose of a wing can tolerate some amount of suction—and the
associated adverse pressure gradient—without separating. Since σL is defined so that it is generally
positive, they proposed that no vorticity be released as long as σL is smaller than a critical
value, σL
max:
σL(δ; S(t)) ≤ σL
max. 15.
If, instead, the instantaneous σL exceeds the allowable range, then a new vortex element is released
from the edge, with a strength δ proportional to σL − σL
max to ensure that σL is brought back
within bounds. Clearly, the Kutta condition is equivalent to setting σL
max to zero, and Equation 15
can be viewed as a generalization of this condition. The Kutta condition leads to a prediction of
circulation growth that is too strong except at very large angles of incidence (Wang & Eldredge
2013). Larger values of σL
max naturally lead to the release of weaker vortex elements from the leading
edge, and there is necessarily a maximum value of σL
max that σL never exceeds and that reverts the
model to shedding from the trailing edge only. It is important to stress that, for any nonzero value
of σL
max, the leading edge of a thin airfoil will have singular flow velocity and nonzero leading-edge
suction.
To determine the appropriate value of σL
max, Ramesh et al. (2014) postulated that, for a given
airfoil section and Reynolds number regime, there is a unique value of σL
max (when nondimensionalized
by a characteristic velocity)that holds across a wide variety of airfoil kinematics. They
obtained this value by carrying out both a high-fidelity simulation and a vortex model of some
representative motion, noting the value of σL in the vortex model at the instant when the highfidelity
simulation first exhibits the formation of a vortex at the leading edge. Vortex models of
other canonical motions, using this same σL
max, agreed reasonably well with high-fidelity results.
Although there is evidence from the study of Ramesh et al. (2014) that σL
max is insensitive
to the motion of the wing, there is also an advantage in assuming that σL
max adapts to changing
conditions. For example, a time-varying value of σL
max can serve as a trigger for the transient
aerodynamic response to an incident disturbance or actuation (Darakananda et al. 2018). In fact,
as is discussed further in Section 5.4, σL
max provides a natural opportunity for constructing a datadriven
vortex model that is responsive to empirical measurements: Rather than prescribing the
value of σL
max from offline calibration, its time-varying value might be obtained through data
assimilation.
5.4. Data-Driven Models
Because of the ambiguity in succinctly describing the physics of leading-edge flow separation
in a mathematical model, many investigators have proposed semiempirical approaches in which
measurements are used to inform some phenomenological model of the flow. The underlying
model, termed a dynamical template model in this review, captures much of the underlying physics
but leaves one or more parameters unspecified and to be determined from measurements. These
approaches represent a form of system identification, familiar in the field of control theory. Such
a procedure might be carried out as a postprocessing of data from experiments or high-fidelity
computations in order to obtain a simple closed-form description of the flow and force behavior,
or, by restricting the measurements to those obtained from onboard sensors and employing tools
from data assimilation, it might serve as a framework for real-time dynamic estimation of the
current flow state.
Inviscid vortex models that are informed from empirical data represent a useful and intuitive
approach to distilling the flow field. For example, Pitt Ford & Babinsky (2013) constructed a
basic potential flow template of a 2D flat plate accelerating from rest but left the positions and
strengths of a general set of point vortices unspecified. Each of these parameters was determined
by applying vortex identification tools to experimental flow data. This allowed them to deduce
aspects of the flow field unavailable from the direct measurements, such as the value of bound
circulation.
The approach taken by Pitt Ford&Babinsky (2013) omits the vortex dynamics (i.e., the motion
of point vortices with the local velocity field) from the template model, leaving the instantaneous
vortex position to be determined empirically along with its strength. Alternatively, the model
could include these dynamics and rely on measurements only to determine the strengths of vortex
elements. The aforementioned study of Ramesh et al. (2014) represents a particularly light touch:
Empirical measurements—from high-fidelity simulation—were used to determine a single static
parameter, σL
max, that helped determine the element strengths while the underlying dynamical
model was otherwise unmodified. Other approaches are more heavily reliant on data. For example,
Hemati et al. (2014) used the variable-strength vortex model of Wang & Eldredge (2013) as
a dynamical template but did not specify constraints at the edges of the wing. Instead, the rates of
change of the two vortex element strengths (i.e., the growth rates of theLEVand trailing-edge vortex
circulation) were found by minimizing a cost function—the error between the model-predicted
force and the “true” force obtained from high-fidelity computation over a time window—subject
to the constraint that the vortices obey their usual dynamics about the wing. This approach successfully
matched empirical force data for multiple cases of wings in pure translation and pitching,
except at low angles of incidence, where the drag was primarily due to skin friction and therefore
impossible to model with vortex dynamics. The constrained minimization approach taken by
Hemati et al. (2014) requires a full empirical measurement history; an a posteriori phenomenological
model is iteratively constructed from these historical data. Darakananda et al. (2016) improved
the efficiency of the technique by building the data-optimized model progressively in short time
increments. The vortex growth rates in each increment are first predicted from a regression over
their recent history; these are then refined by minimizing as before, using the new force data in the
increment.
The investigations of Hemati et al. (2014) and Darakananda et al. (2016) demonstrated that
force measurements can provide meaningful information about the unknown rates of vorticity flux
into theLEVand the wake. However, one drawback was that the underlying model of two variablestrength
vortex elements was too simple to capture the mechanics of shedding. Furthermore, the
strategy they used is not of practical value if the objective of the model is to predict force, as is
typically the case. It is helpful to employ tools from the field of data assimilation, such as the
Kalman filter (KF), to develop a truly predictive strategy that is assisted by practical—and likely,
noisy—sensor measurements. The basic premise of the KF is to advance the estimate of a system’s
state in two steps within each time increment. The state of the flow is first predicted with a
dynamical model, and then this prediction is updated by incorporating new measurements. The
ensemble KF (EnKF) is a variant of the KF designed for dealing with high-dimensional nonlinear
systems, such as fluid flows (Evensen 2009). In the framework of an EnKF, Darakananda et al.
(2018) employed a discrete vortex element model with the leading-edge suction criterion for the
predictive step and used surface pressure measurements obtained from a high-fidelity Navier–
Stokes simulation (the “truth” system) for the update. The state vector comprised the positions
and strengths of the vortex elements and, crucially, σL
max. This framework was used to estimate the
Figure 13
(a, top row) Snapshots of vorticity fields from the “truth” case (computational fluid dynamics at Re = 500) and (bottom row) snapshots of
vortex elements from the ensemble Kalman filter (EnKF)-based model (circles) at the same times. The strengths of the vortex elements
are proportional to the size of the circles, and the colors indicate sign (red is negative). (b, top panel) Normal force coefficient, Cn, versus
time from the EnKF-based model (blue) and computational fluid dynamics “truth” data (red ). (Bottom panel) EnKF estimate of critical
leading-edge suction coefficient, σL
max. Time is measured in convective units, t∗ = tU/c, where U is the plate speed and c is the chord
length. Figure adapted with permission from Darakananda et al. (2018).
flow past a translating flat-plate wing at α = 20◦ and Re = 500. As the results in Figure 13 show,
the estimated vorticity field and normal force agreed well with the truth; the small noise in the
force estimate is due to the fact that the EnKF uses an ensemble of random instances of the model.
As Figure 13b shows, the estimate of σL
max settled to a reasonably constant value, supporting the
hypothesis of Ramesh et al. (2014).
SUMMARY POINTS
1. The leading-edge vortex (LEV) is an essential flow feature of agile flight in nature and
small-scale vehicles. Its mechanics can be understood by distilling complex wing geometries
and kinematics into basic canonical forms.
2. The early development of the LEV and its structure are remarkably common across a
wide variety of wing kinematics and geometries, in both two and three dimensions, and
are relatively independent of Reynolds numbers from O(102) to O(104).
3. The lift generated by the LEV is significantly larger than achievable in steady, attached
flow, but this enhancement is short lived except on rotating wings.
4. The LEV inevitably sheds from the wing in nearly all kinematics and geometries, except
over limited spanwise ranges in rotating wings, where Coriolis tilting and vorticity
annihilation collectively balance the growth of LEV circulation.
5. Vortex models can capture the inherent nonlinearities of the LEV flow field with a
modest number of degrees of freedom. Their accuracy can be systematically improved
by data assimilation using sensor measurements.
FUTURE ISSUES
1. The canonical motions of rigid wings described in this review set only a foundation of
understanding; ultimately, these motions will be combined and applied to flexible wings to
form agile maneuvers. These higher-order combinations and the resulting fluid–structure
interactions will require systematic study, possibly with help from machine learning to
navigate the large parametric space toward optimal maneuvers.
2. LEVformation or response in incident gusts and other transient disturbances has only recently
received systematic attention. We have little understanding of how LEV behavior
will change in an uncertain environment.
3. Data-driven vortex modeling of theLEVhas the potential to provide real-time estimation
of the flow state in a closed-loop control setting. Furthermore, 3D vortex models have
been largely unexplored but have the potential to capture the essential influences of the
tip vortices and spanwise flow on low–aspect ratio wings.
4. Closed-loop control of massively separated flows, effected with pulsed actuators or variations
in wing geometry, remains a primary objective for achieving agile flight vehicles
that remain robust to incident gusts. The LEV is likely to play a central role in this
control.
DISCLOSURE STATEMENT
The authors are not aware of any biases that might be perceived as affecting the objectivity of this
review.

















The inuence of edge undulation on vortex
formation for low-aspect-ratio propulsors
Experiments to study the effect of edge undulation on vortex formation have been
conducted on impulsively accelerated plates. Abstractions of propulsors found in
nature are produced by imprinting undulatory features with varying wavelengths
onto the circumferential vortex-forming edge of circular plates. The effects of the
small-scale disturbances introduced by these modifications are accessed by means of
force measurements and time-resolved particle image velocimetry. Investigations of
four different geometries at two different Reynolds numbers reveal an insensitivity
of the flow towards length scales smaller than or similar to the thickness of the
feeding shear layer. However, the instabilities in the shear layer and the coherence of
the vortex wake are influenced when the wavelength of the undulation exceeds the
shear-layer thickness by a significant margin. This results in a force augmentation due
to enhanced entrainment into the turbulent vortex core, and thus an associated faster
vortex growth rate. Yet, contrary to prior expectations, the time of vortex pinch-off
remains constant for all edge modifications. The causeâ€“effect relationship behind the
stability of the vortex wake is further investigated. While for small edge undulations
a turbulent transition of the vortex core results in vortex pinch-off, for larger edge
undulations the turbulent vortex core is found to be fed constantly with additional
circulation from the shear layer.
Key words: propulsion, free shear layers, mixing enhancement
1. Introduction
Thy dynamics of swimming and flying differ significantly between the observable
behaviour in nature and engineering solutions. Flow separation in quasi-steady
flows (e.g. on fixed wings and rigid bodies) usually leads to significant losses of
propulsive efficiency and control authority, and hence is generally avoided in most
engineering applications. In contrast, during a propulsive stroke in nature, animals
utilise accelerating and morphing propulsors to actively trigger flow separation, as
sketched in figure 1. At the vortex-forming edge (VFE; marked green in figure 1),
a shear layer separates from the propulsor. Subsequently, the shear layer rolls up
FIGURE 1. Undulatory modifications of the VFE (depicted in green): (a) leopard frog
(sketch abstracted from Johansson & Lauder (2004)); (b) sea lion (sketch abstracted from
Sawyer, Turner & Kaas (2016)); and (c) humpback whale (sketch abstracted from Fish &
Lauder (2006)).
to form a vortical structure. Fluidâ€“structure interaction between the vortex and the
propulsor leads to the production of lift and thrust, while maintaining manoeuvrability.
Comprehensive overviews of animal locomotion are given by Azuma (2006), Shyy
et al. (2007) and Biewener & Patek (2018) to name but a few.
According to Pitt Ford & Babinsky (2013), the importance of the unsteady vortex
to lift and thrust is defined by its circulation, as the vortex contains most of the
circulation bound to the propulsor. However, when the vortex sheds, the associated
forces are reduced significantly. As a consequence, the influencing factors on the
circulation of the vortex wake and its stability have been widely addressed in
recent studies. Examples include the role of rotational accelerations (see e.g. Lentink
& Dickinson 2009), propulsor flexibility (see e.g. Vanella et al. 2009), spanwise
curvature (see e.g. Taira & Colonius (2009), Hartloper & Rival (2013)), swept wings
(see e.g. Wong & Rival (2015)) and the interaction of the vortex with secondary
boundary layers (Eslam Panah, Akkala & Buchholz 2015; Akkala & Buchholz 2017).
This study aims to extend the above studies towards a characterisation of the
effects of small-scale structures in unsteady flows, and their relevance towards the
resulting vortex dynamics. In particular, we strive to draw insight into the influence
of turbulence and associated coherent flow structures of different length scales on
the vortex formation process. Inspired by undulated modifications on the VFE of
multiple propulsors in nature (see figure 1), this problem is tackled with a combined
study of vortex formation over a wide range of Re superimposed with various VFE
modifications. While an increase of Re is known to extend the range of turbulent
substructures towards additional smaller vortices, VFE modifications allow for the
introduction of disturbances with distinct wavelengths , which are predetermined by
the propulsor itself.
1.1. Background on the Re scaling of vortex formation and shear layers
Various swimmers and flyers across diverse species utilise separated flows during
their locomotion. Inspired by this evolutionary convergence, prior studies applied
dimensional analysis to extract commonalities in shape and kinematics for a large
variety of propulsors. These common shapes and kinematics developed independently
and from different lineages. For instance, Chin & Lentink (2016), among others,
characterised leading-edge vortices (LEVs) through dimensionless quantities such as
Rossby number (Ro), Strouhal number (St) and Reynolds number (Re). In particular,
Ro, i.e. the relation between centripetal and Coriolis forcing due to rotational
accelerations of the wing, was emphasised as crucial for long-term LEV stability.
Furthermore, Lentink & Dickinson (2009) revealed a limit to values of Ro < 4 in
nature across diverse species.
Plunging and pitching kinematics are characterised by the reduced frequency (k)
and the Strouhal number. Again, a narrow band of values 0:2 6 St 6 0:4 tends to
deliver maximum efficiency and can be found for both flyers (Taylor, Nudds &
Thomas 2003) and swimmers (Triantafyllou, Triantafyllou & Gopalkrishnan 1991). In
contrast, propulsors ranging from fruit flies to large cetaceans use vortical structures
for efficient propagation, which according to Gazzola, Argentina & Mahadevan
(2014) corresponds to a rather large Reynolds-number range of 102 6 Re 6 108. The
convergence over a wide range of Re found in nature suggests a small influence of the
small-scale structures in the flow onto the vortex formation process and the associated
loadings. The above result that small scales do not influence vortex formation is
unexpected since various studies indicate a Re-dependent formation of separated shear
layers. The review on quasi-steady shear layers by Ho & Huerre (1984) indicates
an essentially inviscid mechanism behind the dominant instability mechanism in free
shear layers: Kelvinâ€“Helmholtz instability (KHI). However, small-scale mixing and
entrainment are still dominated by viscous effects, as discussed by Wolf et al. (2013)
and Chauhan, Philip & Marusic (2014). Furthermore, Rosi & Rival (2018) extended
this observation for accelerating shear layers and showed that for acceleration the
spacing of KHIs varies strongly with Re.
By conducting experiments over a wide range of Re, the present study attempts to
address the paradox that, on the one hand, the Re influence based on dimensional
analysis seems to be small, yet, on the other hand, the entrainment in free shear
layers is influenced by the small structures and thus Re. Particular focus lies on the
interaction of the Re scaling with the aforementioned VFE modifications.
1.2. Background on vortex-forming edge modification
For quasi-steady flows, applications of passive flow control by means of geometrical
modifications are well established, as already outlined by Carmichael (1981). For
instance, Lissaman (1983) demonstrated significant delay of flow separation on
stalling airfoils by means of tripping strips, fixed at the respective leading edges.
Similarly, vortex generators and bio-inspired serrations have been applied for airfoil
flow control by Lin (2002) and Ito (2009), respectively. Furthermore, lobe mixers have
been proven by McCormick & Bennett (1994) to reduce the noise through mixing in
the wake of jet engines. These examples underline the importance of the nonlinear
impact of convective effects, where the introduction of small-scale structures leads to
a significant change in behaviour of the global system.
In the category of separated flows, Usherwood & Ellington (2002), Rival et al.
(2014) and Leknys et al. (2018) investigated the influence of mild VFE modifications
for accelerating propulsors. Motivated by the reports of Hertel (1966) on the
dragonfly-wing leading edge, Usherwood & Ellington (2002) imprinted a spanwise
saw-like structure on the VFE under consideration. This study â€“ most likely due
to the choice of a single wavelength â€“ did not reveal notable influences of VFE
modifications on the flow and the measured forces. Rival et al. (2014) and Leknys
et al. (2018) focused on small changes of the leading-edge curvature and reported
small yet measurable delays on vortex formation.
The high impact of nonlinear effects onto quasi-steady flows and the already
measurable influence of VFE modifications in the case of small VFE modifications
suggest that there is merit in further investigation of a wider range of introduced
disturbances in separated flows. This is supported by the observation of significant
VFE modifications on propulsors in nature. The list of examples â€“ even though from
strongly differing lineages â€“ includes webbed frog feet (e.g. Johansson & Lauder
2004), the hind limbs of sea lions (e.g. Sawyer et al. 2016) and the tubercles on the
flippers of humpback whales (e.g. Fish & Lauder 2006); see also figure 1. Note that
each of those VFE modifications is of distinct wavelength , all significantly smaller
than the length scale L of the propulsor itself.
1.3. Objectives and procedure
The combined effects of VFE modification and Re scaling are investigated on the
reference case of an impulsively accelerated, rigid circular plate of diameter D.
This canonical flow for vortex formation was recently characterised by Fernando &
Rival (2016b) and combines several features, qualifying it as a suitable base case
for this study. First, the lack of leading-edge sweep, no rotational accelerations
of the propulsor and the absence of wing-tip or wing-body effects minimises
three-dimensional effects during vortex formation. Thus, the influence of small scales
in the flow can be observed without being superimposed by complex, global and
three-dimensional flow features. Second, Fernando & Rival (2016a) showed that, in
contrast to non-circular propulsors, the circular case produces a stable vortex, which
only pinches off after multiple diameters D travelled. The stable vortex allows a
longer observation time and provides the possibility to investigate the effects of VFE
modifications onto vortex stability. The bio-inspired VFE modifications are abstracted
from the undulatory examples mentioned in Â§ 1.1 and modelled by adding a cosine
function of different wavelengths  with amplitude a D =4 onto the circular edge
geometry; see Â§ 2.1 for more parameter details.
Early on during the onset of vortex formation, a thin layer of vorticity is produced
along the VFE on the varying plates, as indicated in figure 2(aâ€“c). While the
frontal area remains constant for all plates, the effective perimeters vary due to the
superimposed waves. Therefore, it is expected that the magnitude and orientation of
vorticity generated on the edge should vary between the different geometries. During
the subsequent vortex formation, the shear layer detaches, rolls up and forms a vortex.
For the circular plate, the vortex evolution is well known (see figure 2d). The KHIs
appear during the plateâ€™s acceleration and are continuously produced during the whole
vortex formation process, as has been shown by Wong et al. (2017) and Rosi & Rival
(2017). A high-vorticity core remains stable until azimuthal instabilities appear and
destabilise the system. The wavelength of the azimuthal instabilities and their onset
in a circular vortex ring depend on Re, which has been shown by Maxworthy (1977)
for vortices produced by a classical pistonâ€“cylinder vortex generator. Fernando &
Rival (2016b) confirmed the existence of the azimuthal instability for vortex rings in
the wake of an accelerated circular plate.
For the geometries with VFE modifications, however, the formation of the vortex
wake is yet unknown. Various wakes seem possible, all of which are outlined in
figure 2. It is hypothesised that the relation
BD=2 (1.1)
of the undulatory VFE modification height =2 to shear-layer thickness  is a crucial
relative quantity. Therefore, the flow around the plates is colour-coded accordingly
in figure 2, where red indicates flow around a smooth circular plate, and green
FIGURE 2. Summary sketches of possible flow topologies around various plate geometries;
(aâ€“c) the shear layer directly after the onset of acceleration for different plate geometries;
and (dâ€“i) different possible vortex formation topologies for varying Re and plate
geometries.
corresponds to VFE wavelengths in the order of magnitude of the shear-layer thickness
or smaller (B61). The flow around plates with VFE modifications with significantly
larger wavelengths is visualised in blue (B1). Three flow topologies seem possible
for small  (B 6 1). As a trivial solution, the effects of the VFE modifications
might be irrelevant or directly damped out such that the vortex formation would then
appear similar as for the unmodified circular plate (see figure 2e). The disturbances
introduced by the undulated VFE could also promote transition to a turbulent shear
layer, where the presence of turbulent small-scale structures directly corrupts the
forming KHIs (see figure 2g). Recent reports by Buchner, Honnery & Soria (2017)
suggest that a change of the dominant instability mechanism towards centrifugal
instabilities (CIs) is also possible. Small-scale VFE modifications could favour the
formation of CIs, as indicated in figure 2(h). The shear layer of larger undulatory
VFE modifications  (B1) might reorient towards a circle and roll up as discussed
for the smaller perturbations (similar to figure 2e). In contrast, a complete disruption
of the vortex formation process and an expedited transition to the fully separated
and turbulent state also seems possible (see figure 2f ). However, a mixture between
these above cases is most likely: a destabilised shear layer that rolls up to a complex
turbulent flow with a diffuse vortex core, as sketched in figure 2(i).
FIGURE 3. (a) Sketch of the chosen vortex-edge modification; wavelength , undulatory
disturbance amplitude a and mean plate diameter D. (b) Geometries with number of
undulations np D1, np D12, np D50 and np D200.
As a consequence of the above-outlined variety of possible flow conditions and
hypotheses, the objective of the present work centres around which VFE modifications
influence the unsteady vortex wake behind accelerating propulsors. A systematic
variation of  across a wide range of Re is performed by means of time-resolved
particle image velocimetry (PIV) and force measurements in an optical towing tank.
The resulting data serve as a basis to study the instability mechanisms as they
occur with varying wavelength  and/or Re. In particular, the footprint of the VFE
modifications in the vortex formation is evaluated with respect to the resulting loading
on the propulsor and the stability of the vortex wake itself.
2. Methods
2.1. Kinematics and parameter space
The kinematics and the Re space are chosen to match the previous study by Fernando
& Rival (2016b). The plates of mean diameter D are impulsively accelerated
perpendicular to their orientation to a terminal velocity of U1 within a distance
of s D s=D D 0:5, where s is the physical distance travelled. Two terminal velocities
U1 are compared, which correspond to Re D U1D= D 50 000 and Re D 350 000
based on constant values for plate diameter D and kinematic viscosity of water .
Three plates with undulatory VFE modifications of varying wavelengths  are
compared to a smooth circular plate (np D 1). The mean perimeter pD is divided
into np waves of length  D pD=np. To avoid the introduction of additional (and
confusing) length scales into the system, the wavelength  and amplitude a of the
introduced undulations are held constant at a ratio of a= D 1=4. Consequently, the
radius of the modified plates is defined as
R.'/DD=2C=4 cos.np'/; (2.1)
which is indicated in figure 3(a). The frontal surface area A of all plates is held
constant, which is achieved by iteratively adjusting D. Note that the addition of
the cosine function in combination with a constant frontal area A implies different
perimeters P for plates of different wavenumbers kp D 1= D np=pD. In particular,
the influence of the number of undulations, np D 12, np D 50 and np D 200, is tested.
FIGURE 4. Optical towing tank (a) and PIV set-up (b). The light sheet enters the
tank from the bottom window; multiple high-speed cameras (A, B and C) capture the
accelerating plate in a lab-fixed frame of reference.
Plate D (m) a (m) np (â€“) kp .mô€€€1/ A .m2/ P (m)
Base 0.300 0 1 1 0.707 0.942
200 0.300 0.012 200 212.2 0.707 1.390
50 0.300 0.047 50 53.1 0.707 1.381
12 0.298 0.195 12 12.8 0.707 1.423
TABLE 1. Parameters describing the various plate geometries tested here.
Additionally, the experiments for a circular plate (np D 1) from Fernando & Rival
(2016b) are repeated for reference and comparison. All plate geometries considered
are shown in figure 3(b); the geometrical details are also listed in table 1. The
plate with np D 50 leads to a length scale 2a similar to the spacing of KHIs and
the shear-layer thickness  (B  1). Accordingly, undulatory VFE modifications with
larger or smaller wavelengths (np D 200 and np D 12) introduce structures that are
smaller (B<1) or larger (B>1) than the shear-layer thickness, respectively.
2.2. Experimental set-up
The experiments were conducted in the 15 m long optical towing tank facility at
Queenâ€™s University, as shown in figure 4(a). The cross-section spans 1 m  1 m
and is optically accessible from three sides. An overhead traverse is used to
tow models, where an additional non-transparent semi-enclosed ceiling minimises
free-surface effects. In the present study, the four plates considered were mounted
onto a cylindrical sting with a diameter of 0:08D and length 2D, which was further
connected to the traverse by a symmetric profile of thickness 0:08D. A six-component,
submersible ATI Nano force transducer was applied between the sting and the plates
to record force data at 1000 Hz with a static resolution of 0:125 N (see figure 4b).
Every parameter combination was repeated 20 times for s 6 33 and the data were
ensemble-averaged accordingly.
Complementary to the force measurements, the flow fields in the wake of all plates
were captured by means of time-resolved planar PIV, as shown in figure 4(b). A 2 mm
FIGURE 5. Overlapping FOVs and campaigns of the conducted PIV experiments (plate
motion from right to left).
Experiment Re Force I II III IV V VI VII
Base 50 000 X X X X â€” â€” â€” â€”
200 50 000 X X X X â€” â€” â€” â€”
50 50 000 X X X X â€” â€” â€” â€”
12top 50 000 X X X X â€” â€” â€” â€”
12bot 50 000 X X X X â€” â€” â€” â€”
Base 350 000 X â€” X X X X X X
200 350 000 X â€” X X â€” â€” â€” â€”
50 350 000 X â€” X X â€” â€” â€” â€”
12top 350 000 X â€” X X X X X X
12bot 350 000 X â€” X X X X X X
TABLE 2. Overview of the conducted experiments; roman numerals Iâ€“VII correspond to
different FOVs of the PIV set-up; cf. also figure 5.
thick light sheet was created through a 40 mJ pulseô€€€1 Photonics high-speed laser, and
then introduced into the tank through the bottom window. The light sheet was centred
parallel to the sidewalls in the tank and was tilted towards the towing direction to
avoid shadows in the wake.
Three multi-camera PIV campaigns were performed in a lab-fixed frame of
reference, as illustrated in figure 5 and listed in table 2. Raw images were recorded
at 200 frames per second (f.p.s.) for Re D 50 000 and 1400 f.p.s. for Re D 350 000.
Note that for plate 12 (np D12) the PIV measurements were performed twice per Re,
where the light sheet was either aligned with the maximum or with the minimum of
the plate radius (r D 169:3 mm, r D 129:3 mm). These two measurements are from
here on referred to as 12top and 12bot. Similar to the force measurements, 20 runs
of PIV data collection were performed for each set of parameters throughout all
campaigns.
Three cameras were used during the first measurement campaign to cover a
combined field of view (FOV) of s D 0â€“2:3 on the bottom half of the plate (see
figure 5). The early acceleration stage was captured in a small FOV (I in figure 5) of
0:7D  0:7D with a Photron Fastcam Mini WX100 (2048  2048 pixels, camera A)
to ensure sufficient spatial resolution near the VFE. Two additional Photron SA4
cameras (10241024 pixels, cameras B and C) captured the further evolution of the
vortex, where larger FOVs of 0:85D  0:85D (II and III in figure 5) were chosen to
account for the vortex growth. All cameras were equipped with AF-S Micro-Nikkor
60 mm 1:2.8G ED lenses, which resulted in a resolution of 9:8 pixel mmô€€€1 (FOV I)
and 4 pixel mmô€€€1 (FOVs II and III).
Owing to frame rate limitations, camera A (Mini WX100) was only used for the
lower-Re cases. The evaluation of the first campaign (i.e. the force measurements),
however, provided evidence to investigate later vortex formation stages â€“ particularly
for the high-Re case. This insight will be elaborated in more detail in Â§ 3. Therefore,
two additional PIV campaigns were performed for the circular and the np D 12 plate
and the higher Re (ReD350 000). The two Photron SA4 cameras (cameras B and C)
were readjusted to allow for an even larger FOV of 1:0D  1:0D and were used to
measure the vortex evolution from s D2:3â€“4:2 (campaign 2 â€“ FOVs IV and V) and
s D4:2â€“6:1 (campaign 3 â€“ FOVs VI and VII). The frame rates were kept identical to
campaign 1, but, owing to the larger FOV, the resolution decreased to 3:4 pixel mmô€€€1.
The raw images were pre-processed in MATLAB in terms of ensemble-based
median subtraction, edge detection and plate tracking, and masking of the shadowed
area. DaVis 8.4 was used to calibrate and process the data of each camera separately.
A multi-grid scheme with a final interrogation area of 32  32 pixels and 75%
overlap was chosen for FOVs IIâ€“VII. FOV I was evaluated with an interrogation
area of 48  48 pixels and 75% overlap. The resulting velocity fields of all 20 runs
were ensemble-averaged for each FOV. Finally, all FOVs were merged on a common
plate-fixed grid. Note that the interplay of accurate calibration, edge tracking and
good reproducibility of the flow even allows stitching of the separate campaigns to a
single FOV.
2.3. Analysis methods
To further analyse the ensemble-averaged and merged velocity fields, various
post-processing strategies are applied, each of which is outlined in this section
and indicated in figure 6.
Hunt, Wray & Moin (1988) introduced the second invariant Q of the velocity
gradient tensor as a robust means to locate vortical structures. Its maximum Qmax is
frequently applied to localise the core of such structures. More recently, Graftieaux,
Michard & Grosjean (2001) proposed two criteria (ô€€€1 and ô€€€2) to identify the core
and boundaries of vortical structures, respectively. Huang & Green (2015) showed
that ô€€€ max
1 and Qmax lead to similar core locations in cases where the reference frame
for ô€€€1 was chosen appropriately, as indicated in figure 6(a) for a plate-fixed frame
of reference. However, the location rc.t/; zc.t/ of the vortex core is estimated based
on the Galilean invariant Qmax throughout the present work to avoid any possible
uncertainties resulting from reference-frame issues.
The Galilean invariant ô€€€2-criterion is chosen to identify the boundaries of vortical
structures for ô€€€2 D 2=p. The core locations rc.t/; zc.t/ are enclosed by one such
boundary (see black line in figure 6a), which is referred to as vortex core area Aô€€€2 .
The equivalent diameter d and corresponding circulation ô€€€c of the vortex core are
determined by
FIGURE 6. Sample processing for case 12top, s D3:5. (a) Overview of applied methods:
.x0; y0/ coordinate system aligned with the shear layer (red); vortex core centres (rc; zc)
determined with ô€€€1 and Q; horizontal line through the vortex core (light blue); vortex core
boundaries evaluated with ô€€€2 (black); control volume for total vortex circulation (orange).
(b) Velocity vectors and vorticity contours of the .x0; y0/ domain (red box). (c) Spatially
averaged profiles of velocity huy0 iy0 and vorticity h!'0 iy0 . Black lines in panels (b) and (c)
indicate the domain for the calculation of circulation flux ô€€€Psl.
An estimate of total vortex circulation ô€€€ including the shear layer is derived from
the integration of vorticity across a large control volume behind the plate, which is
highlighted orange in figure 6(a). The lower boundary of the control volume is fixed at
r=DD0:19 to exclude the secondary vortex from the circulation budget. The secondary
vortex occurs during the later stages of the flow and near the centre of the plate;
see Â§ 3.
Recent reports on vortex formation evaluated the convective fluxes through a cut
perpendicular to the propulsor and directly behind the flow separation to estimate the
circulation flux ô€€€Psl from the shear layer into the vortex; see for instance Eslam Panah
et al. (2015) or Akkala & Buchholz (2017). The present work attempts to advance
beyond this plate-oriented approach to overcome two shortcomings. First, the optical
access to the shear layer is blocked in the immediate vicinity of the VFE for 12bot,
since the plate is an obstruction in front of the light sheet due to radius changes; see
(2.1). The velocity information in this region, therefore, remains unknown. Second,
instabilities in the shear layer lead to a strong fluctuation of the circulation flux over
time due to KHIs.
Consequently, the flux evaluation is shifted slightly leeward and away from the solid
structure, and a spatial average of the shear layer is evaluated to smooth the effects
of instabilities. This four-step approach is illustrated in figure 6. First, the average
velocity of the shear layer is evaluated in the red dashed area behind the plate to
determine the shear-layer orientation. This direction is then aligned with the y0-axis
of a .x0; y0/ coordinate system. A close-up of this tilted .x0; y0/ domain (solid red box)
is shown in figure 6(b). Profiles of spatially averaged velocity huy0 iy0 and out-of-plane
vorticity h!'iy0 along y0 are shown in figure 6(c). The origin of x0 is set to collapse
with the maximum of h!'iy0 . The spatial filter along y0 minimises the influence of
remaining PIV uncertainties and small-scale instabilities in the shear layer.
FIGURE 7. Force history Cd.s/ for all plate geometries and Re; the shaded area indicates
the scatter through a 2 uncertainty margin. Significant force deviations for the np D 12
plate are emphasised in the close-up of panel (b).
Despite slight shear-layer thickness variations, a band of ô€€€0:025D < x0 < 0:025D
was found to be a reasonable and robust domain for the circulation-flux calculation,
since !' 0 in the vicinity of the shear layer:
ô€€€Psl D
Z 0:025D
x0Dô€€€0:025D
huy0 iy0 h!'iy0 dx0: (2.4)
Note that more accurate strategies to approximate the shear-layer thickness (see e.g.
Brown & Roshko 1974) do not apply for the present data, as only  5â€“6 velocity
vectors are found within the width of the shear layer itself.
3. Results
First, the impact of edge undulations and Re scaling on the overall force histories
is evaluated in Â§ 3.1. Complementary to force measurements, velocity and ensembleaveraged
vorticity fields provide insight into the underlying flow patterns for varying
VFE modifications and Re, as addressed in Â§ 3.2.
3.1. Forces
The force histories of each run were temporally filtered with a least-squares estimator
as per Savitzky & Golay (1964) and ensemble-averaged across 20 runs. Subsequent
normalisation of the force (drag) was performed as follows:
Cd D
2jFzj
AU21
; (3.1)
as presented in figure 7. To further check the repeatability of the measurements and
uncover significant case-to-case variations, two standard deviations .2/ are added
to the plots. The small uncertainty margins during acceleration, with its associated
added-mass peak (0:06s60:5), relaxation stage (0:5<s<2:5) and the stable vortexgrowth
stage (2:5 6 s 6 5 for Re D 50 000, 2:5 6 s 6 9 for Re D 350 000) indicate
good repeatability. Once the flow destabilises and detaches from the plate, the run-torun
scatter increases by an order of magnitude, which indicates the sensitivity of this
instability-triggered topology change. Finally, beyond s >20, the fully separated and
turbulent flow collapses to similar terminal values Cd, which implies that this stage is
stable and highly repeatable again.
Interestingly, no significant variations of the drag coefficient are found between
the circular reference plate and the small-scale modifications np D 50 and np D 200
for both Re â€“ despite considerable changes in the perimeter. Only the force history
of the np D 12 plate deviates from the circular base case, which still lies within the
uncertainty margin for ReD50 000 (see figure 7a). For ReD350 000, in contrast, the
np D12 plate generates a significantly higher force during the vortex-growth stage as
compared to the reference and small-scale cases. A close-up of this difference is added
to figure 7(b) so as to emphasise the 20% offset in relation to the small uncertainty
margin. Note that the abscissa of the inset starts from s D 2:0, since the forces
are significantly higher during the early acceleration (s 6 0:5) and relaxation
(0:5 < s < 2:5). During the acceleration, the forces collapse for identical plate
cross-sections A.
The force deviation for the stable vortex-growth stage implies that the largeamplitude
VFE modification of the np D 12 plate significantly influences the vortex
formation process. However, the vortex topology remains similar in the wake.
Consequently, the wavy shear layer still rolls up into a vortex, which remains
attached to the plate for a comparable duration as with the circular reference case.
To explore these differences, we now turn to the flow fields.
3.2. Field data
To compare the early vortex formation process (1:25 6 s 6 2:25) of the various
plate geometries, the vorticity fields for all cases are shown in figures 8 and 9 for
Re D 50 000 and Re D 350 000, respectively. Note that only selected vorticity fields
are displayed here for the sake of brevity. The complete temporal evolution (s <2:3)
of all cases is provided in a supplementary movie (see Movie1.mp4) available at
https://doi.org/10.1017/jfm.2019.908.
The circular plate (Base, np D 1) and the high-wavenumber plates (np D 50 and
np D 200) reveal similar results in the force measurements for both Re (see figure 7).
The corresponding PIV measurements confirm these results, where the similarity of the
extracted vorticity fields (see figures 8 and 9) indicates that similar topologies during
the early relaxation stage (s D 1:25) and during the late relaxation stage (s D 2:25)
are equivalent for the three geometries (np 2 f1; 200; 50g) and both Re. The strong
repeatability of the measurements, furthermore, leads to clear visualisations of the
dominant KHI. Neither disturbances smaller than the shear-layer thickness  (B < 1,
np D200) nor disturbances of the same order of magnitude (B1, np D50) affect the
instability mechanism or lead to significant corruption of the KHI. Also, the influence
of Re onto the instabilities is small, since the same spatial offsets of consecutive
KHI structures are found in the shear layer for both Re; compare figures 8(a)â€“(i)
and 9(a)â€“(i). It is important to mention at this point that this finding does not hold for
the earlier stage of plate acceleration (s <0:5), as reported by Rosi & Rival (2017).
The similarity between the circular case and the small-scale VFE modifications
(np 2 f1; 200; 50g) indicates that the shear-layer formation smoothes the spanwise
FIGURE 8. Early vortex formation at Re D 50 000: vorticity fields in the range of
1:25 6 s 6 2:25 for all plate geometries. Similar results for no and small-scale VFE
modifications (np 2f1; 200; 50g) â€“ also applies for ReD350 000, cf. figure 9. KHI length
scale variation and convection of oppositely signed vorticity from the leeward boundary
layer for large-scale VFE modifications (np D12).
spatial disturbances smaller than or in the range of the shear-layer thickness (B 6 1).
The force histories (see Â§ 3.1) and also the vorticity fields of the smallest tested
wavenumber np D 12 lead to the observation that such geometrical disturbances
become influential once their length scale  exceeds the shear-layer thickness itself
(B1).
1:25 6 s 6 2:25 for all plate geometries. Similar results for no and small-scale VFE
modifications (np 2 f1; 200; 50g). Note that the KHI spacing remains constant for
the small-scale VFE modifications (np 2 f1; 200; 50g), but is reduced for np D 12 at
ReD350 000 compared to ReD50 000, cf. figure 8.
Figures 8( j)â€“(o) and 9( j)â€“(o) present the early evolution of the vortex in the
12top and 12bot measurement plane, where the laser sheet cuts the vortex at the
maximum and minimum radius of the plate, respectively. The non-circular shape
of the vorticity maximum in the vortex core can be associated with significant
vortex stretching. Figure 10 presents the out-of-plane gradient of the out-of-plane
velocity .1=r/ @u'=@'. Furthermore, in contrast to the high-wavenumber geometries
(np 2 f1; 200; 50g), the main part of the vortical structure in the 12top plane is
located at a smaller radius than the maximum radius of the undulated vortex edge at
R D D=2 C a. Recent surface pressure measurements on wings by Eslam Panah et al.
(2015) and Akkala & Buchholz (2017) provide evidence that this vortex position
leads to an adverse pressure gradient in parts of the boundary layer between vortex
and plate, which in turn results in a flux of (positive) vorticity from the plate into
the vortex (see also Lighthill 1963). While the positive vorticity in the boundary
layer between the leeward side of the plate and the vortex wake is not sufficiently
well resolved in the PIV measurements, the advected positive vorticity is clearly
visible in figures 8( j)â€“(l) and 9( j)â€“(l). The advected vorticity cross-annihilates with
the shear-layer vorticity and thus reduces the circulation growth of the vortex itself.
In addition, this cross-annihilation also changes the wavelength of the KHI, which
â€“ in contrast to the geometries np 2 f1; 200; 50g â€“ varies significantly between the
low- and high-Re case. Interestingly, the 12top case comprises significant amounts of
engulfment from the irrotational outer region between the shear layer and the vortex.
Figures 8 and 9 reveal differences during the early stages of vortex formation
between the high-wavenumber cases (np 2 f1; 200; 50g) and the low-wavenumber
case .np D 12/. Despite the topological differences between these cases, the drag
coefficient Cd is similar for all geometries and Re during the plates acceleration and
the relaxation stage. Only for the high-Re case .Re D 350 000/ and for later stages
(s > 2:5) do significant differences between the cases exist, which in turn motivated
the additional PIV experiments for 2:3 6s 6 6 and ReD350 000; cf. Â§ 2.2.
The resulting vorticity fields for the circular and np D 12 plates are shown in
figure 11 and are available in a supplementary movie (see Movie2.mp4). A secondary
vortex appears near the plate centre in all measurements. In comparison with figure 9,
the shear layer weakens for both cases. Yet the shear layer remains more pronounced
for the non-circular plate (np D12). However, the interaction of shear layer and vortex
core varies between the circular and the non-circular plate. At s D3, the shear layer
of the circular plate gets deflected by the vortex core, convects around the vortex
centre, and eventually merges with the low-vorticity region in the core vicinity. At the
largest plate radius of the small-wavenumber plate (12top), the shear layer is furthest
from the core, where parts of the shear layer even start to convect into the wake
from s D 3; see figure 11(d) and supplementary movie Movie2.mp4. In contrast, the
shear layer at the smallest plate radius (12bot) directly convects into the vortex core,
FIGURE 11. Differences in the long-term vortex formation for the circular plates and
np D 12 at Re D 350 000: stronger shear layer, higher turbulence level and more diffuse
vortex core for both np D12 measurements compared to the circular case. Counter-rotating
vortex near the axis for both cases. Black lines show vortex boundaries of ô€€€2 D 2=p.
The connected area Aô€€€2 around the maximum vorticity core is defined as the vortex core
(cf. Â§ 2.3).
as indicated in figure 11(g)â€“(i). The vortex core behind the circular plate remains
coherent within the measured FOV s <6. The vortex core boundaries â€“ as introduced
in Â§ 2.3 â€“ enclose a small area where rotation dominates shear. The corresponding
core boundary for the np D12 plate appears larger and accordingly comprises a more
diffuse vortex core. Furthermore, the area of the overall vortex is larger, while its
maximum vorticity is smaller due to turbulent mixing. The secondary vortex near
the np D 12 plate centre is found to be corrupted and, therefore, less pronounced as
compared to the circular plate.
4. Discussion
This section focuses on a quantitative comparison between vortex formation on the
plate with wavenumber np D12 and the circular base case. Simple models are applied
to elucidate the physics behind the most important observations stated in Â§ 3 and the
influence of the ratio B; see (1.1). First, the changes in shear layers for varying plate
geometries and their effects on the vortex circulation are analysed (Â§ 4.1). Second, a
discussion on how the vortices stay attached to the plate for similar periods of time,
even though the vortex wake behind the np D 12 plate is significantly less coherent
FIGURE 12. Vorticity distribution directly after the plate and perpendicular to the flow
(see figure 6) for ReD50 000 (dotted lines) and ReD350 000 (solid lines) for different s
and geometries. Left of the shear layer: vorticity-free outer flow. Right of the shear layer:
vortex wake.
(Â§ 4.2), is presented. Finally, the physical mechanism behind the changes in force
is addressed (Â§ 4.3) by considering the momentum and size of the vortex wake and
estimating the pressure in front of and behind the plates.
4.1. Feeding shear layer and vortex wake circulation
When recalling the similar spacing of KHIs (figures 8 and 9), it was hypothesised that
the shear-layer thicknesses  for both Re under consideration would be similar. This is
confirmed in figure 12. Applying the methods introduced in Â§ 2.3, figure 12 presents
the averaged vorticity distribution perpendicular to the shear layer. No significant
influence of Re on the shear-layer thickness  is observed.
The similarities between all high-wavenumber plates (B 6 1, np 2 f1; 50; 200g),
as observed in Â§Â§ 3.1 and 3.2, are confirmed. In contrast, the maximal vorticity in
the shear layer is consistently higher for the np D 12 plate (B  1). However, with
regards to the temporal change of circulation in the vortex, the higher vorticity in the
shear layer is balanced by a flux of oppositely signed vorticity from the leeward side
of the plate during the early stages (figure 12b,c), as parts of the fed vorticity are
directly annihilated. For the later stages of the vortex formation process (s > 2:5),
less vorticity diffuses from the leeward boundary layer (see figure 12d). Yet the shear
layer is still more pronounced compared to the circular plate. As a consequence, the
circulation fed through the shear layer is higher for np D12 than for the other plates.
Applying (2.4), the circulation flux through the shear layer is evaluated. Figure 13(a)
shows, as expected, high ô€€€Psl during the early stages, where the forces are also higher.
FIGURE 13. (a) Circulation flux through the feeding shear layer ô€€€Psl for 1 < s < 6 for
the varying cases at Re D 350 000. Blue lines depict the average of 12top and 12bot.
(b) Accumulated circulation in the vortex wake for 1<s <6.
During the stable vortex growth (s > 2:5), ô€€€Psl remains fairly constant for each
geometry. However, the circulation flux of the small-wavenumber plate (np D 12) is
higher than for the circular reference case. The overall circulation behind the plate,
and as such in the vortex wake, is captured in figure 13(b). Despite the differing
circulation flux through the feeding shear layers, the temporal development of overall
circulation in the vortex is similar. A superposition of various circulation-reducing
mechanisms in the vortex itself is hypothesised as mechanisms to limit the circulation
growth:
(i) reorientation of vorticity, followed by cross-annihilation in the turbulent vortex
wake;
(ii) loss of circulation due to convection of vorticity into the wake;
(iii) interaction and cross-annihilation of vorticity in the primary vortex with the
vorticity in the secondary vortex; and
(iv) interaction of the vortex with the boundary layer on the leeward side of the plate.
The above mechanisms are more pronounced in the highly turbulent wake of the noncircular
geometry (B1, np D 12) and, as such, compensate for the higher ô€€€Psl. As
a consequence, there only remain small differences in the overall circulation budget
(see figure 13b).
4.2. Vortex core and its stability
The position of the vortex centre is estimated by the maximum of the Q-criterion (see
Â§ 2.3) and is presented in figure 14(a). The undulatory shape of the small-wavenumber
plate (np D12) influences the vortex core radius rc and its distance from the plate zc.
Relative to the circular plate (Base), rc is either larger (12top) or smaller (12bot). Yet
the average position (blue solid line) is similar to the radial position of the circular
plateâ€™s vortex centre. However, the axial position zc increases more quickly for the
small-wavenumber plate np D12 than for the circular plate (dashed lines).
The distinct rc values in the 12top and 12bot planes suggest an undulatory
shape of the vortex wake, which most likely results in a significant radial vorticity
FIGURE 14. Results for ReD350 000. (a) Vortex centre position based on the maximum
of the Q-criterion. The blue line depicts the average of 12top and 12bot. (b) Magnitude
of radial velocity along a horizontal cut through the vortex centre estimated by the Qcriterion,
for s D5. The distance between the two maxima is defined as the vortex core
diameter dz. (c) Vorticity along a horizontal cut through the vortex core, for s D5.
component !r in between the measurement planes 12top and 12bot and induces
stretching of the vortex core as depicted in figure 10. The stretching contributes
to the qualitative differences between the different plates concerning the coherence,
symmetry and size of the vortex core reported in Â§ 3.2 and further discussed utilising
figure 14(b) and (c). The magnitude of velocity (jurj) and the vorticity (!') are
presented in a horizontal cut through the vortex centre .rc; zc/ (see dashed blue
line in figure 6). Note that, while ur is the radial velocity in the coordinate system
introduced in Â§ 2, it also represents the azimuthal velocity of the vortex wake itself.
Maxworthy (1977) defines the horizontal distance between the velocity peaks on
both sides of the vortex core as the core diameter dz (see figure 14b). This dz provides
similar results to the reduced vortex core diameter d defined in Â§ 2 (see figure 15a,b).
The vorticity peak of the circular plateâ€™s vortex is higher, while its diameter dz is
significantly smaller as for the plate with np D12.
As mentioned in Â§ 1.2, Maxworthy (1972) related the laminar-to-turbulent transition
of a vortex ring with an azimuthal instability. The vortex core of diameter d develops
a wavy structure along the ring with radius rc (similar to the shape of the geometries
in this study), which amplifies in time and finally corrupts the vortex core itself. The
onset and wavelength of the azimuthal instability depends on Re. Counter-intuitively,
smaller Re lead to an earlier onset of instability (Maxworthy 1977). While the
findings of Maxworthy are based on careful analysis of pistonâ€“cylinder vortex
generators, Fernando & Rival (2016b) captured the same instability mechanism
for the vortex formation on a circular plate and observed an earlier vortex pinch-off
for smaller Re. Combining the earlier vortex pinch-off for smaller Re observed by
Fernando & Rival (2016b) with the earlier onset of the instability for smaller Re
observed by Maxworthy (1977) suggests a causal relationship between the vortex
core corruption and the vortex pinch-off. The core transition leads to reorientation
of vorticity, followed by cross-annihilation and eventually a reduced circulation ô€€€c in
(based on ô€€€2); and (b) dz estimated by the distance of the maxima in figure 14(b).
(c) Circulation in the vortex core as identified by the boundary ô€€€2 D2=p.
the vortex core. Furthermore, the additional mixing results in a more diffuse (and as
such larger) vortex core.
Figure 14(b) supports modelling the vortex core as a Rankine vortex, and thus its
core as a solid-body rotation. Applying this model, the pressure difference between
the vortex centre p.rc; zc/ and the pressure at the core boundary p.rc; zc C d=2/ can
be estimated by
p.rc; zc/ô€€€p

rc; zc C
d
2

Dô€€€
ô€€€ 2
c
2p2d2 ; (4.1)
where ô€€€c is the circulation of the core. Equation (4.1) clarifies that both effects of
transition, the reduction of jô€€€cj as well as the increasing core diameter d, decrease
the magnitude of the pressure minimum in the core. This leads to vortex pinch-off, as
the attracting forces from the vortex core towards the plate are eventually insufficient
to avoid pinch-off.
While explaining the Re scaling of the vortex stability in the wake of a circular
plate, the chain of events described above contradicts at first glance the fact that the
vortex behind the np D12 plate is stable. For this geometry, even before the onset of
an azimuthal instability, the vortex core is already turbulent. This turbulence leads to
a growth of the vortex core size: d and dz (figure 15a,b). Hence, according to (4.1), a
comparable core pressure minimum similar to the circular plate can only exist if the
core circulation also grows in time. The magnitude of core circulation jô€€€cj (see (2.3))
is shown in figure 15(c). While jô€€€cj slightly decreases over time for the circular plate,
jô€€€cj increases in the vortex core of the np D12 plate. The growth of jô€€€cj results from
the direct merging of shear layer and vortex core at the lowest plate radius (12bot,
see Â§ 3.2 and in particular figure 11gâ€“i). As a consequence, even though the vortex
formation and as such the physics behind the stabilisation are distinct, the magnitude
of the pressure minimum is similar for both plate geometries and both vortex cores
stay attached to the plate.
The quantitative evaluation of the vortex wake circulation ô€€€ (figure 13b), the radial
position of the vortex centre rc (figure 14a) and the approximate pressure at the vortex
centre (4.1) provide similar results. As such, the physical phenomenon behind the
force enhancement up to 20% (see Â§ 3.1) for the small-wavenumber plate (np D 12)
as yet remains unclear and is addressed in the following. Two distinct approaches to
reconstruct forces from the measured data are applied. First, the momentum of the
flow is discussed. It provides an intuitive explanation for the force enhancement but
does not resolve the measured propulsion force to its full extent. This is achieved by
the second approach, i.e. the evaluation of the pressure at the plateâ€™s surface.
Motivated by the different observed vortex sizes during the later stages of the
measurement (see figure 11), a simple model is applied to estimate the influence of
vortex volume growth on the rate of change of momentum and as such the propulsion
force. By assuming the vortex of growing volume V travels with approximately the
same velocity as the plate U1, the momentum in the axial direction can be split into
the contribution of the vortex momentum, Iv DVU1, and the momentum of the also
time-dependent flow outside of the vortex, Io:
Iz DIv CIo DVU1 CIo: (4.2)
During the later stages of vortex formation (s > 2:5), where distinct forces
were measured for the different geometries, the plate velocity U1 is constant. The
approximated force results in
Fz DIPz DFv CFo DVPU1 C P Io; (4.3)
where Fv D P Iv DVPU1 and Fo D P Io. The fluid mass V, which travels with the plate,
and as such can be considered as part of the vortex, is analysed in the following.
Multiple mechanisms lead to a growing vortex mass V. The vorticity-containing
shear layer feeds mass into the vortex volume (Wong, Kriegseis & Rival 2013). In
addition, turbulent entrainment (Rosi & Rival 2018), as well as engulfment of inviscid
fluid (e.g. figure 9), adds additional mass to the vortex core. The additional fluid,
which subsequently travels with the propulsor, also requires momentum. Similar to
the findings of McPhaden & Rival (2018), equation (4.3) suggests that the change
of vortex volume influences the propulsion forces. To approximate the instantaneous
vortex volume V, the topology of the flow is analysed. The streamline originating
in the node behind the plate is identified (red line in figure 16a) and the underlying
vortex volume is defined as V. Figure 16(b) shows the temporal evolution of the vortex
volumes. The large disturbances due to the undulatory VFE of plate 12 result in a
turbulent vortex wake and thus in additional entrainment. For the small-wavenumber
plate (np D 12), the volume is estimated by linear interpolation between the results
from 12top and 12bot. A linear fit on V (figure 16b) is utilised to estimate the force
contribution Fv of the growing vortex wake of the respective geometries (F12
v and
FBase
v ) in the interval 3 6 s 6 6; see figure 17(a). The trends of the reconstructed
forces Fv are consistent with the trends of the forces measured by the force transducer
Fz, i.e. F12
v >FBase
v . However, Fz is not reproduced to its full extent, as the contribution
of Fo is unknown and missing.
To obtain a more accurate force reconstruction, the pressure distribution at the plate
is estimated. Multiple steps are performed for each dataset: the circular plate (Base),
12top and 12bot.
FIGURE 16. (a) Vortex size estimation based on the outer streamlines of the vortex; shown
here at s D 6 for the circular plate at Re D 350 000. The red line denotes the estimated
vortex boundary. (b) Vortex volumes shown for the varying cases at Re D 350 000 over
travelled distance of the plate s. The blue line represents the average of 12top and 12bot.
for the pressure evaluation: (a) shadowed area in front of the plate and (b) the far field
extending the FOV of the measurement. The areas of potential flow solution and their
respective Neumann boundary conditions are highlighted: measured velocity data (solid
lines), ur D0 (dashed lines), uz D0 (dotted lines). The plate position is depicted in green.
(c) Propulsion force at Re D 350 000 measured by the force transducer (Fz; same as
figure 7b), estimated by the plateâ€™s surface integral of the pressure (Fp) and by the rate
of change of vortex momentum (Fv DVPU1).
(1) The area directly in front of the plate was not captured, as the plate itself
casts a shadow (see figure 4b). The flow field in the shadowed area is
reconstructed by employing a two-dimensional potential flow solver. As presented
in figure 17(b), the measured velocity data (solid lines), the impermeability of
FIGURE 18. Results for ReD350 000. Pressure fields evaluated by means of the ensemblebased
approach of Kling et al. (2019) at s D 4:5 for (a) the circular plate, (b) 12top
and (c) 12bot. The vortex wake of the circular plate shows steeper pressure gradients,
resulting in a higher pressure at the leeward side of the plate and, as such, a smaller
propulsion force. Owing to the unknown out-of-plane velocity/gradients in the highly threedimensional
vortex core of the undulated plate (see figure 10), the core area is excluded
from the pressure calculation for 12top and 12bot.
the plate (dotted line) and the symmetry of the flow (dashed line) are utilised as
Neumann boundary conditions (NBC).
(2) In addition, the potential flow solver is used to extend the limited FOV of the
measurement. NBC are applied at the boundaries of the flow area of interest
(see figure 17c). While the measured velocity data provide NBC at the FOV
boundaries (solid lines), the far-field NBC are estimated to be ur D 0 at the
upper and lower boundaries (dashed lines) and uz D 0 at the windward and
leeward boundaries (dotted lines). Note that these far-field NBC are only an
approximation. To reduce the influence of these imperfect NBC, a large area of
13D4D is selected to reconstruct the far field.
(3) The pressure field p is evaluated by applying the ensemble-based pressure
estimation method of Kling et al. (2019). An ensemble-based Reynolds
decomposition is applied on the divergence of the momentum equation. Kling
et al. (2019) calculate the Reynolds stresses by means of the variance and
covariance of the ensemble-averaged data at a single time instant. As only 20
runs were performed in the present experiment, the method is extended by
an additional moving temporal average in the plate-fixed frame of reference
over 100 time steps. Furthermore, due to the two-dimensional measurements
in the present study, u', @ur=@' and @uz=@' are unknown. Therefore, for the
undulated plate, the vortex core is excluded from the pressure reconstruction, as
significant out-of-plane gradients exist in this region (see figure 10). The far-field
solution estimated in the previous step of the calculation allows application of
the required Dirichlet boundary condition p0 Dconst. far away from the plate, in
particular at the upper boundary of the far-field solution. The NBC are derived
from the momentum equations. The estimated pressure fields 1p D p ô€€€ p0 are
presented in figure 18 for s D4:5.
4) The reconstructed axial force resulting from the pressure field Fp is evaluated by
integration of the pressure along the plateâ€™s front and back surfaces:
Fp D
Z
Afront
p dAô€€€
Z
Aback
p dA: (4.4)
The net force on the undulatory plate (npD12) is estimated by averaging between
the two measurement planes F12
p DF12bot
p CF12top
p .
Despite the limitations of the two-dimensional data and the limited number of
measurement planes, the estimated forces Fp closely resemble the forces measured by
the transducer Fz (see figure 17a). This good agreement of the integral value provides
evidence to further interpret the instantaneous pressure fields presented in figure 18:
the larger and more diffuse vortex core in the wake of the undulatory plate results in
less steep pressure gradients and thus in a lower pressure at the leeward side of the
plate, which in turn explains the higher propulsion force.
5. Conclusions
The convergence of unsteady propulsion in nature for a broad range of Re at first
glance suggests a minor impact of turbulence and small-scale structures on vortex
formation. Conversely, recent studies on free shear layers and the influence of Re
on entrainment refute this argument. This study explores the impact of small-scale
coherent structures on vortex formation. This problem is, on the one hand, addressed
by performing measurements for a wide range of Re, where at higher Re it is expected
that the size of the smallest structures in the flow will be reduced. On the other
hand, disturbances of distinct wavelengths  are introduced into to flow. Motivated
by varying undulations on propulsors of swimming animals, the disturbances are
introduced into the flow by modifying the VFE of accelerating low-aspect-ratio plates.
The relation BD=.2/ (see (1.1)) hereby describes the height of the undulation =2
relative to the length scale of the shear-layer thickness . As such, this study explores
the influence of varying edge undulation on vortex formation over wide ranges of B
and Re.
Conclusions relating to the scaling of B and Re are categorised below with regard
to vortex stability and forces on the propulsor.
(1) The influence of edge undulations strongly depends on the relation B. The
shear layer and, as such, the forming vortex are found to be insensitive to VFE
modifications smaller than or similar to the shear-layer thickness . The vortex
formation process is only affected significantly for large-scale VFE modifications
(B1).
(2) For B  1, the force is increased during the stable vortex-growth stage. Up
to 20% higher forces are measured in the present study. A change in vortex
volume growth and a more diffuse vortex core are identified as the cause of
these variations. This observed increase in force is likely one cause for the
undulatory VFE modifications observed in nature.
(3) For all geometries tested here, the wake vortices remain stable over an equivalent
normalised distance. Yet a different mechanism stabilises the vortex for large 
(B  1). The vortex wake for the circular base case depends on the coherence
of its vortex core to remain stable. Transition due to azimuthal instabilities leads
to vortex pinch-off. For larger VFE modifications B  1, the vortex core is
turbulent from the start. However, the circulation of the turbulent and spatially
expanding core grows continuously, preserving the magnitude of the required
pressure minimum inside the core.
(4) For small VFE modifications (B61) other than the known Re scaling associated
with the azimuthal instability (Fernando & Rival 2016b), no additional scaling
was observed. In accordance with findings on quasi-steady free shear layers
(Ho & Huerre 1984), neither the spacing of KHIs nor the shear-layer thickness
 vary within the available accuracy of the data and within the Re range
under consideration. The existence and the effects of coherent structures that
are significantly smaller than the observed KHIs cannot be evaluated with the
available data. Repeating the high-resolution particle tracking experiments of
Rosi & Rival (2018) for the quasi-steady regime of the flow could provide
insight if additional entrainment due to very small structures occurs at high Re.
In contrast, for the case of B  1, higher Re leads to smaller spacing of the
KHIs. Following the model of Rosi & Rival (2017), the smaller spacing could
result in higher entrainment, a faster-growing vortex volume and consequently a
higher overall force. As such, the higher impact of coarse VFE modifications in
cases of larger Re can be motivated by the observed smaller KHI spacing.
By selecting an impulsively accelerated, rigid circular plate as the canonical
base flow of the present study, the effects of VFE undulations are isolated from
other influencing factors such as propulsor flexibility, varying propulsor shape and
complex kinematics, as found in nature and engineering applications. It is, therefore,
hypothesised that proper VFE undulation design (B1), convective mixing inside the
vortex and entrainment along the vortex boundaries should also increase and affect
the overall forces for more complex base flows. However, future detailed studies
will be required to test these complex interactions. As a final remark, it is also
worth mentioning that the present study mainly focuses on the later stages of vortex
formation, due to the distinct propulsion forces measured for s > 2:5. Future work
could address the early stages of vortex formation s < 0:5, where the plate is still
accelerated, and extend the work of Rosi & Rival (2017) by assessing the influence
of distinct acceleration rates on the vortex wake of undulated plates.
Acknowledgements
The authors greatly acknowledge the help of N. Kling, who provided his code for
the pressure evaluation from unsteady PIV data and contributed significantly during
the code modification and its application to the present data. Furthermore, the authors
acknowledge the support by the Karlsruhe House of Young Scientists (KHYS) for
F.K.â€™s scholarship to support his research abroad in Canada. D.E.R. would also like
to acknowledge NSERC.
Supplementary movies
Supplementary movies are available at https://doi.org/10.1017/jfm.2019.908.




















A new approach to estimate the aerosol scattering ratios for the atmospheric
correction of satellite remote sensing data in coastal regions
Aerosol scattering reflectance is the most uncertain term to be determined in the atmospheric correction of
satellite remote sensing data. The values in the visible bands depend on the aerosol scattering ratios (the epsilon
spectrum). The epsilon value in the Near-infrared (NIR) band is estimated on the dark pixel assumption
of the water-leaving reflectance in the two NIR bands and then the epsilon spectrum is determined from the
aerosol models. This assumption usually becomes invalid for turbid coastal waters, leading to lost regions in
the satellite imagery masked by the failure of the atmospheric correction. A new approach was developed to
accurately estimate epsilon from turbid coastal waters. This method is based on the idea that the aerosol scattering
reflectance and the epsilon values can be obtained from the known water-leaving reflectance of in situ
measurements. The water-leaving reflectance is determined from the choice of a look-up table of the
water-leaving reflectance based on the Angstrom law of the candidate aerosol scattering reflectance using
the best non-linear least squares fit function. In this approach, the entire epsilon spectra can be obtained
and used to determine the two closest aerosol models which are used to interpolate the actual epsilon values.
It is demonstrated that the results from matching the entire spectra are more robust than that obtained from
using only one epsilon value. The performance of the approach was evaluated using the simulated reflectance
at the top of the atmosphere, the Sea-viewing Wide Field-of-view Sensor (SeaWiFS) imagery, and in situ
measured aerosol optical thickness. This approach is based on the assumption of the aerosol scattering reflectance
following the Angstrom law instead of the standard dark pixel assumption, named as the ENLF model.
This new assumption is valid for both Case 1 and Case 2 waters, even over terrestrial regions. Therefore, the
ENLF model provides a potential approach for a universal algorithm of the atmospheric correction of satellite
remote sensing data.
1. Introduction
While the Rayleigh scattering radiances can be accurately determined
for use in the atmospheric correction of satellite ocean color
remote sensing data, the aerosol scattering reflectance is the main uncertain
term to be determined. Normally, measurements in two
near-infrared (NIR) bands are used to retrieve the aerosol properties
on the assumption of zero water-leaving reflectance at these bands.
However, this assumption is not valid in either Case 2 or highly productive
Case 1 waters, leading to the failure of the atmospheric correction
(Mao et al., 2012; Wang & Shi, 2005). Several approaches
have been developed to overcome this problem for relatively ideal
satellite imagery over turbid waters. These include using the aerosol
scattering reflectance over neighboring clear waters for Case 2 waters
(Hu et al., 2000; Mao et al., 2001), assuming spatial homogeneity of
the NIR band ratio (Ruddick et al., 2000), employing spectral shape
matching methods (Shanmugan & Ahn, 2007), or using a regional
empirical relationship between water reflectance at the red and NIR
bands (Ahn et al., 2012). However, since the spatial and temporal distributions
of aerosols usually vary greatly in the satellite imagery,
these approaches have limited use in operational satellite data processing
systems. The two short wave infrared (SWIR, 1.24 and
1.64 μm) bands were used for atmospheric correction for Moderate
resolution Imaging Spectroradiometer (MODIS) data because the
dark pixel assumption (DPA) was valid in the SWIR bands over turbid
waters (Wang, 2007; Wang & Shi, 2005). However, Knaeps et al.
(2012) pointed out that slight radiance contributions in the 1.24 μm
band were observed in extremely turbid waters and a modification
of the SWIR algorithm was needed.
The ratio of the aerosol scattering reflectance in two NIR bands is
used to obtain the epsilon spectrum on the matchup of aerosol
models. This spectrum is used to extrapolate the magnitudes of the
aerosol scattering reflectance at visible bands from the value at NIR.
According to Wang (2007), when the Sea-viewing Wide Field-ofview
Sensor (SeaWiFS) Band 8 (0.865 μm) is used as a reference,
the epsilon values at Band 1 (0.412 μm) vary between 0.9 (O99
model) and 2.6 (T50 model), implying that the magnitudes of the
aerosol scattering reflectance can vary about 3 fold depending upon
the choice of aerosol models. Therefore, correctly estimating epsilon
becomes one of the critical factors affecting the estimate of the
water-leaving reflectance. More concerning, when the SWIR band
(2.130 μm) is used as a reference band, the magnitudes of epsilon at
0.340 μm vary between 0.7 and 22.6, leading to a more than 30 fold
variation of the aerosol scattering reflectance for the same aerosol optical
thickness (AOT). It should be noted that the accuracy of the atmospheric
correction is much more sensible to epsilon values when
SWIR bands are used and high signal to noise ratio of the sensor is
needed for these bands. In fact, since SWIR bands have in general
lower SNR values than visible and NIR bands (Knaeps et al., 2012),
it obviously limits the accuracy of the atmospheric correction when
the dark pixel assumption (DPA) method of SWIR bands is applied.
The problemof the atmospheric correction in Case 2waters is caused
by that both the aerosol scattering reflectance and thewater-leaving reflectance
become unknown terms. If the water-leaving reflectance becomes
a known term, the aerosol scattering reflectance can then be
extracted and the epsilon values are accurately estimated. Therefore, it
is critical to determine the water-leaving reflectance in the approach
of epsilon estimation for Case 2 waters.
In the standard atmospheric correction procedure, twelve aerosol
models have been used to define the epsilon values for different geometric
angles (Gordon & Wang, 1994). The epsilon value extracted
from the aerosol scattering reflectance in the two SWIR (or NIR)
bands is used to find the two closest aerosol models for interpolation.
Due to amplifying effects of epsilon extrapolated from SWIR to visible
bands, the aerosol scattering reflectance in the visible bands is sensitive
to the small variations of epsilon in the SWIR band. For example,
a small calibration error in MODIS data for the two SWIR bands will
lead to a small error of the retrieval epsilon in those bands (Wang &
Shi, 2005), but the error may be amplified by 10 times for other
bands. If the epsilon spectra over all bands could be obtained and
used to find the two closest aerosol models, the results would become
much more robust.
In this paper,we developed a newapproach to estimate epsilon that
is suitable for both turbid coastal regions and oceanic waters. The AOT
measurements from the Aerosol Robotic Network (AERONET) are
used to analyze the relationship of the aerosol scattering reflectance
among different bands in Section 3. A look-up table of the typical
water-leaving reflectance is established using the in situ measurements
fromthe East China Sea (ECS) in Section 4. A detailed description of the
epsilon approach is given in Section 5. The performance of the approach
is evaluated in Section 6, with the first subsection (Section 6.1) analyzing
simulated reflectance data, the second subsection (Section 6.2)
comparing results applied to SeaWiFS data with the standard method,
and the third subsection (Section 6.3) comparing the match-up between
SeaWiFS data and in situ AOT results, and the accuracy of epsilon
is evaluated. Comparisons are made among different values of the
water-leaving reflectance in the look-up tables presented in the Discussion
section (Section 7).
2. The epsilon problem of the standard atmospheric correction in
Case 2 waters
It is well known that the processing of satellite data using the
standard atmospheric correction usually fails over turbid coastal regions.
One of the main reasons is the difficulty in accurately determining
the epsilon values because the water-leaving radiance in the
two NIR bands are usually much higher than zero in the coastal waters.
An example of epsilon computed from a SeaWiFS image using
the DPA method is shown in Fig. 1.
In coastal regions, the epsilon values in the NIR band are around
1.4, which is roughly 30% higher than its mean value. If these values
are used to extrapolate epsilon to the other wavelength bands, the
value in SeaWiFS Band 1 (0.412 μm) becomes 7.63 which may overestimate
the aerosol scattering reflectance by at least 4.5 times of its
actual value. The overestimation of the aerosol scattering reflectance
will lead to significantly underestimating the magnitudes of the
water-leaving reflectance in visible bands, even generating some negative
values (Shanmugan & Ahn, 2007; Shi &Wang, 2007). The failure
of the atmospheric correction leaves lost regions in the satellite imagery,
over 52,000 km2 in this image. These regions cover the estuary
and coastal zones, very important for the interaction between the terrestrial
and marine environments.
This overestimation is easily detected and its primary cause is that
the water-leaving reflectance is wrongly attributed as part of the
aerosol scattering reflectance in the two NIR bands. There are also
other factors that affect the estimation of epsilon in NIR band, such
as the bias of satellite sensor calibration, uncertainty of the Rayleigh
scattering reflectance, and absorption effects of atmospheric gases.
For example, the O2 A-band absorption may reduce the SeaWiFS measured
reflectance in Band 7 by more than 10–15% (Ding & Gordon,
1995; Wang, 1999). These factors definitely affect the accuracy of
the epsilon estimates, although they are usually difficult to be
detected in satellite imagery. However, due to the amplifying effects
of the extrapolation of epsilon, a small bias in epsilon in the NIR
band may easily lead to a relatively large error in the water-leaving
reflectance at shorter wavelengths. An uncertainty of 5% is assumed
to exist in the NIR band. It then causes the choice of different aerosol
models with a variation of epsilon 0.77 at Band 1, 34% higher than the
true value. A poor selection of the aerosol models may also produce
artifact errors in the processed products (Wang, 2003b). It is important
to establish a suitable approach to accurately estimate the epsilon
for the atmospheric correction.
3. Nonlinear fit of the aerosol scattering reflectance from AOT of
AERONET measurement
As epsilon is used to extrapolate the aerosol reflectance to visible
bands, it is important to understand the relationship of epsilon values
over different bands. The Angstrom constant is usually used to estimate
the epsilon relationship (Gordon & Castano, 1987). However,
Wang and Gordon (1994) concluded that an exponential relationship
would be more accurate for extrapolating epsilon to the other bands.
It is necessary to precisely determine the relationship of epsilon in
visible bands to the NIR band. The AOT dataset of AERONET was
compiled from global distributed data, providing an excellent data
source to analyze the aerosol properties.
AERONET is a ground-based, optical aerosol monitoring network
used to validate the remote sensing algorithmand to ensure the quality
of aerosol products (Holben et al., 1998). The network hardware consists
of automatic Sun–sky scanning radiometers to measure both the
direct and diffuse radiance,which can be used to retrieve aerosol optical
properties such as the AOT, aerosol particle size distribution, complex
refractive index and single scattering albedo (Dubovik et al., 2000).
Data quality control is employed to eliminate errors caused bymeasurement
conditions, such as clouds and biomass burning events (Smirnov
et al., 2000). AERONET has been extended to other applications. For example,
the ocean color component of AERONET (AERONET-OC) has
been used to support long-term satellite ocean color investigations of
autonomous radiometer systems deployed on offshore fixed platforms
(Zibordi et al., 2009a,b).
We obtained AOT data from the AERONET project web page
(http://aeronet.gsfc.nasa.gov). There are a total of 554 measurement
sites in the world, with a total of 7.69 million AOT spectra having
been obtained. Measurements are taken in 16 wavelength bands
ranging from 0.340 to 1.640 μm, with 13 bands selected for wavelengths
from 0.412 to 1.020 μm. Some sites started measurements
from 1999 to now, whereas some just came on-line in 2010. The
number of AOT spectra at each site varies widely, from a maximum
of 65,530 spectra at one site down to a minimum of 12. Because several
different types of instruments are used among these sites, the
spectra bands of AOT measured are different from one site to another.
The site locations are distributed over islands, coastal areas, inland
sites and mountains. Because our focus is on the aerosol types around
the ocean, the 165 sites selected are at altitudes less than 50 m above
sea level with most located near the coastal regions. Some statistical
information on the chosen sites is listed in Table 1.
The mean of the AOT over all spectra is around 0.2, with values decreasing
from0.265 to 0.112 as the wavelength increases. The standard
deviation (STD) of AOT shows a similar pattern to themean values. The
5% lower tail of the total spectral distribution occurs around 0.03 with
minimaclose to zero. The 95% upper tail of the total spectral distribution
occurs around 0.5 with the maxima exceeding 2.0. Since most sites
measured the AOT at 0.440, 0.500, 0.675, 0.870 and 1.020 μm, a set of
AOT collected from 120 sites was established with more than 1.48 million
spectra in these five bands.
According to Gordon and Castano (1989), the aerosol single scattering
reflectance can be calculated from the AOT spectra as:
where ρAS(λ) is the aerosol single scattering reflectance at the top of
the atmosphere (TOA), τa(λ) is the aerosol optical thickness measured
by AERONET, ωa(λ) is the aerosol single scattering albedo,
and Pa(λ) is the aerosol scattering phase function. As many of the
AOT values are higher than 0.1, multiple scattering effects need to
be considered. The aerosol scattering reflectance can be obtained
from where a, b and c are the coefficients of the least squares polynomial fit
which depend on the aerosol models.
As the spectral variations of the AOT of AERONET are quite similar,
it is possible to find a simple equation to describe them. For example,
ρA(λ) is usually assumed to follow the Angstrom law:
where n is the wavelength independent Angstrom exponent, λi is the
wavelength of AERONET band i and λ0 is the reference bandwavelength.
Using the aerosol models from the LOWTRAN-6 models, Wang
and Gordon (1994) concluded that the exponential equation provides
more accurate results than the Angstrom law:
where c is also a constant that depends on the aerosol models chosen
and the viewing geometry.
The extensive measurement set from AERONET provides an excellent
data source for evaluating spectral variations of the aerosol scattering
reflectance. To compare the quality of the spectral fitting functions
for both the Angstrom law and an exponential equation, four aerosol
scattering reflectance of the Mean, Mean plus STD (MS), 5% (P5) and
95% (P95) are selected. The results based on the Angstrom law are
shown in Fig. 2a and those based on the exponential equation are
shown in Fig. 2b.
There are some differences between the observed aerosol spectra
and the fitted spectra for both the Angstrom law and the exponential
equation. The mean relative errors of the P5, Mean, MS, and P95 for
the Angstrom fit are 1.21%, 1.41%, 3.53%, and 0.95%, respectively,
while those for the exponential fit are 4.02%, 4.05%, 6.32%, and
3.71%, respectively. Comparing the fitted situations in Fig. 2, The Angstrom
law is more suitable to describe the spectral shapes of the aerosol
scattering reflectance than the exponential equation.
The nonlinear fit is applied to individual spectrum of AOT and the
mean relative error is calculated from every site, with the results
shown in Fig. 3. The mean relative errors of the fitted aerosol scattering
reflectance are relatively small with most values less than 0.5%.
Half of the sites have errors that are less than 0.16% and 0.30% for
the Angstrom and exponential fits, respectively, with the mean
value of errors of 0.27% and 0.43%, respectively.
When the fitting function is applied to individual spectra the mean
errors become much smaller. The overall average error of the Angstrom
fit is 0.29% and of the exponential fit is 0.47%. Therefore, both
the Angstrom law and the exponential equations provide suitable
fits to the aerosol scattering reflectance data, and because it has a
slightly lower mean error the Angstrom law is used in the remainder
of this paper.
4. The look-up table of in situ water-leaving reflectance
Four cruises were conducted to directly measure the water-leaving
reflectance in the summer and winter of 2006 and in the spring and autumn
of 2007. Each cruise employed two ships (Haijian 46 and Haijian
49) and took about two months to complete the 514 pre-planned stations
in the ECS. The reflectancewas measured by a hyper-spectral spectrometer
manufactured by Analytical Spectral Devices Inc. (ASD). The
ASD instrument was operated following the protocol of the abovewatermethod
(Mueller et al., 2000). As the reflectance can only bemeasured
in the daytime, the actual number of measurement stations was
136, 48, 132, and 143 during the summer, winter, spring, and autumn
cruises, respectively. The spatial distribution of stations is shown in
Fig. 4.
The ASD instrument measures the downward irradiance incident
on the top of a gray plate, the reflected radiance from the sea surface,
and the sky radiance. Each radiance measurement consists of a group
of 20 continuous, simultaneous spectra which can be used to help detect
and remove spectral results contaminated by changes in the environmental
conditions. The methods of Hooker et al. (2002) were
followed to obtain the water-leaving reflectance. The wavelengths
measured by the ASD are from 0.325 to 1.075 μm and the reflectances
in eight bands are computed by the spectral averaging method
according to the SeaWiFS response functions (Mao et al., 2012). A
total of 1496 spectra were measured and 100 spectra were selected
to establish a look-up table of the water-leaving reflectance (Fig. 5).
To make the water-leaving reflectance in the look-up table represent
different types of water in the ECS, a method of selection was developed.
The in situ measured spectra were sorted by the magnitudes
of the reflectance and 100 spectra were selected according to their
position in the magnitude distribution from 0.5% to 99.5% with a 1%
increment. From Fig. 5, the range of the reflectance is large in all 8
bands, even the two NIR bands. The reflectance in Bands 7 and 8
ranges from 0 to 0.06 sr−1, which exceeds the variability of the reflectance
even at visible bands. Obviously, the assumption of zero
water-leaving reflectance is invalid for most of these spectra. These
spectra are used to establish the look-up table of the water-leaving
reflectance to determine the distribution of the actual reflectance in
the ECS.
5. A new approach to estimate the epsilon value from satellite data
The Rayleigh scattering reflectance, the sun glint and whitecap reflectance
can be computed in the atmospheric correction procedure
of satellite data. When the water-leaving reflectance in the two NIR
bands is assumed to be zero, the aerosol scattering reflectance can
be obtained and used to compute epsilon. However, over turbid waters,
this assumption is not valid and its use leads to errors in the estimation
of the epsilon. On the other hand, if the water-leaving
radiance can be determined, the epsilon can be estimated from the
aerosol scattering reflectance over turbid waters.
We define the aerosol–water reflectance, ρAW(λ), including the
aerosol scattering reflectance and the water-leaving reflectance at
TOA, as:
where ρA(λ) is the aerosol scattering reflectance (including Rayleigh–
aerosol interactions) and ρw(λ) is the water-leaving reflectance at
TOA. The aerosol–water reflectance can be deduced from the following:
where ρt(λ) is the satellite measured reflectance at the TOA, ρr(λ) is
the Rayleigh scattering reflectance, T(λ) is the diffuse transmittance
of the atmosphere, and t(λ) is the direct transmittance of the atmosphere.
The term ρwc(λ) is the ocean whitecap reflectance, and
ρg(λ) represents the effects of sun glitter off of the sea surface. The
Rayleigh reflectance can be computed exactly from the Rayleigh lookup
tables based on the vector radiative transfer theory (Wang,
2003a). The whitecap reflectance can be modeled from the sea surface
wind speed (Moore et al., 1999). The sun glint reflectance can
be computed from the sea surface slope distribution (Cox & Munk,
1954). Therefore, ρAW(λ) can be accurately estimated by Eq. (6).
When the water-leaving reflectance is known, ρA(λ), which can be
obtained from the aerosol–water reflectance using Eq. (5), can be
used to estimate the aerosol single scattering reflectance from
where a, b and c are the same coefficients as in Eq. (2). The epsilon spectrum
of the aerosol single scattering reflectance is then defined as
The approach to estimate the epsilon value from the reflectance at
TOA can be summarized as follows:
(1) The Rayleigh scattering reflectance is accurately computed from
themultiple scatteringmethod and the reflectance caused by the
sun-glint and whitecaps is estimated. Then, the aerosol–water
reflectance is obtained from the satellite measured reflectance
using Eq. (6).
(2) A look-up table of the water-leaving reflectance is established
from in situ measurements over the ECS to represent different
types of reflectance.
(3) The candidate aerosol scattering reflectance can be obtained by
subtracting the aerosol–water reflectance from all water-leaving
reflectance in the look-up table, fromwhich with negative values
are removed. The number of the candidate aerosol scattering reflectance
is the same as the reflectance in the look-up table if no
spectra are removed.
(4) The nonlinear least squares fit function based on the Angstrom
lawis used to obtain the best fitted spectrumfor every candidate
aerosol scattering reflectance to compute the difference. The one
with the smallest difference is selected as the actual aerosol
scattering reflectance.
(5) The aerosol single scattering reflectance is estimated from the
aerosol scattering reflectance using Eq. (7), which is then used
in Eq. (8) to obtain the epsilon spectrum.
(6) The epsilon spectrum is used to match the two closest aerosol
models for interpolation to obtain the actual epsilon values.
Matching of the entire spectrum instead of just one value in the
NIR band provides a more stable estimate of epsilon.
Overall, this approach to estimate the epsilon primarily relies on
the assumption of the aerosol scattering reflectance following the
Angstrom law by using the non-linear fit function of the reflectance
under the help of the look-up table of the water-leaving reflectance,
named as the ENLF model.
6. Evaluation of the ENLF model
6.1. Evaluating the ENLF model using the simulation method
The sensor reflectance at the TOA can be simulated from the sum
of its components (Mao et al., 2010). To evaluate the performance
of the ENLF model, only the aerosol–water reflectance needs to be
simulated. The water-leaving reflectance at the TOA can be estimated
from the spectra of the look-up table using:
ρwðλÞ ¼ tðλÞρwnðλÞ cos θs ð Þ; ð9Þ
where ρwn(λ) is the normalized water-leaving reflectance from the
look-up table, θs is the solar-zenith angle and t(λ) is the direct transmittance
of the atmosphere which can be estimated by the method of
Gordon and Wang (1992). The aerosol scattering reflectance in the
reference band is calculated using Eq. (1) with an AOT of 0.12 and
the reflectance in other bands are obtained using Eq. (3) with an Angstrom
constant of 1.1. These parameter values are close to the mean
of the AOT in the AERONET. This aerosol scattering reflectance and
epsilon are taken as the reference to test the performance of the
ENLF model for simulated data. The simulated aerosol–water reflectance
at the TOA is obtained using Eq. (5).
Following the procedure of the ENLF model, the aerosol single
scattering reflectance is obtained from the simulated data using
Steps 2 to 5, and then the epsilon is computed. A total of 1496 epsilon
spectra were obtained, and it is infeasible to show them in one figure.
Therefore, five typical epsilon spectra with different relative errors
are selected and shown in Fig. 6, together with the reference epsilon.
In general, most of the epsilon spectra (not shown) are relatively
close to the reference one and are similar to Spectra 2 results. Still,
there are some spectra with larger variability such as the result of
Spectra 3. The aerosol–water reflectance at the TOA is sensitive to
the settings of several parameters and these parameters become unknown
when the reflectance is used to retrieve the epsilon with the
ENLF model. For example, the water-leaving reflectance at the TOA
simulated from the in situ measurements depends upon the transmittance
of the atmosphere, which becomes difficult to be extracted exactly
from the simulated reflectance. This situation will also happen
when retrieving the epsilon from satellite remote sensing data using
the ENLF model.
Most of the retrieved epsilon spectra followthe Angstromlaw,while
some shapes of epsilon modulate according to the wavelengths, for example,
Spectra 1, 3 and 5 in Fig. 6. These spectra need to be corrected.
Since the epsilon shapes in the aerosolmodels can be taken as the actual
spectra distribution of the aerosol scattering reflectance, the retrieved
epsilon spectra are used tomatch the two closest aerosol models for interpolation
to obtain the actual values. The 12 aerosol models are defined
as O99, M50, M70, M90, M99, C50, C70, C90, C99, T50, T90 and
T99, where the symbol O, M, C, and T refer to oceanic,maritime, coastal
and tropospheric models, respectively, and the numbers designate the
relative humidity. These models produce the epsilon spectra in the
SeaWiFS bands under different solar-sensor geometry. Normally,
these epsilon spectra are used to extrapolate the values in visible
bands depending on the selection of the aerosol models in the NIR
band. The selections are based on the match-up conditions of epsilon
between the measured values from the remote sensing data and the
modeled values. As the spectra of epsilon are obtained, they can be
used to match the two closest spectra of the aerosol models instead of
one value in the NIR band. These two aerosol models are used to
interpolate the actual epsilon values. The five spectra in Fig. 6 are used
to test this match-up method and the results are shown in Fig. 7,
superimposed on the original ones.
Fig. 7 shows that the match-up method of epsilon can significantly
modify the shape of the epsilon spectra and improve the accuracy of
the retrieved epsilon. For example, the retrieved Spectrum 3 is obviously
abnormal from the shape of epsilon and it becomes similar to
the reference one after the use of the match-up method. The relative
error of the retrieved Spectrum 3 is 12.5% and it is 2.3% for the
matched epsilon. As the satellite data are affected by many different
measured conditions, it is difficult to ensure that all reflectance are
perfect. The retrieval spectra depend on many factors; such as the
matched situation of the water-leaving reflectance between the actual
values and the look-up table, the calibration accuracy of the satellite
sensor, the Rayleigh scattering reflectance and the effects of
atmospheric absorption. In fact, some abnormal spectra similar to
Spectra in Fig. 6 are usually found in the satellite imagery. The
match-up method is useful in removing some abnormal variations
of the spectra. The results in Fig. 7 show that the matched epsilon
does follow the Angstrom law closely and fit the epsilon spectra produced
by the aerosol models. As a consequence, the accuracy of the
epsilon estimate can also be improved.
Normally, only one value of epsilon in the NIR band is used to determine
the two closest aerosol models and to extrapolate to other
values in the visible bands. However, the results depend directly on
the accuracy of the aerosol scattering reflectance in the two NIR
bands. For example, the matched epsilon value of Spectrum 3 at
Band 1 (0.412 μm) will be 7.5, 3.4 fold of the true value by the one
value matched method. The mean relative error of the spectrum is
111.1%. The variations of the five spectra at Band 1 are between
1.95 and 7.49, much higher than the results of spectra matched method
which range from 1.54 to 2.34. A small error in the reflectance will
easily lead to a bias of epsilon in the NIR band and an amplified error
in reflectance at shorter wavelengths. If the entire epsilon spectrum
instead of one value at the NIR band is used to determine the two
closest aerosol models, the results are more robust. One advantage
of the ENLF model is that it matches the two aerosol models by
matching the entire epsilon spectrum and thus improves the accuracy
of the epsilon values.
6.2. Evaluating the ENLF model using SeaWiFS imagery
The ENLF model was used to process the SeaWiFS imagery and an
example of an epsilon image in the NIR band is shown in Fig. 8. This
image is produced from the SeaWiFS data from March 27, 2007 and is
the same data used in Fig. 1. To produce this image, first, the Rayleigh
scattering reflectance was computed using a multiple scattering approach
together with the reflectance components from the ocean
whitecaps and the sun glitter off of the sea surface. The aerosol–water
reflectance was obtained from the satellite measured reflectance using
Eq. (6). Finally the aerosol scattering reflectance and the epsilon spectra
were obtained from the ENLF model
From the refined epsilon distribution in Fig. 8, we can see that the
high values of epsilon in the coastal regions are significantly reduced,
with values more closely matching to Case 1 waters. The values in the
NIR band are reduced from around 1.4 to 1.05 in the coastal regions.
To compare the differences of the epsilon spectra between Figs. 1
and 8 in detail, the whole region of the image is classified into 4 classes
according to the range of the epsilon values at Band 7 by the DPA
method. Class 1 represents the range of 1.0 to 1.1, 2 is from 1.1 to 1.2,
3 is from 1.2 to 1.3 and 4 is from 1.3 to 1.5. The epsilon spectra belonging
to the same class are averaged to obtain the mean spectra
of this class, which is shown in Fig. 9.
From Fig. 9, the mean epsilon values between the ENLF model and
the DPA method are large. The ranges of the epsilon values by the
ENLF model are from 1 to 1.7, while those by the DPA method are
from 1 to 5.7. The values by the DPA significantly increase when the
wavelengths shift from the NIR to the blue band, while those by the
ENLF keep much more stable. The spectra by the DPA are significantly
different from Classes 1 to 4, while those by the ENLF are similar.
Checking the true image (not shown in this paper) produced from
SeaWiFS data of this day, the aerosol distributions in the image are relatively
homogeneous. The epsilon values should be close to each other
among the 4 classes. As a result,we conclude that the ENLFmodel is suitable
for application in coastal regions to overcome the overestimation
problem for the epsilon.
Checking carefully the epsilon image in Fig. 8, we can see that this
image shows some information on the spatial structure of the waterleaving
reflectance. However, because the number of the waterleaving
reflectance in the look-up table is limited, there may be a small
difference between the actual water-leaving reflectance and the selected
one from the look-up table. This difference will cause a small error
in extracting epsilon. The match-up method of the ENLF model can improve
the accuracy of epsilon in different bands, but it cannot remove
the effects caused by this difference. It is important to build up a good
look-up table of thewater-leaving reflectance to cover enough actual reflectance
of the regions. It is also necessary to evaluate how the look-up
table of the reflectance affects the accuracy of the epsilon values.
6.3. Evaluating the ENLF model using in-situ measurements
The globally distributed AERONET data can be used to validate the
aerosol products derived from satellite ocean color remote sensing
data such as SeaWiFS (Mélin et al., 2010). The epsilon can be estimated
from the AOT data to validate the results from SeaWiFS data with the
ENLF model. The AOT was measured during the 2006 winter and 2007
autumncruises in the ECS using a handheldmulti-band sun photometer
(MICROTOPS)manufactured by the Solar Light Company. Two different
types ofMICROTOPS instrumentswere used. One is used tomeasure the
total ozone column, the water vapor column and the AOT at 1.020 μm.
The other measures the AOT at the five wavelengths of 0.380, 0.440,
0.500, 0.670 and 0.870 μm. A total of 17 stations during the winter
cruise and 18 stations during the autumncruisewere measured. The instrument
was connected to a GPS system to record the locations and
was used during the transit between stations, significantly increasing
the number of AOT spectra. A total of 4233 spectra were measured at
the locations shown in Fig. 2.
To compare the epsilon values between SeaWiFS and AOT, a
matched data set was assembled using a time window for the satellite
overpass of ±2 h from the in situ measurements. A total of 263
matching spectrawere obtained,withmany SeaWiFS reflectances missing
due to excessive cloud or fog cover. After applying a data quality
check, a set of 49 matched spectra was obtained under the cloud-free
condition of SeaWiFS data. The SeaWiFS reflectances were processed
to obtain the epsilon using the ENLF model and the results are shown
in Fig. 10. The AOT spectra were used to calculate the aerosol scattering
reflectance using the geometric angles of SeaWiFS, to estimate the single
scattering reflectance and finally to obtain the epsilon values which
are also shown in Fig. 10.
Fig. 10 shows that the two types of epsilon are relatively comparable.
While the magnitudes and the spectra shapes are similar to each
other, there are some important differences. Four epsilon spectra of
AOT with green color look abnormal because of their large slopes between
band 1 (0.380 μm) and band 3 (0.500 μm), which are three
times higher than the mean value. The other five spectra have the
similar characteristics, but with relatively small slopes. The slopes of
the AOT epsilon also show some difference from the SeaWiFS epsilon,
especially for the shorter wavelengths.
Because the processing of the data from AOT to epsilon is rather
complicated with several estimated parameters, the accuracy of the
AOT epsilon itself naturally exists some uncertainty. As the AOT data
of AERONET are retrieved from the ground based radiance measurements,
the data quality of AOT can also be affected by the instrument
calibration and the measurement conditions (Dubovik et al., 2000;
Smirnov et al., 2000). The translation fromAOT to the aerosol scattering
reflectance is related to the parameters of the aerosol single albedo and
its phase function, which are determined by the absorption, as well as
the type and particle size distribution of aerosols. The coefficients in
Eq. (3) are determined from the aerosol models, so they rely on different
aerosol models. These parameters were not measured and typical
values were used in the computation of epsilon from the AOT. Finally,
horizontal distributions of aerosolsmay change and a 2-hour timewindow
obviously brings some differences between SeaWiFS and in situ
measurements. Considering these inherent limitations in obtaining
both of the epsilon values, we are satisfied with the results of the comparison
of the two epsilon spectra.
7. Discussion
We find that the epsilon values resulting from satellite data strongly
depend upon the accuracy of the look-up table from the in situmeasured
reflectance. The epsilon can be estimated accurately when the look-up
table covers the actual water-leaving reflectance. Some errors are produced
by the ENLF model when only the closest reflectance is searched
out from the table with some difference between this reflectance and
the actual value. To evaluate how the table affects the accuracy of the epsilon,
a test wasmade. The test was designed to compute the accuracy of
the epsilon using different look-up tables based on different quantities of
the water-leaving reflectance. Seven tables were established including
10, 20, 50, 100, 200, 500, and 1000 spectra from the same data set of
the in situ reflectance. The method to establish these tables is that the
spectra are randomly selected with unequal distribution, different from
that in Section 4. All reflectance were used to obtain the aerosol–water
reflectance at TOA using the simulation method in Section 6.1. The epsilon
spectra were estimated from the aerosol–water reflectance by the
ENLF model. The accuracy of the epsilon was computed on the reference
epsilon in Fig. 6 to obtain the mean relative errors shown in Fig. 11.
Fig. 11 is the result of relative errors of the epsilon from7 look-up tables.
It is clear that the errors varywidely among different tables. For example,
when the table only includes 10 spectra, most of the actual
reflectance cannot be found from the table with some large differences
fromthe selected spectra. The mean relative error of the retrieved epsilon
is 64.4%, with the maximumerror of 89% at Band 5.When the numbers
of spectra in the table increase, more actual reflectance can be
found in the table and meanwhile the difference between the actual reflectance
and the selected spectrumbecomes smaller, leading to smaller
errors. The mean relative error is 2.34% when the table includes 1000
spectra. Therefore, the accuracy of the epsilon depends on the look-up
table of the water-leaving reflectance.
The look-up table of the in situ reflectance is the basis for estimating
the epsilon fromthe satellitemeasured reflectance at the TOA using the
ENLF model. As the actual water-leaving reflectance is unknown before
the epsilon is estimated, it is determined from the look-up table with
the best nonlinear least squares fit function using the Angstrom law of
the aerosol scattering radiance. If the actual reflectance exists in the
table, the epsilon can be exactly estimated. Otherwise, a difference between
the actual reflectance and the matched spectra introduces a
bias of epsilon using the ENLF model. If the model is used in other regions,
it is important to obtain enough in situ water-leaving reflectance
data over different seasons in these regions to obtain good estimates.
The atmospheric correction over land is still difficult to establish
with high accuracy. The ENLF model provides a new practical method
to estimate the epsilon spectra for the satellite imagery over land
cover. It depends upon neither the two atmospheric correction bands
nor the dark pixel assumption. It only assumes that the aerosol scattering
reflectance follows the Angstrom law and this assumption is also
valid over land. It depends on the look-up table of in situ reflectance
to separate the aerosol scattering reflectance from the satellite measured
reflectance. The accuracy of the model depends on the distribution
of the reflectance in the look-up table. When a well designed
look-up table is established with different reflectance to represent all
land cover types in different seasons, the ENLF model can then be applied
to the atmospheric correction of satellite remote sensing data
over terrestrial regions.
8. Conclusions
Using awell designed look-up table of thewater-leaving reflectance,
the aerosol scattering reflectance can be determined using the best
non-linear least squares fit function based on the Angstromlaw. The reflectance
is then used to estimate the aerosol single scattering reflectance,
which is then used to determine the epsilon values. The ENLF
model can improve the accuracy of the atmospheric correction of satellite
remote sensing data over turbid coastal waters. The assumption of
the atmospheric correction is changed from the dark pixel assumption
to that the aerosol scattering reflectance following the Angstrom law.
As this new assumption is valid for oceanic, turbid waters and the terrestrial
regions, the ENLF model provides a new potential universal approach
of the atmospheric correction of satellite remote sensing data.
Furthermore, the two NIR atmospheric correction bands are not used
for the ENLF model, which can then be used to estimate the total
suspended matters over turbid waters.
The epsilon spectra are used to determine the two closest aerosol
models instead of relying on one epsilon value from the NIR band. The
results show that this match-up method is more robust and overcomes
the epsilon error caused by small variations of the aerosol scattering
reflectance in the two NIR bands. Meanwhile, this method can
also reduce some abnormal epsilon errors caused by factors including
the data quality of satellite measured reflectance and the mismatch of
the actual water-leaving reflectance with the spectra in the look-up
table. Further, it can also make the shape of the epsilon spectrum
more closely follow the Angstrom law.
The performance of the ENLFmodelwas evaluated by using simulated
reflectance at the TOA, SeaWiFS imagery, and in situ measured AOT
in the ECS. The results show that it can work well under the help of a
properly designed look-up table of the water-leaving reflectance. This
approach can be applied in an operational satellite remote sensing
data processing systemto improve the accuracy of the atmospheric correction
over turbid coastal regions and terrestrial environments.Title:
Defning active, inactive, and extinct sea?oor massive sulfde deposits

Author:
J.W. Jamieson, A. Gartman

Abstract
Hydrothermal activity results in the formation of hydrothermal mineral deposits, including sea?oor massive
sulfde deposits, at oceanic spreading ridges, arcs, and back-arcs. As hydrothermal systems age, the mineral
deposits eventually become severed from the heat source and ?uid-?ow pathways responsible for their formation
and become extinct. The timescales and processes by which this cessation of activity occurs, and the resultant
distinction between hydrothermally active and inactive deposits has recently taken on policy implications related
to the potential issuance of exploitation leases for sea?oor massive sulfde deposits by the International Seabed
Authority in Areas Beyond National Jurisdiction. Here, we discuss the scientifc rationale behind designating
hydrothermal systems as active, inactive, or extinct, with the aim of applying a scientifc underpinning to
ongoing policy discussions, which often lack a common set of criteria and use the same descriptions for opposing
phenomena. We apply the simple defnition that active vent felds currently exhibit ?uid ?ow above ambient
seawater temperatures, inactive vent felds are not currently exhibiting ?uid ?ow but may potentially become
active again, and extinct vent felds are not expected to become active again. We suggest these terms can only be
correctly applied at the vent feld scale and defne a vent feld as a geologically continuous entity that may
include both actively and formerly venting hydrothermal deposits. Finally, we propose criteria and techniques
for determining activity and reasonably bounding the extent of a vent feld for classifcation purposes.

Introduction
Hydrothermal vents are sites of ?uid discharge on the sea?oor that
occur along or associated with submarine tectonic boundaries such as
mid-ocean ridges and subduction zones. In these geological settings,
magmatic heat sources beneath the sea?oor drive convective circulation
of hydrothermal ?uids along permeable pathways in the crust such as
faults and fssures (Fig. 1) [1]. If the hydrothermal vents are discharging
high-temperature (>~250 ?C), metal- and sulfur-rich ?uids, a sea?oor
massive sulfde (SMS) deposit may develop over time on and/or
immediately below the sea?oor from the precipitation and accumulation of metal-rich minerals that precipitate from the ?uid when it mixes
with cold seawater [1,2]. In rare places, where these
geologically-favorable conditions persist for an extended period of time,
these deposits can grow to be large enough and contain high enough
concentrations of Cu, Zn, Au, and/or Ag to be potentially economically
viable sources for these metals and targets for the emergent industry of
deep-seabed mining [3,4]. Eventually, hydrothermal ?uid venting at
any given site will cease, and the site will become hydrothermally
inactive and ultimately extinct. Currently, very little information is
available for inactive or extinct SMS deposits. InterRidge maintains an
online ※Vents Database§ of all known and inferred hydrothermal systems discovered to date [5]. The current version (v. 3.4) contains 707
individual records of ※submarine hydrothermal activity§, with only 54
of these records listed as ※inactive§. The paucity of documented inactive
sites is largely because inactive deposits are diffcult to fnd as they do
not have an associated hydrothermal plume, the detection of which is
the primary exploration tool for SMS deposits [6]. The scientifc focus on
active hydrothermal vent sites, which host unique chemosynthetic taxa
and which are locations of scientifc interest beyond the economic interest driving SMS exploration further contributes to the low number of
known inactive sites [7每9]. However, there is an increasing focus on
inactive SMS deposits driven by the possibility of marine mining.
In Areas Beyond National Jurisdiction (ABNJ), any mining of SMS
deposits (also referred to as polymetallic sulfde deposits) will be
managed by the International Seabed Authority (ISA). As the ISA moves
closer toward fnalizing exploitation regulations for mining in ABNJ,
discussion of environmental considerations, including what
environmental protections are needed, has been a key component. A ban
on mining of active SMS deposits has been suggested by parts of the
scientifc community, due to the minimal areas of the sea?oor these
vents occupy, their scientifc value, and low estimations of their resource
potential [10每12]. This has been followed by assertions that mining of
active hydrothermal vents would likely take place in a limited manner, if
at all, and that inactive deposits will be the main target of this emergent
industry [13]. However, all current ISA exploration contracts include
areas of active hydrothermal venting and, while only a fraction of each
10,000 km2 exploration area will be subject to exploitation, a workable
defnition for active hydrothermal vents in a manner that could be legally applicable for mining regulations does not currently exist. Here we
propose a classifcation similar to that used for volcanoes, which uses the
designations active, inactive (dormant), and extinct. We consider how this
classifcation can be reasonably applied to hydrothermal systems and
associated SMS deposits, in order to ensure a consistent and meaningful
geologic framework for the evolving regulations. Known active hydrothermal systems are diverse, and many do not result in SMS formation.
Here we discuss those associated with high-temperature sub-sea?oor
?uid circulation that has the potential to mobilize and deposit base and
precious metals as sulfde minerals (e.g., temperatures above 250 ?C)
[14]. We specifcally do not include systems that would not result in SMS
formation, and so exclude low-temperature, low-?ux systems associated
with off-axis areas and seamounts.

Method
2. Terminology
For clarity, we use the following terminology when referencing hydrothermal mineral accumulations on the sea?oor at different scales
(Table 1). Focused sources of high temperature hydrothermal venting
typically emanate from chimneys, or individual, in places interlinked,
vertical pipe-like mineral accumulations that have one or more orifces.
Hydrothermal edifces or mounds are larger structures that form from the
progressive accumulation of hydrothermal material on the sea?oor at
vent sites. Edifces and mounds are defned by their morphology, with
edifces generally having steeper sides, and mounds having a distinct
conical shape. Both structures may host chimneys. The spatial clustering
of chimneys, edifces, and/or mounds on the sea?oor defne a vent feld,
which describes a discrete region that contains hydrothermal accumulations distributed over an area with dimensions that can vary from 10s
to 1,000s of meters (Table 1).
When referring to sea?oor hydrothermal deposits as a resource, we
use well-established language typically used by the land-based mineral
resource sector (Table 2) [15]. If a sea?oor hydrothermal site contains a
spatially continuous accumulation of hydrothermal material of large
enough size to be potentially mineable (i.e. a large edifce or mound), it
is referred to as a deposit (e.g., the Solwara I deposit, in the Manus Basin)
[16,17]. An SMS deposit refers to a subset of hydrothermal deposits that
is composed dominantly of metal sulfde minerals (e.g., pyrite, chalcopyrite, sphalerite; note that the term massive in sea?oor massive sulfde is
a mineralogical textural term to indicate a mass abundance of sulfde
minerals of >~60% and is not a reference to deposit size) [18]. Therefore, the term SMS deposit should be applied only to large accumulations
of hydrothermal minerals (deposits) that are known from sampling or
other methods to be composed largely of massive sulfde material. In
other words, although every SMS deposit is a hydrothermal deposit, not
every hydrothermal deposit is necessarily an SMS deposit. Petersen et al.
[4] estimated that, at probable minimum ore grades (e.g., ~5 wt% Cu,
15 wt% Zn, 5 ppm Au), an SMS deposit would have to be at least 2 Mt to
be an economically viable target for mining. This would correspond to a
massive sulfde mound with a basal diameter of approximately 200 m
and height of 60 m, similar to the active TAG mound [19]. Using these
defnitions, a vent feld may contain no deposits, a single deposit, or
several deposits, some or all of which may or may not be SMS deposits.
Volcanoes are informally classifed as active, dormant, or extinct, where
an active volcano has erupted in the past ~10,000 years, a dormant
volcano has not erupted in the past 10,000 years, but is expected to erupt
again, and an extinct volcano has not erupted in the past 10,000 years,
and is not expected to erupt again [20]. This classifcation can serve as a
guide for the activity of vents felds, where active felds will contain
hydrothermal venting at temperatures above ambient seawater, inactive
vent felds lack apparent activity but were recently active, remain near a
heat source, and have the potential to become active again, and venting
is unlikely to resume at extinct vent felds. The classifcation of a volcano
as dormant, rather than extinct, is normally due to the presence of indicators of heat and/or magmatic activity in the subsurface, such as
seismic events within the volcano, elevated heat ?ow, or presence of
fumaroles. Several examples of the eruption of an ※extinct§ volcano (eg., Fourpeaked Volcano, Alaska) point to the lack of certainty associated
with this informal classifcation, associated with an evolving understanding regarding deep-Earth processes. Likewise, the classifcation of a
vent feld as extinct should not be considered an absolute certainty, but
instead a low probability that hydrothermal venting will reactivate in
the future.

Result
3. Indicators of hydrothermal activity
Defning an individual hydrothermal vent, deposit or vent feld as
active is conceptually simple enough: it is hydrothermally active when
?uid with a temperature above that of ambient bottom water is venting
from the sea?oor. Hydrothermal plume surveys use CTD (Conductivity,
Temperature and Depth) casts that also include optical sensors
(turbidity) and sensors for chemical anomalies (e.g., salinity, Eh, CH4)
associated with the hydrothermal plume in the water column above and
down-current from an active vent site, at distances of up to 100s of kilometers away from their source [6]. Collecting seawater on CTD casts
also allows for the detection of 3He, an unambiguous tracer of hydrothermal activity. Once a plume is detected, the hydrothermal site is
typically visually located using a camera tow or cameras on a Remotely
Operated Vehicle (ROV) or Autonomous Underwater Vehicle (AUV).
Visual signs of venting may be obvious, with vigorous discharge of
high-temperature black or white smoker ?uid, often from multiple vents
(Fig. 2A). Subsurface mixing of ascending hydrothermal ?uid with local
seawater prior to venting also typically occurs, and results in cooler,
diffuse-?ow ?uids that may be shimmering due to temperature and
salinity differences relative to seawater. Both ※focused ?ow§ and
※diffuse ?ow§ styles comprise active hydrothermal venting.
Even when venting is not visibly obvious, the presence of live hydrothermal vent-endemic species is another clear indicator of active
venting. Vent biota have a tendency to colonize even the most diffusely
venting, low-temperature sites, and, due to the vivid colors these organisms may display against a dull sea?oor substrate, can be a particularly effective indicator of hydrothermal activity when ?uid ?uxes
and/or venting temperatures are low (Fig. 2B).
On sites that have been sampled by dredge and therefore lack visual
information regarding sea?oor context, the presence of anhydride
(CaSO4) can be used as a mineralogical indicator of recent hightemperature hydrothermal activity and indicates that the site may still
be active. Anhydrite precipitates when Ca-rich hydrothermal ?uids mix
with and/or heats up Ca- and SO4-rich seawater at temperatures of
above ~150 ?C [21]. However, anhydride has retrograde solubility, and
will dissolve back into seawater when it is no longer exposed to elevated
temperatures. The rate of anhydride dissolution is not well constrained,
and it is therefore not known for how long anhydride remains as a
mineral phase within cold vent deposits. However, surfcial areas of old,
inactive or extinct deposits will not contain anhydride, although it may
remain in areas not in contact with seawater (e.g., at depth) as it is found
occasionally in volcanogenic massive sulfde deposits (VMS, i.e. the
ancient equivalent of SMS deposits) [22].
4. Indicators of hydrothermal inactivity
There are several sea?oor observations that indicate prolonged hydrothermal inactivity at a deposit or vent feld and can therefore be
important criteria for distinguishing inactive sites from extinct sites.
Over long time periods (e.g., 1000s of years) it is likely that inactive
chimneys will eventually collapse. The oldest known vent felds (e.g.,
>40,000 years old) are characterized by a notable lack of upright
chimney structures, and instead are composed primarily of low-relief
mounds (e.g., Peterburgskoe and Semyenov IV on the Mid-Atlantic
Ridge) [23每25]. Chimney collapse occurs due to increasing structural
instability as chimneys grow taller, the dissolution of thermodynamically unstable minerals in seawater, and because most hydrothermal
systems occur in seismically active areas where ground motion can lead
to collapse. However, timescales for chimney collapse may vary for
different deposits according to tectonic and oceanographic setting (i.e.
intensity and frequency of seismic activity; temperature and dissolved
oxygen content in bottom waters), and the mineralogy and morphology
of the deposits.
Oxidation of sulfde minerals within hydrothermal deposits indicates
exposure to seawater. Oxidation begins as soon as the minerals are in
contact with oxygenated seawater and thus oxidation occurs even during periods of active venting. Likewise, the dissolution of anhydride
begins once the precipitated mineral is no longer hot, and therefore
dissolution can occur on cooler parts of otherwise high-temperature vent
structures. Prolonged periods of exposure to ambient, low temperature
seawater can lead to substantial oxidation of sulfde minerals and anhydride dissolution, and both processes may be considered qualitative
indicators of deposit aging, with the important caveat that these transformations do not a priori indicate inactivity. However, over long time
periods loss of these minerals may fundamentally reshape hydrothermal
deposits.
Inactive vents display an absence of chemosynthetic fauna and, if
they have been inactive for long enough, will often host deep-sea slow
growing sessile taxa such as sponges and corals (Fig. 2C). For a more
detailed description of biological indications of activity and inactivity
see Van Dover [26].
When applied independently, the indicators of inactivity described
above (lack of upright chimneys, extensive oxidation, and lack of vent
biological communities) may not necessarily be diagnostic for classifcation of inactivity on their own. However, taken together, these observations provide a reliable, unambiguous indication of prolonged
hydrothermal inactivity.
5. Defning hydrothermal activity at the vent feld scale
Although hydrothermal inactivity may be described at the scale of
individual vents or deposits using the criteria in the preceding section,
defning a vent feld as active, inactive, or extinct is much more complex
because we do not yet fully understand the spatial scale of hydrothermal
circulation cells in the sub-sea?oor. A vent feld is generally described as
a cluster of hydrothermal vents or deposits on the sea?oor. The size,
morphology, and distribution of hydrothermal deposits that make up a
vent feld can vary signifcantly, largely according to tectonic environment, making a simple defnition based on deposit size or footprint
impractical [2]. We use the concept of geological connectivity to defne
the extents of individual vent felds based on a common magmatic heat
source and subsea?oor permeability structure, such as normal faults or
detachment faults associated with rifting along mid-ocean ridges, or ring
faults associated with caldera collapse within arc volcanoes [27每29].
Using this defnition, any spatially associated currently or formerly
active vent sites and associated chimneys, edifces and/or mounds that
are geologically connected would be considered part of a single vent
feld. Each vent feld would be subject to a single classifcation of activity
that would apply to the entire feld, and any active vent feld may
contain more than one hydrothermal circulation cell. The evaluation of
geological connectivity at a specifc site would be possible by interpretation of geological features on the sea?oor (e.g., faults, volcanic centers) identifed through ship-based and high-resolution bathymetric
mapping (e.g., ~40 m and 1 m resolution, respectively). These datasets
would commonly be generated as part of any exploration and resource
evaluation program.
The Endeavour Segment of the Juan de Fuca mid-ocean ridge provides an illustrative example of defning the limits of individual vent
felds using the concept of geological connectivity. The Endeavour
Segment is described as containing fve major active vent felds that
occur along 15 km of the ridge axial valley, each separated by 1.5每2 km,
with several smaller felds, and other sites of diffuse venting occurring
near or between the major active felds (Fig. 3A) [30]. However, this
current clustering of active venting represents only a present-day
snapshot of the evolution of hydrothermal circulation along this 15 km
length of ridge segment. This stretch of ridge axis also contains over 400
inactive chimneys, edifces and mounds that occur outside of the active
vent felds and record over 3000 years of spatially and temporally dynamic venting at Endeavour (Fig. 3A) [31,32]. Seismic imaging along
the ridge axis reveals that this venting was, and continues to be, driven
by a continuous axial magma chamber beneath the ridge segment [33].
The vents occur in close proximity to the along-axis normal faults that
defne the axial valley and are likely an important control on subsurface
permeability and hydrothermal ?uid ?ow. Thus, although the active
vent clusters each likely represent currently discrete hydrothermal circulation cells, the entire 15 km length of axial valley represents a
geologically continuous zone of present or recent hydrothermal venting,
driven by a common heat source and structurally controlled permeability regime. Endeavour should therefore be considered as a single
continuous vent feld, and, consequently, since parts of Endeavour are
hydrothermally active, the entire 15 km long vent feld should be
considered active.
The fve Semyenov vent felds associated with the 13?30＊ oceanic
core complex on the Mid-Atlantic Ridge provide a contrasting example
to Endeavour. Semyenov 1-5 are roughly equally spaced along the axis
of the core complex (Fig. 3B) [34]. Apart from Semyenov 3 and 5, which
are closely spaced and are geologically similar, these vent felds have
different morphologies, compositions, and local tectonic controls and
substrates. All vent felds are separated by stretches of substrate that
show no indication of past hydrothermal activity. Here, it is appropriate
that each vent feld be considered as separate from the others, and
individually classifed as active, inactive, or extinct, as there is no
indication of geological continuity between the fve Semyenov felds
(again, except for Semyenov 3 and 5, which should be grouped
together). Based on extensive plume surveys and several ROV dives,
only Semyenov 2 is known to be active, and Semyenov 1, 3, 4, and 5
appear to be extinct [35,36].
The TAG hydrothermal feld presents a more challenging case study.
The hydrothermal mounds that defne the TAG feld are distributed over
a ~8 km2 area on the eastern edge of the axial valley (Fig. 3C) [38].
Some of the mounds coalesce with adjacent mounds (e.g. Double
Mound) or are directly next to each other (e.g., Shinkai Mound and New
Mounds #2 and #3; Southern Mound and Rona Mound). Other mounds
appear to be isolated deposits more than a kilometer away from other
known mounds (e.g., Mir Zone, active TAG Mound, and Shimmering
Mound). The TAG feld has been used as a case study for the exploration
for inactive deposits and understanding their physical and chemical
evolution [37]. With the exception of the active mound, which is
vigorously venting black smoker ?uids, the mounds display characteristics of inactive deposits, including lack of venting or vent-related
macrofauna, partial burial by sediments, paucity of upright chimney
structures, partial dismemberment by faulting, and internal collapse,
and Southern, Shinkai, and New Mounds have been described as ※eSMS§
(extinct SMS) [37]. However, at Southern Mound, the presence of bacterial mats suggests that diffuse, low-temperature venting is occurring
(Fig. 2B). The Shinkai and New Mounds lack evidence of bacterial mats,
but host abundant upright chimney spires and do not display internal
collapse features that characterize many of the other mounds. The lack
of collapse features is interpreted to indicate that anhydride dissolution
has yet to occur and heat is still retained within these mounds, suggesting that these mounds have only recently become inactive [37].
Radioisotope dating of hydrothermal precipitates indicate that large
deposits form on the order of 1,000s to 100,000 s years [32,39每41].
Evidence from radioisotope dating of the active mound at TAG indicates
that hydrothermal activity is cyclical, with prolonged periods of activity
and inactivity [39], suggesting that, using the defnitions proposed here,
Shinkai and New Mounds may not be extinct but only currently inactive,
whereas Southern Mound appears to be active. Overall, the cluster of
mounds that defne the TAG feld are interpreted to have formed
through hydrothermal circulation driven by a common ridge axis heat
source and channeling of ?uid along a common subsea?oor detachment
fault system [42,43]. Based on this interpretation, the entire vent feld
should therefore be classifed as hydrothermally active, as the individual
mounds are linked by the same underlying hydrothermal permeability
structure.
The examples from Endeavour and TAG provide evidence that,
although individual chimneys can in places be stable on decadal timescales, this is not universally true, and hydrothermal venting can be
spatially variable over the lifespan of a vent feld. Results from deepdrilling projects reveal that, in some places, drilling into the sea?oor
adjacent to an active black smoker does not intersect the system and
results in no change to ?uid ?ow in the active smoker. Drilling by ODP
leg 169 at Escanaba Trough, a sediment covered spreading center,
resulted in no intersection of hydrothermal ?ow nor changes to the
adjacent active vent, less than 10 m from the drill site (Fig. 4A) [45,46].
In other places, however, drilling resulted in the creation of a new black
smoker. In the Iheya North hydrothermal feld, in the Okinawa Trough,
for example, drilling adjacent to active hydrothermal vents during IODP
Expedition 331 resulted in the formation of artifcial, active hydrothermal vents, some of which persisted for more than 40 months and
resulted in rapid hydrothermal chimney growth (Fig. 4B) [47]. Similar
creation of artifcial hydrothermal vents and associated chimneys were
also reported during ODP Legs 139 and 169 at Middle Valley on the Juan
de Fuca (Fig. 4C) [48,49]. At Iheya North, the ?uids discharging from
these artifcial vents were higher salinity than those forming the natural
hydrothermal vents in the region prior to drilling, as a result of segregation of high and low salinity ?uids in the subsea?oor, with only low
salinity ?uids venting in the absence of drilling [50]. This demonstrates
that sea?oor penetration in an active region may release not only ?uids
that are currently venting in other parts of the vent feld but may connect
with existing subsurface ?uid reservoirs that are currently not venting.
Given our current understanding of subsurface ?uid ?ow, the dynamic
and unpredictable nature of hydrothermal systems indicates that drilling
into an inactive deposit in an overall active vent feld has the potential to
initiate ?uid ?ow.
6. Global inventory of inactive SMS deposits
The InterRidge Vents Database is a researcher-driven compilation
that aims to provide an international standard for documentation of all
sites of submarine hydrothermal activity and includes both hightemperature ※black smoker§ ore-forming systems discussed here, and
low-temperature systems associated with seamounts and fracture zones.
Only 8% of the sites documented in the InterRidge database are classifed as inactive [5]. Of the 54 sites classifed as inactive, 36 are
low-temperature systems that occur on seamounts (e.g., Brown Bear
Seamount; Dellwood Seamount), Fe- or Mn-oxide deposits (e.g., Carlsberg Ridge, 1.67 S; Shikurose Bank), or disseminated sulfdes in quartz
veins in exposed crustal rocks (e.g., DSDP Hole 504B; Vema Fracture
Zone) and are not considered high-temperature, ore-forming systems
comparable to systems that could generate an SMS deposit and so are not
considered here. Of the remaining 19 ※inactive§ sites, one site
(Zenith-Victory, Mid-Atlantic Ridge) should be classifed as active due to
a report of a hydrothermal plume at this location [23]. Another ※inactive§ site from the Mid-Atlantic Ridge, Ashadze 4, is also not considered
because the only published description of this site is a report of an
inactive sulfde boulder with a maximum dimension of about 1 m that
was observed and sampled during an ROV transect ~1.5 km downslope
from the active Ashadze 1 feld [52,53]. There are two sites classifed as
※inactive§ for which we judge there to be not enough information for
activity to be considered: the 24?N 30＊ site on the Mid-Atlantic Ridge;
and the site on the EPR 14 N Seamount that was discovered and sampled
by a single dredge that recovered hydrothermal material [54]. Finally,
there is a record for an inactive site at Mata Nima in the Lau Basin;
however, no published record of hydrothermal vents, either active or
inactive, can be found for this specifc volcano.
Among the list of ※active§ sites in the InterRidge database are also
sites that are known to be inactive. The Petersburgskoe site (MidAtlantic Ridge) is listed as ※Active, inferred§, but no indications of activity at this feld have been reported [23]. Also, ※Semyenov§ (Mid--
Atlantic Ridge) is listed as a single record, yet as discussed above,
represents a grouping of 5 vent felds, with Semyenov 2 the only active
feld. We know of two other inactive sites (Surprise and Yubileinoe,
Mid-Atlantic Ridge; Bel＊tenev et al., 2017) that are not yet listed in the
database [55].
We therefore identify only 20 sites in the global InterRidge database
that should be considered inactive using the criteria described above
(Table 3). A detailed examination of all 707 records would be required
to evaluate whether other potentially inactive sites are hidden or not
individually listed in this dataset. However, from this initial analysis, the
documented inventory of inactive deposits is very low, and points to 1)
how little information we have on inactive deposits, including inactive
SMS deposits; and 2) the need to improve our ability to locate inactive
deposits.
Within the current inventory of inactive deposits that formed along
mid-ocean ridges (i.e. excluding deposits that formed on seamounts) all
occur within or on the ?anks of the ridge axial valley (i.e. on-axis), which
represents the neovolcanic zone, which can be tens of kilometers wide,
and is associated with the locus of volcanic and hydrothermal activity
along mid-ocean ridges [3]. For example, the Petersburgskoe deposit on
the Mid-Atlantic Ridge is located on the western wall of the axial valley,
16 km from the ridge axis [23]; Lost City, on the Mid-Atlantic Ridge is
14 km from the ridge axis [8]; and the Penumbra deposit on the Central
Indian Ridge is 14 km from the ridge axis [50]. As sea?oor spreading
continues, deposits that form on-axis will eventually migrate off-axis,
and away from the heat source that drives the hydrothermal circulation. Off-axis deposits are therefore likely extinct, and attractive
exploration targets for contractors. In fact, since all oceanic crust formed
at mid-ocean ridges, the entire ocean basins may be prospective for
extinct SMS deposits. The preservation of such deposits, which is likely
controlled by the interplay of sulfde oxidation rates and burial by
sediment or lava, will be a major factor in understanding the off-axis
resource potential, although the abundance of VMS found in abducted
oceanic terrains on land indicates that at least some fraction of these
deposits is preserved.


Conclusions and implications for mining and exploration

The classifcation of hydrothermal activity, inactivity, and extinction
that we defne here is simple and geologically concise. Defning the
extent of hydrothermal vent felds, which is necessary for delimiting the
boundary to which a classifcation of activity would apply, however,
presents a greater challenge. For the purposes of defning hydrothermal
activity in relation to mining activities, it is important to note that,
although mining will occur at the deposit scale, the geological connectivity that defnes a vent feld necessitates that the criteria used to
classify the state of activity must extend beyond the limits of the deposit
itself and be considered at the vent feld scale. If an inactive deposit
occurs within a vent feld that is hydrothermally active (i.e. any regions
within the feld show evidence of activity or recently activity), the
overall feld must still be hydrothermally connected to its heat source,
and all deposits within the feld have the potential to resume activity.
Therefore, deposits within an active vent feld may be described as
active or inactive, with the caveat that inactive sections of an active vent
feld may be expected to resume activity; and the classifcation of any
deposit as extinct requires that the entire vent feld be inactive and not
geologically connected to any active venting. It is expected that any
extinct hydrothermal feld would satisfy the following three criteria: 1)
absence of ?uid discharge from the sea?oor at temperatures above that
of ambient bottom water; 2) a lack upright chimneys; 3) extensive
oxidation of hydrothermal material; and 4) absence of live vent-?uid
dependent biological communities.
Although designating areas or individual deposits within an overall
active system as ※active§ or ※inactive§ may be biologically relevant over
observed timescales, this approach lacks a connection to subsea?oor
processes and the validity of these designations may not persist when the
subsea?oor is disturbed by both natural events (e.g., magmatic or
tectonic activity) and human modifcations (drilling, mining). This
emphasizes that such designations must take place at the vent feld scale.
Our simple defnition avoids ambiguities associated with other potential criteria for activity classifcation that would rely on continuous,
non-discrete attributes such as vent ?uid temperature or ?ow rates, and
is consistent with an understanding of hydrothermal activity that at the
primary level is defned by the presence of hydrothermal ?uid ?ow. In
the absence of a detailed sub-sea?oor understanding of the plumbing of
hydrothermal systems, including the dynamism of reservoirs, surface
parameters are described that can indicate cessation of hydrothermal
activity at the vent-feld scale, including the lack of a hydrothermal
plume and the absence of upright chimneys; as well as geological parameters that can be used to consider the extent of a vent feld and
differentiate geologically discrete felds on a ridge segment. Generally,
felds on faster spreading segments exhibit more dynamic variability and
felds on slow spreading ridges may be stable over thousands of years.
However, indicators that hydrothermal activity can cease and restart,
with hiatus lasting between less than one and several thousands of years,
as well as begin in ※inactive§ regions of an active feld as a result of
drilling, demonstrate that we should label entire hydrothermal felds
extinct with caution.
The implication of this binary geologic defnition of activity is that,
in the vast majority of cases where mineral exploration is occurring,
both within EEZs and Areas Beyond National Jurisdictions, the main
area of interest is an active hydrothermal feld. However, the small
number of known inactive deposits does not necessary re?ect a low
resource potential for inactive SMS deposits. Instead, it highlights our
only cursory understanding of the distribution, preservation and ability
to explore for these deposits. This provides abundant motivation for
further research into the processes of SMS formation from hydrothermal
?uids and reinforces the need to continue to explore the ocean basins,
especially further away from the ridge crest.

Detecting changes in high-resolution satellite
coastal imagery using an image object detection
approach
This article presents a spatial contrast-enhanced image object-based change detection
approach (SICA) to identify changed areas using shape differences between bi-temporal
high-resolution satellite images. Each image was segmented and intrinsic image objects
were extracted from their hierarchic candidates by the proposed image object detection
approach (IODA). Then, the dominant image object (DIO) presentation was labelled
from the results of optimal segmentation. Comparing the form and the distribution of
bi-temporal DIOs by using the raster overlay function, ground objects were recognized
as being spatially changed where the corresponding image objects were detected as
merged or split into geometric shapes. The result of typical spectrum-based change
detection between two images was enhanced by using changed spatial information of
image objects. The result showed that the change detection accuracies of the pixels
with both attribute and shape changes were improved from 84% to 94% for the strong
attribute pixel, and from 36% to 81% for the weak attribute pixel in study area. The
proposed approach worked well on high-resolution satellite coastal images.
1. Introduction
Remote-sensing change detection is the process of identifying differences in the state of an
object or phenomenon by observing it at different times (Singh 1989). It is one of the most
important applications for analysing multitemporal remotely sensed images. Change detection
is also a meaningful tool for monitoring and managing natural resources and urban
development in a coastal zone because it can quantitatively measure the spatial variation
of relevant ground objects during a given period (Malthus 2003; Bontemps et al. 2008).
Furthermore, a shape-based approach has been introduced to change detection (Li and
Narayanan 2003). However, the most common approach has been to classify each image
pixel as an independent observation regardless of its spatial context (Marcal et al. 2005).
Unfortunately, this is fraught with the limitations imposed by pixel-based image analysis.
With the increasing spatial resolution of remotely sensed imagery, new applications in
coastal zones require the generation of change detection maps characterized by both a high
geometric resolution and the capability of properly modelling the complex areas present in
the scene (Bovolo 2009).
Remote sensing has made enormous progress over the last few years from what has
been predominantly a per-pixel, multispectral (MS)-based approach to the development and
application of multiscale object-based methods (Hay et al. 2005). Blaschke (2010) gave an
overview of the development of object-based methods. Recently, the development of geographic
object-based image analysis (GEOBIA or OBIA) has nevertheless provided a new,
critical bridge between the spatial concepts applied in multiscale landscape analysis. In the
segmentation process, natural complexity can be effectively explored using spatial analysis
tools based on the concept that landscapes are process continuums which can be partially
decomposed into objects or patches (Burnett and Blaschke 2003). Walter (2004) adopted
object-based classification and a geographical information system (GIS) to improve the
capability of change detection. Desclée, Bogaert, and Defourny (2006) detected forest landcover
change using a statistic object-based method. Yu et al. (2006) found that using objects
as minimum classification units helped overcome the problem of salt-and-pepper effects
resulting from pixel-based classification methods. Tian and Chen (2007) proposed a framework
to find optimal segmentations for a given feature type using an image object-based
method. Object-based applications have been performed for mapping and change analysis
in mangrove ecosystems (Conchedda, Durieux, and Mayaux 2008) and in urban sprawl
areas (Durieux, Lagabrielle, and Nelson 2008). Im, Jensen, and Tullist (2008) introduced a
change detection method based on object/neighbourhood correlation by image analysis and
image segmentation techniques. Some unsupervised approaches have been presented based
on the comparison between features computed on homogeneous regions that are obtained
according to segmentation procedures (Hall and Hay 2003; Bruzzone and Carlin 2006).
Even if these object-based change detection methods are implicitly suitable to analyse highresolution
images, they get the final change detection by pixel-to-pixel comparison, which
is not an effective region descriptor like object-to-object comparison. Partly, this is because
most of these techniques do not generate explicit object topology, nor even incorporate the
concept of an object within their analysis (Hay et al. 2003).
The relatively homogeneous and semantically significant groups of pixels produced
after multiscale segmentation have been designated ‘object candidates’, which are to be recognized
by further processing steps and to be transferred into meaningful objects (Burnett
and Blaschke 2003). Considering the oversegmentation or the undersegmentation problem,
there exists a large matching gap between the theoretically hierarchical network of image
object candidates and real-world objects of a landscape. In many cases, the delineation of
relatively homogeneous areas is the basic method, and the common denominator of various
realizations of image object-based change detection is the objective to derive meaningful
objects. Moreover, the dominant landscape objects have their intrinsic scales and are composed
of structurally connected parts. In this article, the image object detection approach
(IODA) (Chen, Pan, and Mao 2006, 2009) is adopted to choose appropriate scales and
produce meaningful image objects from any possible image candidates, and to distinguish
intrinsic scale from any segmentation scale. The newly developed method is also an update
to the work presented in Chen, Pan, and Mao (2009) by the same authors.
2. Materials and methods
2.1. Related work
Multiscale segmentation is a procedure by which an image is precisely separated into mutually
exclusive region sets by computing the defined heterogeneity. The criteria for stopping
the segmentation procedure are the degree of difference between two regions. These differences
are optimized in a heuristic process by comparing the attributes of the regions
(Baatz and Schäpe 2000). Only one segmentation scale was set as the criterion in the segmentation
procedure. As may be known, there are various optimal segmentation scales
with respect to diverse intrinsic scales of ground objects in remotely sensed images. So,
the IODA method chooses the meaningful object candidates as image objects from whole
multiscale representations (Chen, Pan, and Mao 2009).
In the IODA, for MS remotely sensed images, the spectral mean distance (SMD) was
defined commonly to describe the distance of two adjacent candidates using their spectral
mean values, μd and μ d, for spectral band d:
The combined standard deviation (CSD) which characterizes the homogeneity for each
candidate can be defined as follows:
where wd is the user-defined weight for spectral band d, and σd is its standard deviation of
candidate a. Here, the difference distinctive feature (DDF) was defined as follows:
where the DDF of candidate a is determined by the SMD and the standard deviation. The
minimum DDF (MD) is used in the measurement of the distinguishability of an object
candidate against its surrounded candidates at one segmentation scale:
In the segmentation procedure, the pixel p belongs to a series of object candidates, along
with the increasing of scale s. Each candidate has its minimum distinctive, which makes up
the MD curve. The maximum distinctive feature (MDF) is defined as the maximum MD
from the initial scale to a given scale in the MD curve. The scale order is constructed by a
series of the original scale of ordinal MDF. For a simple object, the original scale at which
its meaningful image object formed must be in scale order, and this can be adopted as the
segmentation scale at the intrinsic scale (Chen, Pan, and Mao 2009).
2.2. Image object-based spatial change detection
The proposed approach was dependent on the homogeneous image objects yielded from
the IODA method (see Figure 1), which was based on the multiscale segmentation result;
scale order sequences for each pixel were then established. Subsequently, the nonsignificant
candidates were removed and the potential image objects in the MDF object candidate set
were retrieved from multiscale representations. As described above, the image object identification
here was making heterogeneity of inter-neighbourhood image objects maximized
as well as the homogeneity of intra-image object individually.
According to the provocative question that Blaschke and Strobl (2001) raised, ‘What’s
wrong with pixels?’, an increasing dissatisfaction was identified with pixel-by-pixel image
analysis. Under the assumption that the landscape of interest is a finite population of objects
(Bian 2007), the spatial information about these objects is integrated in the ultimate change
detection method. Considering the Shannon sampling theorem, we expected that an object
should be of the order of one-tenth of the dimension of the sampling scheme – the pixel –
in order to ensure that it would be completely independent of its random position and its
orientation relative to the sampling scheme (Blaschke 2010). Accordingly, image objects
larger than twice or three times the minimum needed by Shannon’s sampling theorem and
the pixels satisfying 3 × 3 pixels distribution were focused mainly in the application. These
image objects are called dominant image objects (DIOs). Spatial information of these DIOs
are included in the following image object-based change detection procedure.
The workflow of the image object-based change detection algorithm is shown in
Figure 2. Two temporal remotely sensed images were geo-corrected using in image registration
mutually in image preprocessing. At the second stage, two parallel processes were
performed for traditional change detection based on spectral information and image objectbased
spatial contrast-enhanced change detection, respectively. In the latter approach, both
images were segmented using the IODA. Then, the DIO presentation was identified from
the product of optimal segmentation and labelled with a unique reference number. Next, two
DIO sets from bi-temporal images were compared by the raster overlay function by using
their unique reference numbers. The corresponding image objects were recorded as shape
changed when they were detected as merged or split in the overlay function. Concurrently
the typical change detection approach was performed through calculating the image objectbased
average spectral data. At the third stage, the difference map that was created based
on object-to-object comparison with the change of shape as a result was integrated into the
result of traditional detected change using the spectral approach. Finally, the change map
was produced by enhancement of the spatial information of the image object.
The overlay function was executed by taking each DIO as a raster zone by using a
unique reference number. The image objects from two temporal images which merged or
split were divided into small parts in the overlay function. If the DIO from the former
image contained or intersected with more than one latter DIO and the intersected parts also
met the condition of DIO, we considered the situation as the split procedure of the former
image object.When the image object from the latter image contained or intersected with the
former DIOs, their intersected parts were taken as the DIO, and the merged state occurred.
Admittedly, there were both merging and splitting procedures when we compared with the
image objects from two temporal images. The merged or split state means that the change
of shape has occurred.
2.3. Integration of spectral and spatial information
Shape change means some or all parts of ground objects are changed. Other information
is also required for identification of the changed parts or for other regions where the
shape is not changed. Change detection algorithms in the remote-sensing literature can be
roughly divided into two categories (Yuan and Elvidge 1998; Mas 1999): preclassification
and postclassification change detection. Image object-based change detection adopting a
strategy of postclassification takes three steps: segmentation, classification, and comparison.
Such high-resolution imagery presents a new class of change detection problems to
which existing remote-sensing techniques are not well suited. Classification techniques for
thematic labelling, based on spectral and textural attributes, cannot describe within-class
changes like new construction in cities, since the area is classified as urban before and after
such changes (Pollard et al. 2010). This meant that most were detected as changed areas
because of misclassification. In our approach, the segmented results were classified using
the supervised maximum likelihood classifier (MLC) method. The MLC is the most common
supervised classification method used with remote-sensing image data and is based
on the Bayesian probability theory (Richards 1995). Then, the overall accuracy, the kappa
coefficient, producer’s accuracy, and user’s accuracy were calculated for each class. For
illustrating the performance of spatial change detection, the classes were divided into two
groups: high accuracy classes (above the overall accuracy) and low accuracy classes (below
the overall accuracy) in the case study. Therefore, the former classes were called obvious
classes and the latter classes were called unobvious classes. When the change happened,
the change among obvious classes or between obvious classes and unobvious classes was
assumed as a strong attribute pixel change. Only the change among the unobvious classes
was assumed as a weak attribute pixel change. Then, the strong or weak attribute pixel
change was enhanced by a shape changed result derived from DIOs. Therefore, the detected
results were divided into strong attribute pixel and shape change, strong attribute pixel
change only, weak attribute pixel and shape change, and weak attribute pixel change only.
2.4. Study site and data
Two high-resolution satellite images covering the same area were used in this study, which
were both 1024 × 1024 pixel subimages of an IKONOS scene (see Figure 3). The area represents
a portion of the highly fragmented agro-waterfront landscape. IKONOS provides
16-bit MS data in blue, green, red, and near-infrared (NIR) channels at 4 m spatial resolution
and a 16-bit panchromatic (PAN) channel at 1 m resolution. The used image resolution
is generated by fusing MS bands with the PAN band. Herein, the Gram–Schmidt spectral
sharpen function was utilized supported by sensor type. The auxiliary data include field
survey data and a topographic map. The study area is located in the Xiejiadao Peninsula,
Huangdao District, Qingdao City, between latitudes 35◦ 5410 N and 35◦ 5540 N, longitudes
120◦ 1030 E and 120◦ 1200 E. There are mudflats, aquacultures, boats, seawater
below the coastline, residential areas (including building and yard), ponds, and agrarian
fields above the coastline, and the agrarian fields are in different stages of planting. Two
images were acquired at different years and during the same season. The acquisition of former
images was at 02:43 GMT, 17 April 2001 and the latter was acquired at 03:00 GMT,
25 April 2007.

3. Results
3.1. Result of change detection
The image registration procedure was carried out in the preprocessing, and the root
mean square error (RMSE) was 0.54 pixels. As a reference for change detection, the
land-cover change of the study area was identified in the vector layer (see Figure 4)
through image interpretation under the support of commercial GIS software (ArcGIS;
Esri, Redlands, CA, USA). The land-cover classification scheme referred to the standard
used in Chinese offshore investigation and assessment, which was slightly different
compared to recent research (Teodoro et al. 2011). About half of the coastline had
been modified in a short period. Two of the three aquaculture areas also disappeared
during this period. The proportion of residential areas was increasing and almost simultaneously,
the small ponds were changed either in part or as a whole. The agrarian
fields were changed not only into the other types but also changed with respect to
the planting and farming state. The result indicated that land-cover changed rapidly.
The change pattern was similar to the closer coastal area in East China (Liu, Liu, and
Tian 2005). The total area of the Xiejiadao Peninsula is 28.26 km2 and it increased
10.7% between 2001 and 2007, mainly from the tidal zone. Meanwhile, 11.0% of arable
land area decreased and the main cause of occupation was built-up and settlement
land.
In the postclassification change detection procedure, the segmentation scale was 30 for
both MS images. The result of segmentation was oversegmentation for most objects. The
overall accuracy of the 2001 image was 88.65%, and the kappa coefficient was 0.84.
The overall accuracy of the 2007 image was 84.59%, and the kappa coefficient was
0.79. Actually, the producer and user accuracies (see Table 1) of waterbody, cropland,
soil (reaped crop), woodland, and uncovered area were all larger than average. But the
accuracies of building, yard (area between buildings), road, and silt were less than the
average accuracy. The total accuracy of postclassification change detection without spatial
information supported was 73% (see Table 2). The strong attribute pixel change accuracy
increased to 84% and the weak attribute pixel change accuracy decreased to 36%
when the changed area was partitioned according to the classes. After integration with
spatial change information (see Figure 5), the pixels that were of weak attribute pixel
and shape changed occupied 22,452 pixels and its accuracy was 0.81. And the pixels
that were only weak attribute pixel changed occupied 101,702 pixels and its accuracy
was down to 0.26. In contrast, the pixels that are of strong attribute pixel and shape
changed occupied 256,037 pixels and its accuracy was up to 0.94, and the pixels that
were only strong attribute pixel changed occupied 161,563 pixels and its accuracy was
0.68. Furthermore, the result of change detection of the proposed approach has potential
improvement in case more meticulous works are performed like relative radiometric
normalization.
3.2. Traditional change detection enhanced by spatial change information
The IODA had been applied on the hierarchical network of image object candidates with
the ultimate scale 800. Here, the DIOs had different segmentation scales with different
DDFs (see Figure 6). In the vector layer shown in Figure 4, most of the filtered DIOs
were anthrop-impacted ground objects. The proposed method has produced improved
results (see Table 2), but not all land covers benefitted equally. The image object detection
results varied in the ability to reveal different ground objects (see Table 3). The
exhibitions of image object candidates were perfect, especially in residential areas and
agrarian fields. The changed inland aquacultures were detected well by the signal that the
merge and/or split of the corresponding image objects occurred (see Figure 6, below).
The land covers consisting of those image objects had a larger improvement in accuracy.
In contrast, the changed aquacultures under the coastline were not detected ideally
because in the fused image their small or narrow border could not be protected from
merging with their surroundings because of the same materials. The image objects in
the areas where land cover was changed from agrarian fields to settlements were acceptable
although their ground object size was relatively small. However, the built-up areas
of the settlement could not be identified well because of their size, and they were not
considered as DIOs. The accuracy for land covers with those image objects improved
less. Many agrarian fields were detected as changed areas according to their changed
shape. Such a case meant that the planting and farming state would change the result of
IODA. The segmentation is relative to the accuracy of the classification (see Table 1).
The higher accuracy of classification indicated that the ground object could be identified
from other classes, that also meant they were segmented from the surroundings
easily.
As a traditional preclassification method, the spectral change vector approach (CVA)
helped to determine whether there were any DIOs changed on the whole or whether there
were parts changed in the merge or split procedure (Johnson and Kasischke 1998). The
result of the image object-based change vector is clearer than the pixel-based result of the
CVA method (see Figure 7). On comparison with the pixel-based CVA method, the image
object-based method avoids the ‘salt-and-pepper’ phenomenon. The false small polygon,
which usually emerged in the result without accurate geometric registration, did not emerge
when we used the concept of DIO. Generally, the larger value of the spectral change vector
indicates the changed region and the lesser value indicates unchanged parts. The result
of the proposed approach would be more significative in high-resolution remote-sensed
images, where the DIOs can be obtained. In the study area, the traditional change detection
method is still important in image object-based change detection. But the image objectbased
change detection method adds more extra information on spatial information such as
merge or split.
4. Discussion
4.1. Changed shape means changed spectrum
Changed shape means either a merge or a split or both have occurred. It also indicates the
spectral difference in the parts of them. For multiscale segmentation, the spectral features
of two image objects f 1 and f 2 are considered to be similar when they are near to each
other. For an l-dimensional spectral space, the heterogeneity h is described as follows:
where d represents bands. These distances of f 1 and f 2 can be further normalized by
the standard deviation σfd of the spectrum. There are several possibilities for describing
the heterogeneity change before and after a virtual merge (a detailed discussion
is described by Baatz and Schäpe (2000)). Given an appropriate definition of heterogeneity,
the growth in heterogeneity of a merge should be minimized. Obviously,
the adjacent pixels set belonging to the same class would be segmented together
before.
The derived criteria from multiscale segmentation can convey useful information concerning
the discriminatory capability associated with an adopted spectral space. If one
candidate can be separable from surrounding candidates, on the classification view it can be
reflected by the derived criteria of classification such as the maximum inter-class distance
and the minimum intra-class diversity. For each image candidate, there are three phases
passed from being a one-pixel object to being an undersegmented object at a relatively
large segment scale, along with the multiscale segmentation procedure. In the oversegmentation
phase, for each object candidate in the range of one image object, there must be one
or more neighbourhood candidates in the same image object. The spectral mean of the candidate
can be considered as the sample’s mean ¯x and the variance of the candidate is the
variance of the sample’s standard deviation S. For innate merge, the sample’s means were
¯x
1, ¯x2 and standard deviations were S1, S2 before and the same statistics were ¯xm and Sm
after merge. The DDF of two candidates which came from the same image object, the candidates
were oversegmented, was relatively small in this phase. The optimal segmentation
phase followed the ending of the oversegmentation phase; there was a case where both the
candidate a and the candidate a belonged to the same image object. Apparently, the only
two bordering candidates would merge together at the optimal scale. This is related to the
spatial resolution of the image being used. The heterogeneity in the oversegmentation phase
was less than in the optimal phase. Subsequently, it was in the undersegmentation phase.
When two candidates included more than one image object, and n and m were the sample
numbers of these two candidates, the merged candidate had the difference in feature as
follows:
However, as is known, the variance of the candidate increased monotonously, so the DDF
tended to decrease. Two adjacent regions belonging to different classes were obliged to
merge together in the segmentation procedure. But they were not satisfying the condition
of optimal segmentation in IODA.
If DIOs existed and were neighbours together, they were considered to be belonging to
different classes. When the shape changed, the merge procedure indicated that two DIOs
separated in early time but did not separate in latter time. That also meant that two DIOs
did belong to two classes in former time but they belonged to the same class later. The
split procedure indicated that DIOs need to be classified into one class before but should
be classified into different classes later. That meant the shape of the image object could be
used in detection of the spectral change.
4.2. The meaningful image object is critical in using spatial information
In image object-based analysis, the pixel entities that are expected in remote-sensing images
can be related to real entities in the landscape. These regions are later related to geographic
objects through some object-based classification. To extract meaningful image objects,
users are required to focus on different intrinsic scales because all attributes of image
structure are almost highly scale dependent. Spatial information that we want to obtain
from the overlay function should be derived only from the image objects of intrinsic scales
but not of any general scales. Otherwise, it will lead to unreasonable results. This is why the
general result from traditional multiscale analysis cannot be adopted directly. And the optimal
segmentation and intrinsic scale identification are the predetermination of the image
object-based change detection.
Actually, these more important simple meaningful objects are the explicit homogeneous
objects, which have similar spectral characteristics yet differ from surrounding objects in
high-resolution remote-sensing imagery. In our study area, these objects included waterbodies,
residential areas, agrarian fields, and such anthropogenic-impacted ground objects.
Furthermore, the Shannon sampling theorem should be considered. In fact, if the border of
two ground objects or the size of the ground object is similar to the pixel size, the spectral
average of the image candidate will not be a real spectrum of a ground object because of
the problem of mixed pixels. Therefore, many man-made or anthropogenically influenced
objects are coincident with the defined term meaningful objects. The basic task of segmentation
algorithms is to merge image elements based on homogeneity parameters or on the
differentiation to neighbouring regions (heterogeneity), respectively. In other words, these
criteria can be explained as follows: in the segmentation procedure, the increased heterogeneity
can be interpreted as a border between the segments, which has to be overcome for
merging. The higher this border is for a certain segmentation parameter combination, the
more stable is the result (Benz et al. 2004). As long as the scale is the best, meaningful
objects can be obtained. Furthermore, in many cases significant objects appear at different
scales of analysis of the same image.
5. Conclusions
Image object-based change detection can be enhanced by spatial information in change
identification for both postclassification and preclassification approaches. Obviously, the
important information for understanding an image is not represented in a single pixel but
is embodied in meaningful image objects as well as their mutual relationships. The spatial
information of image objects is also an important cue to identify the change. In this
study, an image object-based method is presented to identify the changed areas using shape
difference of an image object between bi-temporal images of a coastal zone. We proposed
the concept and its definition of DIO. The DIO presentation was identified from
the results of optimal segmentation through the IODA. By using raster overlay analysis to
bi-temporal DIOs, the shape of image objects can be used to recognize the change evolution
by knowing its merge or split procedure. The classes conducted in postclassification
change detection were divided into a strong attribute pixel group and a weak attribute
pixel group according to its accuracy. After integration with spatial change information,
the accuracy of both weak attribute pixel and shape change was 0.81, and the only weak
attribute pixel that was changed was decreased from 0.36 down to 0.26. In contrast, the
accuracy of both strong attribute pixel and shape change was increased from 0.84 up to
0.94, and the accuracy of only the strong attribute pixel change was 0.68. The improved
accuracy depended on the ability that the obtained DIOs revealed the different ground
objects. The present method would be more significant in high-resolution remote-sensed
images. The spatially unchanged information can reduce overdetected areas and the spatially
changed information can certify the judgement of change. By analysis of the three
phases of the image segmentation procedure, the IODA was considered as a rational way
to derive intrinsic image objects from a hierarchic candidate. Thereby, the typical spectral
difference based change detection between bi-temporal images was instead of spatial
change detection between ground objects based on the spectral difference which would
be compared in an optimal segmentation procedure. The latter can be helpful in avoiding
some spectral reflection corrections caused by many image acquisition conditions in typical
change detection processes
Title: 
Geological fate of sea?oor massive sulphides at the TAG hydrothermal feld (Mid-Atlantic Ridge)

Author:
Bramley J. Murtona, Berit Lehrmanna, Adeline M. Dutrieuxb, Sofa Martinsc,d,
Alba Gil de la Iglesiaa,f, Iain J. Stobbsb, Fernando J.A.S. Barrigac, J?rg Bialasd, Anke Dannowskid,
Mark E. Vardya,g, Laurence J. Northa, Isobel A.L.M. Yeoa, Paul A.J. Lustye, Sven Petersen

Abstract:
Deep-sea mineral deposits potentially represent vast metal resources that could make a major contribution to
future global raw material supply. Increasing demand for these metals, many of which are required to enable a
low-carbon and high-technology society and to relieve pressure on land-based resources, may result in deep sea
mining within the next decade. Sea?oor massive sulphide (SMS) deposits, containing abundant copper, zinc,
gold and silver, have been the subject of recent and ongoing commercial interest. Although many sea?oor
hydrothermally systems have been studied, inactive SMS deposits are likely more accessible to future mining and
far more abundant, but are often obscured by pelagic sediment and hence difcult to locate. Furthermore, SMS
deposits are three dimensional. Yet, to date, very few have been explored or sampled below the sea?oor. Here,
we describe the most comprehensive study to date of hydrothermally extinct sea?oor massive sulphide (eSMS)
deposits formed at a slow spreading ridge. Our approach involved two research cruises in the summer of 2016 to
the Trans-Atlantic Geotraverse (TAG) hydrothermal feld at 26∼N on the Mid-Atlantic Ridge. These expeditions
mapped a number of hydrothermally extinct SMS deposits using an autonomous underwater vehicle and remotely operated vehicle, acquired a combination of geophysical data including sub-sea?oor seismic re?ection
and refraction data from 25 ocean bottom instruments, and recovered core using a robotic lander-type sea?oor
drilling rig. Together, these results that have allowed us to construct a new generic model for extinct sea?oor
massive sulphide deposits indicate the presence of up to fve times more massive sulphide at and below the
sea?oor than was previously thought.

Introduction
Demand for mineral raw materials is increasing as a result of population growth, 
rising living standards, urbanisation and, more recently, the transition to a low-carbon economy (Zepf et al., 2014). Yet
the mining industry faces numerous challenges including rising costs,
reducing ore grades and declining discovery rates of new deposits
(Calvo et al., 2016). As a result, alternative sources of these minerals
are being considered, including the deep-ocean.
Sea?oor massive sulphide deposits, formed through hydrothermal
venting, are considered to be the modern analogous of ancient volcanogenic massive sulphide deposits preserved on land. Although the
number of hydrothermal vent discoveries has been steadily increasing
since the frst discovery at the Galapagos Rift in 1977 (Corliss et al.,
1979), and more than 600 sites are known today (Beaulieu et al., 2015),
the economic importance of the associated SMS deposits is poorly
known. Recent estimates suggest modern hydrothermally active sites
account, globally, for at least 650 million metric tonnes (Mt) of massive
sulphides containing 10 Mt of copper (Cu), 29 Mt of zinc (Zn), 1 Mt of
lead (Pb), 33 Mkg silver (Ag), and 750,000 kg gold (Au) (Hannington
et al., 1999, 2011). Published bulk geochemical data from 95 of these
modern SMS deposits suggest a median grade of 3 wt.-% Cu, 9 wt.-% Zn,
2 g/t Au and 100 g/t Ag (Hannington et al., 2011; Monecke et al.,
2016). However, these analyses are mainly derived from easily recoverable surface grab-samples, such as high-temperature sulphide
chimney and related talus material, and may not be representative of
the average composition of the deposits at depth. It is likely that the
vast majority of SMS deposits are no longer hydrothermally active and
are increasingly covered by pelagic sediment as they age. It remains
unknown what low temperature geological and environmental processes a?ect hydrothermally extinct SMS (eSMS) deposits once the ?ow
of chemically reduced hydrothermal ?uid ceases, whether the metal
tenor becomes enriched, depleted or disappears with time, and what
the structure and composition of these deposits is beneath the sea?oor.
To enhance our understanding of modern eSMS deposits, two research expeditions (M127 on RV Meteor and JC138 on RRS James
Cook) were conducted at the TAG hydrothermal feld at 26∼09∩N,
49∼30∩W, on the Mid-Atlantic Ridge, in 2016, as part of the EU-funded
Blue Mining project. The objectives were to determine the sub-sea?oor
morphology, mineralogy, composition and extent of the sulphide mineralisation in eSMS deposits. Here, we report the results of that work
and combine observations based on surface geology, sub-sea?oor drilling and seismic imaging, to yield a new model for eSMS deposits at
slow-spreading ridges.

Geological setting
The Mid-Atlantic Ridge is a slow-spreading (22 mm/year) ridge
(Kleinrock and Humphris, 1996), and the associated TAG hydrothermal
feld is one of the most intensively studied sea?oor hydrothermal systems (e.g. Rona et al., 1975, 1984, 1986; 1993a,b; Humphris et al.,
1995, 2015; Humphris and Kleinrock, 1996; Evans, 1996; Tivey et al.,
1995, 1996; Petersen et al., 2000; DeMartin et al., 2007; Zhao et al.,
2012). It lies at water depths ranging from 3,430 to 3,670 m on the
eastern and shallowest part of a 75 km-long, second-order spreading
segment bounded by two right-lateral non-transform discontinuities at
25∼58∩N and 26∼17∩N (Fig. 1A). The TAG segment is characteristic of
most slow-spreading ridge segments, having a fault-bounded graben,
bound by abyssal hills and faults (here oriented NNW-SSE) forming a
deep axial valley that hosts a neovolcanic zone comprising young lavas,
hummocky volcanic ridges and isolated volcanoes (White et al., 1998).
The active TAG mound is the current locus of venting of high
temperature (up to 363 ∼C) hydrothermal ?uids, that has resulted in the
formation of a 200 m diameter, 50 m high circular deposit topped by a
number of 12 m-tall black-smoker chimneys (Humphris et al., 2015).
During ocean drilling programme (ODP) Leg158, in 1994, seventeen
holes were drilled at fve locations into the active TAG mound
(Humphris et al., 1995). The maximum depth of penetration was 125 m
below sea?oor (mbsf). The holes intersected massive pyrite breccia in
the upper part of the mound overlying anhydrite-rich breccia, below
which a silicifed, pyritised, altered basaltic stockwork was encountered. The lowermost part of the drill holes intersected highly-altered basaltic host rock. Metals of economic interest such as Cu, Zn, Ag
and Au were only found to be enriched in the upper 5 m of the mound
(Hannington et al., 1998) whereas mainly barren massive pyrite body
was identifed below this depth.
Radiometric dating indicates hydrothermal activity at the active
TAG mound frst started 50,000 years ago with episodic periods of activity of tens to hundreds of years (Lalou et al., 1990) characterised by
accumulation and recrystallisation of sulphides, precipitation of substantial amounts of anhydrite, and remobilisation of metals. Intervening
periods of hydrothermal inactivity lasted 3,000 to 5,000 years, during
which mass wasting, anhydrite dissolution and interior brecciation
occurred. The current period of hydrothermal activity at the active TAG
mound commenced about 80 years ago (Lalou et al., 1998).
In addition to the active TAG mound, at least seven other hydrothermally inactive or eSMS deposits have previously been identifed
(Fig. 1B) in an area of ‵2.5 km2 (Rona et al., 1993a,b). The largest
cluster of deposits is known as the Alvin Zone and lies ‵1.5 km northnortheast of the active TAG mound, at a depth of 3,400 to 3,600 m. It
consists of a number of roughly circular individual mounds (Fig. 2A)
that are typically up to 60 m high, 100 to 300 m in diameter, formed
from weathered sulphide talus, variably covered in pelagic and hydrothermal sediment (Humphris et al., 2015 and references therein),
and are between 6,000 to 75,000 years old (Lalou et al., 1990, 1993,
1998). These mounds include Double, Shinkai and Southern (Fig. 2B,C).
Another 200 m diameter area of iron-oxide and weathered sulphide
material, called Shimmering Mound, occurs 1 km to the north of
Shinkai Mound where low temperature ?uids (≒22 ∼C) di?use through
iron-bearing silicates, iron-oxyhydroxides (FeOOH) and iron-manganese oxides (Humphris et al., 2015). During cruise M127 (Petersen and
Shipboard Scientifc party, 2016), further hydrothermal structures were
discovered including a 30 m high, 100 m diameter, dome-shaped
mound, named Rona Mound (Fig. 2D) after the late Peter Rona, who
was the frst to discoverer hydrothermal activity at the Mid-Atlantic
Ridge (Rona et al., 1975, 1984). Two smaller mounds (~20每30 diameter) on the north-eastern and south-eastern side of Shinkai Mound,
are called New Mound #2 and New Mound #3 (Fig. 2A-B).
In addition to the extensive Alvin Zone, a second hydrothermally
inactive site occurs, called the MIR Zone (Fig. 2E). This site is located
two kilometres to the east-northeast of the active TAG Mound in water
depths of 3,430 to 3,575 m. In comparison to the Alvin Zone, this area
lacks mound-like features, and instead comprises a raised area of irregular and undulating sea?oor, several 100∩s m in diameter and a few
10∩s m high, composed of an accumulation of weathered sulphide material and iron-rich sediments with a discontinuous pelagic sediment
cover (White et al., 1998). Rona et al. (1993a) divide the MIR Zone into
three distinct areas. The western part comprises weathered sulphide
debris and hydrothermal sediments that are underlain by hydrothermal
breccia. In contrast, the central zone is dominated by toppled and
standing relict chimneys. No sulphides are recorded from the eastern
zone where FeOOH, iron-rich clays and manganese oxy-hydroxide
(MnOOH) crusts occur. Radiometric dating indicates that hydrothermal
activity commenced 140,000 years ago with the last active episode
ceasing only 600 years ago (Lalou et al., 1993; Humphris and Tivey,
2000).
The TAG hydrothermal feld is a basalt-hosted system (Rona et al.,
1993a,b) and the composition of the vent ?uids and their mineral deposits are consistent with ?uid interaction with a mafc host rock
(Campbell et al., 1988). As such, the TAG hydrothermal feld can be
considered a classic example of a modern, volcanogenic massive sulphide deposit and is distinct from deposits formed in intra-continental
rift zones, back-arc basins and volcanic arcs. Recent studies have shown
that the TAG hydrothermal feld is formed on the hanging wall of an
active detachment fault (Tivey et al., 2003), with seismic data indicating the footwall comprises lower-crustal or serpentinised uppermantle lithologies (DeMartin et al., 2007).
No accurate resource estimates are available for the whole TAG
hydrothermal feld, although attempts have been made using bulk
geochemical data from the literature and combining them with the
volume of the mounds above the surrounding sea?oor under the premise that Alvin and MIR zones consist of similar lithologies to those
observed in the sub-sea?oor beneath the TAG mound. Based on these
data, the mass of sulphides in the active TAG Mound is thought to be up
to 3.9 million tons (Hannington et al., 1998) and the Alvin and MIR
zones each range between 1 to 4 million tons. However, many of the
samples used to estimate the composition of the eSMS mounds are
surface grab material, such as chimney fragments enriched in Cu, Zn,
Au and Ag, and may not be representative of the three-dimensional
deposit at depth.
Until now, the focus of the majority of research in the TAG hydrothermal feld has been on the high-temperature active TAG mound.
Here, we focus on the hydrothermally eSMS deposits in the Alvin and
MIR zones, with the aim to characterise their structure and composition
at depth, and determine common processes a?ecting their evolution,
degradation and preservation on and below the sea?oor.

Method
The data reported here were collected during the EU-funded Blue
Mining programme expeditions M127 (RV Meteor) and JC138 (RRS
James Cook) in May to August 2016. An important and novel aspect of
our study was the collection and integration of geological and geophysical data acquired from the same area. The study area was initially
mapped using the ship＊s multibeam echosounder, followed by autonomous underwater vehicle (AUV) missions for high resolution mapping
of the eSMS mounds. These were then imaged using a variety of geophysical methods, that occupied the same profle lines across eSMS
mounds, followed by surveys using a robotic underwater vehicle (RUV)
and drilling by a robotic lander-type sea?oor drilling rig (RD2). From
this body of complementary data we are able to describe the common
features of the eSMS deposits, starting from the surface geology (including exposures of basement in fault scarps) and progressing through
drilled core to a maximum depth of 12.5 mbsf. Whole-rock material
from the core was geochemically and petrologically analysed for major
and trace element variations and sulphide mineralogy. To obtain information on the deeper subsurface, i.e. below the maximum drilling
depth, geophysical methods were used including seismic data acquired
by 25 ocean bottom seismometers placed on and around the eSMS
mounds. These mainly recorded internal re?ections from within the
deposits and the underlying crust, down to depths of ‵400 mbsf.
Refractions from deeper in the crust, down to 3.5 km, were also acquired. Controlled source electromagnetic surveys were also conducted
but the results are presented elsewhere.
3.1. AUV, RUV mapping and surface sampling
Given their water depth (3,600 m) and relatively small size
(100每300 m diameter), high-resolution bathymetry data were acquired
from near the sea?oor using an AUV operated by GEOMAR (Petersen
and Shipboard Scientifc party, 2016) during M127. Multibeam swath
sonar data were acquired from a RESON Seabat 7125 operated at 200/
400 kHz frequencies and processed to a resolution of between 2 and
0.5 m. The AUV was navigated using a long-baseline acoustic system,
and later dead-reckoning using Doppler velocity logging and an Inertial
Navigation System. Multiple missions were undertaken over the TAG
hydrothermal feld, from a variety of altitudes, and the data geographically merged for internal consistency, gridded, and geographically co-registered by locating the images to the 3 m-diameter
steel re-entry cones left by the ODP drilling sites on the active TAG
mound, which have well-known locations. High-resolution maps of the
individual eSMS mounds were generated at a resolution of 0.5 m and
reveal details of the deposits including fault scarps, collapse structures
and extinct chimneys.
The surface geology of the eSMS mounds was studied during the
second cruise (JC138) by videography and sampling from the RUV
HyBIS (Murton et al., 2012), focussing on the Shinkai, Southern and
Rona mounds (Murton and Shipboard Scientifc Party, 2018). Missions
were planned on the basis of the AUV-derived bathymetry maps and
RUV navigation used ultra-short baseline (USBL) with a precision
of ㊣ 5 m and an accuracy of ㊣ 10 m. In total, 52 h of HD video imagery
were obtained during seven dives. A total of twenty-nine HyBIS surface
samples were obtained mainly from Southern Mound, with a few
samples from Rona and Shinkai mounds. These provide information on
the near-sea?oor composition and allowed validation of observations
made from the video images.
3.2. Coring of sediments
Sediment samples were largely collected by 3 m-long gravity coring
to characterise the superfcial sea?oor geology. The thickest sediment
accumulations were identifed from the multibeam bathymetry (being
smooth and having low acoustic backscatter) and shipboard sub-bottom
profler data. Gravity cores were navigated subsea by USBL and acquired from the top and base of the eSMS mounds, and up to several
kilometres away, targeting sediment ponds and areas with thinner sediment. The sediments are classifed as being hydrothermally in?uenced when iron and base metals, such as copper and zinc, exceed the
value found in the background pelagic sediments (carbonate ooze).
Variation in iron and manganese content relate to hydrothermal plume
fall-out, and high-sulphide concentrations are the result of massive
sulphide erosion, transport and deposition.
3.3. Drilling
Following the mapping phase, JC138 deployed the RD2, operated
by British Geological Survey. The RD2 system is capable of coring up to
55 mbsf in water depths up to 4,000 m. Several sensors allow real-time
monitoring of drilling parameters such as revolutions per minute,
torque, bit weight, feed force, feed pressure, feed speed, water ?ow and
water ?ush rates. Landing RD2 on surfaces on the mounds that were ?at
enough and could support the weight of the rig to ensure a fnal inclination of less than 5∼ proved difcult. Despite the relatively precise
navigation provided by our USBL system, these ?at target benches were
no more than 10 m in diameter. To facilitate the fnal landing, ?ashing
beacons were placed on the sea?oor by the RUV HyBIS, following drill
site selection.
3.4. Petrological and geochemical analyses
Full drill cores and sediment gravity cores were described, photographed, split and subsampled on board. Polished thin (30 米m) and
thick sections (200 米m) were made from selected pieces for petrological
and micro-analytical studies using transmitted and re?ected light microscopy.
Bulk geochemical data were obtained from individual or composite
quarter-core pieces of the same lithology. These were crushed, powdered and homogenised with 10 g of each sample being analysed by
Activation Laboratories Ltd. (Actlabs, 2017) in Canada, using: Instrumental Neutron Activation Analysis (INNA), sodium peroxide fusion
with Inductively-Coupled Plasma Optical Emission Spectrometry (ICPOES) & Mass Spectrometry (ICP-MS) analyses, total sulphur infrared
spectroscopy, and cold vapour ?ow-injection atomic absorption spectrometry. Sediment analysis performed on approximately 100 mg of
acid-digested dried and ground material, after applying an internal
spike of Be, Re and In. Analysis was performed by ICP-OES for major
elements (e.g. Ca, Cu, Fe, Mn and Zn) and by ICP-MS for minor elements and trace element (e.g. Cu for some depths). Precision and accuracy were determined for each analytical run by repeat analysis
(n = 3) of the two Certifed Reference Materials: (i) marine sediments
MESS-1 (National Research Council of Canada) and (ii) sulphide ore
mill tailings RTS-1 (National Research Council of Canada). Elemental
precision for each run was less than 4% and 2.7%, respectively, except
for Zn which was up to 13% by ICP-OES due to a low concentration in
RTS-1 and MESS-1. Silica concentrations in jasper samples were determined by X-Ray Flourescence (XRF) using approximately 0.5 g of
dried, ground and homogenised material, mixed with Lithium Tetraborate ?ux and fused into a glass beads. These were analysed on a
Philips? MagiX-Pro 4 kW using a Rh X-ray tube.
Mineral identifcation in sediment was performed on homogenised
dried powder samples using an X-Ray di?ractometer (MiniFlexII,
Rigaku, Japan) equipped with MiniFlex2 + goniometer and detector.
CuK汐1 radiation (1.541 ?) was applied at 30 kV, with 15 mA of beam
current. The 2牟 incidence angle spanned from 5∼ to 60∼ with a scan
speed of 1.2∼/min using a continuous scan mode. Quartz standards were
also run to ensure calibration. The data were analysed using the
※Panalytical Highscore§ software with reference to the ICDD minerals
database 2018.
3.5. Geophysics
To assess the dimensions and structure of the deep interior of the
TAG eSMS mounds, active seismic experiments were performed using
two di?erent air-gun source arrays: a G-gun array (760 cubic-inch, 2 m
depth, 12 s shot interval) for lines up to 10 km-long, and a GI-gun array
(210 cubic-inch, 6 m depth, 9 s shot interval) for the shorter lines up to
5 km-long, respectively. Twenty Ocean Bottom Seismometers (OBS) and
fve Ocean Bottom Hydrophones (OBH) recorded the airgun shots. To
obtain the internal velocity and gross geometries of the deposits, 10 of
the 20 OBS were placed on top of the Shinkai and Southern mounds by
the RUV HyBIS, with an o?set between instruments of about 75 m for
the central part of the mounds. The other instruments (10 OBS and 10
OBH) were deployed by free-fall along crossing seismic profles with
o?sets of 350每750 m between instruments.
Following acquisition, the OBS records were processed by separating the di?erent shot lines, accurately locating the ocean bottom instruments on the sea?oor, fltering to remove source ringing and ?attening of the records to the frst arrival for easier arrival identifcation/
picking. P-wave velocity-depth models for the profles were then generated by forwarding modelling the travel-times using the RAYINVR
ray-tracing software (Zelt and Smith, 1992). Starting conditions, used
to set the initial boundary conditions in the inversion modelling, were
based on laboratory measured seismic velocities for materials recovered
during ODP drilling of the active TAG mound (Table 1). The model was
iterated to establish new velocities for the eSMS mounds that minimise
the misft between the observed and modelled arrivals. Fig. 3 shows
examples for OBS #04, located on the ?ank of Shinkai mounds, and the
best-ft solutions (in black) between the velocity model and the arrivals
picked from the seismic records

Result
4.1. Geology of the TAG hydrothermal feld
Multibeam swath bathymetry mapping of the TAG hydrothermal
feld shows signifcant variability in tectonic and volcanic processes.
The central part of the TAG hydrothermal feld is characterized by
hummocky volcanic lavas that coalesce into mounds, 100每500 m in
diameter, forming a neovolcanic zone ‵1 km to the west of the eSMS
mounds (Fig. 1B). The westernmost part of the area generally comprises
unfaulted hummocky lava mounds and ridges, including a frozen lavalake ‵500 m to the east of the active TAG mound. In the centre of the
TAG hydrothermal feld, a zone of extensional faulting is dominated by
NNE-SSW strikes (Fig. 2A). At ‵29∼09.5∩N, these fault traces bend towards a N-S trend southward, indicative of left-lateral shear. The convergence of these fault orientations coincides with the Alvin Zone,
where the most prominent eSMS mounds are located, and they are
thought to control hydrothermal ?uid pathways from deep within the
crust (Karson and Rona, 1990; Kleinrock and Humphris, 1996).
4.2. Morphology of eSMS mounds
The highest resolution bathymetry reveals changes in slope and
mass wasting features that indicate relative ages for the eSMS mounds
in the Alvin Zone (Fig. 4). Of these, the Shinkai Mound and the adjacent
New Mounds appear to be younger, based on their steeper slopes and
sharp conical shapes with meter-high pinnacles on the summits, indicative of relict hydrothermal chimneys. Double, Southern and Rona
mounds appear to be older, despite being located only 500 m further to
the east, having much smoother and more rounded features, more dome
like than conical, and being intensely dissected by curved faults. The
gentle slopes and smoothness of these older eSMS mounds re?ects the
presence of a thicker sediment cover and the lack of relict chimney
structures. Shinkai and the New Mounds have average slope gradients
(height/radius) of about 0.5 with average slope angles of 27 ㊣ 3∼. In
contrast, Southern, Double and Rona mounds have average slope gradients of about 0.34每0.39, with average slope angles of 20 ㊣ 1∼, respectively (Table 2). Although there is some uncertainty ( ㊣ 10%) in
these slopes due to asymmetry of the mounds and the true extent of the
edge of the sulphide apron, the di?erences are consistent with the relative age of the mounds, described above.
The MIR Zone lies in the south-east corner of the TAG hydrothermal
feld, on an elevated block formed by a west-facing fault that runs for
over a kilometre in a N每S direction (Fig. 2E). It comprises a
(450 m ℅ 300 m), oval-shaped zone of inactive sulphides and relict
chimneys, and the constructive feature is smaller than previously
thought (Rona et al., 1993b). Small mound-like features are arranged
around the circumference of the MIR Zone that is incised by slump
structures and scours on both the SE and NW sides where mass-wasted
blocks and rubble appear to have ?owed down-slope.
4.3 Surface geology of the eSMS mounds
Five of the eSMS mounds (Shinkai, the New Mounds, Southern and
Rona) were surveyed by the RUV HyBIS in detail, and the geological
map produced for Southern Mound is the most comprehensive (Fig. 5).
The majority of Southern and Rona mounds are covered by thin pelagic
sediment. NE每SW trending, conjugate inward dipping and arcuate, low
4.4. Surface lithologies from the eSMS mounds
Surface samples from the eSMS mounds comprise both chimney
fragments and massive sulphide blocks (Fig. 7A每C). Chimney fragments
were only recovered from Shinkai Mound and the New Mounds and
include intact ?uid conduits lined by marcasite, idaite, covellite with
chalcopyrite closer to the outside. Other chimney fragments are composed of chalcopyrite with traces of isocubanite, marcasite and pyrite
hosted in a matrix of amorphous silica.
At Southern Mound, the surface material includes massive pyrite
breccia and layered massive sulphides comprising pyrite and/or marcasite, chalcopyrite and minor chalcocite. The external surfaces of these
samples are coated by a thin FeOOH layer and associated atacamite,
jarosite and minerals of the quartz-group or opal, indicating low-temperature seawater-related weathering. The interiors comprise mainly
porous massive pyrite, with either cubic or coarse-grained subhedral
textures with the latter resulting from high-temperature recrystallization (Fig. 7G). Locally, micro-sized inclusions of sphalerite and/or
chalcopyrite occur within massive, porous, colloform and coarsegrained subhedral pyrite (Fig. 7N,O). Sometimes, this pyrite is overgrown by chalcopyrite,
 that has altered to chalcocite along micro-
fractures, or is surrounded by marcasite (Fig. 7N).
At Rona Mound, surface samples comprise mainly massive sulphide
breccia and material from extinct hydrothermal chimneys. These samples contain colloform and recrystallised pyrite, also overgrown by
chalcopyrite and with chalcocite replacement along zones of microfracturing. Here the matrix of the sulphide breccia consists of quartz
with, locally developed, acicular needles of barite.
4.5. Metalliferous sediments
Sediment thickness varies signifcantly across the TAG hydrothermal feld (Fig. 8). Gravity coring, using a 3 m-long corer, recovered
either the full thickness of the sediment column where the basement
was intersected, or penetrated sediments in excess of 3.1 m, based on
recovery from overshoot by the corer where the basement was not intersected. Sediment thickness (Fig. 8A), facies (based on hydrothermal
in?uence) and Cu concentration across the TAG hydrothermal feld is
shown on maps derived from analyses of the core material (Fig. 8B).
The variation in sediment thickness re?ects a combination of topographical and sea?oor age variation. For example, on the summits of
the eSMS mounds, pelagic and hydrothermal sediment thicknesses
range from being largely absent on the summit of Shinkai Mound, to
between 1.67 and 2.78 m on top of Southern Mound, reach up to 2.78 m
on top of Rona Mound, and between 0.7 and 2.0 m on top of the MIR
Zone. This variability correlates with the relative ages of the mounds, as
discussed above.
The mineralogy of the sulphide-rich sediments, closest to the base of
the mounds, consists mainly of pyrite and chalcopyrite, with subordinate sphalerite and seawater alteration products (e.g. paratacamite). These sediments contain up to 39 wt.-% Fe, 17.2 wt.-% Cu
and 203 ppb Au. In a proximal channel, extending to the south-east of
Southern Mound, a thickness of at least 10 m of metalliferous sediment
was found exposed in fault scarps several hundred metres from the base
of the mound. Here, the sediments comprise alternating centimetrescale layers of fning-upward, fne-grained FeOOH and sulphide (pyrite
and chalcopyrite) sands intercalated with pelagic carbonate ooze enriched in iron oxides (Fig. 9). These mineralogically and texturally
immature sediments indicate rapid deposition as a result of mass
wasting of weathered and oxidised hydrothermal chimney materials
that are rapidly buried by further sediment deposition. Copper concentrations in these sediments are relatively high, reaching up to
10 wt.-%. Pelagic carbonate layers indicate pauses in hydrothermal
sediment deposition. The dip of layers within some of these cores decreases from ‵30∼ at the base to near zero at the top, re?ecting local
and progressive tilting of the basement, probably in response to NW
dipping faults that dissect the channel. Coring and sea?oor video surveys traced the channel for at least 1 km to the south-east of the eSMS
mounds.
In summary, the surface geology reveals a similar structure for a
number of eSMS mounds in the TAG hydrothermal feld. The tops and
?anks of the mounds are composed of unsorted sulphide-talus with
iron-oxide sediment, and often largely draped in a veil of pelagic carbonate ooze. The extent and thickness of the pelagic sediment, the
presence of relict hydrothermal structures, and the aspect ratios and
slopes of the mounds re?ects their relative age. The lower slopes and
margins of the mounds are covered by hydrothermal sediments forming
upwards-fning layers of sulphide sands and iron oxides. Some of these
are intercalated with pelagic carbonates indicating pauses in hydrothermal sediment deposition. The presence of inward dipping, arcuate
faults appears to be a common feature and also correlates with increasing age and size of the mounds, with the oldest and largest having
the most prominent fault structures. Exposure of the interior of the
mounds is limited to these fault scarps and reveals a sequence of pelagic
sediment overlying metalliferous turbidite sediments that in turn overly
massive sulphide breccia that on-laps a basaltic basement of pillow
lavas.
4.6. Drilling results
A total of eight holes were drilled on the top of the eSMS deposits at
Southern, Rona and MIR (Fig. 2C每E), yielding a total core recovery of
9.51 m, with an average recovery of 29.2%, and a maximum penetration of 12.5 mbsf (Table 3). A general down-hole stratigraphy is shown
in Fig. 10, with depths of lithologies reconstructed by correlating recovered core with drilling telemetry (e.g., weight on bit, torque, rotation speed and penetration rate). Geochemical compositions are reported for averages of quarter core sections, and discrete facies, up to
0.25 m long. A common lithological sequence was observed at all three
eSMS mounds including: a layer of pelagic and metalliferous sediment
overlying a layer of dense jasper, several metres thick, that transitions
downwards into massive brecciated sulphide dominated by pyrite with
chalcopyrite overgrowths and veins.
4.6.1. Southern Mound
At the summit of Southern Mound, core was obtained from three
drill holes (stations 022, 031 and 050) (Fig. 10A). The uppermost layer
comprises pelagic carbonate sediment, up to 20 cm thick, underlain by
2.5每3.5 m of orange-brown to reddish coloured, soft and
unconsolidated hydrothermal sediment with oxidised, relict chimney
fragments, confrming observations and samples obtained by the RUV
HyBIS. This material mainly comprises iron-rich clays and rare, centimetre-thick, blackish MnOOH layers, e.g., at 1.8 mbsf. Beneath the ironrich sediments, a layer up to 3.1 m thick of jasper (silica-rich iron
stained material) occurs. This merges downwards into an intercalated,
0.6 m-thick layer of brecciated jasper at around 5.40每5.85 mbsf. The
jasper is quite heterogeneous in appearance, showing a range of colours
from orange (upper layers 每 Fig. 11A-B), to dark red (main portion 每
Fig. 11C-D), to mottled red每orange and grey (e.g. near the base 每
Fig. 11E), and it has variable porosity of 5每20%, measured from thin
sections. The upper-most portion of the jasper layer has vuggy and
sponge-like texture (Fig. 11G) and contains trace quantities of disseminated pyrite that become increasingly massive with depth
(Fig. 11I). The main minerals in the red jasper and jasper breccia are
quartz (‵55每90%) and hematite (‵5每40%), whereas the more massive
orange coloured jasper is typically composed of opal-CT, quartz, and
goethite, with minor hematite. The lower-most parts of the jasper layer
are composed of silicifed breccia containing disseminated pyrite and
rare grains of chalcopyrite (Fig. 11F,K, L). Massive sulphide was encountered at the bottom of the hole (6.7 mbsf) and consists of colloidal
pyrite surrounded by recrystallised pyrite that is locally surrounded by
traces of chalcopyrite (Fig. 7D,M)
4.6.2. Rona Mound
At the summit of Rona Mound, cores were obtained from three drill
holes (stations 057, 065, 068 (Fig. 10B). Although the unconsolidated
pelagic and hydrothermal sediments observed from the RUV HyBIS
images were not recovered, drill telemetry data (rapid penetration rate
and low torque) suggest these are about 2.80 m thick. RUV HyBIS observations indicate they are composed of ‵ 30 cm pelagic ooze overlying 2.5 m of iron-rich sediment. Beneath this, a layer up to 2.5 m thick
of dark red, massive textured jasper was encountered, similar to that
found at Southern Mound, but with a lower total porosity of 5%. The
predominant minerals in the jasper are cristobalite/tridymite
(‵80每90%), with subordinate hematite (‵5每10%) and the deepest
part of the jasper contains patches of intermixed goethite and haematite
with traces of pyrite. This transitions into a zone of mixed red and orange-jasper-silica breccia that consists of cristobalite/tridymite with
minor hematite, goethite, disseminated pyrite and (rare) sphalerite.
Beneath this transitional zone, brecciated massive sulphide was encountered down to a depth of 12.5 mbsf. The upper (6每9 mbsf) part is
composed of porous (5每15%) and silicifed marcasite with minor pyrite,
sphalerite and traces of barite (Fig. 7N). The sulphides have a brownish,
colloidal core of coliform pyrite with overgrowths of marcasite and
sphalerite. The lower (9每12.5 mbsf) part is composed of massive colloidal textured pyrite, frequently surrounded by recrystallised pyrite
that, in turn, is surrounded by chalcopyrite overgrown by sphalerite
(Fig. 7L,O)
4.6.3. MIR Zone
At the MIR Zone, two holes were drilled (stations 073, 076) on a
small platform on the inner north-eastern side of the deposit (Fig. 10C).
Pelagic sediments, with a thickness of 18 cm, are underlain by hydrothermal iron-rich sediments extending down to a depth of 70 cm,
overlying a ‵10 cm thick layer of FeOOH. This is underlain by a 2.8 m
thick zone of jasper, similar to that encountered at Southern and Rona
mounds. Here, the jasper ranges from red to orange and has a mean
porosity of 15每5%. The red jasper consists of quartz (‵55每85%) and
hematite (‵5每35%), with traces of pyrite. The orange jasper is mineralogically distinct, containing quartz (‵60每70%), hematite
(‵5每15%) and FeOOH (‵10%) but no sulphides. At about 3.6 mbsf,
slightly porous massive sulphide breccia is present, dominated by pyrite
with minor chalcopyrite occurring in veins and as overgrowths.
4.7 Metal distribution
Although the three SMS mounds drilled have similar lithostratigraphy, the metal distribution between the mounds is variable (Fig. 10,
Table 4). With the exception of Mn-rich horizons or encrustations, both
on the surface and within the sediments, in which Cu concentrations
can reach 1.42 wt-%, the Fe-rich sediments from the summits of the
three sulphide mounds typically have low concentrations of Cu and Zn
(i.e. less than 0.5 wt-%). At Southern Mound, the Fe-rich sediments
have a Mn-rich horizon (14.9 wt-%) at 1.8 mbsf. At the MIR Zone,
Mn〞rich horizons are found at the top (2.47 wt-%) of the Fe-rich sediments and at its base (3.38 wt-%). The Mn-rich horizon deeper in the
sediment profle coincides with a peak in Cu (1 wt-%), which otherwise
remains low (< 0.2 wt-%) until encountering the massive sulphide
zone. At Southern Mound and MIR Zone, the jasper appears to be
barren of metals of economic interest, whereas at Rona Mound, just
above the brecciated transition zone, it displays some enrichment in Zn
(0.98 wt-%).
At all the eSMS mounds we drilled, the massive sulphide zone has
the highest base metal concentrations. For example, at Southern
Mound, Cu concentrations increase to 0.98 wt.-% at the base of the hole
(6.6 mbsf). At Rona Mound, Cu and Zn concentrations also increase
towards the base of the hole, with two peaks in Cu occurring at
8每9 mbsf (5.8 wt.-%) and 12 mbsf (3.2 wt.-%), while Zn concentrations
reach a peak of 8.4 wt.-% between 7.4 and 9 mbsf. At the MIR Zone, Cu
concentrations in the massive sulphide increase downwards, reaching
16.1 wt.-% (at 4每4.5 mbsf), and then remain high, but variable,
reaching a peak of 20.4 wt.-% at 6.4 mbsf (Fig. 10). Covariation of
metals in samples from both the drill core and the surface material is
summarized in Fig. 12. The manganese-rich horizons are distinguished
by having < 1 wt.-% Zn, < 10 wt.-% Si and S, up to 28 wt.-% Cu, and
up to 35 wt.-% Fe. The jaspers and silicifed jasper breccia have up to
45 wt.-% Si and up to 42 wt.-% Fe, but low S, Cu, and Zn. Surface
massive sulphides have low Zn and S concentrations, but some samples
contain elevated values of Cu and Fe, with a clear trend indicating a
mixture of pyrite and Cu-sulphides (Fig. 12). These sulphides contain
the highest concentrations of Cu (up to 42 wt.-%) re?ecting both the
presence of chalcopyrite and possibly bornite. The sub-sea?oor
sulphides are dominated by pyrite with low Zn (one sample has up to
8.6 wt.-%) and low Si concentrations. Some samples contain up to
20 wt.-% Cu, especially those from the greatest depth below sea?oor. A
single sample has up to 8.6 wt.-% Zn.
4.8. Geophysics 每 seismic results
To assess the dimension, structure and lithology of the deep interior
of the eSMS mounds, coincident geophysical lines were occupied including active seismic studies. Here, we focus on the result of one of the
shorter, 5 km-long (NW-SE) seismic lines shot using GI-guns over both
Shinkai and Southern mounds, with re?ections and refractions recorded
on a number of ocean bottom seismometers placed on and around the
mounds (see Fig. 2A for the orientation of the line and locations of the
OBS＊s discussed here). Fig. 13 shows examples of processed arrivals (see
Methods for details) from record sections from four OBS sensors (OBS-
01, 04, 08, 09) located along the Line 1 (the NW-SE profle line (white
dashed-line in Fig. 2A). The record sections show similar basement and
intracrustal re?ections (PbP and PmP), from within the altered upper
oceanic crust and from the within the unaltered oceanic crust, respectively, which informs us of the geometry and thickness of each layer. In
addition, a crustal refraction (Pb) yields information about the velocity
of the deeper layers. OBS-04 and OBS-08 (located on the crest of the
mounds) have an additional re?ection horizon (PeP), which corresponds to a P-to-P re?ection from immediately beneath the eSMS
mounds.
OBS sensors located on top of the mounds reveal a P-wave velocity
model, derived using the iterative method to best ft the observed arrivals (described in the Methods section above) for the eSMS mounds
and their sub-sea?oor geometry (Fig. 14 and inset detailed panels). The
deeper structure was obtained from 10 km-long G-gun profles. The
shallow structure reveals a ‵350 m wide, 150每210 m thick downward
narrowing cone, with inward-dipping boundaries between relatively
higher and lower velocities that we interpret as corresponding to the
mineralised zone and altered basaltic host, respectively. Our travel-time
inversion indicates the average P-wave velocity within the high-velocity sub-sea?oor mineralised zone beneath the mounds is c.
5.0 ㊣ 0.4 km/s. Additional, very shallow PeP arrivals from within that
zone (Fig. 13B,C) suggest a potential 2-layer architecture, composed of
a higher velocity (4.2每6.6, average 5.4 km/s), 50每100 m thick upperlayer (zone 1) and lower velocity (4每5.5 average, 4.75 km/s), ‵100 m
thick deeper root (zone 2). This transitions into a lower velocity
(3.65 km/s) material (zone 3), which we interpreted as the host rock.
Outside the eSMS mounds, the crustal structure appears as two
distinct layers forming a 2每2.5 km thick section with upper crustal velocities of 3.5每4.7 km/s (See refraction Pb, Fig. 13A,C,D) and a lower
crustal velocity of 5.7 to 7.2 km/s (Fig. 14). The upper crustal layer
thins towards the southeast where it is separated from the lower crustal
section by a marked velocity change with the higher velocities (7.0 km/
s) almost emerging at the sea?oor.
In summary, the seismic data yield signifcant details about the
geometry of the eSMS mounds and their associated velocity, and hence
lithological, structure at depth. Both Southern and Shinkai mounds
have a similar high velocity, cone-shaped body that narrows
downwards, host within a lower velocity surrounding basement.
Shinkai mound has a maximum height of 65 m, a basal radius of 130 m,
a maximum depth for zone 1 of 80 mbsf ㊣ 10% and a maximum depth
for zone 2 of 220 mbsf ㊣ 10%,. Southern mound has a maximum
height of 55 m, a basal radius of 160 m, a maximum depth for zone 1 of
150 mbsf ㊣ 10%, and a maximum depth for zone 2 of 270 mbsf ㊣
10%. The ratio between mound radius, zone 1 and zone 2 maximum
depths for both these examples is ‵0.61 and 1.4每1.7, respectively.

Discussion
5.1. Towards a generic model for eSMS deposits at a slow spreading ridge
Combining the surface geology information and drill core with the
geophysical data from across the TAG hydrothermal feld allows us to
construct a composite model for the structure of the eSMS mounds
(Fig. 15A). The three eSMS mounds have many similarities; they are
draped in a veil of pelagic sediment, underlain by 1每3 m of barren, ironoxide/oxyhydroxide-rich sediments, with intercalated thin layers of
manganese oxyhydroxide. The iron-oxide/oxyhydroxide-rich sediments
have low base metal content, but appear to be derived from weathering
of hydrothermal sulphides from the summits of the mounds (e.g.
chimneys), which have been variably altered through interaction with
seawater. Sediments at the base of, and distal from, the mounds are
composed of sulphide sands and iron oxides, and often have a fning
upwards sequence, typical for turbidite deposits resulting from mass
wasting of the eSMS mounds. The preservation of higher Cu contents,
and pyrite, chalcopyrite and sphalerite grains indicates that seawater
penetration is restricted in these sediments, possibly due to their rapid
burial, lower permeability, and the absence of residual hydrothermal
?ow. Pelagic layers, intercalated with the metalliferous sediments, record hiatuses in the hydrothermal activity. The Mn-rich horizons on
and within the pelagic sediments are likely derived from hydrothermal
plume fall-out, while those within the shallow metalliferous sediment
sequence, and their elevated copper concentrations, are probably the
result of precipitation at a radox front. Tilting of some of the sediments,
with dip decreasing up section, indicates syn-depositionary deformation, probably in response to rotation of the basement by faulting in the
TAG hydrothermal feld. Together, these metalliferous sediments may
present a resource potential in their own right, with a mixture of Cuand Zn-rich material dispersed over an area of several square kilometres, and deserve further investigation.
The formation of a ubiquitous and 2.5每3.5 m thick layer of jasper at
the top of the eSMS mounds, below the sediments, is unusual and has
not previously been reported from hydrothermally active SMS systems.
However, the jasper material has many textural similarities to minor
occurrences at modern hydrothermal vent sites, including the flamentous and sponge-like micro-textures that are possibly the result of
bacterial involvement (Juniper and Fouquet, 1988; Hannington and
Jonasson, 1992; Duhig et al., 1992; Little et al., 1999; Hofmann and
Farmer, 2000; Trewin and Knoll, 2000). The presence of overgrowths of
sulphides in the jasper, often comprising euhedral pyrite and minor
chalcopyrite, combined with the change in colour from dark red to a
lighter shade of red, with bleached mottled areas towards the base of
the layer, associated with a change from hematite to goethite, indicate
the infusion of late-stage, chemically reduced, sulphidic ?uids. It is
thought that jasper formation is a result of silica precipitation from lowtemperature, di?use hydrothermal venting that has resulted in silicifcation of pre-existing iron-rich sediments (Alt, 1988; Sto?ers et al.,
1993). This implies that the formation of the thick jasper layers at the
TAG eSMS sites is a result of late stage, low-temperature, hydrothermal
?uid ?ow, focused on the summits of the waning hydrothermal systems.
The thickness and relative impermeability of the jasper layer is also
signifcant. The rapid transition into fresh, unoxidised pyrite and
chalcopyrite (and copper-rich) massive sulphides immediately below
the jasper layer suggests it acts as an impermeable barrier to the ingress
of seawater. The jasper layer bears some resemblance to occurrences of
silica encountered below the active TAG mound, in stockwork breccia
at depth (Fig. 15B; Humphris et al., 1995). It is also similar to the siliceous layer found above a copper-rich zone, deep within the sedimenthosted Bent Hill deposit (Fig. 15C; Zierenberg et al., 1998). The IRINA
II Mound at the ultramafc-hosted Logatchev vent site (Fig. 15D) also
has a silicifed sulphide breccia cap, although the interior of the deposit
is heterogeneous with sulphide formation restricted to the immediate
vicinity of the active vent sites and the majority of the mound＊s interior
being composed of altered, chloritised and serpentinised ultramafc
lithologies (Augustin, 2007; Petersen et al., 2009). Similar jasper materials are found elsewhere, in the geological record. For example, in
the Iberian Pyrite Belt they are considered vectors to massive sulphide
ore deposits, especially when the jasper is altered into grey chert containing pyrite and chlorite, which grow at the expense of hematite,
attesting to the contribution of reduced ?uids at elevated temperatures
(Barriga and Kerrich, 1984; Barriga and Fyfe, 1988; Carvalho et al.,
1999).
At Southern and Rona mounds, the japers transition in to a weakly
mineralised zone of silicifed breccia. The brecciation is interpreted to
result from volume reduction and collapse, possibly related to anhydrite dissolution at depth. Below the breccia, the deposits are composed
of low-porosity, massive sulphide with little or no gangue material. The
upper 3每6 m of the massive sulphides contains minor amounts of barite,
indicating interaction with seawater at temperatures up to ‵250 ∼C
(Jamieson et al., 2016). At Rona mound, a shallower zinc-rich zone also
occurs (at 7.4每9 mbsf). Copper concentrations generally increase with
depth where primary pyrite is overgrown, and cut by, chalcopyrite
veins, interpreted as evidence for later mobilisation and recrystallisation of copper at elevated temperatures above 300 ∼C. This is probably a
result of high-temperature hydrothermal ?uids mobilizing copper upwards as the mounds grow, similar to that proposed for the active TAG
mound (Humphris et al., 1995). Whether higher grades of copper
continue below 12.5 mbsf, or are restricted to the depths encountered
here, remains unknown.
In contrast to the active TAG mound (Humphris et al., 1995), where
anhydrite dominates the interior, anhydrite is not present in any of the
eSMS cores. This is expected for hydrothermally extinct deposits where
the retrograde solubility of anhydrite at temperatures below 140 ∼C
(Blount and Dickson, 1969) causes it to dissolve once hydrothermal
?uids cease circulating. The inward dipping, arcuate faults that dissect
the oldest and largest eSMS mounds (e.g. Southern, Rona and Double
mounds) are unusual in that they curve around the mounds, have a low
angle (20每30∼), and form graben structures centred on the summits of
the mounds. We interpret their presence as the result of volume reduction and collapse within the mounds caused by anhydrite dissolution, 
and is probably a common phenomenon in the older eSMS deposits where interior temperatures have fallen below 140 ∼C 
(Humphriset al., 1995; Petersen et al., 2000). Shinkai Mound and the New
Mounds, which have little pelagic sediment cover and host many
standing but inactive chimneys, do not show these features, and we
speculate volume reduction within these younger mounds has not
commenced due to higher temperatures maintaining the stability of
anhydrite. The signifcance of the curved faults is in terms of the potential for seawater ingress and alteration of the sulphide at depth. As
we did not drill on the ?anks of the mounds, this possibility remains
untested.
5.2. Estimates of the mass of the sulphide resource
The combined geological and seismic data for each of the eSMS
mounds indicate the deposits extend to depths of up to ‵200 mbsf
where they comprise a layered structure with higher velocity layers
(zones 1 and 2) embedded in a lower velocity host (zone 3). The following description is based on Southern and Shinkai mounds: Zone 1
has rapidly increasing seismic velocity with an average of 5.4 km/s that
extends to ‵100 mbsf and is consistent with the presence of a mainly
sulphide-rich ore body of the type intersected by our drilling. Zone 2
also has a moderately high average velocity, of 4.75 ㊣ 0.3 km/s, and
extends to ‵200 mbsf. We interpret it as a ＆stockwork＊ composed of a
mixture of lithologies potentially including altered basalt, silica and
sulphide (e.g. similar to the TAG active mound stockwork). Zone 3 has a
lower average velocity of 3.65 ㊣ 0.2 km/s and is consistent with an
altered and brecciated basaltic host.
We can estimate the proportion of lithologies forming the di?erent
zones identifed from the seismic model by assuming a mixture of pure
end-member lithologies, (sediment, sulphide (pyrite), jasper (silica) and
chloritised altered basalt) using their seismic velocities measured on
core material recovered from the SMS (Table 5). For example, the
proportion of massive sulphide (XS) in zone 1, can be estimated as:
X (V V )/(V V ) S 1 Z S Z = (1)
where: XS = proportion of massive sulphide; VS = P-wave velocity of
massive sulphide (6.1 km/s); VZ = average P-wave velocity of nonsulphide phase; V1 = average P-wave velocity of zone 1 (5.4 km/s)
Although we do not know the actual composition of zone 1, we can
estimate the maximum and minimum proportions of sulphide by assuming the non-sulphide component is either the highest velocity (silica) or lowest velocity (water-flled pore space) material encountered
during the drilling. This assumption yields an envelope for the upper
and lower estimates of the sulphide present, respectively.
Assuming the non-sulphide in zone 1 is silica (i.e. 4.5 km/s), then
the estimated proportion of sulphide is 56%. Alternatively, assuming
the non-sulphide is pore-water flled porosity, (i.e. 1.5 km/s) then the
proportion of sulphide is 85%. Alternatively, assuming the non-sulphide is altered basalt, (i.e. 3.47 km/s), which we did not encounter in
the drilling but that surrounds the eSMS mounds, then the proportion of
sulphide is 73% (Table 6). Hence in zone 1, the proportion of massive
sulphide may vary between 56 and 85% by volume. The drill core indicates that silica, in the form of jasper, occurs as a 3每5 m-thick layer at
the top of the massive sulphide and then decreases rapidly where it
appears to be absent 3 m below the base of the jasper layer, whereas
open pore space remains present. In addition, and in contrast to the
active TAG mound, there was an absence of anhydrite in the drill core
(i.e. having already dissolved), meaning the non-sulphide component is
most likely to represent water-flled porosity in a brecciated massive
sulphide layer. Hence the proportion of massive sulphide in zone 1 is
likely skewed towards the higher-end of the estimated range.
Repeating this for zone 2 (with an average P-wave velocity,
V3 = 4.75 km/s), and assuming options for the non-sulphide as either
(a) silica, (b) pore water, and/or (c) altered basalt, yields the following
proportions of sulphide: (a) 16%, (b) 71%, (c) 49%, respectively
(Table 6). Using all three non-sulphide options yield signifcantly less
sulphide than in zone 1. Porosity in zone 2 might be assumed to be
closer to zero, because it is greater that 100 mbsf, making the upper
estimate of the amount of sulphide unrealistic. Instead, the concentration of sulphide is likely to be closer to the lower estimate of 16%, given
that a silicifed stockwork containing up to 46% pyrite was encountered
during drilling beneath the TAG active mound (Humphris et al., 1995).
While our calculations suggest this could be achieved for our eSMS
deposits assuming a 1:1 mixture of altered basalt and silica as the nonsulphide matrix to a sulphide breccia, it is also possible to reproduce the
average velocity in zone 2 from a barren mixture of 26% silica and 74%
altered basalt (i.e. no sulphide).
While our new data do not permit predictions about the chemical
composition, or metal tenor, of the interior of the mounds at depths
greater than 12.5 mbsf, the model does predict the total mass of sulphide in the eSMS mounds in the TAG hydrothermal feld (excluding
MIR Zone and Shimmering Mound).
The volume of zone 1 comprises the volume of the eSMS mound
(calculated from its bathymetric expression above the surrounding
sea?oor) plus the volume of an inverted cone that extends below the
base of the mound. Similarly, for zone 2 we can calculate its volume as
an inverted cone extending below the base of the mound, minus the
volume of zone 1 that lies below the base of the mound. Applying the
same geometry to the other eSMS mounds in the area, we can also make
an estimate of the total mass of sulphide in the ore body and stockwork
(Table 7).
Zone 1 for Shinkai Mound has an estimated volume of 1.2 million
cubic metres, including the topographic expression of the mound itself,
and for Southern Mound it is 2.4 million cubic metres. Zone 2 for
Shinkai Mound has an estimated volume of 2.5 million cubic metres
and for Southern Mound it is 3.2 million cubic metres. Assuming the
lower limit for sulphide contents, based on the minimum P-wave velocities and thickness of the zones, and a density of pure sulphide of
5 g/cc, then we estimate the total mass of sulphide for zone 1 (56%) in
and below Shinkai Mound of 3.1 Mt, and for Southern Mound of 9.4 Mt.
This assumes the sulphide in zone 1 is in the form of a silicifed breccia,
similar to that encountered during the TAG active mound drilling
(Humphris et al., 1995). Zone 2 is devoid of any sulphide, if we assume
it is and composed of silicifed altered basalt. Assuming the upper limit
for sulphide content (85%), based on the maximum P-wave velocities
and thicknesses of the zones in and below the mounds, yields an estimated sulphide mass for zone 1 of 5.9 Mt for Shinkai Mound and
17.7 Mt for Southern Mound. Zone 2 has a maximum sulphide content
of 71%, which yields corresponding masses of sulphide of 9.7 Mt and
12.6 Mt for Shinkai and Southern mounds, respectively. In this scenario,
zone 1 and 2 would be composed of massive sulphide breccia with open
pore space, and zone 2 having a larger proportion of void space than
zone 1, which is unlikely given its depth below the sea?oor.
We propose a more realistic scenario for the mass of sulphide, based
on a mean value for the seismically determined P-wave velocities and
thicknesses of the zones, our shallow drilling results and drawing a
comparison with lithologies encountered during the full penetration
during drilling of the active TAG mound (Humphris et al., 1995). In this
scenario, we assume zone 1 is composed of brecciated massive sulphide
with open pore space at the top (as encountered by our drilling), with
an increase in silica flling the open pore space with depth (as encountered by the active TAG mound drilling 每 Humphris et al., 1995)
but with anhydrite being absent. This yields an average proportion of
sulphide in zone 1 of 70%, corresponding to a mass of 3.2 Mt and
10.5 Mt in Shinkai and Southern mounds, respectively. Zone 2 we assume is composed entirely of a silicifed breccia and disseminated sulphide forming a stockwork (as encountered by the TAG drilling 每
Humphris et al., 1995), but with an average proportion of sulphide of
only 16% (compared with 46% beneath the active TAG mound)
yielding 2.0 Mt and 2.6 Mt of sulphide in zone 2 at Shinkai and
Southern mounds, respectively. Together, zones 1 and 2 yield a total
mass of sulphide at Shinkai and Southern mounds of 6.3 Mt and
15.8 Mt. This is approximately 3 times greater than the mass of sulphide
calculated from the topographic expression of the eSMS mounds alone
(Table 7 and Fig. 16). It also indicates that mounds with a radius of
50 m or less are likely to represent deposits of < 1 Mt.
Extrapolating these results to the other eSMS mounds studied in the
TAG area, our estimates yield a range in total tonnage of sulphide in
and below the mounds of between 14 Mt and 54 Mt. Our preferred estimate, based on the assumptions above, yields a total tonnage of 26 Mt.
While these estimates are considerably larger than previous estimates
for the TAG massive sulphide deposits, we stress that there remains an
absence of information about the metal grades below 12.5 mbsf at the
eSMS deposits we have studied. It remains possible that the subsurface
sulphides are barren pyrite, and as such their resource potential is
minimal.

Conclusions
Sea?oor massive sulphides provide a modern analogue of volcanic
massive sulphide deposits found throughout the geological record. They
also have the potential to be a resource for base metals, especially
copper, zinc and precious metals such as gold and silver. This paper
brings together four years of data collection including survey design,
instrument development, deployment across 10 weeks of ship time, and
deep-sea?oor drilling. The geological and geophysical data and their
interpretation described here form, to date, the most detailed, threedimensional characterisation of hydrothermally extinct, sea?oor massive sulphide deposits, hosted by volcanic systems, at a slow spreading
mid-ocean ridge. This research clearly demonstrates a distinct difference between the structure and composition of hydrothermally active
SMS deposits on slow spreading ridges, and hydrothermally extinct
eSMS systems. The main differences are found in the change from active
SMS deposits, where anhydrite dominates an interior comprising massive sulphide overlying a silicifed altered basalt-hosted sulphide
stockwork (Humphris et al., 1995), compared with the eSMS deposits
that have a 1每5 m thick carapace of iron-rich oxide sediments overlying
a 3每6 m thick impermeable silica cap that in turn overlies a dense
massive sulphide ore body of about 100 m thickness, that overlays a
moderate density silicifed sulphide stockwork zone, surrounded by
lower density host rocks of altered basalt. Our combined geological and
geophysics data show that the main ore body is composed of brecciated
and partially silicifed sulphide that extends to a sub-sea?oor depth of
about 100 m with an average sulphide content of about 70% that increases with depth. The underlying stockwork extends to another 100 m
depth and likely comprises ‵16% sulphide in a silicifed matrix that
grades into an altered basalt host rock. These data indicate a resource
that is 2 to 5 times larger than that predicted from the surface expression of the deposits alone. This estimate is even larger if the metal
rich sediment-aprons that surround the eSMS mounds are included.
Our results also give insights into the processes leading to the formation of the eSMS deposits, and especially those that occur at the
closing stages of the hydrothermal cycle. High-resolution AUV-bathymetry (0.5 m resolution) and RUV HyBIS surveys show that the eSMS
mounds have partially collapsed, probably due to anhydrite dissolution
and the resulting volumetric contraction within the main ore body. The
upper ore body is protected from seawater penetration by late stage,
low-temperature hydrothermal silicifcation of iron-rich hydrothermal
sediments, generating a silica cap. Remobilisation and recrystallisation
of the massive sulphide, especially the copper-rich phases such as
chalcopyrite, have led to an enrichment in the upper ten metres of the
ore body, but the impact on metal tenor at depth remains unknown.
The implication of this work is a signifcant upward revision of the
sulphide volume for eSMS deposits at slow-spreading ridges. Whereas a
global database of SMS deposits indicates the majority are relatively
small (Hannington et al., 2011), this study indicates that many of these
are actually larger and extend sub-sea?oor. Depending on their, as yet
undetermined, metal tenor at depth, these deposits may represent more
economically attractive targets than their resource estimates, based on
the volume of the mound alone, would suggest.
Abstract〞We describe the development of feedback control for 
autonomous underwater gliders. Feedback is introduced to make 
the glider motion robust to disturbances and uncertainty. Our 
focus is on buoyancy-propelled, fixed-wing gliders with attitude 
controlled by means of active internal mass redistribution. We 
derive a nonlinear dynamic model of a nominal glider complete 
with hydrodynamic forces and coupling between the vehicle and 
the movable internal mass. We use this model to study stability 
and controllability of glide paths and to derive feedback control 
laws. For our analysis, we restrict to motion in the vertical plane 
and consider linear control laws. For illustration, we apply our 
methodology to a model of our own laboratory-scale underwater 
glider. 
Index Terms〞Autonomous underwater vehicles, buoyancy control, 
glider control, glider dynamics, movable mass, underwater 
gliders.
I. INTRODUCTION 
IN RELATION to existing methods of ocean sampling, 
autonomous underwater gliders offer a host of technical 
advantages: superior spatial and temporal measurement density, 
longer duration missions, and greater operational flexibility. 
These advantages are expected to be greatest when multiple 
gliders are operated cooperatively in a network. The underwater 
glider concept, initially conceived by Henry Stommel 
, has motivated the development of several operational 
gliders, including the SLOCUM glider, the ※Spray§ glider 
and the ※Seaglider§. These are all buoyancy-propelled, 
fixed-winged gliders which shift internal ballast to control 
attitude. Each has many useful features ranging from low 
operational and capital costs, and low noise and vibration to 
high reliability due to simplicity of design, minimal reliance on 
battery power, and low vulnerability of actuator mechanisms 
to the harsh effects of seawater. 
In order for the advantages in ocean sampling using underwater 
gliders to be fully realized, an accurate and reliable glider 
control system should be developed. Most importantly, the use 
of feedback control provides a measure of robustness to uncertainty 
and disturbances. In this paper, we derive a nonlinear 
dynamic model and describe the first steps in development of 
model-based control for a class of underwater gliders, namely 
those with fixed external surfaces which can control buoyancy 
and center of gravity (CG). We emphasize an approach that 
is widely applicable rather than exclusively vehicle-specific; 
accordingly, this work complements the efforts on SLOCUM, 
Spray and Seaglider. 
On SLOCUM, the ballast tanks are configured within the vehicle 
to provide the proper pitching moment during upwards and 
downwards glides. A sliding battery mass is used for 
fine adjustments in pitch and roll. Sensors measure depth, pitch, 
roll, and compass heading. Vehicle position at the surface is determined 
through GPS fix. The pitch angle, an assumed angle 
of attack, and a vertical speed computed from depth measurements 
are used to estimate the horizontal speed of the glider. The 
glider control system periodically checks the glider attitude and 
adjusts the position of the sliding battery mass. The timing of 
mass position adjustments can be changed depending on glider 
performance during the glide. Switching between downwards 
and upwards glides is performed open loop, i.e., the ballast is 
changed and the sliding mass is moved to a new position. 
Sensing and control on other gliders is similar. For example, 
Spray performs active control of pitch and roll every 40 s 
using measured pitch and heading errors. In the case of pitch, 
a low-gain proportional control law is used, and in the case of 
heading, proportional plus integral control is used. 
Our program initiated in this paper to develop a model-based, 
feedback control design methodology is intended to improve 
upon the currently implemented glider control strategies. A systematic 
design methodology that provides control in the full 
state-space is expected to make it possible to design glider controllers 
that require less experimentation and tuning and provide 
more robustness to fouling, payload changes and other uncertainties 
as compared to current techniques. Additionally, with 
a model-based approach, a dynamic observer can be designed 
to estimate states such as glider velocity. These estimated states 
could then be used to determine horizontal glider motion instead 
of the current methods which rely on assumptions of constant 
angle of attack. A model-based approach may also prove useful 
in determining optimal glider motions. 
The dynamic glider model we derive here describes a glider 
with simple body and wing shape. Control is applied to two point 
masses inside the vehicle: the first point mass has variable mass 
but fixed position while the second point mass has fixed mass but 
variable position relative to the center of buoyancy. One control 
input changes the mass of the stationary point and another control 
input vector corresponds to the force applied to the movable 
mass. The model describes the nonlinear coupling between the 
vehicle and the shifting and changing mass. Analysis and control 
law design is performed for the dynamics specialized to the vertical 
plane. Continuous feedback laws are developed. However, 
when energy is at a premium, we envision a scenario in which one 
might occasionally and temporarily turn off the active feedback 
control routine (including sensors), e.g., during periods of relative 
calm or when tight control is less critical. 
The model we derive, although simplified, does capture 
the essential dynamic features of underwater gliding. The 
simplicity allows for development of general control design 
strategies. These strategies along with the insights gained from 
the development are expected to be relevant to the design of 
control laws for the more complex operational gliders. Feedback 
provides robustness to uncertainty, and this uncertainty 
may include unmodeled dynamics. 
Specialization of the glider dynamics to the vertical plane 
constitutes a first step toward a systematic understanding and 
methodology for complete glider control design. In restricting 
to the vertical plane, we ignore, for example, challenges associated 
with currents traveling transverse to the motion of the vehicle. 
Analysis of tail volume requirements, heading corrections 
and the capabilities of shifting mass in this context will be next 
steps in developing methodology to meet these challenges. The 
successful SLOCUM glider experiments at LEO-15 during the 
summer of 2000 suggest promise for this program. In these experiments, 
SLOCUM performed excellent tracking in the presence 
of strong (transverse) currents. 
Throughout the paper, we illustrate our results on a model 
of a small, laboratory-scale underwater glider called ROGUE 
(Remotely Operated Gliding Underwater Experiment) that we 
have built and that we operate in a freshwater tank and pool, see 
Fig. 1. In the first version of this glider, CG position is controlled 
by shifting a lead weight inside the vehicle. In its most recent 
incarnation, ROGUE controls buoyancy and CG position 
by means of a distributed array of independently actuated ballast 
tanks (syringes). 
In related and forthcoming work with colleagues, we address 
issues in optimal path planning for underwater glidersand in 
coordinating control for multiple autonomous underwater vehicles. 
There is a great deal of literature on dynamics, stability, and 
control of airplanes, including [11], [12], [13]每[16], which is 
clearly of interest in the study of underwater gliders. We note, 
however, that added mass forces, variable buoyancy and controlled 
mass redistribution, which play a central role in our study 
of underwater gliders, are not typically relevant for airplanes and 
therefore not included in the airplane literature. 
In Section II, we derive the equations of motion for a 
buoyancy-driven, fixed-wing underwater glider. Controllability 
and observability of steady glide paths in the vertical plane are 
studied in Section III. Linear control laws are developed in 
Section IV for stabilizing these glide paths in the presence of 
disturbances. A simulation of the controlled glider modeled to 
resemble ROGUE is also presented. We give final remarks in 
Section V. 
II. GLIDER DYNAMICS 
The variables used in this paper are defined in Table I. 
A. Equations of Motion in 3-D 
We model the underwater glider as a rigid body with fixed 
wings (and tail) immersed in a fluid with buoyancy control and 
controlled internal moving mass. We take the hull to be ellipsoidal 
with wings and tail attached so that the center of buoyancy 
(CB) is at the center of the ellipsoid. We assign a coordinate 
frame fixed on the vehicle body to have its origin at the CB and 
its axes aligned with the principle axes of the ellipsoid. Let body 
axis 1 lie along the long axis of the vehicle (positive in the direction 
of the nose of the glider), let body axis 2 lie in the plane 
of the wings and body axis 3 point in the direction orthogonal 
to the wings as shown in Fig. 2. 
The total stationary mass, , (also referred to as body mass) 
is the sum of three terms: is a fixed mass that is uniformly 
distributed throughout the ellipsoid, is a fixed point mass that may 
be offset from the CB, and is the variable ballast point mass which 
is fixed in location at the CB. The vector from the CB to the point
mass is . The vector from the CB to the center of mass of the stationary mass is 
The moving internal point mass is . The vector 
describes the position of this mass with respect to the CB at time . 
The total mass of the vehicle is then 
The mass of the displaced fluid is denoted and we define 
so that the vehicle is negatively (positively) 
buoyant if is negative (positive). The different masses and 
position vectors are illustrated in Fig. 3. 
 DEFINITION OF VARIABLES 
Let denote the inertia matrix, with respect to the body 
frame, for the uniformly distributed mass . Define the operator 
so that for a vector , 
Equivalently, for vector i.e., the operator maps a vector to the (skew-symmetric) matrix 
representation of the vector cross product operator. The inertia 
matrix for the stationary (body) mass expressed with respect 
to body frame coordinates is 
Since the variable ballast mass is a point mass located at 
the CB, it does not contribute to , and in particular is a 
constant. 
The orientation of the glider is given by the rotation matrix 
. maps vectors expressed with respect to the body frame 
into inertial frame coordinates. The position of the glider 
is the vector from the origin of the inertial frame to 
the origin of the body frame (vehicle CB) as shown in Fig. 4. 
The vehicle moves through the fluid with translational velocity 
and angular velocity , expressed with respect to the body frame. [Note that we have 
diverged from the notation typical of the submarine literature 
where and . The notation that we use here is taken from texts in classical mechanics such as 
[17] and is more convenient for the derivation and analysis.] In 
this notation, the kinematics of the glider are given by 
Fig. 4. Glider position and orientation variables. 
Let  represent the total translational momentum of the 
vehicle-fluid system and  the total angular momentum of 
the system about the inertial coordinate origin, all expressed 
with respect to the inertial frame. Let represent the total 
momentum of the movable point mass with respect to the inertial frame. 
Then Newton＊s laws state that where is a unit vector pointing in the direction of gravity,
is an external force applied to the system, and is a pure
external torque. These external forces and torques include those
due to gravity and buoyancy; however, gravity is included explicitly
in the third set of equations as it is the only external
force acting on the movable point mass. The force is a
force applied from the vehicle body onto the point mass (a control
force). All vectors are expressed with respect to the inertial
frame. The vector locates the point of application of the force
with respect to the inertial coordinate frame.
Let be the momentum of the vehicle-fluid system expressed
with respect to the body frame. Let be the total
angular momentum about the origin of the body frame. Let
represent the point mass momentum with respect to the body
frame. Then
Differentiating (4)每(6) and using the kinematic expressions (2) 
and (1) gives (7) 
Substituting (3) into (7) for the rate of change of inertial momenta 
gives the following dynamic equations in body coordinates: 
 where is the internal force acting on the 
point mass in body coordinates. Let  
so that 
To derive expressions for , , and , we determine the
total kinetic energy of the glider-fluid system. The kinetic energy
of the rigid body with mass and inertia matrix is
Let be the absolute velocity of the movable point mass
expressed in body coordinates. Given that the velocity of
relative to the body frame is , we compute
The kinetic energy of the movable point mass is then computed 
to be 
Kirchhoff [18] showed that the kinetic energy of an unbounded 
volume of ideal fluid due to the motion of an immersed 
rigid body takes the form where is the added mass matrix, is the added inertia 
matrix, and is the added cross term. These matrices depend 
upon the external shape of the body and the density of the fluid. 
This form of kinetic energy comes from a potential flow solution 
in which the fluid is assumed to be inviscid, incompressible, 
irrotational and motionless at infinity. We incorporate viscous 
effects into the model as external forces and torques (lift and 
drag) below. 
We assume that at low angle of attack, the contribution of the 
wings is dominated by lift and drag forces. Thus, we make the 
simplifying assumption that the added mass and inertia terms 
can be computed solely from the vehicle hull. These assumptions 
are not so critical for the analysis in this paper, especially 
since it is expected that feedback control will provide robustness 
to unmodeled dynamics. Nonetheless, it may be of interest in 
future work to consider more detailed modeling of added mass 
associated with the hull plus wings using methods such as from 
computational fluid dynamics. 
The total vehicle fluid kinetic energy is 
computed to be 
We can then compute momenta as
Since the vehicle hull is ellipsoidal (we neglect the wings in this 
instance), and are diagonal and . Let and . Define 
Inverting the relationships (13) then gives the body velocities in 
terms of the body momenta: 
where is the 3 3 identity matrix. Let and . Furthermore, assume that so that. 
Then,Inverting the relationships (13) then gives the body velocities in
terms of the body momenta: 
To get the equations of motion in terms of body velocities, we 
differentiate (14) with respect to time. We assume that buoyancy 
is changed in a symmetric way (e.g., ballast is pumped on and 
off board in streams with the appropriate symmetry) so that there 
is negligible associated thrust or moment on the glider. Let the 
ballast control input be defined as 
Differentiating (14) then gives 
where 
With the substitution into (16) of (8)每(11) for the derivatives 
, and , (17) for and (13) for the relationship between 
momenta and velocity, the complete equations of motion 
for the underwater glider moving in three-dimensional space are  
Here, 
refer to external forces and moments, in this case lift and drag, 
with respect to the body frame. 
B. Equations of Motion in the Vertical Plane 
We now specialize the model to the vertical plane, the 每 
plane in inertial coordinates and the 每 plane in body coordinates. 
Accordingly, 
where is pitch angle. 
The equations of motion (18) for the gliding vehicle restricted 
to the vertical plane become 
Here, is the angle of attack, is drag, is lift and is the 
viscous moment as shown in Fig. 5. These forces and moment 
are modeled as 
where the s are constant coefficients. This model is a standard 
one, derived using airfoil theory and potential flow calculations 
and then verified using experimental observations, see for example 
[12]. The method for determination of the coefficients 
is described in Section III-C. 
As shown in Fig. 5, we denote the glide path angle by where 
Fig. 5. Lift and drag on glider. 
Fig. 6. Planar gliding controlled to a line. 
We also denote the glider speed by where 
We also denote the glider speed by where
We will typically specify a glide path by desired glide path angle 
and desired speed . We define inertial coordinates 
such that coincides with the desired path: 
Then, measures the vehicle＊s perpendicular distance to the 
desired path. We define two gliding objectives: 
GO1 
The objective is to control only the direction and speed 
of the vehicle＊s glide path. In this case we need not 
consider and at all. 
GO2 
The objective is to control gliding along a prescribed 
line (see Fig. 6). In this case we will include (but 
exclude ) in our analysis and we aim to make . 
The dynamics of the state are 
In the above model, the movable point mass can be controlled 
in all directions (2 degrees of freedom in the planar case). However, 
it may sometimes be the case that control over the CG location 
is restricted, for example, a battery may be shifted in a 
limited way. For the planar case, this might translate into a movable 
point mass with only one degree of freedom. To model this 
we consider the case in which is fixed ( ), i.e., we 
can only move the point mass in the direction.then 
The new equations of motion are (19)每(29) excluding (26) and
(28). Further, is replaced by (32) and is replaced by
which is computed by differentiating (32) with respect to time.
In particular, (22) and (24) are replaced with 
where 
III. CONTROLLABILITY OF STEADY GLIDE PATHS 
In this section, we compute steady glide paths. We then study 
controllability and observability of these glide paths.  
A. Gliding Equilibria 
We prescribe a desired glide path by specifying the desired 
, we see that we must choose glide path angle and the desired speed 
. We denote with subscript ※ § the value of all dynamic variables at 
the glide equilibria. 
To get the conditions for such planar gliding equilibria, 
we set the left hand side of (31) and (21)每(29) to zero. This gives 
or 
and . Note that 
which are all dependent on the equilibrium value of the variable
mass .
Since the drag model is valid only at small angles of attack, we 
take 
Given , (35) and (36) may be solved for . We can then
compute
can then be solved again using (35) and (36). Finally, (34)
gives a one-parameter family of solutions for .
First, we compute from (35) and (36). Note that these
equations reduce to
The first equation of (39) is a quadratic equation in . Provided
and , we have
Equation (40) may be solved for a realizable provided
Evaluating (41) for permissible values of in the range
, we see that we must choose
or
Since the drag model is valid only at small angles of attack, we
take as the solution of (41) with smaller magnitude,
If , then we simply have
For a vehicle which is symmetric about the body 每 plane, 
which are all dependent on the equilibrium value of the variable 
. In this case, for equilibria corresponding to vertical . flight ( ), 
the desired angle of attack is zero. 
Fig. 7. Family of possible movable mass locations for a steady glide. 
We may determine from the latter equation in (39) as 
Finally, we may solve for a one-parameter family of sliding mass locations 
solutions is which satisfy (34). The family of 
where 
and where is a real number. The vector is a particular solution 
of (34). Since is the direction of gravity in body coordinates, 
is orthogonal to the direction of gravity and measures the vehicle＊s ※bottom-heaviness§ as 
shown in Fig. 7.
 For an experimental vehicle, may be a more physically 
relevant choice of parameter than . In this case, 
for a given value of the parameter , provided . 
If , there is an equilibrium of the desired form if and 
If (47) is satisfied, 
is a free parameter. In fact, we should not choose 
to satisfy condition (47) in general because this 
will require that be small. Since contributes to the vehicle＊s 
※bottom-heaviness§ at shallower glide path angles, satisfying 
condition (47) will affect stability of these other equilibria. 
[For a vehicle symmetric about the body 每 plane, 
implies that and . In this case, condition (47) requires that .] 
B. Linearization 
We determine the linearization for the planar glider about a 
steady glide path. 
Let and let . Define 
Then the linearized system is 
(48) where and 
are defined as shown in (49) and (50), shown at 
the bottom of the next page. Here, 
where we have abbreviated as , etc., and
The notation indicates that the quantity is to be evaluated 
at the desired equilibrium. 
This linearization can be used to check features of a given 
vehicle design, e.g., to check stability or controllability of a desired 
glide path given a choice of vehicle design parameters. 
One can also use this linearization to help automate the design 
procedure. For example, consider a vehicle that has been fully 
designed but for a choice of the position of the movable mass 
for a given glide path. Application of the Routh criterion to the 
characteristic polynomial of the matrix gives conditions for 
stability of the glide path. These conditions can be written in 
terms of the free variable. would then be chosen for behavior 
with desired stability and would then be computed according to (46). 
C. Controllability and Observability 
In this section we describe controllability and observability 
of steady glide paths for a model of our experimental vehicle 
ROGUE. ROGUE has an ellipsoidal body with axes of length 
18, 12, and 6 in. The wings are symmetric airfoils from [21] for 
low Reynolds number. We note that the body and wings have not 
been designed for optimal gliding performance but rather in consideration 
of available facilities and other manufacturing constraints. 
Each wing has span of 28 inches with aspect ratio 9.3. 
The glider body, wings and tail are all machined from UHMW 
(ultra-high molecular weight) plastic. Mass and inertia properties 
were measured directly. Added mass and inertia properties 
can be found, for example, in [20]. Lift and drag for the body 
were found experimentally as described in [8]. Lift and drag for 
the wings were taken from the data in [19]. Lift for the body 
plus wings was then computed using Schrenk＊s method [21], 
and drag was computed as the sum of the drag on the wing and 
the body. The lift moment was approximated by taking into account 
the tail. The vehicle model parameters are given as follows: 
The first three masses, , , and were measured with a high 
degree of accuracy. The other terms have less precision because 
they are based on look up tables and approximation methods. 
Four steady glide paths are calculated using the method of 
Section III-A. The glide paths are at glide angles 30 , 45 , 
30 and 45 . We compute the glide path at 30 by choosing 
a desired glide speed m/s and a desired vertical location 
of the movable mass given by cm. This results 
in an equilibrium variable mass given by kg. The 
glide path at 45 is computed for these same values of 
and . The corresponding equilibrium speed for this glide is 
computed as m/s. Similarly, we computed the two steady upward glide paths, for 
the same value of and the same buoyant force magnitude, i.e., the value of is held 
constant. Recall that is the mass of the vehicle less the mass of the displaced fluid . 
The full description of each of the four glide paths is given in Table II. 
Local properties of these steady glide paths can be studied 
using the linearization of Section III-B. By plugging in the equilibrium 
values, we can examine the linearization for stability, 
controllability and observability. The four glide paths listed in 
Table II all have a relatively slow unstable mode. They are all, 
however, locally controllable. That is, and as given by (49) 
and (50), when evaluated at any of the four equilibria, satisfy 
the controllability rank condition. Note that the linearization includes 
the state meaning that controllability extends to the 
variable . Accordingly, we can successfully design a linear 
controller that will locally accomplish not only glide objective 
GO1 but also GO2. 
It is of interest to check the controllability rank condition in 
the case that the movable mass can only move in one direction 
(i.e., is fixed). To do this we have linearized the equations of 
motion for the single degree-of-freedom moving mass described 
at the very end of Section II. Again the new and matrices 
for this case, when evaluated at any of the above four glide paths, 
satisfy the controllability rank condition. Thus, it seems that at 
least for linear type control action, not much is lost in restricting 
the degrees of freedom of the movable mass from two to one. 
On the other hand, for large motions, such as switching from 
an upward to a downward glide path, care needs to be taken 
if restricting the degrees of freedom of the movable mass. For 
instance, while motion of the movable mass restricted to the 
direction would be sufficient for sawtooth maneuvers, motion 
restricted to the direction would not allow for both upward 
and downward glide motions. Because of the glider shape, mass 
motion restricted to the direction will also typically have 
more limited travel as compared to motion in the direction. 
The movable mass for ROGUE is approximately 1/6 of the 
vehicle displacement . This is of the same relative order as the 
movable mass in the gliders SLOCUM, Spray and Seaglider. 
Variations in this mass or its location will not in principle affect 
local controllability of a feasible glide path. In practice, however, 
there are clear tradeoffs associated with moving a large 
mass a short distance versus moving a small mass a large distance. 
For example, a large mass may be energetically expensive 
to move and the necessary motion control resolution required 
may be difficult to achieve. On the other hand, since only short 
travel would be required, there would be a smaller demand on 
vehicle volume. Variation in mass and location will also affect 
the range of feasible glide paths and the nature of the switching 
between them. The variable mass in ROGUE at the equilibrium 
described above is larger relative to vehicle displacement 
as compared to these other gliders. This mass controls glide 
speed; thus, for example, if significant vehicle load required designing 
for a smaller variable mass then maximum glide speed 
would be reduced. 
Observability of the linearized model about the four glide 
paths listed in Table II was also investigated. If GO1 is our objective, 
i.e., if we are interested in controlling only the direction 
and speed of the vehicle＊s glide path, then we need not measure 
. The nine-dimensional dynamic model (which excludes ) is completely observable 
with measurements limited to movable mass position ,and variable mass. In this case, 
pitch angle , pitch rate , linear velocity components and 
and the momentum of the movable mass, need not be 
sensed. Observability means that with the measurements of, 
and , a dynamic observer could be designed to give an estimate 
of the unmeasured states , , , , and .of course, is typically already measured and 
is not so hard to measure, so the real advantage is in the estimation of ,, 
and which are more difficult to measure. The nine-dimensional 
dynamic model is also completely observable with measurements 
limited to , (or ) and . Again this means that using these three measurement signals,
an observer could be designed to estimate the rest of the states. 
If GO2 is our objective, i.e., if we want to control the glider to 
a prescribed line in the plane, then we need a measurement of . 
Recall from (30) that depends on both depth , which is easily 
measured, and horizontal position , which is not so easily measured. 
The measurements , and , together with a measurement of (or alternatively , , and ), 
do not render observable. This means that without an initial condition measurement 
, the trajectory can not be computed. However, with an initial measurement of 
given say from a GPS fix taken when the glider is at the surface, the horizontal motion of 
the glider can be calculated, for example, by making use of the 
estimate of linear velocity from the observer. We note that this 
approach has the potential to improve the accuracy of horizontal 
motion determination over current methods which are based on 
assumptions of constant angle of attack, etc. For example, on 
SLOCUM, the horizontal motion of the glider during the glide 
is estimated from GPS fixes taken at the surface, measured pitch 
angle , an assumed angle of attack and vertical speed computed 
from depth measurements [6]. Similarly, on Spray, horizontal 
flight distance is calculated based on a constant pitch, heading 
and angle of attack to which the vehicle is being controlled [4]. 
IV. CONTROLLED PLANAR GLIDING 
In this section we demonstrate, in simulation, controlled 
gliding in the vertical plane by designing and testing a linear 
controller for the glide path moving 30 downward as described 
in Table II. Since the controller is linear, we expect that it 
should take initial conditions nearby to the 30 downward glide 
path. We demonstrate this result by starting the glider at the 
45 downward steady glide and using the linear controller to 
move it to the 30 downward glide solely by feedback. 
The controller is designed for the linearization about the 30 
downward glide using the LQR (linear quadratic regulator) 
method. This is a standard linear optimal control design method 
which produces a stabilizing control law that minimizes a cost 
function that is a weighted sum of the squares of the states and 
input variables. We assume that all of the states are available for 
feedback. If this were not the case, then, as described above, it 
is possible to design an estimator to determine the unmeasured 
states.
 The cost function to be minimized is defined as 
where and are state and control penalty matrices. and 
were chosen to ensure well-behaved dynamics and to prevent 
large motions in the movable mass position and variable mass 
that would exceed physical limitations. Taking into account real 
or desirable maximum state values, the states associated with 
vehicle and movable mass velocity and variable mass and pitch 
angle were weighted most heavily. No significant tuning was 
performed. The weight selections are given by 
The corresponding control law is where is computed
using MATLAB as the solution to the Riccati equation
given .
 In Figs. 8每10, we show a MATLAB simulation of the glider 
moving first along the 45 downward glide and then switching 
to the 30 downward glide path. This was accomplished by 
turning on the linear controller at seconds. In Fig. 8 we 
show the glide path, in Fig. 9 we show plots of position, pitch, 
linear and angular velocity as a function of time and in Fig. 10 
we show the position of the movable mass, the net buoyant force 
as well as the control inputs as a function of time. In these latter 
two figures we show the results of the controller as applied to 
the linear dynamic model and the results as applied to the nonlinear 
dynamic model. 
The figures show that the 45 downward glide path is in the 
region of attraction of the linear controller designed for the 30 
downward glide path. Furthermore, the transient is very well 
behaved. 
However, we do not expect that the upward equilibria would 
be in the region of attraction of a linear controller designed for 
a downward glide (and vice-versa). This means we would not 
want to use a pure linear feedback solution for switching in a 
sawtooth glide path. Instead, we could consider complementing 
the feedback law with a feedforward term which drives the movable 
mass and the variable mass in a predetermined way from 
initial to final condition. Alternatively, we can consider a nonlinear 
control design approach that would include feedback and 
may or may not include a feedforward term. A feedforward term 
would make it possible to introduce an optimal path between 
two different glides. 
Laboratory experiments of controlled gliding with ROGUE 
will be described in a future publication. 
V. FINAL REMARKS 
We plan to realize the results described in this paper as well 
as future control design developments on experimental gliders 
both of our own construction, e.g., ROGUE, and with our colleagues 
who build sea-worthy gliders, e.g., SLOCUM. 
Fig. 10. Simulation of movable mass, variable mass, and control inputs. 
In future work we intend to further develop the gliding control 
methodology by investigating nonlinear feedback control laws, 
feedforward control and path planning. We are already working 
with colleagues on optimal control theory that is applicable to 
underwater gliders [7] and that may be useful for path planning 
purposes. 
Other future directions include extending our work to gliding 
motion in the horizontal plane, e.g., waypoint following, and to 
glider network maneuvers. Work is already underway to develop 
decentralized control laws to produce underwater vehicles that 
school [9], [10], and these we hope to integrate with control of 
glider dynamics. In support of this effort, we are building an 
experimental, underwater test-bed for multiple-vehicle control 
[22]. Schooling fish function like an integrated sensing system 
and by emulating their traffic rules, we can hope to produce a 
network of gliders that can similarly serve as a fast and effective 
adaptive ocean sensing platform. River piracyâ€”the diversion of the headwaters of one stream into another oneâ€”can dramatically change the routing of water
and sediment, with a profound eect on landscape evolution. Stream piracy has been investigated in glacial environments,
but so far it has mainly been studied over Quaternary or longer timescales. Here we document how retreat of Kaskawulsh
Glacierâ€”one of Canadaâ€™s largest glaciersâ€”abruptly and radically altered the regional drainage pattern in spring 2016.We use
a combination of hydrological measurements and drone-generated digital elevation models to show that in late May 2016,
meltwater from the glacier was re-routed from discharge in a northward direction into the Bering Sea, to southward into
the Pacific Ocean. Based on satellite image analysis and a signal-to-noise ratio as a metric of glacier retreat, we conclude
that this instance of river piracy was due to post-industrial climate change. Rapid regional drainage reorganizations of this
type can have profound downstream impacts on ecosystems, sediment and carbon budgets, and downstream communities
that rely on a stable and sustained discharge. We suggest that the planforms of Slims and Kaskawulsh rivers will adjust
in response to altered flows, and the future Kaskawulsh watershed will extend into the now-abandoned headwaters of
Slims River and eventually capture the Kluane Lake drainage.
River piracy, the diversion of the headwaters of one stream
into another with a higher gradient1, can alter the location
of a drainage divide and dramatically change the routing
of water and sediment, with a profound eect on landscape
evolution24. Although some researchers have examined stream
piracy in glacial environments57, relatively little attention has been
given to proglacial drainage changes. Furthermore, previous studies
of river piracy have dealt with capture over Quaternary or longer
timescales, with no one, to our knowledge, having detailed the
phenomenon in the modern era. Herein, we document rapid river
piracy at the front of Kaskawulsh Glacier, Yukon (Fig. 1), which
occurred in May 2016.
The toe of Kaskawulsh Glacier lies at a drainage divide that
separates the Alsek River watershed, which drains to the North
Pacific Ocean, from the Yukon River watershed, which drains to
the Bering Sea. Prior to May 2016, some of the water flowing
from the toe of the glacier marked the headwaters of Kaskawulsh
River, a major tributary of Alsek River. The remainder of the water
issuing from the glacier flowed via Slims River into Kluane Lake,
the largest lake in Yukon and part of the Yukon River watershed.
Ongoing thinning and retreat of the glacier, caused by over a
century of climate warming812, triggered the river piracy, which was
geologically instantaneous and is likely to be permanent. Substantial
long-term hydrological and ecological impacts13,14 may be expected
as a result, including a reversal in outflow from Kluane Lake,
diminished outflow at the north end of the lake, and perhaps even
seasonal closure of the lake basin.
Recent history of Kaskawulsh Glacier and its terminal lakes
In the early Holocene, when the climate of southwest Yukon was
warmer than today, Kluane Lake discharged to the south towards
the Alsek River15. Kaskawulsh Glacier achieved itsmaximum extent
between 1717 and the mid-1750s1618. During this Little Ice Age
advance, sediment fromthe glacier deposited an outwash fan, which
raised the base level of a river that formerly drained south from
Kluane Lake (Fig. 1), perhaps through much of the Holocene19. By
the middle of the eighteenth century, the glacier completely blocked
this south-flowing river, initiating the north-flowing Slims River,
which rapidly built a delta and floodplain intoKluane Lake. The lake
rose about 12m above the 2006 datum and flowed over a debris fan
at the north end of the lake, rapidly returning the lake to its present
level15,19. Kaskawulsh Glacier began to retreat in the nineteenth
century, with retreat accelerating in the late twentieth and early
twenty-first centuries. Between 1956 and 2007, for example, the
glacier retreated 655m (ref. 9). Roe et al.12 recently developed
a method to test a glacier's retreat against the null hypothesis
that retreat was due to natural climate variability. Applying this
analysis to theKaskawulshGlacier (seeMethods and Supplementary
Fig. 1), we find there is only a 0.5% chance that retreat over the
past centuryand by extension, the observed piracycould have
happened under a constant climate. We therefore conclude that
retreat of Kaskawulsh Glacier is attributable to observed warming
over the industrial era.
Recent retreat of Kaskawulsh Glacier has been accompanied
by the growth of terminal lakes, the largest of which we name`Slims Lake' and `Kaskawulsh Lake' (Fig. 2). Outflow from Slims
Lake has historically been northward into Kluane Lake via Slims
River, whereas outflow from Kaskawulsh Lake has historically been
southward to Alsek River via Kaskawulsh River (Fig. 1). A 1956
aerial photograph9 shows a small Slims Lake on the west side of the
glacier terminus. Only isolated ponds are visible in 1972 and 1980,
with no lake at all in 1990 (Fig. 2). However, by 2000, Slims Lake
had grown to 1.1106 m2, and by 2015 its area had increased more
than threefold to 3.9106 m2 (Fig. 2b,h). ByAugust 2016 (Fig. 2b,i),
Slims Lake had partially drained, its area decreasing to 1.0106 m2,
stranding the outlet of Slims River 17m above the level of the
remnant water body.
The earliest Landsat scene (2 September 1972) shows a small
(1.1106 m2) Kaskawulsh Lake at the east margin of the glacier
terminus (Fig. 2). The lake expanded to1.9106 m2 by 2010, and
to2.7106 m2 by 2015. By early July 2016, its size was three times
that in 1982 (3.3106 m2).
Drainage reorganization in 2016
Although the Slims River gauge record has gaps (Fig. 3g), a clear
reduction in river level is apparent in May 2016. Normal diurnal
fluctuations ended with an abrupt four-day drop in river level
commencing on 26 May 2016. Over the remainder of the summer
of 2016, the level of Slims River was 0.71.0m below the average for
those days. Our acoustic Doppler current profiler measurements at
the Slims River bridge on 3 September 2016, indicated a discharge of
11m3 sô€€€1, less than that reported following the 1970 (21m3 sô€€€1)
diversion. Although we have no measurements immediately prior
to the 2016 piracy, pre-diversion flows of 130m3 sô€€€1 have been
reported for 197020.
The reduction of Slims River flow has had substantial eects at
Kluane Lake. The lake had a record minimum water level in early
May 2016 (Fig. 3ac), about 0.1mbelow its previously recorded low
inMay 2015. By the time of its normal seasonal peak (early August),
the lake was 1.7m below its long-term mean level and 1m below
its lowest recorded level for that time of year. The lowering ofKluane
Lake in 2016 equates to a volumetric reduction of0.67 km3 relative
to the historic average.
Alsek River discharge (Fig. 3df) fluctuated around the historic
mean until early July 2016, after which it rose sharply and fluctuated
around the historic maximum level for that time of year, at times
exceeding it by 1 m. The peak discharge in 2016 occurred on
20 July (1,470m3 sô€€€1), which is nearly as high as any previously
recorded peak flow (1,550m3 sô€€€1, 13 July, 1989) and 87% higher
than the average peak discharge during the 41-yr period of record
(783m3 sô€€€1). The elevated discharge was probably the result of
the emptying of the eastern part of Slims Lake and lowering of
its western part, with the consequent re-routing of water from
Slims River into Kaskawulsh River. By October 2016, Alsek River
discharge had dropped to its historic mean, reflecting the reduction
in discharge from glacier melt at the end of the summer.
About 15mm of rain fell at Haines Junction on 2 May 2016,
but there was little other precipitation during the first half of
2016 (Supplementary Fig. 2). It is therefore unlikely that rainfall
contributed significantly to the mid-summer increase inAlsek River
discharge. Meteorological records from Haines Junction and the
Kaskawulsh Glacier nunatak indicate that mean daily temperatures
during the period JanuaryApril 2016 were, respectively, 4.3 C
and 3.6 C warmer than the 20072016 decadal means for the
same three-month period (Supplementary Fig. 2), which itself was
probably the warmest decade of the past century21. At Haines
Junction,which is 300mlower than the glacier terminus,mean daily
temperatures were above freezing almost continuously after mid-
March, approximately one month earlier than the decadal mean.
At the nunatak, which is 1,000m higher than the terminus, the
main period of above-freezing conditions started in early May,
1.5 weeks earlier than normal, and air temperatures were
unusually warm immediately before the drop in the level of Slims
River (Supplementary Fig. 2). It thus seems likely that Kaskawulsh
Glacier experienced unusually high surface melt in the spring of
2016, which lead to development of an ice-walled canyon (Fig. 4
and Supplementary Fig. 3) and increased flow in Kaskawulsh and
Alsek rivers.
The ice-walled channel that was established across dead ice at the
terminus of Kaskawulsh Glacier (Fig. 4 and Supplementary Fig. 3)
was rapidly enlarged by meltwater and localized collapse of the
channel walls in the summer of 2016, resulting in the 17mlowering
of Slims Lake. Even if the current canyon walls were to collapse and
temporarily block the flow of meltwater into Kaskawulsh Lake, the
blockage could not pond water in Slims Lake to the level required
to re-establish Slims River. For example, at transect AA0 near the
upstream end of the canyon (Fig. 4), the channel floor is at785m
a.s.l. For an ice blockage to raise water levels to reconnect to Slims
River (head at 805m a.s.l.), Slims Lake would need to be >20m
deep, which is more than the height of the canyon walls (Fig. 4b).
Only a re-advance of Kaskawulsh Glacier could block the channel
and refill Slims Lake, but this is very unlikely given recent and
current climate trends. We therefore conclude that the drainage
change is permanent.
Previous workers19 had predicted the capture of Slims River by
Kaskawulsh River, although they were unable to predict when, or
how rapidly, that change might happen. These predictions were
based, in part, on the fact that Kaskawulsh River has amuch steeper
gradient than Slims Riverthe uppermost reach of Kaskawulsh
River has a gradient of 6.1mkmô€€€1, almost five times that of
upper Slims River (1.3mkmô€€€1, Fig. 5). Furthermore, the surface
of Kaskawulsh Lake is approximately 50m lower than that of
Slims Lake (755m versus 805m a.s.l. in September 2016), and also
approximately 25m lower than that of Kluane Lake (780m a.s.l.
in September 2016).
Implications of drainage reorganization
The piracy of Slims River has five important downstream
implications. First, the level of Kluane Lake has fallen and may
fall further, potentially below its outlet at the north end of the
lake (Fig. 1). If this happens, Kluane Lake will become a closed
basin. Second, the large supply of sediment to Kluane Lake from
Slims River2224 has ended, with unknown eects on the structure
and chemistry of the lake and its ecosystems25. For example, in
summer 2016,massive afternoon dust storms occurred almost daily
on the nearly abandoned Slims River floodplain (Supplementary
Fig. 3) due to the lower lake level, possibly altering nutrient fluxes
to the lake. Third, Slims, Kaskawulsh and Alsek rivers must now
all adjust to altered discharges. For example, as flow and sediment
transport in Slims River have decreased greatly, channel stability
there may increase, resulting in conversion from a braided to a
wandering or meandering river planform26. Increased flows in
Kaskawulsh River may increase sediment conveyance and bank
erosion. Fourth, large-scale changes to drainage basin geometry
and re-routing of meltwater may introduce considerable changes
to fish populations and habitat, including eects produced by
the timing of flows27. Fifth, assuming that Kaskawulsh Glacier
continues to thin and recede, Kaskawulsh River may advance its
drainage headward (northward) into the easily erodible Slims River
valley fill and towards Kluane Lake, thereby capturing the discharge
of the small mountain streams that previously joined Slims River. It
is noteworthy that the present head of Kaskawulsh River is 25m
below the current level of Kluane Lake and, as a consequence,
there is gravitational potential to drive continued expansion of
the Kaskawulsh River catchment towards the lake (Fig. 5). Thus,
over time, headward erosion could result in Kaskawulsh River
reaching the south end of Kluane Lake and re-establishing its
former southerly drainage to the Pacific Ocean15,19.
These eects highlight the consequences of climate-induced
glacier retreat, and river piracy triggered by such retreat, on the
routing of water and sediment across a deglaciating landscape. Such
changes undoubtedly were common, and probably occurred on a
larger scale, during the decay of ice sheets in North America and
Eurasia at the close of the Pleistocene, as shown by abandoned
and buried valleys that are common features of formerly glaciated
landscapes.We contend that radical reorganizations of drainage can
occur in a geologic instant, although they may also be driven by
longer-term climate change. The potential consequences of such
channel re-routing and environmental change are amplified by
anthropogenic activity and settlement, and these eects can be
expected to have significant eects on both ecosystem functioning
and economic infrastructure, for example hydroelectric power
generation and water supply, as glacier retreat proceeds in a
future warming climate. Most studies of the eects of climate
change on glacial environments deal with enhanced melt or contri-
butions to sea-level rise. We suggest that the eects can be more
far reaching.Structural Control, Evolution, and Accumulation Rates of Massive Sulfides in the TAG Hydrothermal Field 
 
Sebastian Graber1, Sven Petersen1, Isobel Yeo2, Florent Szitkar3, Meike Klischies1, John Jamieson4, Mark Hannington1,5, Marcel Rothenbeck1, Emanuel Wenzlaff1, Nico Augustin1, Iain Stobbs6 
 
1	GEOMAR ¨C Helmholtz-Centre for Ocean Research Kiel, Germany. 
2	National Oceanography Centre, Southampton, UK. 
3	Geological Survey of Norway (NGU), Trondheim, Norway. 
4	Memorial University of Newfoundland, St. John¡¯s, Canada. 
5	Now at the University of Ottawa, Ontario, Canada. 
6	University of Southampton, Waterfront Campus, Southampton, UK. 
 
 
Corresponding author: Sebastian Graber (sgraber@geomar.de) 

 
Key points: 
	Intersecting fault and fissure populations dominate the structural control of seafloor massive sulfide mineralization 
	90 percent of the sulfide material is contained in the inactive and weakly-active mounds (26 Mt) compared to the high-temperature mound (3 Mt) 
	The structural complexity in the TAG area has lead to a large accumulation of sulfide material in a confined area 	 

This article has been accepted for publication and undergone full peer review but has not been through the copyediting, typesetting, pagination and proofreading process which may lead to differences between this version and the Version of Record. Please cite this article as doi: 10.1029/2020GC009185 

Abstract 
The Trans-Atlantic Geotraverse (TAG) hydrothermal field on the Mid-Atlantic Ridge is one of the best-studied hydrothermal systems to date. However, high-resolution bathymetric data obtained in 2016 by an autonomous underwater vehicle (AUV) reveal new information about the distribution of active and inactive hydrothermal deposits, and their relation to structural features. The discovery of previously undocumented inactive vent sites contributes to a better understanding of the accumulation rates and the resource potential of seafloor massive sulfide deposits at slow-spreading ridges. The interpretation of ship-based and high-resolution AUVbased data sets allowed for the determination of the main tectonic stress regimes that have a first-order control on the location and distribution of past and present hydrothermal activity. The data reveal the importance of cross-cutting lineament populations and temporal variations in the prevalent stress regime. 
A dozen sulfide mounds contribute to a substantial accumulation of hydrothermal material (ca. 29 Mt). The accumulation rate of ca. 1500 t/y is comparable to those of other modern seafloor vent fields. However, our observations suggest that the TAG segment is different from many other slow-spreading ridge segments in its tectonic complexity, which confines sulfide formation into a relatively small area and is responsible for the longevity of the hydrothermal system and substantial mineral accumulation. The inactive and weaklyactive mounds contain almost ten times the amount of material as the active high-temperature mound, providing an important indication of the global resource potential for inactive seafloor massive sulfide deposits. 
 
Plain language summary 
Previously unknown hydrothermal mounds were discovered by an autonomous underwater vehicle in the Trans-Atlantic Geotraverse (TAG) hydrothermal field in the central Atlantic Ocean. The location of the hydrothermal mounds is controlled by crossing fault and fissure populations that have been active at different times. The material in the newly discovered mounds together with that of the previously known mounds accumulates to ca. 29 Mt of hydrothermal material. Most of the tonnage is contained in inactive or only weakly-active mounds where hydrothermal activity has already ceased or is in a waning stage. The hydrothermal activity likely occurs periodically and we assume an accumulation rate of ca. 1500 t/y, which is comparable to rates from other hydrothermal fields. Within the TAG hydrothermal field, however, the sulfides seem to accumulate over long periods of time and in a relatively small area which is a result of the complex tectonic history of the area. 
 
1.Introduction 
Since the discovery of inactive and active massive sulfide chimneys along the East Pacific Rise (Francheteau et al., 1979; Hekinian et al., 1980), these systems and their associated biological communities have been in the focus of international research. Today we know of more than 600 active and inactive sites of hydrothermal venting, either inferred from the detection of hydrothermal plumes (active only) or confirmed from visual ground-truthing (active and inactive) (Beaulieu et al., 2013), with more than 900 additional undiscovered sites estimated to occur along the young mid-ocean spreading ridges (Beaulieu et al., 2015). Seafloor hydrothermal systems play a major role in the input of metals to the ocean, affecting not only chemical but also biological processes (German et al., 2016). Furthermore, seafloor massive sulfide (SMS) deposits, which form at sites of high-temperature hydrothermal venting, are, together with manganese nodules and cobalt crusts, considered as a possible contribution to the future global metal supply (Rona, 2003; Cathles, 2011; Hannington et al., 2011; Petersen et al., 2016). 
Our current understanding of the resource potential of SMS deposits is, however, severely limited by our lack of knowledge about the distribution and size of these deposits. Most of the known marine sulfide occurrences are small (<< 1 Mt each; Hannington et al., 2010; Beaulieu et al., 2013), and clusters of larger deposits, a common observation in land-based sulfide districts (Sangster, 1980), have rarely been documented along mid-ocean ridges (Rona et al., 1993a, 1993b; Cherkashev et al., 2013; M¨¹ller et al., 2018).  
Global resource estimates for SMS deposits are lacking several of the fundamental answers that need to be addressed. There are currently several orders of magnitude between estimates based on observations at the seafloor and those based on calculated metal fluxes (Cathles, 2011; Hannington et al., 2011). In order to produce realistic resource estimates, we must understand how much metal is released by high-temperature fluid convection over a given length of a ridge segment, the specific geological time frame in which it is deposited as massive sulfide, and how much is preserved after hydrothermal activity ceases. Inactive deposits are assumed to be more common and larger than active deposits as they have gone through a full cycle of hydrothermal activity (Jamieson et al., 2014, Van Dover et al., 2018). However, as a consequence of current exploration technologies (i.e. primarily searching for active deposits through investigations of geochemical and geophysical anomalies in the overlying water column), there have been only a few systematic regional surveys for massive sulfide accumulations that target both active and inactive deposits. These recent surveys mainly investigated areas close to the spreading axis using AUVs (Jamieson et al., 2014; Clague et al., 2018; Paduan et al., 2018). Large-scale (tens of square kilometers), high-resolution surveys at a tectonically complex slow-spreading ridge had never been performed. Within the framework of the Blue Mining project, an EU-funded, interdisciplinary project for the development of scientific, technical and legal solutions concerning deep-sea mining, regional survey methods were developed and tested, providing us with the tools to assess the amount and the accumulation rate of sulfides along a significant portion of a slow-spreading mid-ocean ridge segment. In this paper we use ship-based and AUV-based bathymetric data from the TAG hydrothermal field on the central Mid-Atlantic Ridge (MAR) and its geological interpretation to document the distribution of active and inactive hydrothermal mounds in order to investigate the geological processes that govern their location, and, using deposit age data from previous work, to determine deposit growth rates. 
1.1.Geological setting 
The TAG hydrothermal field is located at 26¡ã08¡¯N, on a 40 km-long segment of the MAR between the Atlantis Fracture Zone (30¡ãN) and the Kane Fracture Zone (24¡ãN) (Fig.1). Since 2014 this area has been part of the French exploration license area granted by the International Seabed Authority (ISA). The TAG segment is bounded by two right-stepping non-transform discontinuities (NTDs), which offset the ridge axis by about 4-5 km to the east (Purdy et al., 1990; Semp¨¦r¨¦ et al., 1993). While the segment ends are marked by deep nodal basins, the axial valley floor rises from 4500 m to about 3600 m towards the segment center, which is also marked by a decrease in the width of the axial valley from 9 to 6 km (Semp¨¦r¨¦ et al., 1993).  
The TAG segment shows a strong morphological asymmetry between the western and eastern ridge flanks (Zonenshain et al., 1989). The western flank is characterized by axis-parallel ridges and terraces, which are bounded by major normal faults, extending for more than 10 km along- axis and accommodate throws of more than 100 m (Zonenshain et al., 1989; Kleinrock and Humphris, 1996a). The eastern flank shows a steeper but smoother morphology and rises to water depths of less than 1,400 m (Karson and Rona, 1990). Widespread parts of the lower eastern flank are covered by mass-wasting deposits (White et al., 1998), with only a small number of exposed scarps in the upper section that define major fault blocks (Zonenshain et al., 1989; Karson and Rona, 1990). No ultramafic material has been found to date, however, Zonenshain et al. (1989) reported the exposure of gabbros and dikes at the eastern flank at water depths of 3000 m, indicating that vertical movement along the bounding faults was sufficient to expose lower crustal rocks. 
The valley floor hosts abundant volcanic centers (Zonenshain et al., 1989; Rona et al., 1993a). The western half is dominated by hummocky volcanic ridges that are partially covered by sheet flows (Kleinrock et al., 1996; Kleinrock and Humphris, 1996b). In contrast, the eastern section of the valley floor, which hosts parts of the hydrothermal field, is a highly tectonized volcanic terrain (Kleinrock and Humphris, 1996b) characterized by dense normal faulting and fissuring (Karson and Rona, 1990; Bohnenstiehl and Kleinrock, 1999). 
1.2.	The TAG hydrothermal field 
The TAG hydrothermal field is considered to be basalt-hosted (Rona et al., 1993b). However, based on a near-bottom magnetic survey and the occurrence of gabbros on the eastern slope (Zonenshain et al., 1989), Tivey et al. (2003) suggested that the TAG hydrothermal field lies on the hanging wall of an oceanic detachment fault. Subsequently, several seismic studies (Canales et al., 2007; DeMartin et al., 2007; Smith et al., 2008; Schouten et al., 2010; Zhao et al., 2012) confirmed this and reported the presence of a dome-like fault surface that dips at a steep angle but flattens to about 20¡ã at depths shallower than 3 km below the seafloor under the eastern side of the axial valley. While earlier studies (Tivey et al., 2003; Canales et al., 2007; DeMartin et al., 2007) proposed a young oceanic detachment system, Smith et al., (2008) and Zhao et al. (2012) suggested the presence of an older fault system, extending over 15 km to the east. Therefore, detachment faulting may have shaped the eastern ridge flank for more than 1.35 million years (Zhao et al., 2012). 
Early studies of the TAG field described hydrothermal Mn-crusts in a low-temperature zone on the upper eastern flank (Rona, 1976; Rona et al., 1984). Subsequent video-sled surveys documented the active TAG mound on the valley floor (Fig. 2), venting hydrothermal fluids with exit temperatures greater than 360¡ãC (Rona et al., 1986). During follow-up side scan sonar, manned submersible and video-sled surveys, several inactive, as well as weakly-active mounds, were discovered north-east and east of the active TAG mound (Lisitsyn et al., 1989; Rona et al., 1993a, 1993b; Kleinrock et al., 1996b). The Mir Mound, the largest mound structure in the more extensive ¡°Mir Zone¡± (Rona et al., 1993a), shows weak signs of hydrothermal activity and is located 2 km ENE of the TAG mound, on an uplifted crustal block (Rona et al., 1993b). In the north, several inactive mounds cluster together with another weakly-active mound in the so-called ¡°Three Mound area¡± (southern part of the former ¡°Alvin Zone¡±). Here, the three large mounds, Southern Mound, which shows evidence for lowtemperature fluid flow and associated bacterial mats (Murton et al., 2019), Double Mound (former Alvin Mound) and Shinkai (former New Mound), together with the smaller New Mound 2 and New Mound 3, form a closely spaced group of sulfide occurrences, representing a large accumulation of sulfide material (Fig. 2). In 1998 after more than two decades of research in the TAG area, another weakly-active low-temperature vent site with sulfide mineralization, called Shimmering Mound, was found 1.5 km north of the Three Mound area (Fig. 2a) by side scan sonar and video-sled surveys (White et al., 1998).  
Previous studies documented a local tectonic control on the location of the active TAG mound and its association with an intersection of different fault zones, extending from the eastern flank into the valley floor (Karson and Rona, 1990; Kleinrock and Humphris, 1996a). Following the discovery of the detachment fault, Canales et al. 2007 concluded that the hydrothermal field is located in a high-porosity zone in the hanging wall of the detachment fault, where episodic movement along the fault surfaces may have resulted in increased permeability of the hanging wall, periodically reactivating the hydrothermal system and contributing to its longevity (Tivey et al., 2003). 
Dating of hydrothermal samples suggests that the hydrothermal activity in the TAG area started about 140 ka ago in the Mir Zone and at 125 ka in the low-temperature zone and later formed sulfide mounds (Three Mound area, TAG active mound, and Shimmering Mound) by episodic hydrothermal activity (Lalou et al., 1986, 1993, 1995). The current phase of high-temperature venting is confined to the active mound and is thought to have initiated nearly 80 years ago (Lalou et al., 1995, 1998).  
The heat source driving hydrothermal circulation in the TAG hydrothermal field for the past 140 ka as well as the geometry of the circulation cell are still poorly constrained (Humphris et al., 2015). Both a magmatic body close to 26¡ã06¡¯N (Kong et al., 1992) and heat delivered from the root zone of the detachment several kilometers below the seafloor have been suggested as the possible heat source for the TAG active mound (Canales et al., 2007; DeMartin et al., 2007). Recent studies, however, indicate that the source may be located below the fault (Zhao et al., 2012; McCaig et al., 2013), suggesting that an intrusion into the footwall of the detachment is the most likely heat source for the TAG hydrothermal field (McCaig et al., 2007, 2013; Zhao et al., 2012).  

2.Data and Methods 
During research cruise M127 on board of RV Meteor in 2016, extensive bathymetric data sets were collected using Meteor¡¯s hull-mounted Kongsberg EM122 system, operating at a frequency of 12 kHz, in order to map the entire length of the 26¡ãN segment including the nontransform offsets to the north and south and the off-axis areas (up to 25 km) on each side. The majority of surveys (710 nm in total) were conducted with a symmetrical beam spacing of 45¡ã to both sides (90¡ã full swath angle), a survey speed of 6-8 kn and a line spacing of 3 km, giving a typical swath width on the seafloor of around 7 km and yielding bathymetric grids with a final grid resolution of 30 m. In total, an area of about 4000 km2 was mapped by the shipmounted multibeam system within the study area. 
Additional high-resolution bathymetric data were gathered with the autonomous underwater vehicle (AUV) Abyss 6000 (owned and operated by GEOMAR), which is equipped with a Reson Seabat 7125 multibeam echosounder. Bathymetric data from 11 AUV dives, using the 200 kHz multibeam configuration, were combined using MB System (Caress, 1999) to create a continuous map over a 7 by 7 km area, with a grid resolution of 2 m, positionally fixed with relation to the location of the well-located drilling re-entry cone on the active TAG mound, left behind after the Ocean Drilling Program Leg 158 drilling project (Humphris et al., 1996). More than 47 km2 of the axial valley and the adjacent lower eastern valley wall were mapped in highresolution by the AUV. In addition, the Mir Zone and Three Mound area were also mapped at a grid resolution of 50 cm during two missions, using the 400 kHz configuration, at three different altitudes (20, 50, and 80 m). Processing of the raw data was carried out using MB Systems (Caress, 1999), false soundings were removed with QPS Qimera and a digital bathymetric model (DBM) was created with QPS Fledermaus.  
A series of criteria derived from the bathymetric data sets were used to define hydrothermal targets and to distinguish them from volcanic mounds. First, slope, surface roughness, and shape in the high-resolution topography data were visually examined in order to identify hydrothermal targets on the seafloor (after Jamieson et al., 2014). Additionally, magnetic lows in the AUV-based magnetic data (Tivey et al., 1993), as well as low backscatter anomalies observed in the ship-based multibeam data, were also considered for the identification of potential targets. Subsequently, the most promising targets were visually inspected using HYBIS, a remotely-controlled multi-purpose vehicle (owned and operated by GEOMAR).  
2.1. Geological and structural mapping  
Extensive mapping has been carried out in the area of the TAG segment and the vicinity of the TAG hydrothermal field (Fig. 3, 8, and 10). Mapped units and features are primarily defined by their characteristics in the bathymetric and backscatter data. To guide the classification of the terrain, derivatives from the DBMs such as slope (angle of inclination), aspect (slope direction), curvature (slope changes) and roughness (topographic variation) and were calculated using the open-source software QGIS (v. 3.8.0 Zanzibar; QGIS Development Team, 2019). Structural lineaments were manually mapped as polylines based on the morphological expression in the DBM (an overview is given in Table 1), using the calculated aspect and curvature grids as an aid. Rose diagrams were generated with the QGIS plugin ¡°Line orientation histogram¡±-plugin (v. 2.6).  
2.2. Volume estimations 
Volume calculations for the hydrothermal mounds were conducted in QGIS, by manually defining the outlines of the mound in the high-resolution DBM and subsequently calculating the volume between the actual surface of the mound in the high-resolution data and a reference base. This reference surface is created by inverse-distance weighted (IDW) interpolation based on the terrain along the defined outline and models the original seafloor before the formation of the hydrothermal mounds. We assume that this method is more accurate than using a flat surface at a single depth or evenly tilted plane as a lower boundary of the mounds (an exemplary figure (Fig. S1) for the comparison of the bathymetry and the reference surface of the Mir Mound is included in the supporting information). However, our method does not account for the possibilities that the mounds have formed on top of a smaller volcanic edifice, in a depression or fissure. Due to the irregular shape of the mounds and the rough topography, defining the outlines and extent of the mounds is difficult and the volume and tonnage calculations should be considered as estimates only. A more detailed and precise estimate is, however, only possible by determining the thickness and composition of the interior of the deposits through extensive drilling. 

3.Results 
3.1. Regional geological setting of the TAG segment 
The structural asymmetry between the western and the eastern ridge flanks of the TAG segment is prominent in the ship-based bathymetry. The western flank is characterized by narrow, axisparallel ridges that extend along-axis for up to 37 km (Fig. 1). These ridges are marked by eastsoutheast dipping normal faults with pronounced fault scarps, which reflect the general trend of the ridge axis (~027¡ãN) (see Fig. 3a). In contrast, the topography on the eastern flank is dominated by three axis-parallel ridges that extend over 30 km along axis. The easternmost and largest ridge rises to a water depth of less than 1,400 m. Each of these ridges exhibits prominent northwest facing escarpments. The easternmost ridge shows a smooth northwest-dipping slope, which contains a few structures that are parallel to the spreading direction (Fig. 1). The origin of these structures is not known. Compared to the fault pattern on the western ridge flank, faulting on the eastern flank is more complex. 
The large fault systems bounding the three ridges are adjoined by an area to the east that is dominated by smaller faults with varying dipping directions. Transverse, E-W trending faults are observed north of the hydrothermal field on the eastern side of the axis, extending from the flank onto the valley floor, creating a step-like morphology towards the northern NTD (Fig. 3b).  
The valley floor shows abundant evidence of volcanism in the form of pillow mounds and hummocky ridges, smoother volcanic plains, and small axial volcanoes. A continuous neovolcanic zone cannot be identified along the TAG segment, however, north of the 26¡ã17N NTD a large axial volcanic ridge (AVR) is apparent that extends for more than 18 km to the north and beyond the area investigated here (Fig. 3a). Based on the rough and mostly unfaulted appearance, the hummocky ridges in the center of the axial valley appear to be the areas of most recent volcanic activity and form small neovolcanic ridges, two of which are located 3-4 km west of the active TAG mound (Fig. 3b). Small volcanoes are scattered throughout the valley floor but also on both ridge flanks. Flat-top volcanoes occur in a range of sizes from only a few hundred meters to over 2 km in diameter. 
The hydrothermal vent field, however, is located at the raised eastern half of the valley floor close to the intersection with the eastern bounding wall fault. 
3.2. Geological and structural setting of the TAG vent field  
The area of the TAG hydrothermal field, located on the eastern part of the axial valley, was mapped in high-resolution by the AUV (Fig. 2a). The western half of this area is dominated by pillow mounds. This volcanic terrain stretches from the center of the valley in the west to about 1 km east of the active TAG mound and extends along the entire length of the area mapped by AUV. The southwestern part of the working area, as well as sections west and northwest of the Shinkai mound show intact pillow mounds and smaller hummocky ridges that are not dissected by faulting (see Fig. 2a).  
In contrast, the central part of the area shows strong axis-parallel lineations caused by extensional faulting, which is restricted to a 1-1.5 km wide band north of the active TAG mound and a smaller section 500 m to the south (Fig. 2a). The regions immediately west and south of the active mound as well as the north-western corner of the mapped area show signs of a second, cross-cutting fault system that is oblique to perpendicular to the spreading axis, trending northeast-southwest (Fig. 4). Therefore, we interpret the active TAG mound to be located at the boundary between an extensional zone in the north and an oblique-fissured zone to the south and west. A small hydrothermal mound (#11) was discovered 750 m west of the active mound, which grew superimposed on a larger volcanic mound (Fig. 2a). This mound is covered in hydrothermal material based on a visual inspection and by gravity coring, recovering Fe-Mn-oxyhydroxides (Petersen and Shipboard Scientific Party, 2016). The area east of the active mound is marked by a relict ponded flow which has a smooth surface and exhibits several collapse structures (Fig. 4). It lacks any faulting or fracturing, so appears relatively young, however, a sediment cover of several decimeters has been documented by visual inspection, indicating that this ponded lava flow was not formed by a recent volcanic event (Murton et al., 2018). The ponded flow and the area immediately to the north of it, which is formed by unfaulted and unfissured pillow mounds (Fig. 4), create an ¡°untectonized island¡± separating the extensional fault zone in the west from the Mir block in the east.  
The south-eastern section of the mapped area is characterized by a major slope, extending from water depths of 3,700 m to 2,800 m (see Fig. 2b). Zonenshain et al. (1989) observed lower crustal rocks in this area, however, different lithologies could not be distinguished in our highresolution bathymetric data. This major slope is marked by a fault scarp upslope, which forms the boundary to a plateau-like area. This plateau features highly sedimented volcanic hummocks, which are faulted in a northeast-southwest trend and represents the oldest volcanic terrain in the entire area.  
The northern section of the survey area is characterized by three distinct areas (Fig. 5). A strongly tectonized ¡°chaotic¡± zone with abundant faults and ridges that show highly variable strike directions, dominates the central region. This area separates the sulfide mound-hosting trough with the Three Mound area in the south from the ¡°smooth northern block¡±. This northern block exhibits a smooth seafloor topography and stretches for 1.8 km from a major westwarddipping fault to a corrugated surface in the east. The west-facing fault and the associated topographic high hosts the weakly-active Shimmering Mound, the northernmost sulfide occurrence in this area. The eastern limit of the smooth northern block is dominated by a sharp ridge, which is inferred to be the termination of a detachment fault (Fig. 5). The corrugated surface of this detachment extends for more than 600 m further eastward and extends over a distance of 1.8 km in the along-axis direction, possibly extending even further north, beyond the area mapped by the AUV. The corrugated surface is bounded by a fault scarp in the east, likely the breakaway of this detachment fault (Fig. 5).  
3.3. Structural setting of the Three Mound area 
Mapping of the Three Mound area in 50 cm grid resolution (Fig. 6) reveals distinct differences in the topography, age, and geological history of sulfide mounds located in this area. Southern Mound, as well as Shinkai Mound and the small sulfide mounds New Mound 2 and New Mound 3, are hosted within an elongated trough that runs in a NW-SE direction. This trough is heavily dissected and appears to be older than the axis-parallel faulting. 
The mounds in the west (Shinkai, New Mound 2, and New Mound 3) appear to be unaffected by the axis-parallel faulting and show relatively smooth, steep slopes (median slopes of 29¡ã for Shinkai Mound; 33¡ã for New Mound 3, and 28¡ã for New Mound 2; Table 2). In contrast, the large Double Mound, located in the chaotic zone (Fig. 5) and Southern Mound, which are both located 500 m further to the east, are intensely cut by faults, are sediment-covered (Murton et al., 2019), and their slopes are more gentle (26¡ã and 22¡ã respectively). Rona Mound, an inactive sulfide mound identified during our cruise M127, is located a further 200 m east of Southern Mound. This mound also shows gentle slopes (median slope of 22¡ã) and is bounded by two opposing faults. The smooth topography and the gentle slope of the mound also contrast with the inactive mounds in the west (e.g., Shinkai) and are more akin to the closer Southern and Double mounds. We, therefore, assume that the low relief mounds in the east are older than the steep-sided mounds to the west. The strike of the faults in the Three Mound area varies from N to ENE, deviating slightly from the general trend of the spreading axis. However, the trends are within the range of regional fault orientations. The dipping directions of the faults reflect a horst and graben system rather than axis-facing normal faults (Fig. 6b). The smaller Mont de Reliques and mound #24, which was discovered during M127, are both hosted in the chaotic zone but appear to be unaffected by faulting and might have formed after the dissection of the older Double, Rona, and Southern mounds. 
3.4. Structural setting of the Mir Zone 
The Mir Mound is located 2 km ENE of the active TAG mound and was also mapped at 50 cm grid resolution (Fig. 7). The mound is located just east of the escarpment of a west-facing fault that stretches for over one kilometer in a N-S direction (Fig. 7b), extending into the trough that runs through the Three Mound area. The extent of the Mir Mound appears to be smaller than previously reported based on the observation of metalliferous sediments during submersible dives (Rona et al., 1993b). The mound has an elongated sickle shape and stretches ~400 m in a northwest-southeast direction. The center is marked by a depression, which is connected to the west by an erosional gully (Fig. 7c). Standing and toppled chimneys are visible in the bathymetric data in the northern, eastern, and southern parts of the mound. The upper part of the west-dipping fault is covered by sulfide talus and chimney fragments resulting in a rough surface texture. The area to the west of the fault is characterized by unfaulted hummocky, volcanic terrain. The areas east and northeast of the Mir Mound are characterized by oblique NNE to NE trending faults and ridges. The origin of a few smaller features and mounds to the SE of the Mir Mound (Fig. 7c) cannot be determined by bathymetric observations alone, although their irregular morphology suggests a hydrothermal origin. Two larger targets (Abyss Mound and target #29), which are located along the west-facing fault scarp north of the Mir Mound (Fig. 7a), were also confirmed to be of hydrothermal origin during a visual survey with the towed HYBIS-system.  
3.5. Volume and tonnage considerations for the hydrothermal mounds 
The high-resolution bathymetric data sets offer the opportunity to calculate volumes of the hydrothermal mounds in order to quantify the amount of hydrothermal material amassed along a section of the slow-spreading Mid-Atlantic Ridge (Table 2) and enables us to estimate the accumulation rate over time. To convert volume to tonnage, we have to assume a density for the material. Published density values for individual SMS samples vary between 1.6 and 4.6 t/m3. For more porous surface sulfides average dry bulk values between 3.1 t/m3 (Tivey et al., 1999; Jamieson et al., 2014) and 3.6 t/m3 (Spagnoli et al., 2016) have been used. The most comprehensive set of dry bulk density measurements was performed by Nautilus Minerals for their resource estimate for the Solwara-1 deposit. The dry bulk density of over 400 samples of sulfide-dominant rock from drill core averages 3.3 t/m3 (range from 2.2 to 4.4 t/m3; Golder Associates Pty. Ltd., 2012). Hannington et al. (1998) used an average density of 3.8 t/m3 in their assessment of the active TAG mound based on values for dominantly massive, recrystallized pyrite samples drilled from the subseafloor at TAG (Ludwig et al., 1998). As pyrite is likely the dominant sulfide within all inactive mounds in the TAG field (Lehrmann et al., 2018; Murton et al., 2019) we use an intermediate value of 3.5 t/m3 for our tonnage estimates.  
The largest tonnage calculated for the mounds above the reference surface is contained in the Mir Mound (5.5 Mt), followed by Southern Mound (4.3 Mt) and Shinkai Mound (3.3 Mt). For the active TAG mound, although one of the largest known seafloor massive sulfide occurrences, our estimate indicates a tonnage of only 2.3 Mt above the reference surface. Substantial (> 1 Mt) sulfide tonnage is also present at Double Mound, Shimmering Mound, and Abyss Mound. The other hydrothermal occurrences do not add significantly to the total tonnage estimate of the study area. Unlike for the newly identified larger sulfide mounds that have been sampled (e.g., Rona Mound and Abyss Mound), it is unclear if the hydrothermal areas from targets #24, #29 and near #11 (Fig. 2, 5 and 7) are underlain by sulfide material at all. The cumulative tonnage for the entire field is estimated to be 22.1 Mt of sulfide material deposited above our baseline reference surfaces with the largest portion (~90 %) contained in the inactive and weakly-active mounds. 
As no true thickness information is available for the inactive and weakly-active mounds, this tonnage estimate has a high degree of associated uncertainty. Our volume calculations assume that the mounds are entirely made up of sulfides. However, it is also possible that these mounds could be composed of sulfide material covering preexisting basaltic pillow mounds. However, the slopes of the mounds, the exposure of massive sulfides along the bounding faults (Murton et al., 2019), and results from drilling into the active TAG mound (Humphris et al., 1996) and the upper few meters of Southern, Rona and Mir mounds (Lehrmann et al., 2018; Murton et al., 2019) all indicate a hydrothermal origin of at least these mounds. Sulfide accumulation also occurs below the seafloor, mainly within the alteration pipes of the hydrothermal upflow zone where alteration, brecciation, and dissolution enhance permeability and allow for subseafloor precipitation of sulfide minerals, including sulfide veins and replacement of the host rock (Petersen et al., 2000). Based on drilling observations from the active TAG mound where the subseafloor stockwork zone accounts for 30 % of the total sulfide tonnage (Hannington et al., 1998) we assume similar proportions for mound/subseafloor tonnage for each of the inactive mounds and add 30% subseafloor sulfides to the tonnage. This will raise the tonnage of the entire TAG vent field to just below 29 Mt.  

4.Discussion 
4.1. Regional geological control 
Similar to many other segments on the MAR, the TAG segment is characterized by the interplay of tectonic and magmatic processes (Tucholke and Lin, 1994). However, compared to magmatically robust segments, magmatic accretion at the TAG segment is not focused at the segment center and appears to be more diffuse and distributed along the axis (Zhao et al., 2012). Furthermore, as previous authors have noted, there is a striking asymmetry in this segment of the MAR. The western side is dominated by axis-parallel ridges, terraces, and prominent fault scarps, reflecting the common morphology of slow-spreading ridge crust (Semp¨¦r¨¦ et al., 1993; Tucholke and Lin, 1994).  
Smith et al. (2008) reported bathymetric corrugations from the areas in between the eastern ridges. We could not confirm the proposed corrugations in the off-axis region based on our ship-based bathymetric data with a grid resolution of 30 m, aside from the spreading parallel structures on the steep flank of the easternmost ridge (Fig. 1). Some of these structures show similarities to corrugations, however, the slope is steeper (>23¡ã) compared to the typical lowangle dip of other corrugated surfaces in the Atlantic (Escart¨ªn et al., 2017; Olive et al., 2019). Furthermore, it should be noted, that the resolution of the older bathymetry, on which the presence of corrugations is based, is coarser than the data presented here. Despite the geophysical evidence, the lack of prominent corrugations contrasts with observations at other detachment systems and OCCs in the Atlantic (Blackman et al., 1998; Ranero and Reston, 1999; Escart¨ªn et al., 2003; MacLeod et al., 2009) where extensive corrugated surfaces can be overserved in the ship-based data sets.  
An explanation for the absence of major corrugations in our ship-based bathymetric data might be that the eastern ridges are ¡°rider blocks¡±, rafted blocks of the basaltic hanging wall, superimposed on a large detachment structure, covering the fault plane (Smith et al., 2008; Schouten et al., 2010). This type of detachment system is reportedly more common at segment centers and more frequent along the MAR than previously thought (Reston and Ranero, 2011; Cann et al., 2015). However, the branches of the detachment fault that are bounding the rafted blocks have not been imaged by seismic surveys yet. Furthermore, the exact location of the original breakaway is also not known. A large flat-top volcano, located 23 km east-south-east of the active TAG mound, at the eastern flank of the massif (Fig. 3), appears to be tilted by more than 15-20¡ã to the east, indicating that the breakaway might be in the eastern section of the massif. An accurate interpretation and classification of the off-axis terrain and the determination of the localization of the original breakaway region are hindered by regional sedimentation at this distance to the spreading axis. Assuming an average sedimentation rate of 1.8 cm/ka (Scott et al., 1978) and the estimated half-spreading rate of between 12 and 13 mm/y in the east (McGregor et al., 1977; Tivey et al., 2003), the sediment thickness in this area could be more than 30 m, thereby obscuring smaller features and minor fault scarps in the data.  
4.2. Local geological controls and implications on the location of the hydrothermal mounds 
The high-resolution bathymetry shows a large morphological variability within the hydrothermal field, indicating a strong interplay of tectonic and volcanic processes in the area. The volcanic valley floor is affected to various degrees by faulting and fissuring and can be divided into different structural domains (Fig. 8). The center of the hydrothermal field is heavily tectonized and dominated by extensional faulting. We mapped more than 1200 lineaments in the AUV bathymetry (2 m grid resolution), and the analysis of the strike direction distribution shows three distinct orientation groups (Fig. 9) of which the axis-parallel group and the obliquely trending group are confined to certain areas within the hydrothermal field, implying different causes for their formation. The same trends can also be observed in the 50 cm bathymetry of the Three Mound area and Mir Zone (Fig. 10). The mapped lineaments in both areas show a strong correlation with the orientation of the spreading axis. However, while the Mir Zone shows only minor secondary peaks in the orientation of the lineaments, the Three Mound area exhibits a strong secondary peak at 10¡ãN, potentially representing an older, deviating fault system associated with the formation of the trough, that runs through the Three 
Mound area. The intersection of this feature with the different fault populations seems to be one of the main controlling factors for the location of the hydrothermal mounds in the Mir Zone as well as the Three Mound area. Assuming that this trough is an older transfer fault connecting the large bounding fault in the west of the Mir Zone with the northern area, Shimmering Mound might be located on the same larger fault system, that is however partially overprinted by the chaotic zone. 
The overall lineament orientations are in good agreement with the trends originally reported by Bohnenstiehl and Kleinrock (1999) from the ¡°fault and fissure zone (FFZ)¡± in the center of the hydrothermal field and by Bohnenstiehl and Kleinrock (2000) from the vicinity of the active TAG mound. Bohnenstiehl and Kleinrock (1999) indicate that the majority of the extensional faults in the area (>65%; n=200) dip eastwards. Our observation deviates slightly from this finding and reflects a horst and graben type of faulting within the extensional zone in the central part of the hydrothermal field.  
The active TAG mound and mound #11 occur at the boundary of the extensional zone in the north and the oblique-fissured area in the south (Fig. 8). While mound #11 is bounded by relatively unfissured, intact volcanic terrain to the south, the active mound is located on the north-western side of a larger pillow mound terrain, which is cut by several axis-parallel, westfacing faults and some smaller oblique ones (Fig. 4). Two major, axis-parallel faults occur within 400 m east of the active mound and extend towards the trough in the north, where they bend east just 200 m south of Southern Mound. These faults were already reported by White et al. (1998) and are, based on the cross-cutting relationships, either younger faults or reflect the rejuvenation of older extensional faults in the center of the hydrothermal field. Our observations show that some of the axis-parallel faults are cut by the oblique faults, indicating that the development of the oblique faults followed extension (Fig. 4). However, the two large, axis-parallel faults near TAG offset the oblique faults. This implies that multiple phases of extensional faulting and oblique fracture development occurred within the hydrothermal field.  
Nonetheless, our interpretation of alternating phases of faulting and fissuring deviates from Kleinrock and Humphris (1996a), who proposed that the oblique fissures were pre-existing before the initiation of extensional faulting. These oblique fissures are most likely not associated with extensional faulting as they deviate from the general trend of the spreading axis by more than 40¡ã. However, this deviation of the stress regime is interpreted to be localized because the oblique trend is not present within the entire southern part of the area mapped by the AUV. As previously mentioned, ¡°islands¡± of unfaulted pillow mound terrain, as well as a smaller section of axis-parallel faulted volcanic terrain, are present in the south. The cause of the oblique trend seems to be confined to smaller sections and corridors in the south and north of the area. Bohnenstiehl and Kleinrock (2000) suggested shallow, laterally propagating dikes and the resulting temporal rotation of the stress field as a source for the oblique lineament population, which is known from other spreading zones (Gudmundsson, 1990, 1998) and mainly occurs during episodes of high magmatic activity and dike emplacement. The dominance of the far-field tectonic stress, associated with the plate movement might prevail after a period during which the strain of the dike emplacement dissipates. Such dikes may originate at the spreading axis south of the area mapped here by the AUV, where neovolcanic ridges can be observed in the ship-based multibeam data (Fig. 3). Such volcanic centers and hummocky ridges are commonly fed by fissure eruptions related to major diking events. Bohnenstiehl and Kleinrock (2000) suggest that secondary dikes originated from on-axis fissure eruptions might be sufficient enough to induce axis-oblique fissuring, but they also reported that only 1% of the observed fissures in the vicinity of the active mound show any association with volcanic extrusives.  
Another explanation for the oblique trend might be the accommodation zone proposed by Karson and Rona (1990). Even though there is no clear structure extending from the flank onto the hanging wall, structural features (fault scarps and smaller faults) show contrasting orientations between the northern section and the major slope south of the Mir block. Furthermore, a few fault lines on the lower flank show similar oblique trends to the valley floor, which might indicate that the deviating stress-field was also present during the time of the formation of the eastern flank and that there is a larger regional-scale influence not related to the intrusion and propagation of dikes.  
4.3. Implications for hydrothermal convection 
Humphris et al. (2015) emphasize that the geometry of the convection cell under the hydrothermal system is still not well understood. The location of the weakly-active and inactive sulfide mounds along the N-S trending fault at Shimmering Mound, the slightly curved NWSE trending trough in the Three Mound area, that extends into the N-S trending fault in the west of the Mir Zone, along with the absence of a magma chamber in the vicinity of the hydrothermal field (Canales et al., 2007), seems to favor an interpretation where hydrothermal fluids are fed by convection driven by a regional heat source at depth and where multiple upflow zones develop in response to the opening of local pathways. This is in agreement with the age dating by Lalou et al. (1995) that indicates simultaneous high-temperature activity not only at the active TAG mound but also at Double Mound. This model explains the widespread distribution of sulfide mounds of various ages, the presence of current low-temperature activity at several mounds, and the focus of high-temperature venting solely at the active mound to date. 
The different stages of erosion and tectonic faulting of the sulfide mounds indicate that the location of the upflow zones is changing over time, likely in response to movement along the dominant faults in the specific areas. The diversion of hydrothermal fluids may be further facilitated by the intersection of the faults with past or present-day low-angle detachment faults at depth, which might increase the permeability of the hanging wall (McCaig et al., 2013). 
The prominent corrugated surface that is visible in the northeast part of the AUV-based highresolution map marks the presence of a detachment fault, indicating that a significant portion of the spreading in this area is accommodated by tectonic movement. This corrugated surface can be traced in seismic data for several kilometers, dipping at a shallow angle, towards the spreading axis in the west, where it seems to be bounded by another detachment fault (Szitkar et al., 2019). The relation between the corrugated detachment fault and the proposed major detachment structure at depth that is supposed to extend further to the east (Smith et al., 2008; Schouten et al., 2010; Zhao et al., 2012) cannot be determined based on our data sets alone. However, it is unlikely that the corrugated surface visible in our AUV data is the upper exposed part of the detachment surface proposed by previous studies, as that surface is thought to be at a depth of 1 km beneath the TAG active mound, steepens towards the center of the axial valley, 2-3 km west of the active Mound (DeMartin et al., 2007; Canales et al., 2007), and dips at a much higher angle compared to the low-angle detachment fault associated with the corrugated surface. This implies that the corrugated surface is either superimposed over a larger detachment faults at depth or that the northern part of the hydrothermal field is structurally different from the southern part as suggested by Szitkar et al. (2019). The TAG massif and especially its western part seems to be highly complex and might be formed by a sequence of detachment faults. 
The chaotic zone enclosed by the corrugated surface in the east, Shimmering Mound in the northwest, and the Three Mound area in the south (Fig. 5) appears to be an important area to understand the different crustal processes between the northern and southern section of the mapped area. This part of the eastern flank has been interpreted to be related to large masswasting events (White et al., 1998). However, it cannot be excluded, that the ridges observed in our high-resolution data are instead related to faulting. Several faults and fractures extend into this region, suggesting that it is unlikely that this terrain formed purely by debris flows from the upper flanks further to the east (Fig. 5). Importantly, there is also no prominent concave headwall scarp visible upslope, which would be necessary to produce such an extensive debris fan with a length of almost 2 km and heights of the individual ridges of more than 25 m. Such scars were reported from landslides in the area of the Krasnov and Ashadze hydrothermal fields in the Atlantic (Cannat et al., 2013). Even though a tectonic overprinting of an older debris field cannot be ruled out, a tectonic origin for the chaotic zone seems likely, based on the high-resolution topography. This agrees with the interpretation of high-resolution magnetic data, acquired during our AUV surveys, that led Szitkar et al. (2019) to highlight this location as a ¡°transition zone¡± between crustal blocks of large differences in tilt. 
Even though the heat source driving activity at the active TAG mound might be located beneath the southern part of the hydrothermal field, the lateral extent of the various upflow zones could also span to the area north of Shimmering Mound and the corrugated surface. Unfortunately, we do not have any high-resolution data from this area, and no sulfide occurrences north of Shimmering Mound and the smooth northern block are known to date. Also, stepping further off-axis with high-resolution surveys beyond the Mir Zone did not result in the discovery of additional sulfide occurrences, as most mounds were found close to the intersection of the valley floor with the eastern bounding fault or on the hanging wall section in the western part of the mapped area. The clustering of all known sulfide mounds in a relatively small area and their relative youthfulness (~100,000 years) in comparison to the long evolutional history of the eastern rift valley flank is intriguing. It should be noted, however, that the areal extent of our survey is limited and it cannot be excluded that similarly favorable conditions occurred outside our working area and between the eastern ridges of the TAG massif, especially if these ridges are indeed ¡°rider blocks¡± on a larger and probably older detachment system.  
4.4. Evolution of the eastern flank and hydrothermal activity 
Some dating of hydrothermal precipitates has been carried out (Lalou et al., 1993, 1995, 1998), providing a frame for the development and evolution of the hydrothermal mounds. However, the observations during the M127 cruise showed that the TAG hydrothermal field comprises additional hydrothermal mounds and that distinct morphological differences related to the evolution of the various parts of the vent field also occur.  
The oldest structural unit found in the high-resolution bathymetric data is the uplifted sedimented hummocky terrain in the south-east corner of the working area, at a water depth of 2,800 m (Fig. 8). This volcanic terrain was formed 300 to 400 ka ago (based on the halfspreading rate of 13 mm/y, McGregor et al., 1977) at the valley floor in a similar setting to the current extensional zone, as indicated by the similar fault pattern. With the initiation of the major bounding fault, during a pronounced tectonic phase, this terrain was uplifted by more than 800 m into the upper section of the eastern flank. During this exhumation, the upper faulted terrain was created in the north by continuous normal and transfer faulting. This episode could also be related to the initiation of the detachment fault and the onset of the exhumation of the corrugated surface by the rotation of a normal fault due to localized stress. 
A following magmatic phase created the Mir block, the smooth northern block, and likely parts of the chaotic zone. These blocks were subsequently uplifted along normal faults and, in the case of the smooth northern block, likely dragged onto the detachment surface. During the uplift, these blocks were also highly tectonized and partially covered by mass-wasting products.  
The general structural pattern and the distance to the spreading axis imply that the tectonized areas and the unfissured sections of the hanging wall were largely formed during the same magmatic episode. Even though the ponded flow is not affected by the different fault populations and therefore appears to be young, visual observations (Murton et al., 2018) showed a sediment cover of at least 10-15 cm on top of the lake, which is similar to the sediment depth observed in gravity core samples taken west of the active mound (Petersen and Shipboard Scientific Party, 2016). Hence, there seems to be no major age progression in the volcanic terrain with distance to the spreading axis in this small area. The magmatic phase that formed the main part of the hydrothermal field was succeeded by a phase of ridge-parallel extensional faulting in the eastern part of the valley floor. Unequal movement between the exhumation of the northern detachment, the Mir block, and the valley floor is accommodated in the chaotic zone, resulting in its rough appearance.  
The most recent magmatic phase led to the formation of the neovolcanic ridges in the center of the axial valley observed in the ship-based data sets (Fig. 3). Magmatic activity at the neovolcanic ridges southwest of the hydrothermal field may be related to the propagation of dikes, which intruded along predefined weak zones into the southern area of the hydrothermal field. These dikes may be associated with the small percentage of observed extrusive fissures (Bohnenstiehl and Kleinrock, 2000). Phases of extensional faulting alternated with axis oblique faulting and fracturing, possibly due to shifts in the dominant stress regime. A recent episode of tectonic extension may have led to the rejuvenation of major axis parallel extensional faults east of the active mound (Fig. 4).  
Manganese-oxyhydroxides are the oldest hydrothermal material from the entire TAG hydrothermal field (about 140 ka) and were recovered from the periphery of the Mir Mound. Hence, hydrothermal activity likely started in the east of the valley floor when the Mir Zone was in a distance to the spreading axis similar to the position of the active TAG mound today. The Mir Zone shows signs of oblique and axis perpendicular faulting, but it seems that the block largely stayed intact after the uplift in the lower valley wall and escaped the deviating tectonic stresses.  
In contrast, the highly dissected and sedimented nature of Southern Mound, Double Mound and Rona Mound indicates that these mounds have formed between the uplift of the eastern blocks and the strong spreading-parallel extensional faulting. Shinkai and the smaller New Mounds seem to be spared from this extensional faulting even though they are located only 500 m to the west of the highly faulted mounds. Shinkai and the New Mounds were therefore likely formed after the extensional period that dissected the other mounds in the east. This is in agreement with the unsedimented nature of these mounds that also infers a younger age of the mounds in the west. Unfortunately, there is no published age data for Shinkai or the New Mounds. However, if Shinkai formed during the same time as Southern and Double Mound we would expect a different shape and that the smaller New Mounds would also show some evidence of faulting. Therefore, Shinkai and the two New Mounds might be younger than 40 ka, the youngest age of sulfides reported from the smooth eastern mounds (Double Mound; Lalou et al., 1995). The formation of the steep sulfide mounds in the west might rather correlate with some of the later stages of activity reported for the active TAG mound (excluding the most recent activity pulse that started ca. 80 years ago). Also, Mount de Reliques and mound #24 seem to be unfaulted and appear to be younger than the adjacent Double Mound. Therefore, they might indicate later stages of hydrothermal activity even in the chaotic zone. However, it should be kept in mind that sampling and age dating was scarce and might not be representative for the highly faulted Double and Southern mounds. 
Visual observations from the lower terrace of the active TAG mound (Rona et al., 1986) indicate that venting is often linked to fractures rather than discrete discharge. This and that recent fractures in the sedimented areas just east of the mound suggest recent tectonic activity (Kleinrock and Humphris, 1996a). However, Kong et al. (1992) detected no earthquakes in the immediate vicinity of the active hydrothermal mound or beneath the actively developing faults to the east. The nearest recorded events were reported more than 2 km away and at depths exceeding 4.5 km below seafloor.  Earthquakes detected by Pontbriand and Sohn (2014) close to the active mound are very small and are likely related to the precipitation of anhydrite within the mound and not associated with tectonic activity. 
4.5. Tonnage considerations 
We infer a total sulfide tonnage of about 29 Mt (including 6.7 Mt in the stockwork zones) to be present in the active, weakly-active, and inactive sulfide mounds of the TAG vent field (Table 2). This estimate is based on the volume calculations derived from the AUV-based highresolution topography and assuming a density of 3.5 t/m3. We can test our estimate by comparing it with published tonnage information obtained by drilling of the TAG active mound in 1994. Hannington et al. (1998) reported that 2.7 million tons of sulfide are present in the mound itself with an additional 1.2 million tons contained in the stockwork zone. Their calculations were based on an average density of 3.8 t/m3 determined by Ludwig et al. (1998) on the drill core samples. Our calculation for the TAG active mound indicates a tonnage of 2.3 million tons above the reference surface. This is only slightly lower than the estimate of Hannington et al. (1998) and mainly a result of a slightly lower mound volume (647,000 m3 versus ~700,000 m3) and the lower density used in our calculations (3.5 t/m3). We, therefore, assume our tonnage estimate for the entire hydrothermal field (29 Mt) to be a reasonable estimate. Overall, these 29 Mt are a significant portion of the roughly 600 million tons of sulfide assumed to be present within the neovolcanic zone of the global mid-ocean ridges (Hannington et al., 2010, 2011). Therefore, the TAG segment hosts a significant proportion (~5%) of the global estimate for the neovolcanic zones, which is substantially larger than most SMS deposits known to date (Hannington et al., 2010). It should also be noted that the presence of additional unknown hydrothermal mounds within the TAG hydrothermal field is likely, as some potential targets could not be investigated further due to time constraints. Their presence would increase the total accumulation of sulfide material even more. It is still unknown whether this elevated tonnage is a unique attribute of the TAG hydrothermal field, due to its tectonically complex regional and local geological setting, or if it is instead related to a lack of detailed investigations in other hydrothermal systems thereby underestimating their tonnages. 
Studies from the Central Indian Ridge, also report significant sulfide occurrences from short, tectonically complex ridge segments (e.g. Kairei and Edmond vent fields, Van Dover et al., 2001; Okino et al., 2015), indicating that the tectonic history of a ridge segment plays an important role in the accumulation of sulfides. Furthermore, the estimated 600 Mt global tonnage is a representation of mainly active sites in the neovolcanic zones (~1000 sites) and does not reflect the potential of SMS occurrences on the entire seafloor (Hannington et al., 2010, 2011). Studies from other hydrothermal fields in the Atlantic, Indian Ocean, and the Pacific have shown that with an increasing number of high-resolution surveys in the vicinity of known deposits, more inactive deposits are being discovered (Jamieson et al., 2014, Paduan et al., 2018).   
The tonnage reported here for the TAG hydrothermal field is comparable to values reported for ophiolite-hosted mining districts on Cyprus, where single deposits have tonnages of 0.05 to 16 Mt (Hannington et al., 1998). However, the tonnages are substantially lower than values reported from other large mining districts on land (e.g. Abitibi Greenstone Belt, Canada). The amount of sulfide-dominated hydrothermal material within the TAG hydrothermal field amounts to an average of 630 kt/km2, based on an area of 47 km2. However, this value is calculated based on the entire area mapped by the AUV, which does not necessarily represent the extent of the hydrothermal field. Calculating the tonnage in relation to the smallest rectangular dimensions possible for the vent field that includes all known mounds (~14 km2), would increase the potential ore volume to about 2.1 Mt/km2. These values are higher than the values reported from the Endeavour Segment of 0.02 Mt/km2 (Jamieson et al., 2014).  
Assuming an average metal content in the inactive mounds similar to that drilled at the active TAG mound (Cu 2.8 %, Zn 0.4 %, Ag 14 ppm, Au 0.53 ppm; N= 45; Hannington et al., 1998) the field would comprise 804 kt Cu, 110 kt Zn, 402 t Ag, and 15.2 t Au. We use here the weighted average metal content of Hannington et al. (1998) for the interior of the TAG active mound due to a lack in sampling from the interior of the weakly active and inactive mounds. Only a few sulfide samples have been drilled from the upper parts of Southern Mound, Rona Mound, and the Mir Mound in 2016 (Lehrmann et al., 2018, Murton et al., 2019) and their geochemical composition is likely, not representative for the entire mounds. Murton et al. (2019) report copper concentrations averaging 3.1 wt.% Cu (N=19), however, they show clear variations between the mounds. A single sample from Southern Mound has 1.0 wt.-% Cu, while average concentrations of 1.5 wt.% Cu at Rona Mound (N=14) and 9.5 wt.-% at Mir Mound (N=4) are reported. With the exception of a single sample from Rona Mound (8.4 wt.% Zn) the drill core sample suite is characterized by low Zn concentrations and averages 0.6 wt.% Zn (N=19). These values are slightly higher than the average value for the interior of the TAG mound, mostly because of a few high-grade samples from the Mir and Rona mounds. Due to the low sample number and the limited penetration into the mounds we consider the average grade from the TAG active mound published by Hannington et al. (1998) to be a more reasonable estimate for the bulk composition of the inactive and weakly-active mounds. Using these metal grades, the surface metal density for the TAG area would be just over 20.000 t Cu+Zn/km2. These values for the TAG field, however, only include the identified hydrothermal mounds and do not account for any potential accumulation of sulfides in the metalliferous sediments surrounding the mounds or the thick (2-3 m) Cu-rich sediments in the channel-like depression in the deepest parts of the trough south of Rona Mound (Fig. 8 and Fig. 10a). The latter was observed in gravity core samples retrieved during cruises M127 and JC138 (Petersen and Shipboard Scientific Party, 2016; Murton et al., 2019). The lateral extent of these metalliferous sediments is not known. However, based on the topography, the retrieved sediment cores, and observations made during M127 and JC138 we inferred that thick hydrothermal sediments occur in the surroundings of all larger mounds (Fig. 10), commonly below a layer of pelagic sediment. The channel-like depression likely received input through mass transport of sulfides and other hydrothermal components from the erosion of the adjacent Southern, Rona, and Abyss Mound, but probably even from the distal Mir Zone (indicated by the white arrows in Fig. 10). 
4.6. Sulfide accumulation rates 
Assuming a constant spreading rate (12¨C13 mm/yr) over the last five hundred thousand years, the crust hosting the oldest known sulfides in the TAG vent field around the Mir Zone (4 km distance to the spreading center), may not be older than 300,000 years. Our results indicate that sulfide formation is related to large scale faulting following this volcanic phase. The oldest record of hydrothermal activity in the area, determined from hydrothermal manganese crusts collected at the Mir Mound and from the low-temperature zone in the east, indicates a minimum age of hydrothermal activity of 125,000 to 140,000 years (Lalou et al., 1986, 1993). Dating of hydrothermal precipitates indicates that high-temperature sulfide formation at the Mir Mound started around 100,000 years ago (Lalou et al., 1993). The formation of 29 Mt of sulfides in a 100,000-year period would result in an accumulation rate of ~300 t/y. However, episodic hydrothermal venting has been proposed by age gaps identified from dating of the hydrothermal deposits in the area. Lalou et al. (1995) suggest that, over a ~50,000-year history, hydrothermal activity at the active TAG mound has been confined to only 20 % of this time. If we assume a similar episodic activity for the other inactive sulfide mounds, sulfide accumulation rates for the entire field over the last 100,000 years could amount to ~1,500 t/y during ~20,000 years of hydrothermal activity. This rate lies above the published range of estimated accumulation rates that vary from 1 to 800 t/y (compiled in Jamieson et al., 2014) that include everything from individual black smokers to entire vent fields. Hannington et al. (1998) calculated a value of between 500-1000 t/y for periods of hydrothermal activity at the active TAG mound, while the accumulation rate for all active and inactive sulfides along the entire Endeavour segment was estimated to be about 400 t/y (Jamieson et al., 2014). The sulfide accumulation rate within the entire TAG hydrothermal field is likely not significantly higher than that of other hydrothermal systems. However, it is the duration of hydrothermal activity in a confined area that raises the sulfide tonnage at the TAG hydrothermal field above average when compared to other vent fields. 

5.Conclusions 
By combining multi-resolution bathymetric data sets, we were able to significantly advance our understanding of the relationships between the volcanic, tectonic, and hydrothermal features and processes associated with the TAG hydrothermal field. Our conclusions can be summarized as follows. 
1.The active TAG mound and the sulfide mounds in the Three Mound area are located at the intersection of different fault and fissure populations that mark distinct tectonic zones that relate to temporal changes in the prevailant stress regime. 
2.The presence of a corrugated surface in the north of the field adds complexity to the structural control on hydrothermal venting and contributes to its longevity.  
3.The Mir Mound, Southern Mound, and Shimmering Mound are located close to major faults that might intersect the detachment faults at depth, enabling the low-temperature hydrothermal activity. 
4.The inactive to weakly-active sulfide mounds are of different age indicating temporal and regional variability in the location of upflow zones from an undefined heat source at depth. 
5.The estimated sulfide tonnage for the TAG vent field is ~29 Mt, including an estimated 30% of subsurface stockwork mineralization. Most (~90 %) of the hydrothermal material lies in inactive and weakly-active sulfide mounds, highlighting the importance of developing new exploration methods that do not rely on water column anomalies caused by a buoyant plume.  
6.The location of the inactive and weakly-active mounds along the margin of a larger detachment system and the discovery of new mounds during cruise M127 indicates that there is still potential for further discoveries within the area and possibly even further off-axis between the large ridges further to the east. 
7.The calculated sulfide accumulation rate is ~1.500 t/y, assuming that hydrothermal activity occurred only during 20% of the estimated 100,000-year history of venting at this site, and is higher than that reported from other modern seafloor vent fields. However, the TAG vent field is not characterized by a different and more powerful type of hydrothermal activity. It is the tectonic control and longevity of the system that confines sulfide formation into a small area. 
There is a need to extend the survey area to the north and east away from the axis in order to fully quantify the extent of hydrothermal deposition. This would also provide additional information about the evolution of the eastern flank and the geographic extent of the hydrothermal convection cell. Petrological sampling along the eastern wall is lacking and would help in reconstructing the tectonic evolution of the TAG segment. Further investigations in the sedimented areas between the large, linear ridges to the east may also provide information about the sequence of hydrothermal activity and shed light on possible hydrothermal activity east of the known TAG vent field.  
Abstract
We present a linear controller and observer design
for autonomous underwater gliders based on a
model of glider dynamics that we have derived. Our
focus is on buoyancy-propelled, fixed-wing gliders
with attitude controlled by means of internal mass
distribution. In cases when some states cannot be
directly measured, such as with oceangoing gliders,
the design of an observer offers possible improvements
over current glider control methods.
1 Introduction
Initially conceived by Henry Stommel [16], autonomous
underwater gliders offer many advantages
in ocean sensing: superior spatial and temporal
measurement density, longer duration missions and
greater operational 
exibility. Several oceangoing
gliders are operational or under development, including
the SLOCUM glider [18], the "Spray" glider
[14] and the "Seaglider" [3]. These are all buoyancypropelled,
fixed-winged gliders which shift internal
ballast to control attitude.
On SLOCUM, the ballast tanks are configured
within the vehicle to provide the proper pitching
moment during upwards and downwards glides
[17, 18]. A sliding battery mass is used for fine adjustments
in pitch and roll. Sensors measure depth,
pitch, roll, and compass heading. Vehicle position
at the surface is determined through GPS fix. The
pitch angle, an assumed angle of attack, and a vertical
speed computed from depth measurements are
used to estimate the horizontal speed of the glider.
The glider control system periodically checks the
glider attitude and adjusts the position of the sliding
battery mass. Switching between downwards
and upwards glides is performed open loop, i.e., the
ballast is changed and the sliding mass is moved to
a new position.
Sensing and control on other gliders is similar.
For example, Spray performs active control of pitch
and roll every 40s using measured pitch and heading
errors. In the case of pitch, a low-gain proportional
control law is used, and in the case of heading, proportional
plus integral control is used [14].
The goal of our work is to improve upon currently
implemented glider control strategies. Our aim is
to develop a widely applicable approach, complementing
efforts on SLOCUM, Spray and Seaglider.
Toward that end we have developed a nonlinear
glider dynamic model and examined the use of linear
controllers and observers. A model-based approachmay
also prove useful in determining optimal
glider motions (see [2] for early work in this direction).
Our glider dynamic model is derived in [10],
where we examine steady glides, controllability and
observability in the vertical plane.
In this paper we review the derivation from [10]
and present a controller design that uses a linear
observer. Because of space and power constraints,
and because of the diffculties of underwater sensing,
many operational gliders do not carry the sensors
necessary to determine the full glider dynamic
state. In particular, glider position and velocity are
costly to measure directly. We design a dynamic
observer to estimate the glider state from a limited
set of measurements. These estimated states can be
used to determine horizontal glider motion instead
of the current methods which rely on assumptions
of constant angle of attack.
Our dynamic glider model describes a glider with
simple body and wing shape. Control is applied to
two point masses inside the vehicle: we control the
mass of a point with fixed position and the forces on
a mass with variable position. The model describes
the nonlinear coupling between the vehicle and the
shifting and changing mass. Analysis and control
law design is performed for the dynamics specialized
to the vertical plane.
In related work with colleagues, we address issues
in optimal path planning for underwater gliders
[2] and in coordinating control for a network of
autonomous underwater vehicles [1, 9, 15].
In Section 2 we describe our laboratory-scale
glider ROGUE, see Figure 1.1. ROGUE controls
buoyancy and CG position by means of a distributed
array of independently actuated ballast tanks (syringes).
In Section 3, we derive the equations of motion
for a buoyancy-driven, fixed-wing underwater
glider. Controllability and observability of steady
glide paths in the vertical plane are studied in Section
4. Linear observers and linear control laws are
developed in Section 5 for stabilizing these glide
paths in the presence of disturbances. A simulation
of the controlled glider modelled to resemble
ROGUE is also presented. We give final remarks in
Section 6.
2 The ROGUE gliding vehicle
ROGUE is a laboratory-scale gliding vehicle designed
for experiments in glider dynamics and control.
The vehicle consists of a water-tight body,
modular wings and tail, internal ballast actuators,
and on-board electronics and batteries. Using its
on-board computer and sensors the vehicle is capable
of autonomous or remote operation. ROGUE's
ellipsoidal body has axes of length 18, 12 and 6
inches. The vehicle body contains the vehicle actuators,
power, computer and sensors, and has mounts
for the modular vehicle wings and tail. Different sets
of wings and tail can be attached to perform experiments
with different vehicle hydrodynamic profiles.
We note that the body and wings have not been designed
for optimal gliding performance but rather in
consideration of available facilities and other manufacturing
constraints. Experiments are currently
planned using the ROGUE vehicle in our twentyone-
foot diameter laboratory tank and in an olympic
sized pool at Princeton University.
In the configuration shown in Figure 2.1, each
wing has span of 28 inches with aspect ratio 9.3.
The wings are symmetric airfoils from [13] for low
Reynolds number. The tail was sized using standard
aircraft rules as in [4] and [11] . The vertical
tail is designed to give yaw stability in forward glides
and is modular, allowing tail volume to be changed.
The glider body, wings and tail are all machined
from UHMW (ultra-high molecular weight) plastic.
In Figure 2.1 the top half of the vehicle is separated
from the bottom so the internal components
are visible. The metal box at the center of the vehicle
contains the vehicle on-board computer. Two
syringe-type ballast tanks are visible, one on each
side of the computer housing. Visible to the left of
each ballast tank is its driving servo (they appear as
black squares). To the right of the computer housing
are two white battery packs.
ROGUE contains three Nickel-Cadmium battery
packs to provide power to the vehicle actuators,
computer and sensors. Two "8-packs" of AA batteries
power the computer, sensors and signal conditioning
electronics. A "4-pack" of AA batteries
powers the servos and vehicle optional R/C receiver.
ROGUE contains four independently actuated
syringe-style ballast tanks aligned along the vehicle
long axis, two mounted on each side of the vehicle
centerline. The two ballast tanks in the upper half
of the vehicle point forwards, while the two in the
lower half point to the aft of the vehicle. Each tank
is made of a clear plastic cylinder with an internal
plunger. Ball screw mechanisms with a high torque
modified RC servo drive each ballast tank. The tank
connects to the vehicle exterior via tubing. Moving
the plunger inside each ballast tank transfers water
from the tank to outside the vehicle or the reverse.
Filling the tanks with different amounts of water
causes ROGUE to become positively or negatively
buoyant and moves the CG relative to the vehicle
centroid. Each tank has a 100 mL capacity for .4"
of plunger travel. Together the four tanks have a
400 g capacity. The relative positions of the four
tanks allows for moderate adjustments in the vehicle
CG position. The high torque servos driving
the plungers connect to linear potentiometers and
use feedback control to regulate the tank's plunger
position. The tanks are filled completely with water
before ROGUE is deployed to eliminate any air
bubbles in the tanks.
ROGUE's on-board computer is a TattleTale
Model 8. The TattleTale Model 8 is based on the
Motorola 68332 chip and also includes a PIC 16C64
chip for low-power operation. The TattleTale allows
up to 8 analog inputs and up to 25 digital input/
output signals. The Motorola 68332 chip contains
a central processor as well as a "time processing
unit (TPU)" capable of performing simple
clock functions independently of the main processor.
There are 12 TPU channels available to the Tattle-
Tale user, each of which can generate a clock signal,
measure the duty cycle of a square wave signal,
etc. Electronics have been added to multiplex the
analog input signals doubling the computer's analog
input capacity to 16 channels. The TattleTale
Model 8 includes 256 kilobytes of RAM (temporary
memory) and 256 kilobytes of 
ash EEPROM (permanent
memory). The memory can be expanded to
any size available on a PCMCIA memory expansion
card.
ROGUE's computer serves three major purposes:
to store sensor data, process sensor data according
to a pre-defined feedback control law, and apply
command signals to ROGUE's actuators according
to a feedback control law or to a remote command
from a modified R/C transmitter.
TattleTale programs are written in C and compiled
on a host computer. Programs may be downloaded
from PC to TattleTale via serial link to the
RAM or to EEPROM. The TattleTale is housed
within a metal box to shield it from interference.
Also mounted within the metal box is an optional
R/C receiver for remote operation of the vehicle.
ROGUE's sensor suite includes the following elements:
two depth sensors, two single-axis inclinometers
mounted to measure pitch and roll angles,
and three angular rate sensors mounted to measure
pitch, roll and yaw rates. The vehicle also contains
analog preamplifiers for each of the sensors.
The amplifiers and angular sensors are mounted in
a metal box in the lower half of the vehicle. The
two pressure sensors are differential wet/wet sensors
located at the fore and aft of the vehicle hull and
are connected to the vehicle exterior and interior by
tubing. The pressure sensors are used to measure
depth. All sensors are connected to the on-board
computer for logging data and performing feedback
control using sensor data.
3 Glider Dynamics
The variables used in this paper are defined in Table
3.1.
3.1 Equations of Motion in 3D
We first review the derivation of the equations of
motion for our underwater glider. For a more detailed
derivation see [10]. Our choice of glider body
and wing configuration is motivated by the designs
of ROGUE, SLOCUM, Spray and Seaglider. We
model the underwater glider as a rigid body with
fixed wings (and tail) immersed in a 
uid with buoyancy
control and controlled internal moving mass.
We take the hull to be ellipsoidal with wings and tail
attached so that the center of buoyancy (CB) is at
the center of the ellipsoid. We assign a coordinate
frame fixed on the vehicle body to have its origin at
the CB and its axes aligned with the principle axes
of the ellipsoid. Let body axis 1 lie along the long
axis of the vehicle (positive in the direction of the
nose of the glider), let body axis 2 lie in the plane
of the wings and body axis 3 point in the direction
orthogonal to the wings as shown in Figure 3.1.
The total stationary mass, ms, (also referred to
as body mass) is the sum of three terms: mh is a
fixed mass that is uniformly distributed throughout
the ellipsoid, mw is a fixed point mass that may be
offset from the CB, and mb is the variable ballast
point mass which is fixed in location at the CB.
The vector from the CB to the point mass mw is
rw. The vector from the CB to the center of mass
of the stationary mass ms = mh +mw +mb is rs.
The moving internal point mass is m. The vector
rp(t) describes the position of this mass with respect
to the CB at time t. The total mass of the vehicle
is then
The mass of the displaced 
uid is denoted m and
we define m0 = mv-m so that the vehicle is negatively
(positively) buoyant if is negative (positive).
The different masses and position vectors are
illustrated in Figure 3.2.
Let Jh denote the inertia matrix, with respect to
the body frame, for the uniformly distributed mass
mh. Define the operator ^ so that for a vector x =
(x1; x2; x3)T ,
Equivalently, for vector y = (y1; y2; y3)T , ^xy =
xy. The inertia matrix for the stationary (body)
mass expressed with respect to body frame coordinates
is
Since the variable ballast mass mb is a point mass
located at the CB, it does not contribute to Js, and
in particular Js is a constant.
The orientation of the glider is given by the rotation
matrix R. R maps vectors expressed with
respect to the body frame into inertial frame coordinates.
The position of the glider b = (x; y; z)T is
the vector from the origin of the inertial frame to the
origin of the body frame (vehicle CB) as shown in
Figure 3.3. The vehicle moves through the 
uid with
translational velocity v = (v1; v2; v3)T and angular
velocity 
 = (
1;
2;
3)T , expressed with respect
to the body frame. (Note that we have diverged
from the notation typical of the submarine literature
where v = (u; v;w)T and 
 = (p; q; r)T . The
notation that we use here is taken from texts in classical
mechanics such as [5] and is more convenient
for the derivation and analysis.) In this notation,
the kinematics of the glider are given by
Let P be the momentum of the vehicle-
fluid system
expressed with respect to the body frame. Let
 be the total angular momentum about the origin
of the body frame. Let Pp represent the point mass
momentum with respect to the body frame. We differentiate
the expressions relating the momenta P,
, and Pp to the momenta in the inertial frame.
Applying the kinematic expressions (3.2) and (3.1),
then applying Newton's laws, gives the following dynamic
equations in body coordinates:
where all vectors are expressed with respect to the
inertial frame. The vector xi locates the point of
application of the force fexti with respect to the inertial
coordinate frame. fexti is an external force
applied to the system and extj is a pure external
torque. k is a unit vector pointing in the direction of
gravity. These external forces and torques include
those due to gravity and buoyancy; however, gravity
is included explicitly in the third set of equations
as it is the only external force acting on the movable
point mass. is the internal force applied from the vehicle
body onto the point mass (a control force) in body coordinates.
Define the control vector
To derive expressions for P, , and Pp, we determine
the total kinetic energy of the glider-
uid
system. Let Ts be the kinetic energy of the rigid
body with mass ms and inertia matrix Js. Let vp
be the absolute velocity of the movable point mass
m expressed in body coordinates. Given that the
velocity of m relative to the body frame is r_p, we
compute
The kinetic energy Tp of the movable point mass is
then computed to be
Kirchhogff [8] showed that the kinetic energy of an
unbounded volume of ideal 
uid due to the motion
of an immersed rigid body takes the form
whereMf is the added mass matrix, Jf is the added
inertia matrix and Df is the added cross term.
These matrices depend upon the external shape of
the body and the density of the 
uid. We assume
that at low angle of attack, the contribution of the
wings is dominated by lift and drag forces. Thus,
we make the simplifying assumption that the added
mass and inertia terms can be computed solely from
the vehicle hull.
The total vehicle 
uid kinetic energy T = Ts +
Tp + Tf is computed to be
Since the vehicle hull is ellipsoidal (we neglect the
wings in this instance), Mf and Jf are diagonal
and Df = 0. Let Mf = diag(mf1;mf2;mf3) and
Jf = diag(Jf1; Jf2; Jf3). Define
where I is the 3 * 3 identity matrix. Let M =
diag(m1;m2;m3) and J = diag(J1; J2; J3). Furthermore,
assume that mw = 0 so that rs = 0.
We can then compute the momenta as
Inverting these relationships then gives the body
velocities in terms of the body momenta. To get
the equations of motion in terms of body velocities,
we differentiate the equation for the body velocities
with respect to time. We assume that buoyancy
is changed in a symmetric way (e.g., ballast
is pumped on and off board in streams with the
appropriate symmetry) so that there is negligible
associated thrust or moment on the glider. Let the
ballast control input u4 be defined as
Dififerentiating the body velocities
then substituting for the derivatives and substituting (3.9)
for the relationship between momenta and velocity, the
complete equations of motion for the underwater glider moving 
in threedimensional space are refer to external forces and moments, in this case
lift and drag, with respect to the body frame.
3.2 Equations of Motion in the Vertical Plane
We now specialize the model to the vertical plane,
the i-k plane in inertial coordinates and the e1-e3
plane in body coordinates. The elements of R, b, v,

, rp, Pp, and u corresponding to the e2 direction
are set to zero. The equations of motion (3.11) for
space are the gliding vehicle restricted to the vertical plane
are then
Here, is pitch angle,  is the angle of attack, D
is drag, L is lift and MDL is the viscous moment as
shown in Figure 3.4. These forces and moment are
modelled as
where the K's are constant coeffcients. This model
is a standard one, derived using airfoil theory and
potential 
ow calculations and then verified using
experimental observations, see for example [4, 11].
The method for determination of the coeffcients is
described in Section 4.3.
As shown in Figure 3.4, we denote the glide path
angle by  where
We also denote the glider speed by V where
We will typically specify a glide path by desired glide
path angle  and desired speed . We define inertial
coordinates such that  coincides with
the desired path:
Then, measures the vehicle's perpendicular distance
to the desired path. We define two gliding objectives:
GO1 The objective is to control only the direction
and speed of the vehicle's glide path. In this
case we need not consider x and z at all.
GO2 The objective is to control gliding along a prescribed
line (see Figure 3.5). In this case we will
include (but exclude) in our analysis and
we aim to make
In several glider designs the moving mass is restricted
to one degree of freedom in the planar case.
This case is addressed in [10].
4 Controllability of Steady
Glide Paths
In this section we compute steady glide paths. We
then study controllability and observability of these
glide paths.
4.1 Gliding Equilibria
We prescribe a desired glide path by specifying the
desired glide path angle and the desired speed
Vd. We denote with subscript "d" the value of all
dynamic variables at the glide equilibria. To get
the conditions for such planar gliding equilibria, we
set the left hand side of equations (3.24) and (3.14)
through (3.22) to zero.
With their left hand sides set to zero and given
, equations (3.16) and (3.17) may be solved for
. We can then compute
mbd can then be solved again using (3.16) and
(3.17). Finally, equation (3.15) with left hand side
set to zero gives a one-parameter family of solutions
for (rP1d; rP3d )T .
First, we compute from equations (3.16) and
(3.17) with left hand sides zero. This yields a
quadratic equation for. 
2, we have
Equation (4.1) may be solved for a realizable fid
provided the discriminant of the quadratic equation
is greater than or equal to zero. Evaluating the
condition on the discriminant in the range (will 
give a range of possible glide path angles d
depending on the glider hydrodynamic parameters.
See [10]. Since the drag model is valid only at small
angles of attack, we take  as the solution of (4.1)
with smaller magnitude.
The special case of is covered in [10].
We may determine mbd from (3.16) and (3.17),
Finally, we may solve for a one-parameter family
of sliding mass locations (rP1d; rP3d )T which satisfy
equation (3.15). The family of solutions is
and where 
 is a real number. The vector 
is a particular solution of equation (3.15). Since
is the direction of gravity
in body coordinates, is orthogonal to the
direction of gravity and 
 measures the vehicle's
"bottom-heaviness" as shown in Figure 4.1.
For an experimental vehicle, rP3 may be a more
physically relevant choice of parameter than 
.
Equilibrium mass locations in this case are addressed
in [10].
4.2 Linearization
We determine the linearization for the planar
glider about a steady glide path. Let 
Then the linearized system is
where we have abbreviated and
The notation  indicates that the quantity is to
be evaluated at the desired equilibrium.
4.3 Controllability
In this section we describe controllability and observabililty
of steady glide paths for a model of
our experimental vehicle ROGUE, described in Section
2. Mass and inertia properties were measured
directly. Added mass and inertia properties can be
found, for example, in [7]. Lift and drag for the
body were found experimentally as described in [6].
Lift and drag for the wings were taken from the data
in [13]. Lift for the body plus wings was then computed
using Schrenk's method [12], and drag was
computed as the sum of the drag on the wing and
the body. The lift moment was approximated by 
taking into account the tail. The vehicle mode
l parameters are given as follows
The first three masses, m, mh and m were measured
with a high degree of accuracy. The other terms
have less precision because they are based on lookup
tables and approximation methods.
Four steady glide paths are calculated using the
method of Section 4.1. The glide paths are at glide
angles 30, 45, 30 and 45. We compute the
glib de path at 30 by choosing a desired glide speed
Vd = 0:30 m/s and a desired vertical location of the
movable mass given by rP3d = 4 cm. This results in
an equilibrium variable mass given by mbd = 1:36
kg. The glide path at 45 is computed for these
same values of rP3d and mbd . The corresponding
equilibrium speed for this glide is computed as Vd =
:37 m/s.Similarly, we computed the two steady
upward glide paths, for the same value of rP3d and
the same buoyant force magnitude, i.e., the value of
jm0d j is held constant. Recall that m0 is the mass of
the vehicle mv less the mass of the displaced 
uid
m. The full description of each of the four glide
paths is given in Table 4.1.
Local properties of these steady glide paths can
be studied using the linearization of Section 4.2. By
plugging in the equilibrium values, we can examine
the linearization for stability, controllability and observability.
The four glide paths listed in Table 4.1
all have a relatively slow unstable mode. They are
all, however, locally controllable. That is, A and B
as given by (4.6) and (4.7), when evaluated at any
of the four equilibria, satisfy the controllability rank
condition. Note that the linearization includes the
state z meaning that controllability extends to the
variable z. Accordingly, we can successfully design
a linear controller that will locally accomplish not
only glide objective GO1 but also GO2.
It is of interest to check the controllability rank
condition in the case that the movable mass m can
only move in one direction (i.e., rP3 is fixed). To do
this we have linearized the equations of motion for
the single degree-of-freedom moving mass derived in
[10]. Again the new A and B matrices for this case,
when evaluated at any of the above four glide paths,
satisfy the controllability rank condition. Thus, it
seems that at least for linear type control action, not
much is lost in restricting the degrees of freedom of
the movable mass from two to one. Several operational
gliders have moving masses that translate in
the vehicle long axis and rotate in the roll direction,
corresponding to a design with one degree of
freedom when considering only the vertical plane.
The movable mass m for ROGUE is approximately
1/6 of the vehicle displacement m. This is of
the same relative order as the movable mass in the
gliders SLOCUM, Spray and Seaglider. Variations
in this mass or its location will not in principle affect
local controllability of a feasible glide path, but
may affect the range of feasible glide paths and the
switching between them.
4.4 Observability and State Estimation
Observability of the linearized model about the four
glide paths listed in Table 4.1 was also investigated.
If GO1 is our objective, i.e., if we are interested in
controlling only the direction and speed of the vehicle's
glide path, then we need not measure z
0. The nine-dimensional dynamic model (which excludes
z) is completely observable with measurements limited
to movable mass position rp1, rp3 and variable
mass mb. In this case, pitch angle , pitch rate 
2,
linear velocity components v1 and v3 and the momentum
of the movable mass Pp1, Pp3 need not be
sensed. Observability means that with the measurements
of rp1, rp3 and mb, a dynamic observer could
be designed to give an estimate of the unmeasured
states , 
2, v1, v3, Pp1 and Pp3. Of course,  is
typically already measured and 
2 is not so hard to
measure, so the real advantage is in the estimation
of v1, v3, Pp1 and Pp3 which are more difficult to
measure. The nine-dimensional dynamic model is
also completely observable with measurements limited
to , rp1 (or rp3) and mb. Again this means
that using these three measurement signals, an observer
could be designed to estimate the rest of the
states.
We note that the use of a dynamic observer to
estimate the glider states has the potential to improve
the accuracy of horizontal motion determination
over current methods which are based on assumptions
of constant angle of attack, etc. For example,
on SLOCUM, the horizontal motion of the
glider during the glide is estimated from GPS fixes
taken at the surface, measured pitch angle, an assumed
angle of attack and vertical speed computed
from depth measurements [17]. Similarly, on Spray,
horizontal 
ight distance is calculated based on a
constant pitch, heading and angle of attack to which
the vehicle is being controlled [14].
If GO2 is our objective, i.e., if we want to control
the glider to a prescribed line in the plane, then
we need a measurement of z. Recall from (3.23)
that z depends on both depth z, which is easily
measured, and horizontal position x, which is not
so easily measured. The measurements rp1, rp3 and
mb, together with a measurement of z (or alternatively
, rp1, mb and z), do not render x observable.
This means that without an initial condition measurement
x(0), the trajectory x(t) cannot be computed,
and so z is not observable. That is, using
any combination of the other nine states, it is not
possible to design a dynamic observer to estimate
the z state. However, with an initial measurement
of x given say from a GPS fix taken when the glider
is at the surface, the horizontal motion of the glider
can be dead reckoned using velocity estimates from
the observer. This introduces some error z ded
into the estimate of the z state. Using the deduced
ded in the feedback control, the glider can perform
GO2, gliding along or parallel to the desired glide
path with some offset in the z direction due to dead
reckoning error.
The dead reckoning approach involves calculating
horizontal velocity, then integrating to obtain
a deduced.
This can then be used to calculate 
ded. The pitch angle and depth z can be measured directly.
Estimates of the velocities, v1est and v3est, are provided
by the observer, while is determined by the desired glide.
The equation for the dead reckoned is then
The nine observable states include v1 and v3 so our
observer estimates v1est and v3est will converge to
the actual states when we are close enough to the
equilibrium glide path for the linearization to be
valid. When there is some error in the observer
estimate of the velocities, integrating (4.8) to find
xded and using (4.9) will result in some error. 
This error depends on the observer state
estimate error, which will vary with different state
trajectories and disturbances.
5 Controlled Planar Gliding With Observer
In this section we demonstrate, in simulation, controlled
gliding in the vertical plane by designing
and testing a linear controller and observer for the
glide path moving 30 downward as described in Table
4.1. Since the controller is linear, we expect that
it should take initial conditions nearby to the 30
downward glide path. We demonstrate this result
by starting the glider at the  45 downward steady
glide and using the linear controller to move it to
the 30 downward glide solely by feedback.
5.1 Controller Design
We address the case where only a limited set of the
states are measured, depending on the sensors on
the glider. We design the controller and then, given
the limited set of state measurements, design a dynamic
observer to determine the full state of the
glider. The controller is designed for the linearization
about the 30 downward glide using the LQR
(linear quadratic regulator) method. This is a standard
linear optimal control design method which
produces a stabilizing control law that minimizes a
cost function that is a weighted sum of the squares
of the states and input variables.
The cost function to be minimized is defined as
where Q and R are state and control penalty matrices.
Q and R were chosen to ensure well-behaved
dynamics and to prevent large motions in the movable
mass position and variable mass that would exceed
physical limitations. Taking into account real or 
desirable maximum state values, the states associated
with vehicle and movable mass velocity and
variable mass and pitch angle were weighted most
heavily. No significant tuning was performed. The
weight selections are given by
Q = diag(:05; :5; 1; 2; 2; :1; :1; 1; 1; :5);
R = diag(1; 1; 1):
The corresponding control law is  where
K is computed using MATLAB as the solution to
the Riccati equation given A;B; Q;R.
5.2 Observer Design
In the case that some states are not provided by
sensor data, as is likely to be the case with velocities
v1 and v3 in an autonomous glider, it is possible
to construct a linear optimal observer to estimate
the unavailable states. As described in Section 4.3
the nine dimensional dynamic model excluding is
completely observable. If is not directly sensed, it
is an unobservable state but can be dead reckoned.
The construction of an optimal linear observer for
the nine dimensional system proceeds in a similar
manner to the construction of the LQR controller.
Our linear system is described by A and B, de-
fined above. The system output is y = Cx + v,
where C is the system output matrix determined
by the available sensors and v is noise. Given a linear
time invariant system subject to additive process
disturbance w(t) and measurement noise v(t)
which are zero mean, gaussian, white noise processes,
an observer which minimizes variance in the
estimate error is derived in a manner similar to the
LQR. The cost function to be minimized is
where is the system zerostate
response to plant disturbance 
and is the zero-state response
to measurement noise. Let and  represent the
white-noise processes w(t) and v(t) , where the
Dirac Delta Function represents the fact that
white noise is uncorrelated in time. Let W =
and  be the disturbance and
noise covariance matrices. Then choosing observer
feedback  minimizes the cost function
J0, where P is the solution to the observer matrix
Riccati equation given A; C;W; V .
In this observer design the W and V matrices
are chosen according to reasonable estimates of the
disturbances and noise. The covariance matrix W
is a diagonal matrix whose elements are the square
of the standard deviation of the state disturbances.
The standard deviation is taken to be ten percent of
the desired value at the equilibrium glide for each
state. In the cases where a state xi has desired
value zero, we determined a maximum deviation
4xi from equilibrium by simulating several switches
between equilibrium glides under full state feedback
control. Ten percent of 4xi is then used as the
standard deviation for the i
th state. The noise covariance
matrix V depends on the noise properties
of the sensors used in the vehicle. In this design
they are taken to be of the same or smaller order of
magnitude as W:
When determining the cost function to be minimized,
if V is large then computed gain L will be
small, so direct measurements have smaller impact
on the state estimate. A large W implies disturbances
dominate the plant dynamics. In that case
computed gain L is large, resulting in an observer
state estimate which depends more heavily on sensor
measurements than the plant model.
5.3 Glider Simulation
In Figures 5.1-5.3 we show a MATLAB simulation
of the glider switching from a 45 downward glide
to a 30 downward glide path. This is accomplished
by turning off the controller for the first glide at
t = 5 seconds and turning on the linear controller
for the second glide. In each figure we show results
using full state feedback and using an observer to
estimate the state used in the control law.
When calculating the controller gain matrix K
and the observer gain matrix L, some plant parameter
error is incorporated into the design. When calculating
the gain matrices using the respective Riccati
equations, the parameters which determine the
A and B matrices are varied by up to ten percent.
Changing the A and B matrices used as the system
model in the controller and observer design represents
imperfect knowledge of the glider dynamic coeffcients.
In Figure 5.1 we show the glide path before and
after the switch. In Figure 5.2 we show plots of position,
pitch, linear and angular velocity as a function
of time and in Figure 5.3 we show the position of
the movable mass, the net buoyant force as well as
the control inputs as a function of time. The figures
show that the 45 downward glide path is in the region
of attraction of the linear controller designed
for the 30 downward glide path. Furthermore, the
transient is well behaved.
6 Final Remarks
Laboratory experiments of controlled gliding with
ROGUE will be described in a future publication.
Experiments with ROGUE are planned to provide
verification of our dynamic model and to test the
controller and observer designs. We also plan to
realize these and future results on sea-worthy gliders
such SLOCUM in collaboration with our colleagues
who build and operate these vehicles.
Other future work includes extending the glider
control design to motions in the horizontal plane
such as waypoint following, control of unsteady motions,
and three dimensional motions such as gliding
in a spiral. We intend to develop the glider control
methodology further by investigating nonlinear
feedback control laws, feedforward control and path
planning. Work with colleagues on optimal control
theory which is applicable to glider path planning
appears in [2].
Work is underway to develop decentralized control
laws to produce underwater vehicles that school
like fish [9, 15]. By developing control schemes for
coordinated group motion of underwater vehicles,
we hope to produce a network of gliders that can
serve as a fast and effective ocean sensing platform.
For example, in [1], decentralized control algorithms
are described that allow a pair of vehicles to climb
a spatially distributed gradient. In support of this
effort we are building an experimental, underwater
test-bed for multiple-vehicle control [1]. Vehicle systems,
including control systems, hardware and software,
and new sensors developed for the experimental
grouping vehicle in [1] may also be incorporated
into the design of future laboratory gliders.
7 Acknowledgements
We would like to thank Monique Chyba, Kristin
Pettersen, Ralf Bachmayer and Craig Woolsey for
helpful discussions on this work.Abstract¡ª Underwater Gliders (UGs) are a type of 
Autonomous Underwater Vehicle (AUV) that uses buoyancy 
engines, an energy efficient locomotion, primarily for 
oceanography. In this paper, control strategies for existing 
underwater gliders are reviewed. A total of 50 papers indexed by 
Scopus with keywords control and underwater gliders were 
reviewed from 1989 to 2014. The majority of gliders use classical 
controllers, which cannot dynamically compensate for unmodeled 
hydrodynamic forces and unknown variations in water 
current and wind. With increasing operational depths and larger 
payloads, control strategies will become an increasingly 
important aspect for these gliders. Control strategies 
implemented in underwater gliders were reviewed and 
alternative control strategies are proposed. 
Keywords¡ª Control strategy, underwater gliders, review; 
I. INTRODUCTION 
An underwater glider is a type of autonomous underwater 
vehicle (AUV) that uses buoyancy control in conjunction with 
wings to convert vertical motion to horizontal motion in a sawtooth 
pattern and thereby propelling itself in forward direction 
with very low power consumption [1]. They use buoyancy and 
wings to produce forward motion for long duration ranging 
from weeks to months and cover distances over 3000 km 
[2].There are more than 50 different AUGs developed for 
research and commercial purposes. Much theoretical and 
experimental analysis has been carried out on glider dynamics 
[3, 4], glider design and development for payload and 
maneuverability [5, 6], performance and stability analysis [7, 
8], trajectory control analysis [9, 10] and path planning [9, 11] 
based on different controller techniques[12].The majority of 
these gliders use classical controllers such as ProportionalIntegral-
Derivative (PID) and Linear Quadratic Regulator 
(LQR). However, These control methods cannot provide 
simultaneously easy control implementation and fast 
convergence speed for stabilization in the presence of 
continuously varying water current [13]. 
In this paper, existing underwater glider control strategies 
are reviewed with emphasis on their actuation system that 
includes wings and rudder control, followed by external effects 
of water current and variable loads which influence their 
motion. 
No work to date achieved the control of fully under actuated 
vehicles via dynamic modelling with the presence of external 
varying forces such as water current and advanced controller 
design to compensate these problems and then implement them 
to underwater gliders. In order to overcome these issues, 
serious design and implementation considerations are required 
for robust control approach. 
II. UNDERWATER GLIDER MOTION CONTROL 
The motion of an underwater gliders depends upon a 
buoyancy control system combined with a set of wings and 
rudder to move in vertical and horizontal direction through a 
water column [14]. The glider descends and ascends in water 
column due to control of buoyancy engine, which consists of 
oil filled bladder or piston evacuated cylinder. Buoyancy is 
controlled to be slightly positive or negative due to which a 
glider can move with almost zero power consumption with 
limited speed [15]. For example, the Spray glider as shown in 
Figure 1, has a maximum speed of 0.35 m/sec but can operate 
for up to six months. 
Regardless of the design, gliders depend on vertical space to 
operate efficiently but cannot resist with strong currents, which 
ultimately creates a problem to control motion and trajectory. 
On the other hand, the average horizontal speed of underwater 
gliders is about 0.3 m/s. The low speed limitation has created 
challenges in controller performance such as control of vehicle 
orientation, position and velocity [14]. There is a direct effect 
of environmental forces including winds, wave and surface 
currents upon the motion of underwater gliders and AUVs [21]. 
These effects may be compensated through an effective control 
strategy. Therefore the control strategy should be chosen to 
reduce the effect of these disturbances. However, the 
challenges associated with control of underwater gliders 
include a highly nonlinear dynamic system and complex 
hydrodynamic effects. Different control strategies for 
laboratory scale and commercial scale have been implemented 
to underwater vehicles which are discussed in table 1. 
The ALBAC glider used its payload to glide downward and 
upon releasing its payload, glided upward [2]. However most 
gliders such as SLOCUM [22], Spray [23] and Seaglider [18] 
use a buoyancy engine instead. These three gliders are fixed 
winged buoyancy propelled gliders which shift their internal 
ballast mass to control their attitude [1]. 
Leonard and Graver [1] developed a laboratory scale 
ROGUE glider to implement a model based feedback open 
loop approach. Another laboratory scale glider , the ALEX 
glider, developed by Arima et al [7] designed an independently 
controllable wings and rudder. 
III. CONTROL STRATEGIES 
Several control strategies have been proposed to control the 
position, velocity and trajectory of UGs such as PID controllers 
by Bhatta and Noh [24, 25], Linear Quadratic Regulator 
(LQR)by Nina and Graver [9, 26], Sliding Mode Controllers 
(SMC) by Yang and Song [27, 28], Neural Network (NN) by 
Isa[19] and Dong[29], Fuzzy logic for self-tuning by Loc [30] 
and Model Predictive Controllers by Cooney [31] and Steenson 
[32].PID control loops used for pitch and roll control by Panish 
[16] allows their UG to be guided in a variety of ways. In such 
control strategies, vertical and horizontal modes are decoupled 
to control the pitch, angle of attack and depth of the glider. The 
robustness of PID controller using nonlinear time invariant 
aspects of glider designs is investigated by Bender [33]. AUG 
dynamics are nonlinear but PID controllers are based-on linear 
control law. Therefore, PID controller cannot dynamically 
compensate for un-modeled vehicle hydrodynamic forces and 
unknown disturbances. As such, combinations of control 
strategies to form hybrid controllers are used to handle the 
complexity that is produced due to nonlinearity and 
hydrodynamics effect. 
Sliding mode control(SMC) for underwater trajectory 
tracking control has been used by Yang and Bessa [27, 34] ; 
and Liu [21]. SMC is a robust system with low sensitivity to 
environmental disturbances. However, chattering is a well-
known downside of this control strategy, which can degrade 
the performance of gliders and may lead to instability. 
A multivariable Multiple input Multiple Output (MIMO) 
control system have been proposed for AUVs and ROVs 
including self-tuning and position control by Goheen [35]. The 
dynamic model using LQR technique was implemented in a 
laboratory scale ROGUE glider in order to examine its motion. 
However, LQR has poor switching performance between 
upward and downward glide paths [26]. 
In a comparative study, Nag [36] discussed the use of an 
adaptive fuzzy control strategy for tracking trajectory system 
combined with PID controller. However, they considered the 
glider as a SISO system only and neglected nonlinearities. A 
recurrent NN based on MPC control strategy for motion 
control problem in longitudinal plane is proposed by Shan [37] 
for convergence to optimal solution. However, in this approach 
the effect of water current has not been considered. 
Due to the inherent non-linearity in the dynamics, and hence 
difficulties in determining an accurate mathematical model of 
the vehicle, an adaptive fuzzy logic based control strategy 
coupled with dynamic compensators have been designed. The 
effect of fuzzy logic for depth controller has been implemented 
on KAUV-1 which has the advantage of fast self-tuning [30]. 
The simulation results showed satisfactory performance but 
again, the effect of external disturbance and nonlinearity were 
not investigated. The nonlinearity of the Odyssey IV for the 
purpose of disturbance rejection using Model predictive 
controller was discussed by Cooney [31] and a hybrid PIDMPC 
controller is suggested for improving and predicting 
future values. However, a communication problem between 
commanded input and actual input was observed during 
implementation of this strategy in Odyssey IV. In [27], Yang 
addressed the robustness issue produced due to nonlinearities 
and proposed the adaptive sliding mode controller. However, 
the resulting chattering effect lead to instability. 
Neural network (NN) control strategy [19] have been 
proposed as a solution to the lack of robustness due to 
nonlinear effect in dynamic models. Due to the ability to 
handle nonlinearity and adapt to changing conditions as is the 
case of varying currents in glider dynamics, NN has been 
implemented in USM hybrid underwater glider [19]. In 
proposed model, all glider parameters e.g. pitch, speed, 
maneuverability, sliding mass, wing, rudder and propeller were 
controlled. A neural network self-tuning PID controller [29] for 
a spherical AUV was proposed for the purpose of controlling 
the vehicle velocity and tracking the desired target in the 
vehicle fixed coordinate system. 
IV. HYBRID CONTROL STRATEGY 
The performance of individual control strategies can be 
improved by combining it with another control strategy to form 
a hybrid control strategy. Different implementations of this 
concept have been applied successfully in AUVs such as 
Adaptive fuzzy sliding mode control strategy. 
A. Combining fuzzy logic and sliding mode 
Fuzzy logic has been used to design a sliding mode 
controller[28, 38]. However, the FLC was implemented in a 
SISO system only. Alternatively, PID and FLC have been 
combined to design a new controller. To eliminate the 
chartering effect of sliding mode, a boundary layer is 
introduced around the sliding surface by Song [28]. A general 
block diagram of adaptive fuzzy sliding mode controller is 
shown in Figure 3. 
B. Sliding mode fuzzy controller 
In this approach, sliding mode is used to design functions of 
the fuzzy logic controller and a switching function is formed 
which can eliminate the chattering effect. This type of 
approach was experimentally applied to OEX series AUVs for 
pitching and heading control. The inputs to SMFC heading 
controller were defined as heading error while the output for 
heading controller was to control the rudder.[39]. 
The control strategy can be further improved using a
combination of nonlinear controllers for the motion of
autonomous underwater gliders based on added mass and
inertia, effect of external varying disturbances and rudder
control inputs[40] . However the effectiveness of an adaptive
fuzzy sliding mode controller has only been investigated for
the Snorkel AUV [38]. Recent work concerning different
control techniques for AUVs and AUGs is summarized in
Table 2.
V. MAJOR LIMITATION AND CHALLENGES 
Very few control schemes are proposed to measure the 
sensitivity analysis of environmental disturbances such as 
Isa[12] and Fan [50], but this is a new idea in undersea 
vehicles to consider both water current estimation and 
payload factor in dynamic modelling and control for UGs. 
The main attempt is to choose a controller that can guarantee 
the suppression or at least the limitation of the overshoot in 
the system response. An appropriate control strategy is 
required that will damp out the external disturbances and will 
follow trajectory with minimum error for varying payloads. 
VI. CONCLUSION 
Different control strategies have been demonstrated for 
acceptable control performance in different AUVs and 
AUGs, but they still have limitations in compensating for the 
high nonlinearity of underwater gliders including external 
disturbances and carrying large payloads which are the main 
reasons that make it a challenging problem to control the 
underwater glider. Due to this it is highly desirable to design 
a controller that is robust and will give real time response. 
For this, combinations of two or more controllers have been 
developed by some researchers for AUVs to form a hybrid 
controller which can minimize the negative effect and will 
improve the performance.
ACKNOWLEDGMENT 
Authors are thankful to Universiti Teknologi PETRONAS 
for providing the resources required for this work. 

Evidence of fluid seepage in Gronfjorden, Spitsbergen: Implications from an integrated acoustic study of seafloor morphology, marine sediments and tectonics
ABSTRACT
This study integrates high-resolution surface and sub-surface geophysical data to investigate possible fluid (gas and/or liquids) migration pathways to the seafloor in Gr?nfjorden, west Spitsbergen. Gr?nfjorden is an N-S aligned tributary fjord, located in the western part of the Isfjorden fjord system. Pockmarks were identified on the bathymetric data in the southern and northern parts of the fjord. Three types of shallow seep-related acoustic features have been identified in the marine sediments: i) enhanced reflections, ii) acoustic turbid zones, and iii) acoustic blankings. They occur beneath some of the pockmarks and circular patches of high-backscatter on the seafloor in the central part of the fjord, which are potential seep-related seafloor features. 6每8 km long NNWSSE striking ridges have been identified in the southern part of the study area. Thrust faults were interpreted on 2Dmultichannel seismic data truncating the Triassic, Jurassic and Cretaceous successions of western Isfjorden, located north of Gr?nfjorden. A possible link has been found between these fault systems and the seafloor ridges which strike in a similar direction. Further, a normal fault has been identified within the shallow marine sediments which drapes the bedrock of the fjord. The fractured bedrock of the fjord and faults associated with the West Spitsbergen fold-and-thrust belt could possibly represent major conduits for fluid migration from deeper stratigraphy to shallow marine sediments and the seafloor.
1. INTRODUCTION
Integrated analysis (geophysical, geochemical and geotechnical) of shallow sub-surface fluids is important for risk assessment of marine geo-hazards, localizing hydrocarbon reservoirs and identification of shallow and deep source rocks (Hovland and Judd, 1988; Hovland et al., 2002; Ostanin et al., 2013). In this paper, ＆fluid＊ means both the liquid and gaseous phases, including pore-water with dissolved gas and/or dispersed gas bubbles (when the pore-water is supersaturated with gas). Fluids from deep and shallow sources can migrate along faults to the seafloor, where features like pockmarks, mud volcanoes or carbonate mounds are formed (Ligtenberg, 2005; Judd and Hovland, 2007; Sahling et al., 2008; L?seth et al., 2009; Barnard et al., 2015). The distribution of these various types of seep features on the seafloor depends on the location and geometry of underlying reservoirs, geological unconformities and seal bypass systems (faults and intrusions) which act as fluid migration pathways (Ligtenberg, 2005; Cartwright et al., 2007; Gay et al., 2007; Judd and Hovland, 2007; Chand et al., 2012; Ostanin et al., 2013).
Several studies have documented seabed fluid flow, seepage features, occurrence of gas hydrates, and associated slope failures in the Norwegian Sea, Barents Sea and the West Spitsbergen continental margin (Plaza-Faverola et al., 2011; Vadakkepuliyambatta et al., 2013; Dumke et al., 2016). Gas flares in the water column of southwest and west of Spitsbergen have been reported by Damm et al. (2005) and Westbrook et al. (2009). Near-surface hydrocarbon anomalies suggest mixed biogenic and thermogenic gases in the fjords and shelf sediments of west Spitsbergen (Knies et al., 2004).
Pockmarks associated with possible shallow gas occurrences have been previously described in Isfjorden (Forwick et al., 2009; Roy et al., 2014). Gr?nfjorden is located in the western part of the main Isfjorden fjord system in Spitsbergen, where the largest pockmark has been reported (Roy et al., 2015). However, shallow gas-related acoustic anomalies and the influence of regional geology on focused fluid flow associated with the pockmarks in Gr?nfjorden have not been studied so far. The objective of this paper is to investigate the sub-surface fluid occurrences, seafloor seepage features and processes in order to better understand how they are connected to the complex deeper geological setting in Gr?nfjorden. This study uses high-resolution multibeam bathymetric and backscatter data, shallow sub-bottom acoustic data and multichannel 2D seismic data to investigate the seafloor seeps, sub-surface fluid occurrences and their link to underlying tectonic and structural features.
2. REGIONAL SETTING
Gr?nfjorden is located 9 km southeast of the mouth of the Isfjorden fjord system in west Spitsbergen (Fig. 1). The mouth of Isfjorden was deglaciated c. 12,300 calendar years BP (Mangerud et al., 1992). The glacigenic stratigraphic units blanketing the bedrock of Gr?nfjorden have been described in Fig. 4C of Forwick and Vorren (2010), comprising subglacial, glacier frontal, and glacimarine deposits. Sedimentary sources to these deposits are the valley glaciers terminating along the southern-southwestern coast of Gr?nfjorden (Fig. 2A). The river Gr?ndalselva has deposited fluvial to paralic sediments along the central- eastern coast of the fjord (Fig. 3). Restricted water exchange and high sedimentation rates have the potential to result in anoxic bottom water conditions in Arctic fjords like Gr?nfjorden (Howe et al., 2010).
The tectonic setting of Gr?nfjorden is in the frontal syncline of a major regional monocline of the WSFTB (Ohta et al., 1992; Braathen et al., 1995) (Fig. 2). This major structure shows faults, mostly low angle to bedding and some steeply dipping, likely related to thrusting. On a small scale, extensional and shear fractures have been found in the Mesozoic and Cenozoic sedimentary sequences (Braathen and Bergh, 1995; Teyssier et al., 1995). A distinct NNW-SSE strike characterizes the thick-skinned basement-involved fold-thrust complex in the study area (Braathen and Bergh, 1995; Bergh et al., 1997). However, the Isfjorden-Ymerbukta fault has an NE-SW trend offsetting the fold complex dextrally (Braathen et al., 1999) (Fig. 3).
The onshore bedrock succession is dominated by the Paleocene Firkanten Formation and the Cretaceous Carolinefjellet and Helvetiafjellet formations on both sides of Gr?nfjorden. The Firkanten Formation consists of sandstone and some shale with intervening coal layers (Steel et al., 1981; Dallmann, 1999). Belowthis, the Carolinefjellet and Helvetiafjellet formations are overlying the Janusfjellet Subgroup (Ohta et al., 1992; Braathen and Bergh, 1995) (Fig. 2B). The Aptian- Albian Carolinefjellet Formation is subdivided into five members according to the predominance of shales, siltstones or sandstones, and correlates with Kolmule Formation of the Barents Shelf (Dallmann, 1999). The Helvetiafjellet Formation consists of 50每100mof largely fluvial sandstones and shales, possessing good reservoir potential in the Barents Sea (Edwards, 1979; Midtkandal et al., 2007; Gjelberg and Steel, 2012). The underlying Middle Jurassic坼Lower Cretaceous Janusfjellet Subgroup is c. 400 m thick shaly unit (Dypvik et al., 1991). The Middle Triassic每Jurassic Kapp Toscana Group (c. 475 m) and Sassendalen Group (c. 700 m) consist of sandstone intercalated with thin shale layers and thick shaly layers intercalated with sandstone and siltstones, respectively (Dallmann, 1999). The organic rich rocks in the study area are the black paper shales of the Middle Triassic Botneheia Formation andMiddle Jurassic坼Lower Cretaceous Janusfjellet Subgroup (M?rk and Bjor?y, 1984; Dypvik, 1985; Bjor?y et al., 2010) (Fig. 4).
3. DATA AND METHODS
This study uses multibeam backscatter and bathymetric data, shallow sub-bottom acoustic profiles and deep 2D multichannel seismic data. ArcGIS and Petrel software were used to interpret, integrate and correlate seafloor and sub-surface features on different geophysical data sets, ultimately offering a 3D platform for a coupled sub-surface to seafloor analysis.
3.1. Multibeam data
The high-resolution multibeam swath bathymetric data were collected with a 95 kHz Kongsberg Maritime EM1002 multibeam echo sounder by the Norwegian Hydrographic Service in 2008 (Figs. 2A and 3). In addition to this, multibeam backscatter data were collected using a Kongsberg Maritime EM300 multibeam echo sounder operating at a frequency of 30 kHz, onboard RV Helmer Hanssen in 2010 (Fig. 3). CTD measurements were done at the mouth and center of the fjord in order to calculate the velocity profile (1464每1472 m/s) in thewater column. References to ※backscatter§ throughout the paper should be regarded as relative backscatter strength. The processed backscatter and bathymetric data were gridded with 0.6 m and 5 m horizontal cell size respectively using IVS 3D Fledermaus v.7.0 software.
3.2. Sub-bottom acoustic data
The shallow high-resolution seismic data were acquired with a 4kW EM3300 hull-mounted sub-bottom profiler on the RV Helmer Hanssen in 2010 (Fig. 1). This instrument uses a frequency range of 2每16 kHz that allows a vertical resolution of 6每10 cm. The sub-bottom acoustic profiles could image the marine sediment layers to a maximum depth of c. 20每25 m below the fjord seafloor. Edgetech Discoverer II software was used for visual interpretation of stratigraphic units and acoustic anomalies related to possible shallow gas occurrences.
3.3. 2D multichannel seismic data 
The conventional 2D multichannel marine seismic data were acquired in Isfjorden by Statoil in 1985 and 1988 using 2400 and 3000 m long streamers, respectively, and 25 m between shot-points (Fig. 1). Acquisition and processing parameters of the 2D seismic surveys (ST8515 and ST8815) are provided in B?lum and Braathen (2012) and Blinova et al. (2012). Due to the large variations in 2D data quality in the western part of Isfjorden, interpretation of the seismic data is problematic. The poor reflectivity (diffuse nature), continuity and undulating nature of the reflections has been attributed to the hard sea floor combined with high velocity rocks and complex structural setting of the WSFTB in this area (Braathen et al., 1995; Bergh et al., 1997; B?lum and Braathen, 2012; Blinova et al., 2013) (Fig. 4B). Acoustic penetration is c. 3每4 s TWT (two-way-travel time), which is equivalent to c. 7每9 km depth (using Vp values provided in B?lum and Braathen (2012)). The structural interpretation of the seismic data was donewith Petrel software.
4. RESULTS AND INTERPRETATION
4.1. Bathymetric data: seafloor morphology and evidence of seepage
4.1.1. Observations
The seafloor of Gr?nfjorden is relatively smooth, as compared to some of the other tributary fjords of the Isfjorden fjord system (Ottesen and Dowdeswell, 2006; Baeten et al., 2010; Roy et al., 2015). The water depth varies from 15 m at the head to 190 m at the mouth of the fjord. The central part is occupied by a 1.7 km wide and 6.6 km long basin with a maximum water depth of 150 m (Fig. 2A). The northern part of the fjord is characterized by a 5.8 km2 plateau, with a relief of 16每20m(Fig. 2A). A 1.5 km long transverse ridge with a relief of 2每12m (from E toW) is located 100m south of this plateau. The southern part of the fjord is characterized by 6每8 km long, parallel to sub-parallel ridges with a relief of 2每60 m, and oriented along NNW-SSE direction (Fig. 2A).
19 circular depressions (100每240 m diameter and 6每10 m deep) have been observed in the northern part of the fjord. The circular depressions do not have raised rims, but their edges are sharply defined. Their slope changes from horizontal to an estimated 20每30∼ within a distance of 0.5每1 m. Some of these have been previously described by Forwick et al. (2009) and Roy et al. (2015). Smaller circular depressions (20每60mdiameter and b3mdeep) have been observed in the southern part of the fjord (Fig. 3). There are no circular depressions in the central basin.
4.1.2. Interpretation
Fluid escape from the seafloor lifts the fine sediments into suspension, and then the suspended fine sediments get redistributed by near-bottom currents, resulting in the formation of circular depressions in the seafloor. These circular depressions are termed as pockmarks (King and MacLean, 1970),which provide evidence of fluid escape. Subglacial landforms are not well preserved in the present day seafloor of Gr?nfjorden, presumably due to the thick postglacial sediment cover in the fjord originating from the glaciers and rivers draining into the fjord (Forwick and Vorren, 2010). Geological maps and cross-sections from the northern part of the fjord show slivers of Paleocene bedrock on either sides of the fjord and a distinct syncline-anticline pair (Ohta et al., 1992; Braathen and Bergh, 1995) (Fig. 2A). The fold in the Paleocene succession coincides with the extent of the plateau on the fjord seafloor, suggesting that Paleocene rocks probably make up the bedrock of the plateau. The E-W striking transverse ridge located 100msouth of the plateau has been interpreted as a thrust moraine based on its orientation perpendicular to ice-flow direction and acoustic facies analysis (Forwick and Vorren, 2010) (Fig. 2A). The sub-parallel ridges oriented NNW-SSE in the southern part of the fjord coincide with the alignment of the steeply dipping Upper Jurassic坼Lower Cretaceous successions and thrust faults observed in the northern part of the study area and adjacent onshore bedrocks (Ohta et al., 1992; Braathen et al., 1995; Bergh et al., 1997; Blinova et al., 2012) (Figs. 2 and 3). Hence, it is suggested that these ridges probably relate to steeply dipping competent Late Mesozoic layers and/or thrust faults of the WSFTB.
4.2. Seafloor backscatter data: anomalies related to possible seepage
4.2.1. Observations
The seafloor acoustic backscatter data in Gr?nfjorden has values ranging from ?6.38 to ?54.88 dB (Fig. 3). In general, the western part of the fjord has higher backscatter values compared to the eastern part. But in particular, high backscatter values are observed in the center of deep pockmark depressions in the northern part (Fig. 5B and C). In addition, 16 circular patches (40每70 m diameter) of relatively high backscatter values (?19 to ?23 dB with a background value of c. ?30 dB) are observed in the central basin, which are devoid of pockmark depressions (Figs. 3, 6B and C).
4.2.2. Interpretation 
Acoustic backscatter intensity is a function of sea bottom character (mostly roughness and sediment type) (Borgeld et al., 1999; Urgeles et al., 2002; Collier and Brown, 2005). Variable grain size distribution, porosity, presence of gas, and bioturbation can influence the seafloor roughness and backscatter character resulting in anomalous backscatter values. For example, Hovland (1990) found higher acoustic reflectivity of the seafloor sediments where gas was present immediately below the seafloor.
The localized high-backscatter values observed in the center of the pockmark depressions in Gr?nfjorden (Fig. 5B and C), are interpreted to be caused by coarse sediments, which are expected to be present inside the pockmarks due to winnowing of fine grained sediments. High reflectivity from authigenic mineral precipitates associated with fluid seepage could also be a possible reason for such results, as observed in other settings (Judd and Hovland, 2007; Naudts et al., 2008; Dandapath et al., 2010).
It is suspected that the circular patches (pockmark-like) of high-backscatter in the central basin could possibly relate to precipitates following diffuse fluid seepage from the seafloor (Fig. 6B). Similar high-backscatter anomalies have been attributed to authigenic carbonates which are formed due to oxidation of seeping methane in other offshore settings (Orange et al., 2002; Sahling et al., 2008; Ivanov et al., 2010). However, bioturbation induced roughness could also have an influence on the backscatter strength at these suspected seep locations. Surface sampling, coring, and geochemical analysis are required to confirm the actual cause of these acoustic anomalies.
4.3. Sub-bottom acoustic data: anomalies related to shallow gas
4.3.1. Description and interpretation of seismic units
Five seismo-stratigraphic units (A1每A5) are distinguished based on the variation of acoustic signatures, bounded by coherent seismic reflections (Table 1). The units are bounded at the base and top by continuous, parallel to sub-parallel, medium-high amplitude seismic reflections (Fig. 7). However, unit geometries and bedding style varies from head to mouth of the fjord. Acoustic penetration is limited over steeply dipping bedrock ridges, resulting in reduced reflection strength and chaotic zones instead of distinct seismic reflectors (Fig. 6A). The acoustic basement is characterized by a medium to high amplitude, partly diffuse reflection, 15每20 m below the seafloor (Fig. 7). It can generally be distinguished over the entire study area and is interpreted as the top of the bedrock.
Closely spaced conformal internal reflectors have been identified in units A3, A4 and A5 (Figs. 6A and 7). It is suggested that these internal reflectors could result from the lithological variations of well stratified deposits originating from the river Gr?ndalselva. A steeply dipping (c. 70∼ dip angle), E-NE striking normal fault has been interpreted, with 4每5 m offset which displaces units A1, A2 and A3 (Fig. 7). Furthermore, three types of acoustic anomalies attributed to possible shallow gas migration/ accumulation, are discussed below.
4.3.2. Acoustic blankings 
4.3.2.1. Observations. Vertical acoustic blankings appear as patches distorting the continuity of seismic reflectors bounding the seismo-stratigraphic units. Acoustic blankings (C1 and C2) distort the continuity of a high-amplitude continuous seismic reflection in between units A4 and A5 (Figs. 6Aand8D). C1 and C2 are 5每10mwide. Several vertical acoustic blanking features (1每4 m high, 15每20 m wide) distort unit A2 in the northern part of the profile (Figs. 7 and 8B). Occasionally, they are associated with up-doming of the reflector above them. Further along the southern slope of the profile, thinner (b10 m wide) acoustic blanking features have been identified. The vertical acoustic blankings within unit A2 are widespread in the eastern part of the fjord and gradually disappear in the western part.
4.3.2.2. Interpretation. Acoustic blankings usually occur in marine sediments due to disruption of sediment layering by the migration of gas, or by the absorption of acoustic energy in overlying gas charged sediments. In the thick units of unconsolidated glacimarine sediments in Gr?nfjorden, it is suspected that these acoustic features are most likely caused by the scattering and attenuation of acoustic energy by upward migrating gas or by the presence of gas-charged sediments in units A1每A2 and A4每A5. The up-doming reflector suggests possible sediment mobilization or dewatering above some of the acoustic blanking features. The combined presence of circular patches (pockmark-like) of high-backscatter values at the seafloor and the underlying acoustic blankings supports this scenario of near-seabed fluid flow.
4.3.3. Enhanced reflections 
4.3.3.1. Observations. Coherent seismic reflections with high-amplitude and abrupt terminations have been observed at several stratigraphic levels within the marine sediments (Table 1, Figs. 6A, 7 and 8A). They occur commonly beneath the circular patches of high-backscatter (pockmark-like) and in association with acoustic blankings. A seismic reflection characterized by high-continuity and high-amplitude bounding A4 and A5 units is diffuse beneath a high-amplitude seismic reflection (marked by red line in Fig. 6A).
4.3.3.2. Interpretation. High amplitude reflections are termed as ＆enhanced reflections＊ by Judd and Hovland (1992) in the shallow marine sediments of North Sea. They usually occur due to gas saturation within porous marine sediments. The presence of free gas in the sediment pore space causes a reduction of P-wave velocity and high impedance contrast, giving rise to a reflection with anomalously high amplitude from the top of the gas (Judd and Hovland, 1992; Schroot et al., 2005). Hence, it is suspected that the high-amplitude enhanced reflections observed in Gr?nfjorden are most likely caused by the strong decrease in acoustic impedance at the top of gas-charged sediments (Figs. 6A and 7). Alternatively, the enhanced reflections within unit A5 just beneath the seafloor could also result from stratified layer of course grained sediments brought in by the river Gr?ndalselva (Fig. 7).
4.3.4. Acoustic turbid zones 
4.3.4.1. Observation. Chaotic reflections resembling dark vertical smear are observed on the acoustic data within unit A5 (Table 1). These vertical acoustic turbid zones occasionally extend 2每5 m vertically below pockmark depressions (Fig. 8C). The upper limit of these features is irregular, diffuse and without a clear boundary, but occasionally is marked by blurred high-amplitude reflections.
4.3.4.2. Interpretation. Acoustic turbidity is interpreted to result from scattering of the acoustic energy by interstitial gas bubbles trapped in the sediment pores (Schubel, 1974). It is suspected, that these zones of acoustic turbidity in Gr?nfjorden are possibly ascribed to irregularly distributed, gas-charged sediments (of low-acoustic velocity) below the pockmark depressions. The top of the turbid zone cannot be clearly resolved possibly due to minor gas seepage from the pockmarks. Alternatively, these features could be a result of ※acoustic focusing§ effect below the pockmark depressions.
4.4. 2D multichannel seismic data: fold-and-thrust belt related structures 
4.4.1. Observations 
Four seismic reflectionswithmedium-high amplitudes and low-medium continuity are identified up to a depth of 1.4 s (TWT) in the 2D seismic profiles (Table 2, Fig. 4B).Majority of these reflections are related to the impedance contrasts between shale sections and thicker sandstone layers. These reflections are tied to the geological unit boundaries in Spitsbergen, as published by Ohta et al. (1992), Braathen et al. (1995), Bergh et al. (1997) and Blinova et al. (2013). The reflectivity patterns of the seismic units and their suggested linkwith the regional stratigraphy are shown in Table 2.
4.4.2. Interpretation Shallower sections (b0.6 s) show high-amplitude continuous reflections due to sharp acoustic impedance contrast between the Janusfjellet Formation shales and the overlying Helvetiafjellet Formation sandstones. Based on observed offset between reflections, thrust faults have been interpreted in the Triassic, Jurassic and Cretaceous successions. This has been done by identifying consistent dip-domains that point to long thrust ramps (Fig. 4B). Two d谷collement (detachment) layers have been identified as surfaces to which the thrust faults are rooted. Blinova et al. (2013) and Roy et al. (2014) have illustrated three d谷collement layers on other seismic profiles in central Isfjorden area. The d谷collement layer within the deeper Gipshuken Formation (Carboniferous) cannot be identified in this seismic profile due to the complex deformation in the western Isfjorden area (Braathen et al., 1995). The thrust faults cannot be mapped further south into Gr?nfjorden due to the lack of 2D seismic data coverage. However, sub-parallel ridges mapped on the bathymetric data aligned along the strike direction of the bedding and thrust faults mark the possible continuation of the subsurface geology within the Mesozoic rocks, sub-cropping below the seafloor of Gr?nfjorden (Figs. 2A and 3).
5. Discussion
5.1. Characterization of shallow gas in marine sediments
Three types of acoustic anomalies attributed to suspected shallow gas occurrences have been identified in the marine sediments of Gr?nfjorden: (i) enhanced reflections, (ii) acoustic turbid zones, and (iii) acoustic blankings. Quantification of shallow gas based on these acoustic anomalies is difficult. Laboratory studies on sediment acoustic properties have indicated that the presence of low concentration (0.5每 1%) of discrete gas voids within fine grained shallow marine sediments could cause zones of acoustic turbidity, whereas higher concentration interconnected gas-filled pore spaces result in enhanced reflections (Schubel, 1974; Hovland and Judd, 1988; Wilkens and Richardson, 1998). The abundance of the latter in Gr?nfjorden could possibly suggest a relatively higher concentration of gas in the topmost 10mof sediments. The physical property logs in the top 10 m of the glacimarine sediments of Isfjorden show up-core decrease in bulk density and increase in fractional porosity (42每83%) (Forwick and Vorren, 2009). Hence in this study, it is suspected that gas can preferentially accumulate within the moderate-high porous, relatively uniform, fine grained topmost 10 m of the glacimarine Holocene sediments and cause the acoustic turbid zones and enhanced reflections.
Deeper in the stratigraphy, vertical acoustic blanking zones distort continuous reflections bounding units A1, A2 and A5; suggesting possible upward gas migration or mobilized sediments (Figs. 6A, 7 and 8B). Similar suggestions have been made by Schroot et al. (2005) and L?seth et al. (2009) in an attempt to describe acoustic gas features above hydrocarbon reserves. The subglacial deposits and glacier frontal deposits of units A1 and A2 consist a mixture of silt, clay, sand, gravel and boulders (Forwick and Vorren, 2010). Different stages of loading (during glacier advance) and unloading (during glacier retreat) could have resulted in the formation of fractures (Forwick and Vorren, 2009). The effective anisotropy caused by the fracture systemcan create fluctuations in the P-wave velocity, resulting in acoustic blanking features (Arntsen et al., 2007). Hence, the presence of fractures could also lead to scattering and attenuation of the seismic waves in addition to the scattering caused by the presence of gas in the pore spaces of sediments.
5.2. Seafloor backscatter anomalies 
Major sediment input from glacially fed rivers, along its south-eastern coast, and fjord water circulation (Coriolis force driven currents), possibly contribute to the differences in bottom sediment distribution 每 resulting in variable backscatter strength between the eastern and western parts of Gr?nfjorden.
The abundant presence of fine-grained, muddy sediments and the high sedimentation rates from the rivers and glaciers could have resulted in the fast burial of the organic matter (Elverh?i et al., 1983; Forwick and Vorren, 2009). Additionally, the anoxic bottom waters in the Arctic fjords (Howe et al., 2010), provide ideal conditions for the generation of biogenic gas in the marine sediments. Under such a scenario, methane derived authigenic carbonates could form within pockmarks, by anaerobic oxidation of methane, which might result in high-backscatter anomalies.
Medium-high intensity bioturbation is found in the topmost 450 cm glacimarine sediments of Isfjorden (Forwick and Vorren, 2009). Pertaining to the high percentage of clayey silts and occurrence of bioturbation in Isfjorden (Forwick and Vorren, 2009), volume scattering caused by bioturbation might be considered an alternative probable cause for these high-backscatter anomalies. Hence, it is suggested that the circular patches of high-backscatter on the seafloor and high-backscatter from the center of pockmark depressions are most likely caused either by bioturbation, coarse lag deposits, or mineral precipitates following fluid seepage. The reason why the circular patches of high-backscatter are lacking a morphological expression is most probably, the annual supply of fluvial sediments into the central part of the fjord from the river Gr?ndalselva, located on the eastern shore of the fjord (Fig. 3). This river is active every summer and autumn, thus, most probably obliterating the depressions and smoothing the central part of the fjord seafloor with fluvial deposits.
5.3. Significance of fault conduits in fluid migration 
More than 1000 mesoscale faults were mapped in the onshore Cretaceous每 Paleocene rocks of the study area, which comprise the bedrock of Gr?nfjorden (Teyssier et al., 1995). The steeply dipping normal fault offsetting units A1, A2 and A3 has a strike direction almost orthogonal to the trend of the WSFTB (Braathen et al., 1995; Bergh et al., 1997) (Fig. 7). Similar striking faults are found in the vicinity of Gr?nfjorden, within the onshore Helvetiafjellet Formation rocks (Braathen et al., 1995; Teyssier et al., 1995). Hence, it is suggested that this relatively young normal fault was reactivated in recent times as an extensional fracture following the intercontinental compression between Svalbard and Greenland during the Late Cretaceous每Early Paleocene (Braathen and Bergh, 1995).
Ligtenberg (2005) has described the importance of sub-seismic faults associated with a major fault zone in directing vertical fluid flow from deeper source rocks, leading to the formation of pockmarks. Role of fault zones and d谷collement layers as conduits for channelled fluid flowdue to permeability anisotropy and contrasting deformation mechanisms has been documented by Tobin et al. (2001). N-E striking strikeslip faults, N-NWstriking normal and reverse faults exist in theMesozoic and Cenozoic strata of study area (Braathen and Bergh, 1995; Braathen et al., 1995; Teyssier et al., 1995). Furthermore, two d谷collement layers (D1 and D2) show splay thrusts cutting up into the overlying succession (Fig. 4B). A localized fracture frequency peak associated with low-angle thrust in the Carolinefjellet Formation has been found in drilled cores along the southwestern coast of Isfjorden (Braathen et al., 2012). Sub-seismic fractures and faults with an offset b40每50 m will not be resolved in the 2D seismic dataset used in this study. However, presence of these sub-seismic faults and fractures evidenced in drill cores and onshore bedrocks near the thrust faults may be considered as locations of concentrated fluid flow. Hence it is suspected that lateral and vertical migration of fluids along various types of faults and natural fractures could have led to the formation of pockmarks and shallow gas occurrences in Gr?nfjorden.
5.4. Suspected origins of fluids 
Sources to possible shallow gas occurrences and seafloor seepage could either be within the shallow glacigenic marine sediments (biogenic origin) or deeper organic rich rocks (thermogenic origin). Pore-water escape from the glacimarine deposits could also lead to the formation of pockmarks in Gr?nfjorden, as found in the North Sea (Harrington, 1985). However, mixture of biogenic and thermogenic signatures from near surface marine sediments have been documented in Isfjorden (Knies et al., 2004). The presence of vertical acoustic blanking zones (suggesting possible gas migration) above the fractured and faulted bedrock prompt us to investigate probable deeper thermogenic fluids originating from beneath of the glacimarine sediments. A possible source for fluids escaping through the pockmarks in northern Gr?nfjorden could be the buried coal seams of the Firkanten and Helvetiafjellet formations (Ohta et al., 1992; Dallmann, 1999). Decrease in pressure within the coal seams, following post-glacial rebound could lead to methane desorption fromthe in-situ coal deposits and release of free methane from the pore matrix of coal (Scott, 2002). However, the possibility of fluids originating from deeper strata cannot be completely ruled out due to the presence of thrust faults and organic rich Mesozoic rocks. The Middle Triassic Botneheia Formation and the Janusfjellet Subgroup in Spitsbergen consist of type II and III kerogens with high production index for hydrocarbons and high total organic contents (Bjor?y and Vigran, 1980; M?rk and Bjor?y, 1984; Bjor?y et al., 2010). Hence, these stratigraphic layers could be possible thermogenic sources to the seeping fluids in Gr?nfjorden. However, it will be necessary to obtain geochemical analysis of pore water and near surface sediments directly from seep locations to achieve a clear understanding of the composition and source of fluids.
6. Conclusions 
The evidences presented herein suggest ongoing- and/or paleo-seepage of fluids (gas and/or liquids) through the seafloor of the Gr?nfjorden, along its whole length. This conclusion is reached by the documented occurrence of pockmark depressions both in the northern and southern parts of the fjord. In the north, these pockmarks have high-backscatter values at their centers. These high-backscatter anomalies are suspected to be either caused by bioturbation, by coarse lag deposits, or by mineral precipitates following fluid seepage.
In the mid-section of the fjord there are no pockmark depressions, but only circular patches with high-backscatter. The reason why these suspected fluid flow features are lacking morphological expressions, could be the annual supply of fluvial sediments from the river which obliterates the depressions and smoothens the central part of the fjord seafloor.
Geophysical evidences of gas migration overlying the faulted and fractured bedrock along with the presence of possible hydrocarbon source rocks have allowed us to develop a conceptual model to explain the fluid flow features observed in Gr?nfjorden (Fig. 9). It can be summarized as follows:
Migrating fluids (gas and/or liquids) of unknown source and composition are suspected to be generated in the sedimentary column beneath the fjord. These fluids migrate upwards through the sedimentary strata, via faults and fractures, including thrust faults. Focused upward migration of fluids above the fjord bedrock results in acoustic blankings, which are also known as acoustic gas/seismic chimneys. Further up in the marine sediments, the fluids form either enhanced reflections, where minute gas bubbles are trapped in porous layers, or they form acoustic turbidity zones where the fluids migrate diffusively through the near-surface sediments. Finally, the fluids escape through the seafloor in a focused manner through pockmark depressions (northern and southern parts of the fjord) and pockmark-like circular patches of high backscatter (central part of the fjord).
Acknowledgement 
Norwegian Hydrographic Service is acknowledged for providing access to the high-resolution multibeam bathymetric data (permission number: 08/620). Thanks to the crew of R/V Jan Mayen (Helmer Hanssen) who supported the geophysical data collection. Statoil generously provided access to the seismic data. Schlumberger provided an academic license of Petrel. Srikumar Roy acknowledges Riko Noormets and Snorre Olaussen for providing the research facilities and guidance for this study. Constructive reviews byMarc De Batist, Adriano Mazzini and Roger Urgeles improved the manuscript to a great extent. Srikumar Roy is also thankful to the Research Council of Norway for the research grant (194868) to conduct this research project.

Geomorphometric Characterization of Pockmarks by Using a GIS-Based Semi-Automated Toolbox
ABSTRACT 
Pockmarks are seabed depressions developed by fluid flow processes that can be found in vast numbers in many marine and lacustrine environments. Manual mapping of these features based on geophysical data is, however, extremely time-consuming and subjective. Here, we present results from a semi-automated mapping toolbox developed to allow more efficient and objective mapping of pockmarks. This ArcGIS-based toolbox recognizes, spatially delineates, and morphometrically describes pockmarks. Since it was first developed, the toolbox has helped to map and characterize several thousands of pockmarks on the UK continental shelf, especially within the central North Sea. This paper presents the latest developments in the functionality of the toolbox and its adaptability for application to other geographic areas (Barents Sea, Norway, and Malin Deep, Ireland) with varied pockmark and seabed morphologies, and in different geological settings. The morphometric characterization of vast numbers of pockmarks allows an unprecedented statistical analysis of their morphology. The outputs from the toolbox provide an objective, quantitative baseline for combining this information with the geological and oceanographical knowledge of individual areas, which can provide further insights into the processes responsible for their development and their influence on local seabed conditions and habitats.
Keywords: pockmarks; automated-mapping; ArcGIS; Glaciated Margin; North Sea; Malin Basin;
Barents Sea
1. Introduction 
First reported by offshore Nova Scotia [1], pockmarks are depressions formed in soft sediments at the seabed by fluid flow processes. Since then, the known occurrences of pockmarks have increased dramatically. The rise of the documented occurrences of pockmarks is largely thanks to technical improvements in marine geophysical survey equipment, particularly of multibeam echo-sounders (MBES), along with a more widespread use of such data for seabed mapping. Pockmarks have been found worldwide, from water depths of less than 10 m in estuaries (e.g., [2]) to over 3000 m in offshore canyons (e.g., [3]). However, the distribution of known pockmarks is currently still geographically biased, with more occurrences reported in economically developed areas of the world where high-resolution geophysical surveys have been undertaken.
Although the first accounts described these features as concave crater-like depressions [1,4], the complexity and diversity of morphologies possible have gradually become evident. Pockmarks have sizes recorded across four magnitudes, although most are between 10 and 200 m diameter and 1每25 m deep [5]. They can occur in both random and non-random distributions controlled by the underlying geology due to, for example, the presence of faults (e.g., [6]) or buried channels (e.g., [7]). The variability of size, spatial distribution, and geometry results from their development depending on a variety of parameters such as fluid type, flow fluxes, thickness and nature of near bottom sediments, underlying structure, and lithology.
Most of the initial descriptions of these features were based on low penetration seismic profiles (mainly Boomer system) and/or from sidescan sonar data (e.g., [1,4,8]). Presently, pockmarks are predominantly mapped manually using seabed digital terrain models (DTM) created from MBES data (e.g., [6]), generally a time-consuming task. In addition, delineating their individual boundaries is subjective and consistency of criterion is hard to achieve. To address this, the British Geological Survey (BGS) developed a semi-automated mapping toolbox. This ArcGIS-based BGS Seabed Mapping Toolbox recognizes, spatially delineates, and morphometrically describes seabed features including pockmarks [9], coral mounds [10], and other confined features. The toolbox is embedded in ESRI? ArcGIS, a geographic information system (GIS) widely used in the field of marine geology, allowing users to work in a familiar and integrated mapping environment. Furthermore, the scripts developed use standard ESRI? algorithms that increase the clarity of the steps taken during delineation and characterization processes. With this approach, human interaction and expert knowledge is still part of the mapping process but is limited to restricting criteria for feature mapping. This allows multiple mapping exercises to be performed with the same criterion, improving comparisons across different areas or quantification of seabed changes over time. The tools also allow an extensive morphological characterization of the mapped features with a fraction of the effort that would be required using manual techniques.
The consistent characterization of vast numbers of pockmarks with multiple morphological characteristics allows an unprecedented statistical analysis of their morphology. Combining this statistical analysis with the geological and oceanographic knowledge of individual areas provides insights into the processes responsible for their development and the influence of local seabed conditions. The application of this method to different datasets, over a wider range of water depths, seabed sediments types, and geological settings, as reported in this study, significantly increases the understanding of the formation, evolution, and preservation of these common, but still poorly understood, seabed features.
This work is the result of a semi-formal collaboration, established in 2015, between national seabed mapping programmes in Norway (MAREANO, www.mareano.no), Ireland (INFOMAR, www. infomar.ie), and the UK (MAREMAP, www.maremap.ac.uk). This collaboration has facilitated the exchange of data and methods for the development of various aspects of geological mapping of the seafloor. Mapping of pockmarks was quickly identified as an objective of common interest to the geological surveys within the three mapping programmes, and one for which quantitative, objective methods were not yet readily available. Testing this mapping approach originally developed for UK seabed data in other geological settings data from the Barents Sea (Norway) and Malin Basin (Ireland) will lead to further developments of the toolbox. In this paper, we explore the applicability of this practical and effective mapping approach, fully integrated within the most used GIS software, and illustrate the potential of the use of morphometric parameters to characterize pockmarks.
2. MATERIAL AND METHODS
2.1. Study Areas and Datasets
A total of 20 MBES datasets were used in this study. These comprise: (1) 18 site survey datasets from the North Sea (UK) held by the British Geological Survey (BGS) and forming part of the MAREMAP data repository; (2) a dataset from the Malin Basin (Ireland) acquired by INFOMAR; and (3) a dataset from the Barents Sea (Norway) acquired by the MAREANO programme (Figure 1).
The selected datasets represent not just a diversity of geological settings, but also a range of data resolutions and quality, and present different mapping challenges. A resolution range from 1 to 10 m and pockmarks from 20 m to almost 800 m wide provided an opportunity to assess the impact of cell size and feature size in the mapping results. The dataset from Malin Deep provided a chance to test the impact of both regional topography and the presence of MBES acquisition artefacts on the delineation and characterization of pockmarks. The Barents Sea example, where pockmarks are particularly prolific, allowed us to explore the of the tools where vast numbers of seabed features are present.
2.1.1. Witch Ground Basin每North Sea (UK)
In the northern part of the North Sea, especially across the Witch Ground Basin, pockmarks can be found in high numbers (Figure 2). The Witch Ground Basin is an extensive area of muds reaching water depths greater than 150 m. It was a depo-centre for fine-grained sedimentation at the end of the Weichselian glaciation, when sediments were deposited very rapidly, creating a thick sequence of very soft muds [11]. Seismic profiles often show acoustic blanking within the shallow section of the Quaternary sequence underlying the Witch Ground Formation, which suggests that shallow gas is trapped at selected horizons [12]; such accumulations support the hypothesis that the pockmarks found in this basin were formed by gas escape at irregular intervals since deglaciation [13]. In addition, since the seabed in the Witch Ground Basin has remained essentially unchanged by erosion or sedimentation once the sea level stabilized after the last glaciation, the pockmarks represent the cumulative effects of gas-escape activity over a period of at least 8000 years [14].
The vast majority of the pockmarks mapped within the Witch Ground Basin are less than 3 m deep [9], with an area of approximately 2000 to 4000 m2. In cross-section, these pockmarks are mainly V-shaped (Figure 2d), with a few of them being U-shaped and very rarely W-shaped with a degree of asymmetry, whereas in plan-view, they are generally circular to elongate. Besides the vast number of unit pockmarks, several unusually large pockmarks were found in the Witch Ground Basin area. The largest of them, and among the largest pockmarks known globally, is the western pockmark of the Scanner Pockmark Complex (Figure 2a,b) [15].
The BGS has gathered numerous MBES datasets over the last decades in UK territorial waters, especially from the North Sea. These include the 18 multibeam datasets used by [9] in their first study http://using the semi-automatic mapping approach (Figure 2). These datasets were collected for the Strategic Environment Assessment programme (SEA programme) and data were collected as part of site surveys for exploration wells commissioned by different operators. Due to the purpose for which the data were acquired, these datasets tend to be of high resolution (of 2 m for most) but cover small areas, ranging from less than 5 km2 up to 36 km2. In total, the 18 datasets cover an area of 306 km2.
2.1.2. Malin Basin (Ireland) 
The Malin Shelf lies immediately north of Ireland and west of Scotland, with typical depths between 100 and 150 m. Seabed morphology is rocky and irregular in the east, while to the west, it is relatively sandy and smooth. The Malin shelf shallow geology is characterized by glacial diamictons, muds, and sands [16]. The Malin Deep pockmark field lies in the outer Malin shelf, approximately 70 km offshore northwest of the Malin Head. The area covers approximately1000km2. The pockmark field lies in a basin characterized by a smooth and soft seabed composed of fine-grained marine sediments, ranging from fine sands to silts. The thickness of the Quaternary deposits varies N-S from 175 to 125 m [17]. Both acoustic and electromagnetic evidence indicates the presence of fluids within these deposits [18]. Geochemical analysis of sulphate profiles indicates that gas from the shallow reservoir has been migrating upwards [18].
The Malin Deep dataset covers an area of c. 865 km2 (Figure 3). The Geological Survey of Ireland and the Marine Institute acquired this dataset in 2003, as part of the Irish National Seabed Survey (INSS), the precursor to the INFOMAR mapping programme. Data were acquired on-board the R.V. Celtic Explorer using a Kongsberg-Simrad EM1002 multibeam echosounder, with an operational frequency of 93每98 kHz. Bathymetric data cleaning was performed on-board and statistical analysis of the data indicates a vertical accuracy of <40 cm across the region. Resulting bathymetric terrain models were gridded at 5 m - 5 m.
2.1.3. Barents Sea (Norway) 
Pockmarks are a frequent occurrence in the Barents Sea, occurring far beyond the area investigated in this study. The distribution and likely origins of the pockmarks in the SW Barents Sea and Finnmark fjords, northern Norway, have been recently documented as part of a broad-scale study by [19], where the study area investigated here falls under their description for the ※Easternmost Norwegian sector§. The study area considered here was classified by [19] as having a high density (300每800 per km2) of pockmarks; however, the individual pockmarks were not delineated in that study. The authors showed that in the Barents Sea, pockmarks occur in areas where soft glaciomarine and marine sediments were deposited after the ice margin retreated [19]. The distribution of pockmarks thereby reflects the distribution of soft, fine-grained postglacial deposits in the SW Barents Sea. They also conclude that the pockmarks formed due to the melting of gas hydrates. It is suggested that this process probably started c. 14,500 cal. years ago, after the ice cap had melted and the bottom water temperature and thus the seabed temperature had increased due to the inflow of warm Atlantic water.
The Barents Sea dataset provided by MAREANO covers an area of 100 km2 near the Norway-Russian border (Figure 4). The data were collected by MMT under contract to the Norwegian Hydrographic Service as part of the MAREANO seabed mapping programme in 2011 using a Kongsberg EM710 MBES on the survey vessel M/V Franklin. Data are gridded at a 5 m resolution, a standard output for MAREANO bathymetric mapping. The data are of good quality and do not exhibit any significant artefacts from data acquisition or processing.
2.2. Manual Mapping 
Marine geological mapping methods have evolved substantially over recent decades, alongside diversification in the uses for this mapping. However, manual mapping of seabed features still represents a huge component of the effort to map and understand the seabed. Almost invariably, manual mapping of seabed features will involve the use of a GIS. It allows the creation, visualization, and analysis of DTMs, hillshade (from one or more directions), and several other rasters derived from the bathymetry data (e.g., slope, aspect, and curvature). GIS also provides a convenient platform for manual, expert-driven digitization of seabed features, based on the analysis of multiple layers of information. The definition of the limits of the geomorphic features is based on expert judgement complemented with classification schemes, but is often subjective. 
The example of manual mapping of pockmarks included in this study is based on the 5 m DTM and slope maps derived from the Malin Basin dataset. Seabed depressions shallower than 0.5 m were dismissed, as this approaches the vertical accuracy of the data (c. 40 cm). This area is particularly challenging due to the artefacts present in the dataset.
2.3. Pixel-Based Calculation of Terrain Attributes as a Basis for Semi-Automated Mapping 
Pixel-based analyses of bathymetric DTMs are often used to produce derived terrain attributes that serve as an aid to the manual digitization of features (as in the Malin Deep region). However, these derived terrain attributes can also be used for automatic pixel-based mapping, based on expert-defined threshold values before manual fine-tuning. 
Here, we explore some terrain attributes, derived from pixel-based analyses, related to slope, curvature, and relative position in order to examine their potential for delineating pockmarks. Other terrain attributes relating to orientation and terrain variability may be useful for describing the nature of the pockmarks but are not so directly relevant to their delineation.
The pixel-based terrain analysis methods presented as part of this study were conducted on SEA2 Box4 of the Witch Ground Basin datasets. This area is particularly challenging due to the different shapes and sizes of the pockmarks. Therefore, it is a good example for examining how well various methods are able to delineate the pockmarks. The methods and analysis scales used (Table 1) were http://selected to provide examples of the various approaches rather than conducting an exhaustive analysis of the area. A selection of the results is presented in Section 3.1, where results from the BGS Seabed Mapping Toolbox are shown for comparison.
2.4. BGS Seabed Mapping Toolbox 
The BGS-developed tools in the BGS Seabed Mapping Toolbox run individual Python scripts that use a sequence of pre-existing ArcGIS geoprocessing tools. The toolbox includes (1) data preparation tools; (2) feature delineation tools; and (3) characterization tools.
2.4.1. Data Preparation 
The basic input to the BGS Seabed Mapping Toolbox is a DTM, either obtained from a multibeam echosounder dataset or from another dataset that can be used to generate a DTM of the seabed or buried surface (e.g., 3D seismic〞[25]). In datasets strongly affected by artefacts, when using the semi-automated mapping toolbox, pockmarks may be incorrectly delineated and spurious values may be captured during their characterization. This was the case for the dataset available for the Malin Basin, where acquisition artefacts markedly affect this dataset (Figure 5). Artificial vertical reliefs (corrugations) of up to 50 cm are detected systematically across the dataset due to tidal shifts in the lines overlap and vessel motion-related artefacts across the swath. These are comparable in magnitude to the vertical relief of some of the pockmarks. For that reason, it was necessary to smooth the initial bathymetric surface. The ArcGIS Focal Statistics tool was used to smooth the original surface. The smoothed surface was then used as the input surface for both the pockmark delineation and characterization tools. The Focal Statistics tool performs a neighbourhood operation that computes an output raster where the value for each output cell is a function of the values of all the input cells that are in a specified neighbourhood around that location.
The overall regional morphology and the presence of overlapping morphological features can also affect the ability of the delineation tools to correctly map pockmarks. This can be addressed by using the BGS-developed Filter-based Clip Tool. This tool automatically identifies and clips areas of special interest where seabed features are likely to be present. The use of this tool is particularly useful in a setting like the Malin Basin (Figure 6), where the pockmarks occur within a regional basin. During the first delineation, the entire small basin was erroneously mapped as a pockmark since it is also a confined depression (Figure 6A). The Filter-based Clip Tool identifies and outlines areas of vertical relief changes, which are then used to clip the original DTM excluding the data from zones of smooth bathymetry (Figure 6B每D). Using the clipped DTM as the input to the delineation tool, this tool is now capable of delineating individual pockmarks within the small basin (Figure 6E).
2.4.2 Feature Delineation
As explained in [9], pockmarks are generally represented by confined depressions in a DTM, therefore it is possible to employ hydrological algorithms such as the Fill function in ArcGIS Spatial Analyst to define what the lowest elevation on the rim of a sink depression point if the depression was being filled). In fact, ※Fill§ is one of the key algorithms of a sequence of steps used by the Feature Delineation [Bathy] tool as described by [9] and is used here to delineate pockmarks. Experience has shown that in certain regional settings, using the rim of the confined depression related to the presence of the pockmark may result in an underestimation of its size, especially in areas with steep slopes. To address this issue, the alternative Feature Delineation [Derived] tool was created, which can use the derived terrain attribute Bathymetric Position Index (BPI) calculated from the bathymetry data using Benthic Terrain Modeler toolbox [22] as the input, instead of the bathymetry data. BPI measures whether a certain location is higher or lower than the surrounding seabed by comparing the depth of each pixel with the mean depth of neighbouring pixels within a user-defined neighbourhood (inner and outer radii). The BPI value obtained for any pixel, however, depends on both the regional setting and the neighbourhood used in the BPI calculation. The values of BPI are also sensitive to data resolution. This alternative approach using BPI rather than bathymetry therefore introduces unavoidable sources of inconsistencies on the criterion used to map pockmarks in different study areas and is only recommended for use in more local studies.
Due to the limitations of the BPI approach mentioned above, in this study involving data from different regions, the pockmark delineation was done directly from the bathymetric data. Five values must be defined to run the Feature Delineation [Bathy] tool; these are the Cut-off Vertical Relief, Minimum Vertical Relief, Minimum Width, Minimum Width/Length Ratio, and Buffer Distance. The Cut-off Vertical Relief defines the contour line that will be used to delineate the features. The Minimum Vertical Relief, Minimum Width, and Minimum Size Ratio define which features will be mapped; only the features that present dimensions above the specified thresholds will be delineated. The Cut-off Vertical Relief and the Minimum Vertical Relief can be the same value. The defined Buffer Distance compensates for the fact that the delineation process is based on the feature＊s internal contour line corresponding the Cut-off Vertical Relief threshold. This parameter should approximate the distance, in plan-view, from the internal contour line delineated based on the Cut-off Vertical Relief to the actual rim of the feature. The greater the value of Cut-off Vertical Relief, the greater the Buffer Distance. Figure 7 illustrates the different mapping results obtained by choosing different Cut-off Vertical Relief and Buffer Distance values.
The North Sea pockmarks were mapped as part of [9] using the first version of the Feature Delineation [Bathy] tool, which did not require the Minimum Vertical Relief and used Minimum Area instead of the Minimum Width as one of the threshold values. The Minimum Width later replaced the Minimum Area threshold because it was found to be more user-friendly and easier to relate to the resolution of the dataset used as input. The thresholds used to map the North Sea pockmarks were Cut-off Vertical Relief (then referred to as Minimum Depth) of 0.5 m and Minimum Area of 100 square metres. Table 2 presents the respective values used for Malin Deep and Barents Sea study areas.
The output of the tool is a polygon shapefile that delineates the mapped features (pockmarks). The shapefile attribute table contains the following fields (1) Area; (2) Perimeter; (3) VRelief; (4) MBG_Width; (5) MBG_Length; (6) MBG_Orient; and (7) MBG_W_L. Area and Perimeter describe the geometry of each delineated feature. The VRelief provides the vertical relief measure for each delineated feature. The MBG_Width, MBG_Length, and MBG_Orient describe the Minimum Bounding Geometry (MBG) envelope that contains each delineated feature. MBG_W_L describes the aspect ratio of these envelope polygons. Jorge et al. [26] assessed the use of different automated methods to measure longitudinal bedform＊s morphometry. Although these authors focus on subglacial positive-relief bedform (e.g., drumlins), their conclusions are still relevant to the morphometry measurements of pockmarks. They established that the MBG approach provides the most suitable measurements, from the tested methods, for both Orientation and Length, only showing a wider range of errors for the measurement of the Longitudinal Asymmetry, which is not used in this study.
The output shapefiles from the delineation tool require visual assessment as part of a semi-automatic workflow. Visual assessment of the polygons can be performed by overlaying the generated shapefile onto both the original bathymetric data and derived surfaces, such as the slope map. This allows a check on the mapping results and assessment of the need to manually edit sporadic polygons and/or to add features that were missed by the automated method. Additionally, the visual assessment can be complemented by an analysis of the values reported in the table of attributes.
2.5. Morphometric Analysis
In addition to the morphological attributes extracted by the characterization tool, extra
morphological size and shape ratios can also be calculated. This includes the Vertical Relief to Area (VR/A) ratio. To characterize the profile of the pockmarks, we also define a Profile Indicator (PI) by the following equation:
This morphologic PI ratio can help to distinguish between depressions with a V-shape profile, more typical of single pockmarks, and the depressions with a U-shape profile, more common on complex pockmarks. V-shape pockmarks will generate lower values of PI compared to the pockmarks with a U-shape (Figure 8).
3. Results
3.1. Witch Ground Basin〞North Sea
A total of 4146 pockmarks, deeper than 50 cm, were mapped in theWitch Ground Basin. Here, we present the general trends and measures for the whole basin, but [9] summarizes the descriptive measurements for each of the individual site survey areas.
The highest pockmark density occurs within the survey site Roisin in the centre of the basin, with a pockmark density of almost 30 pockmarks per square kilometre (Figure 9). The density of pockmarks decreases from the centre of the basin where water depths exceed 150 m, to less than 5 per km2 on the edge of the basin where water depths are around 120 m. However, more than the changes in water depth, the number of pockmarks seems to be controlled by the thickness of the very soft late glacial sediments within which the pockmarks are developed. This is greatest in the centre of the basin but thins towards the edge.
The mean vertical relief of the pockmarks is 1.82 m, with most pockmarks between 1 and 2.4 m deep (Q1 and Q3, respectively). However, in SEA2 Box 4, there are five pockmarks deeper than 12 m and one of them reaches almost 18 m deep (Figure 10A). These unusually large pockmarks, in UK license block 15/25, have long been known as sites of active seepage [27每29]. None of the datasets exhibit a significant variation of vertical relief with water depth (Figure 10B).
The mean area of the pockmarks is 3222 m2, with half of the pockmarks between 1960 (Q1) and 5385 m2 (Q3). The pockmark area does not follow a trend from the centre to the edge of the basin. However, it does show a strong correlation between vertical relief and pockmark area (Figure 11). With the exception of the Rob Roy dataset, all the other datasets studied in the Witch Ground Basin show a marked trend where pockmarks with higher vertical reliefs have greater areas. The reason why the Rob Roy dataset does not present the same trend as the rest of the dataset will be discussed later.
Comparison between the Results from the BGS Semi-Automatic Approach and from the Pixel-Based Analysis
As shown in Figure 12, the Feature Delineation [Bathy] tool successfully delineates pockmarks across a range of sizes and with different sizes and shapes. Results from pixel-based analyses are more varied in their ability to delineate the various morphologies of the pockmarks in this area. Some examples are shown in Figure 13.
The first terrain attribute tested was slope, initially tested using standard 3 * 3 pixel analysis and Horn＊s algorithm [20] in ArcGIS＊ Spatial Analyst. Using a colour ramp classified by natural breaks (Figure 13A), we see that all the pockmarks delineated by the BGS tool are highlighted by the slope map. The difficulty lies in finding a slope cut-off value that would delimit pockmarks of different morphologies, and in representing the entire pockmark. For instance, some pockmarks in SEA2 Box 4 present an asymmetric profile (Figure 2c), with steep northern slopes facing very elongated southern slopes, and a delineation based on slope would tend to delineate a crescent shape whilst missing the areas of very gentle slope. Testing larger analysis windows and using methods for generating multiple scale slopes (Table 1) has shown that increasing the analysis window is of limited value as the extent of the pockmarks can become overestimated. As with all raster outputs of pixel-based analyses, it is important to be conscious of the influence of colour ramp choice on visual interpretation.
Both marine geoscientists and marine biologists have used BPI widely [10,30]. BPI can be computed using the BTM toolbox [22]. For this study, only fine-scale BPI was calculated, testing the inner and outer radii indicated in Table 2. Figure 13B shows fine-scale BPI with inner radius 1 (i.e., 2 m) and outer radius 15 (i.e., 30 m). The areas with negative BPI give a good delineation of the larger pockmarks, including the base. However, using this neighbourhood, we fail to capture the smaller, shallower, pockmarks, due to the integer rounding inherent in the tool. Decreasing the outer radius to try to capture the smaller pockmarks is not successful and results in fewer pockmarks being detected overall. Increasing the inner radius has a negligible effect on the delineation.
http://The small, shallow pockmarks can be detected with a modified version of BPI without integer rounding (Figure 13C, Table 2). This result is more successful at highlighting the small pockmarks as well as most of the larger ones, but it also detects artefacts in the MBES data, which are of a similar magnitude to the BPI values within parts of the pockmarks.
Like BPI, measures of curvature also highlight positive and negative features of the terrain. We first tested standard, 3 * 3 curvature in ArcGIS＊ Spatial Analyst [24] with the results shown in Figure 13D. This, like Figure 13C, shows up artefacts in data in addition to pockmarks, but in this case, it is even more difficult to separate the artefacts and the pockmarks from the curvature values. It is clear that larger, alternate analysis scales are required that overlook the artefacts and find the pockmarks.
We have also tested two measures of curvature that can be generated at multiple scales in the GRASS module r.param.scale (Table 1). Minimum curvature (Figure 13E) should find the inflexion point in the bathymetry surface corresponding to the pockmark and is reasonably successful in capturing entire pockmarks spanning a range of sizes, although some artefacts are also highlighted. Profile curvature [21] describes the rate of change of slope along a profile of the surface and can be useful in highlighting convex or concave slopes in the bathymetry surface. This appears to be one of the most successful pixel-based approaches to delineating the various sizes of pockmarks in the study area; however, we note that artefacts at the eastern edge of the dataset are also detected.
Various properties of curvature are combined in feature classification. We tested the feature classification from r.param.scale [21]. Here, we show only the ＆pit＊ class output which identifies depressions in the surface and seems well matched the detection of pockmarks (Figure 14). Experimenting with analysis windows at multiple scales, it is clear that the 9 * 9 analysis window fails to separate out pockmarks from artefacts in the data. A larger analysis window of 15 * 15 successfully detects the small-medium size pockmarks but fails to capture the larger ones. Consequently, we see that larger analysis windows of 27 * 27 and 51 * 51 cells find the larger and largest pockmarks, respectively, but overlook the small ones.
It appears that some of the pixel-based methods tested here can give good results where pockmarks are of a relatively uniform size. Whereas, multiple scale analysis can be employed to detect features of different sizes if required. It is challenging, however, to find a single scale of analysis and analysis type that detects all pockmarks and delineates the entire feature. Further, with the exception of the feature classification, the user must determine a suitable cut-off value in the terrain attribute (BPI, curvature etc.) to use as the limit of the pockmark. If such a value can be used, then these raster outputs of pixel-based terrain analysis can be used as a basis for conversion to vector features (shapefiles), should these be required for the application in question. Following conversion to polygon features, area attributes can be applied to the pockmarks. However, the areas may not be a consistent representation of all complete pockmarks due to the dependence on the raster cut-off value, which may well be a compromise between pockmarks of different morphometry. Manual editing of the polygons may therefore be a necessary expert-driven step in the mapping process. Pixel-based terrain analysis, based on the type of methods illustrated here, does not provide any estimate of pockmark depth or volume. Methods such as Geomorphons [31], which are designed to simultaneously identify landform elements across a wider range of scales, may be more suitable, although they face similar limitations in terms of the metrics they provide, unless combined with other methods.
3.2. Malin Basin 
Due to the artefacts present in this dataset, it was necessary to smooth the bathymetry before applying the delineation tool (Section 2.5). As a consequence of this smoothing, the vertical relief of the pockmarks was most likely underestimated and shallow pockmarks were not detected, or where detected, their area may well be underestimated (Figure 15).
Nevertheless, after smoothing and clipping the bathymetry using the Filter-based Clip Tool, the Delineation Tool identified 150 pockmarks with a vertical relief greater than 20 cm (Figure 16) within the Malin Basin. Pockmarks were mapped in water depths ranging from 126 to 177 m, but most of the pockmarks are found in water depths below 167 m (Figure 16B). The largest mapped pockmark is at least 5.5 m deep, but 75% of the pockmarks are shallower than 60 cm, and the mean vertical relief is 36 cm (Figure 17). We also remark that all the pockmarks with vertical relief higher than one metre occur at water depths deeper than 168 m (Figure 16A). The pockmark area varies from 2000 m2 to almost 303,000 m2, with a mean value of 32,073 m2. Whereas, the pockmark length varies from the smallest pockmark with less than 50 m to the largest with 785 m long. They tend to be quite concentric, with a mean size ratio of 0.83.
Previously, 214 depression deeper than 0.5 m had been mapped manually [32]. When comparing the manual mapping with automated mapping, it is evident that fewer pockmarks were identified by the Delineation Tool, especially in the steeper areas (Figure 18). Almost 80% of the pockmarks that were not mapped automatically are located flanks of the Malin Deep, where the smoothing of the data has a stronger impact on the automatic recognition of shallow pockmarks. However, within the centre of the basin, the delineation tool detected several pockmarks that were not manually mapped. Some of the missed pockmarks are deeper than 0.5 m and therefore would fulfil the requirements used for the manual mapping. This illustrates the ability of the automatic tools to recognise subtle features that can escape the attention of expert examination.
Based on the manual mapping, the pockmarks were described as being between 40 and 850 m wide, and up to 8.5 m deep, with a mean length of 124 m and mean vertical relief of 1.02 m [32]. These measurements present higher vertical relief than the values extracted by the semi-automatic method. We believe that this difference also results from the need to smooth, in this case, the bathymetry before applying the Feature Delineation [Bathy] tool.
3.3. Barents Sea 
The Barents Sea dataset is characterized by the presence of vast numbers of pockmarks. These are the main topographic features present, but there are also a few iceberg ploughmarks, which can be a kilometre long. After applying the Filter-based Clip Tool, to avoid the erroneous delineation of ploughmarks as pockmarks, the Feature Delineation [Bathy] tool identified and delineated more than 35,000 pockmarks in two hours. These were then characterized by using the Feature Description tool. 
The pockmarks are found in water depths ranging from 255 to 305 m, with half of all pockmarks located between 275 and 291 m. The mean pockmark vertical relief is 2.2 m, but some can be up to 7 m deep. The pockmark area varies from 538 m2 to more than 11,000 m2, but most are less than 1643 m2 (Q3). Their length measure, using the MBG envelope, varies from just 25 m to more than 220 m. They tend to be concentric with a mean MBG_Width/MBG_Length ratio of 0.88. Figure 19 shows the relationship between vertical relief and area for the pockmarks mapped in the Barents Sea dataset. Almost 80% of the pockmark polygons followed a distinct distribution compared to the remaining pockmarks, with a higher VR:A ratio (Figure 19). By selecting these, it became evident in GIS that these polygons corresponded to the single pockmarks (Figure 19), whereas the polygons with a lower VR:A ratio corresponded to pockmarks with a more complex geometry and with multiple possible venting points.
By separating these two populations, using the following function:
It was possible to study their morphologic characteristic separately. Table 3 shows some of the metrics extracted from these two types of pockmarks.
The mean area of the complex pockmarks is almost twice the mean area of the single pockmarks and the larger of the complex pockmarks can be almost one order of magnitude larger than the single pockmarks (Figure 20). However, the mean vertical relief of these features shows only a slight increase from 2.12 to 2.5, from the single to the complex pockmarks group, respectively (Figure 20). The mean value for the profile indicator for the complex pockmarks is 0.38, whereas the mean PI value for the single pockmarks is 0.33 (Figure 20). These values indicate that the single pockmarks will have profiles closer to the V-shape compared to the complex pockmarks.
The stacked histogram in Figure 21A shows that, contrary to the other two regions, the pockmarks with the highest vertical relief tend to occur in deeper waters. This is not the result of a preferential occurrence of complex pockmarks in deeper waters as both types of pockmarks exhibit a trend of increase in vertical relief with water depth (Figure 21B). However, it should be noticed that changes in water depth in itself should not be the reason for this trend. Differences in the depositional environment linked to the water depth (e.g., thickness of soft, fine-grained postglacial deposits) are expected to be the primary parameter driving the observed increase of vertical relief with water depth. Other factors linked to the gas release could also be responsible for such a trend.
3.4. Geomorphometric 
Comparison between the Different Areas Due to the systematic and consistent mapping approach adopted, the morphological attributes used in this study to describe the pockmarks can be employed to compare the morphology and trends between different areas. For example, we see a clear distinction in the vertical relief of pockmarks from the three study areas (Figure 22).
The North Sea study area is the one where the pockmarks exhibit the highest vertical relief. However, these usually large pockmarks are just five outliers to the general trend that exceed 12 m, whilst the mean value of Vertical Relief is only 1.8 m. The Barents Sea dataset shows the highest Vertical Relief mean value, which is almost four times higher than the mean value calculated for the Malin Basin dataset. Nonetheless, the Malin Basin shows the highest Area mean value; it is one order of magnitude bigger than the mean values for the other two datasets (North Sea: 4798 m2 and Barents Sea: 1462 m2).
http://All the study areas show a positive correlation between vertical relief and area, controlled by the angle of repose. However, the strength of this correlation and the ratio between these two morphometric variables varies from region to region (Figure 23). The pockmarks from the Barents Sea present the highest VR/A mean value; where the single pockmarks have and complex pockmarks. Nevertheless, even the complex pockmarks that tend be less deep than their single pockmark counterparts (i.e., with the same vertical relief), tend to have higher VR/A values than the pockmarks from the North Sea. The Malin Basin pockmarks present much lower VR/A values but, as mentioned earlier, their vertical relief is significantly affected by the smoothing applied to the dataset, leading to artificially lower VR/A values. The regional correlation coefficients calculated for the three study areas are, respectively, 0.72, 0.57, and 0.67 for the North Sea, the Barents Sea, and the Malin Basin (Figure 23). The correlation is even stronger if looking either to individual study areas within the Witch Ground Basin〞R * 0.85 for most survey areas, or individual type of pockmarks〞e.g., R = 0.94 for the single pockmarks within the Barents Sea〞the highest observed correlation coefficient in this study (Figure 24).
4. DISSUSSION
It has been widely shown that human-cognitive approaches, i.e., manual mapping, have a number of limitations such as scale bias, azimuth bias, and operator bias e.g., [33,34]. These limitations have encouraged new research into automated and semi-automated GIS-based mapping routines across many sub-disciplines of quantitative geomorphology [35]. In various fields of terrestrial geomorphology, pixel-based analysis techniques have been used with much success to discretize landform elements or the separable constituents of landforms, such as ridges or peaks [36,37]. However, integrating landform elements to characterize individual landforms has proven more problematic and requires subjective operator decision inputs (e.g., [38]). More recently, objectoriented approaches for automated mapping have become favoured in the terrestrial geomorphological community, whereby remote sensing imagery is segmented into meaningful objects, whose characteristics are assessed through spatial, spectral, and temporal scales [36每38].
As highlighted by [39], the availability of high-quality seabed DTMs and quantitative analyses of these data for delineation of seabed features is much more recent. We are aware of several attempts among the seabed mapping community to delineate individual pockmarks using pixel-based analysis of bathymetric data to produce terrain attributes that highlight the negative features (e.g., slope, curvature). However, there are few published studies, which may be due to the limited success of these approaches. A promising feature extraction method, based on kernel matching and machine learning, was developed by [40], but their approach, developed on synthetic data with limited testing on real-world data, does not seem to have made the crossover from computer science method development to further use in applied seabed mapping. Although other feature extraction methods such as [21] or [31] have been applied to other seabed geomorphic features, to our knowledge, they have not been applied to the delineation of pockmarks. The methods of [21] are among those tested here, but the application of the Geomorphon approach of [31] which is starting to gain attention for marine applications (e.g., [41,42]) has not yet been tested for pockmark delineation. Future applications of this method may benefit from the developments to the method reported by [42], especially where the pockmarks also have a characteristic MBES backscatter signature. Object-Based Image Analysis (OBIA) is currently gaining momentum in the seabed mapping community for seabed classification and delineation of features (e.g., [43每45]). The approach seems suitable for application to the delineation of features such as pockmarks, but we are not aware of any published studies.
These methods vary in complexity depending on the data available for the seabed classification and the approach used but are all much more complex than the approach presented here. The BGS Seabed Mapping Toolbox provides a simple extension to ArcGIS functionality that gives the user the possibility to extract morphologic information of a vast number of pockmarks from multiple surveys in a systematic and consistent way. All that is required from the user is the informed definition a limited number of thresholds. This is a significant advantage within the context of a national scale mapping programme, since it would be challenging for multiple interpreters to maintain consistent criteria throughout the laborious process of manual mapping. Such standardisation may also be difficult to achieve for pixel- or object-based classification, and none of the alternative methods identified so far address the need for morphological characterisation in addition to delineation. The BGS toolbox, although simpler than some potential alternative methods, provides both delineation and characterisation of the pockmarks. This appears to be well matched to the needs of national mapping programmes (see also Section 4.4).
Regardless of the mapping approach used for the delineation of the bedforms, it is crucial to understand how the resolution and data quality will influence morphometric studies of bedforms. Although this has been stressed in certain fields of terrestrial geomorphology that rely on the use of DTMs, relatively little attention has been paid to this to date in marine geomorphological studies. As a result, there is currently a lack of studies assessing how seabed mapping and morphometric characterisation are influenced by variations in the grid cell size or data quality. These types of studies could advise on the optimum resolution depending on the dimensions of the features studied, as well as establishing protocols for dealing with datasets where the fidelity of the DTM is sub-optimal.
4.1. Importance of Pockmarks Geomorphometric Characterization 
Although it is widely accepted that pockmarks are a superficial expression of fluid flow, their formation mechanism is still elusive. Quantitative morphological data can be an important source of information in addressing this question. As we have demonstrated, quantitative analysis of pockmark morphology can provide valuable insights into the factors that control their formation and development. Supported by the wider availability of high resolution bathymetry data and appropriate DTM-based methodologies for morphometric analysis, we hope that more studies linking morphological trends to geological processes will be forthcoming in the literature, such as the one in this Special Issue by [46]. We hope the results of such studies will also be carried forward in validating numerical models of the development of seabed features.
Further, we note that with better understanding the factors controlling the morphology and spatial distribution of pockmarks, their morphometric characteristics could be used as proxies to subsurface conditions, local hydrodynamics, or fluid flow regime. As mentioned in Section 2.1, [9] shows that in the Witch Ground Basin, the thickness of very soft late glacial sediments seems to control the number of pockmarks. In this area, pockmark density maps can be used to predict variation in the thickness of the Witch Ground Formation. However, in areas of the South Western Barents Sea, according to [17], pockmark distribution seems to be almost independent of the Quaternary sediment thickness, although the authors do report an inferred correlation between pockmark size and the thickness of fine-grained deposits. This reflects the complex development of these seabed features and the fact that morphometric characteristics of pockmarks extracted from bathymetry data as proxies to subsurface conditions or flow regimes are somewhat restricted to variations within a certain regional setting.
Even if the actual development of pockmarks is not yet totally understood, there is evidence that lateral collapse of the pockmarks sidewall is a key process for the widening of the pockmarks (e.g., [47]). Therefore, it is valid to conclude that the geotechnical characteristics of the sediments affected will have a crucial role in the ratio between vertical relief developed and area affected by the seepage. Areas with sediment packages comprised of stiffer material will sustain steeper slopes and show higher rations between vertical relief and area. Therefore, a gradual and systematic mapping of the pockmarks, complemented with data on the local geotechnical characteristics would be an invaluable resource that could facilitate the use of this morphometric ratio as an indicator of the sediment proprieties. 
Multibeam datasets can provide bathymetric data over vast areas of the seabed more economically than any sub-surface acoustic system. It would seem advantageous for both the scientific community and any commercial sector that requires seabed infrastructures to utilize the bathymetric data for many more quantitative analyses of seabed morphometry than are currently in common use.
4.2. Impact of Data Resolution and Quality 
Although the BGS semi-automated mapping approach provided robust results for most areas, it was affected by the resolution and quality of the bathymetric dataset used. This is particularly evident when comparing the results between the different datasets from the Witch Ground Basin. The results obtained for the Rob Roy survey are a good example of the impact of using a dataset with an insufficient resolution for morphological studies. The lower resolution of the data from the Rob Roy dataset, a 10 m grid, prevented both the correct identification and delineation of the pockmarks at the seabed. This is evident by comparing the pockmark density of the adjacent survey site, Ivanhoe, which has a pockmark density of 13.8 pockmarks per km2, with the pockmark density within the Rob Roy dataset, which is only 4.81 (Figure 9). As noted by [9], this apparent abrupt reduction of pockmark occurrence between these adjacent study areas does not appear to be controlled by: (1) marked changes in the underlying geology; (2) distinct fluid flow regime; or (3) variation in the nature of the seabed sediments. It can best be explained by the lower resolution of the Rob Roy bathymetric dataset. The lower resolution also affected the morphometric characterization of the pockmarks, in particular the measurements of vertical relief. The vertical relief mean value defined was merely 0.87 m, whereas the mean value for the adjacent area, Ivanhoe, is 1.22 m. The Rob Roy pockmarks are described as having significantly lower vertical relief values than any other study area within the North Sea. No significant changes in the morphometry of the pockmarks between the datasets with 1, 2, and 5 m resolution were evident.
This comparison suggests an optimum resolution (i.e., the coarsest resolution, providing efficiency for measurement, computing time and data storage, at which detail is not sacrificed) of at least 5 m for the dimensions of the pockmarks found in the North Sea. However, to accurately define the optimum resolution, multiple resolution datasets from the same area should be used to assess the effect of resolution on the morphometry measurement of pockmarks. That analysis goes beyond the scope of this work, but as seabed mapping continues to integrate morphometric analyses of bedforms based on digital data, it is crucial to understand how bathymetric data resolution influences morphometric variables related to bedforms.
The dataset from the Malin Deep provides a valuable insight into the potential impact of bathymetric data quality on the detection of pockmarks. Whilst reasonable results may be obtained by employing methods such as smoothing to reduce the effect of the artefacts, it is hard to overcome the inherent limitations of the dataset entirely and it should be acknowledged that there are quality issues affecting the results besides those related to data resolution. Lecours et al. [48] recently showed how heave, pitch, roll, and timing artefacts in MBES bathymetry impact habitat maps and species distribution models which utilise bathymetry data and derived terrain attributes as predictor variables. Their results show that artefacts can lead to misleading and counterintuitive results. For similar reasons to those given for data resolution, the assessment of data quality is equally important for the delineation and quantitative characterisation of bedforms.
4.3. Impact of Regional Morphology 
The Malin Basin dataset has demonstrated that the mapping output of the BGS Feature Delineation [Bathy] tool under represents small features located on slopes. These features are not distinguished using bathymetric data alone, and when identified, their vertical relief is frequently underestimated. For that type of setting, a semi-automated approach based on the BPI, as presented by [49], could be more effective. These features are not distinguished using bathymetric data alone, and when identified, their vertical relief is frequently underestimated. However, it should be noted that for statistical comparison, the mapping based on the BPI would not provide the same consistency of delineation criterion as it is based on a relative defined value depending on the surrounding neighbourhood.
4.4. Mapping Strategies at Multiple Scales 
The output of a quantitative tool like the BGS mapping toolbox is well suited to providing information on several levels of detail to suit the particular mapping purpose and scale of map product. The present study has only illustrated the implementation of the toolbox on relatively small datasets, but the method is suitable for application to larger datasets subject to computing resources and/or with tiling of the bathymetric data.
Whilst some investigations may require the delineation of individual pockmarks on detailed maps at a 1:20,000 scale or finer, others rather use this information as a basis for providing a summarized quantification of properties of the pockmarks. For example, in producing maps at a 1:100,000 scale or greater, the MAREANO programme is interested in mapping polygons indicating ※pockmark areas§ to provide information for management authorities on the location and extent of these terrain features, which may be linked to particular habitats. This is currently done by expert judgement, but the output of the BGS toolbox provides a quantitative indication of pockmark density that will be useful for defining objective criteria for defining the limits of a pockmark field. Further, summary statistics on the size and shape of the pockmarks can be attributed to such polygons.
Depending on the scale used, it can be impractical, or even pointless, to represent individual pockmarks in a map. In the context of regional mapping, the use of density contours is often the best approach to capture the number and spatial distribution of these seabed features. Figure 25 shows the type of pockmark density map that can be generated for the Barents Sea study area based on the outputs from the BGS Seabed Mapping Toolbox. This figure was generated using the point shapefile that represents the deepest point in each polygon. The density values range from 60 to almost 600 pockmarks per square kilometre, and the density shading highlights geographical trends in the distribution.
Even in a situation where the end-user does not require the pockmark density distribution displayed on the map and only the outline of pockmark field is needed, there are advantages to using a density contour line to outline the limits of a pockmark field. This approach increases the efficiency of pockmarks fields＊ delineation, improves the consistency of the mapping, and prevents any discrepancies between the mapping of adjacent areas. In addition, subjectivity issues related to manually defining the limits of a pockmark field are greatly reduced by using a more automatic approach.
5. CONCLUSIONS
This study explored the potential of a practical and effective semi-automatic approach for mapping pockmarks that is fully integrated within commonly used GIS software. A total of 39,533 pockmarks were mapped, across three different geological settings within the European Glaciated Margin, each of which can be considered a test case study for that setting. The results from the semi-automatic mapping exercise using the BGS Seabed Mapping Toolbox were compared to other mapping approaches, demonstrating that the semi-automatic mapping approach presented here is a valid and useful alternative. Our approach overcomes the subjectivity intrinsic to manual delineation of pockmarks, and is also considerably quicker. The results of this study indicate that this approach is more flexible than pixel-based delineation methods when dealing with pockmarks of varying sizes. Moreover, this approach incorporates the automatic extraction of the morphometric attributes of the pockmarks. This information facilitates an unprecedented statistical analysis of their morphology and the analysis of spatial trends within the pockmark fields, providing insights into the processes responsible for their development and the influence of local seabed conditions. Whilst the study has focussed on the delineation of individual pockmarks, we have also shown how our quantitative methodology, once applied to larger datasets, can provide a convenient and objective basis for summarizing pockmark distribution at broader map scales in a consistent way, increasing the relevance of the mapping results to multiple end users.
Shallow stratigraphic control on pockmark distribution in north temperate estuaries
Laura L. Brothers, Joseph T. Kelley, Daniel F. Belknap, Walter A. Barnhardt, Brian D. Andrews, Christine Legere, John E. Hughes Clarke
ABSTRACT
Pockmark fields occur throughout northern North American temperate estuaries despite the absence of extensive thermogenic hydrocarbon deposits typically associated with pockmarks. In such settings, the origins of the gas and triggering mechanism(s) responsible for pockmark formation are not obvious. Nor is it known why pockmarks proliferate in this region but do not occur south of the glacial terminus in eastern North America. This paper tests two hypotheses addressing these knowledge gaps: 1) the region's unique sea-level history provided a terrestrial deposit that sourced the gas responsible for pockmark formation; and 2) the region's physiography controls pockmarks distribution. This study integrates over 2500 km of high-resolution swath bathymetry, Chirp seismic reflection profiles and vibracore data acquired in three estuarine pockmark fields in the Gulf of Maine and Bay of Fundy. Vibracores sampled a hydric paleosol lacking the organic-rich upper horizons, indicating that an organic-rich terrestrial deposit was eroded prior to pockmark formation. This observation suggests that the gas, which is presumably responsible for the formation of the pockmarks, originated in Holocene estuarine sediments (loss on ignition 3.5每10%), not terrestrial deposits that were subsequently drowned and buried by mud. The 7470 pockmarks identified in this study are non-randomly clustered. Pockmark size and distribution relate to Holocene sediment thickness (r2=0.60), basin morphology and glacial deposits. The irregular underlying topography that dictates Holocene sediment thickness may ultimately play a more important role in temperate estuarine pockmark distribution than drowned terrestrial deposits. These results give insight into the conditions necessary for pockmark formation in nearshore coastal environments.
Keywords: Pockmarks; Methane; Redoximorphic features; Swath bathymetry; Gulf of Maine; Bay of Fundy
1.INTRODUCTION
Most often interpreted as fluid-escape features, pockmarks, or seafloor depressions, are well studied in areas with gas hydrates, abundant tectonic activity, deltas and petroleum resources (Judd and Hovland, 2007). However, large pockmarks also persist in environments lacking those characteristics (e.g., Kelley et al., 1994; Webb et al., 2009), and that presently exhibit minimal fluid venting (Ussler et al., 2003). Constrained to fine-grained substrates and typically associated with subsurface gas (Judd and Hovland, 2007), the factors that determine why pockmarks scar some gassy seafloors while other muddy embayments remain relatively flat are unknown. In eastern North America pockmark fields are not reported south of Long Island Sound (Fleisher et al., 2001; Poppe et al., 2006), despite the abundance of well-studied, gassy, muddy estuaries (e.g., Reeburgh, 1969; Schubel, 1974; Hagen and Vogt, 1998; Martens et al., 1998). The absence of pockmarks south of the glacial terminus suggests that local and regional heterogeneities, possibly related to glacial or sea-level history or bedrock geology, influence pockmark field distribution (e.g., Rogers et al., 2006). This paper focuses on three estuarine pockmark fields in the Gulf of Maine and Bay of Fundy (Figs. 1每3; Table 1). Using densely gridded, high-resolution geophysical data combined with vibracore results we test the hypotheses that deposits formed during lower-than-present sea level source the gas responsible for pockmarks, and that regional physiography contributes to pockmark distribution in northern temperate estuaries. These results provide insights into origins of regional shallow gas, shallowfluidmigration pathways, and the set up conditions necessary for pockmark formation.
2.REGIONAL SETTING
Metamorphic and igneous bedrock, ranging from Precambrian to Paleozoic age, frame Maine's coast (Osberg et al., 1985), while igneous, metamorphic and sedimentary bedrock typify the Bay of Fundy (Cumming, 1967). Differential erosion of bedrock associated with glaciations, coupled with Holocene sea-level rise, formed an irregular coastal and inner shelf region characterized by numerous estuaries and bays (Kelley, 1987; Kelley et al., 1989), and littered by pockmarks (Rogers et al., 2006). In cross section, the coastal embayments are 10每 60 m deep bedrock-framed depressions filled with sediment termed ＆nearshore b layers would occur at the base of the Holocene in topographic lows
where it was protected from the erosive effects of the last transgression.asins＊ (Kelley and Belknap, 1991). As these topographic lows extend seaward they become scoured, seaward sloping troughs, termed ＆shelf valleys＊ (Kelley and Belknap, 1991; Barnhardt et al., 1996, 2009). The origin of underlying depressions is unknown, though they may reflect faulting (Osberg et al., 1985), fluvial erosion (Johnson, 1925), glacial scour (Shepard, 1931), some combination of the three mechanisms (Uchupi, 1968), or even subglacial meltwater pathways as inferred elsewhere (Boyd et al., 1988).
During deglaciation, approximately 15 ka (Dorion et al., 2001), sea level was ~75 m above the present coastline of Maine (Belknap et al., 1987). By 12.5 ka it had fallen to a lowstand position 60 mbelow present sea level (Kelley et al., 2010). Sea level then rose quickly from the lowstand until ~11 ka when the rate of rise slowed significantly until 7.5 ka. During that 'slowstand' period widespread erosion of glaciogenic deposits occurred and was accompanied by concurrent deposition of reworked sediment into beaches and coastal wetlands. These deposits were subsequently drowned during the rapid sea-level rise after 7.5 ka (Kelley et al., 2010).
Extensive geophysical surveys indicate thatmethane and pockmarks occur in Holocene sediments throughout Maine and New Brunswick's muddy estuaries (Scanlon and Knebel, 1989; Shipp, 1989; Fader, 1991; Barnhardt et al., 1996; Gontz, 2005; Rogers et al., 2006). Inconsistent evidence for venting (Kelley et al., 1994; Ussler et al., 2003), a dearth of methanotropes (Wildish et al., 2008), and nominal seafloor change (Brothers et al., 2011a) suggests these pockmark fields currently experience minimal fluid-escape activity. Geochemical sampling (Ussler et al., 2003) suggest that the region's methane is microbial in origin, although the stratigraphic source of the gas remains undetermined. Barnhardt et al. (1997), Gontz (2005) and Kelley et al. (2010) found micro-and macro-fossil evidence for drowned coastal wetlands in core sampling, and Rogers et al. (2006) proposed that such deposits provided a concentrated source of organic material that evolved into methane, and, by inference, formed regional pockmarks. They suggested such organic layers would occur at the base of the Holocene in topographic lows where it was protected from the erosive effects of the last transgression.
3.METHODS
3.1 Geophysical data
Over the last two decades four organizations collected swath bathymetry and dense grids of high-resolution seismic reflection data in the Belfast Bay, Blue Hill Bay, Maine, and Passamaquoddy Bay, New Brunswick (Table 2). Survey tracklines were spaced 100每200 m apart. Bathymetry data were collected and processed using techniques described in Andrews et al. (2010) and by the Ocean Mapping Group (2012a). Pockmark locations and sizewere extracted fromthe bathymetry data as point features, and the fields spatially examined, using methods presented by Andrews et al. (2010). Where seismic data were available, we generated isopach maps of the Holocene sediment by subtracting interpreted surfaces from each other (Fig. 4). Different seismic data were collected using different systems and frequencies that are described in Table 2. Sediment thickness in Blue Hill Bay could not be mapped due to the prevalence of acoustic turbidity in the upper sediment column that obscures lower units.
3.2 Vibracore samples
Since 2007, 37 Rossfelder vibracore samples (1每7 m in length) have been collected in the Belfast Bay pockmark field (Supplemental material for latitudes, and longitudes) (Fig. 5). Cores were collected with the methods described by Barnhardt et al. (1997), and were evaluated for sediment texture, color, water content, loss on ignition (LOI) by weight and the presence of macrofossils. Three logs of vibracore samples collected by Barnhardt (1994) are also used in this study.
4.RESULTS
4.1 Pockmark field stratigraphy
Seismic reflection profiles and core samples were used to define the stratigraphy of pockmark fields in all three estuaries. Interpretations of units follow those of Belknap and Shipp (1991), Barnhardt et al. (1997) and Belknap et al. (2002).
4.1.1 Unit BR-Bedrock
The basal reflector identified in seismic profiles has a high-intensity return, and a highly irregular surface with relief up to tens of meters over short horizontal distances (Fig. 6). The surface of this unit is shaped into basins that, generally, trend fromnortherly to southerly quadrants. Interpreted as bedrock, this facies is identified throughout all three study areas, but acoustic turbidity related to gas in the upper sediment column can obscure it (Fig. 7). The unit was not sampled by vibracores.
4.1.2 Unit T
Unit T has a strong surface reflector that overlies bedrock and is sometimes difficult to distinguish from bedrock where the unit is thin. Irregularly distributed in ridge-like deposits throughout the study areas, Unit T is interpreted as till. These deposits trend north每south in Belfast and Blue Hill Bay, and northwest每southeast in Passamaquoddy Bay. Large pebbles and cobbles (6-cm diameter) typify this unit in core samples.
4.1.3 Unit GM
With a moderate-intensity, continuous surface return, this unit is either acoustically massive or highly stratified. Filling bedrock basins,Unit GM occurs throughout all three estuaries. Where imaged, deeper deposits drape over bedrock, while shallower deposits exhibit ponded geometry (Fig. 6). GM deposits range from 0 m to >25 m in thickness. The surface of GM is often an unconformity and overlain by Holocene gravel, sand, or mud. Lithologically the unit consists of bluish-gray (5B 4/1, 5BG 4/1) stiff sediments with low organic contents (b5% LOI). Based on these acoustic and lithologic characteristics, we interpret Unit GM as Pleistocene-age glacial-marine sediments, which are well documented in the region (e.g., Belknap and Shipp, 1991; Fader, 1991).
In seven core samples the upper 0.1每0.7 m of Unit GM transitioned from blue-grey (5BG 4/1) to mottled red (10 YR 3/6) sandy mud, and contained indurated mud clasts and soft mud masses (Fig. 8). Mud clasts ranged from 1 to 7 cm in diameter and were rounded, angular pebbles or highly irregular, contorted, pipe-like fragments (Fig. 9). The hardest clasts were generally greenish brown (5G4/2, 5G 5/2, 5GY 6/1) and contained holes ranging from b0.1 cm to 1 cm in diameter. Other clasts had a soft blue-gray surface coating (5BG 5/1), but internally had yellow-red (10YR 3/6) bandings and iron nodules. Soft masses shared the same color patterns as clasts, but were amorphous and easily manipulated by hand. All features tested negative for calcium carbonate when treated with 10% hydrochloric acid, and x-ray diffraction analyses revealed no carbonates. These clasts and masses had LOI values of < 5%.
4.1.4 Basal unconformity (U) 
A high amplitude acoustic reflection on seismic profiles separates Pleistocene and Holocene deposits. This surface commonly truncates the internal reflections in the glacial-marine unit indicating that it is an erosional surface (Fig. 6). In core samples this surface is coarse-grained (Fig. 8).
4.1.5 Unit B
Unit B overlies the basal unconformity and has a maximum thickness of 6 m. Of the three estuaries examined, it was only imaged in Belfast Bay. The unit occurs in patches in the western and northern portion of Belfast Bay, with its most continuous expression as a wedge that thins from 6-m thickness closest to the present shoreline and diminishes in the field's interior (Figs. 5 and 6). The unit also occurs as isolated pools between bedrock, or till, subcrops. The unit lacks internal bedding, and was not sampled by vibracores.
4.1.6 Unit M 
The uppermost acoustic unit is transparent. Unit M occurs in all the examined fields, and locally may exceed 40 m in thickness (Fig. 4). Pockmarks occur solely within this unit. The bottoms of some pockmarks extend to the base of this unit and terminate at the basal unconformity. Unit M is interpreted as estuarine mud of Holocene age. In Belfast Bay the thickest deposits occur in the center of the embayment while Holocene sediment thins in shallow portions of the bay, and in the southern thalweg. In Passamaquoddy Bay, where seismic data were available, the thickest sediments occur along the strike of bedrock. In both fields, zones of minimal Holocene sediment thickness correspond to moraines, drumlins, and bedrock outcrops (Fig. 4). 
Of the 37 cores collected, 28 were entirely within Unit M, while the remaining 9 also sampled an older, deeper facies. Unit M generally has Munsell colors of reddish brown (2.5 Y 4/2 and 10YR 5/2) in the first 0.1 to 0.2 m, transitioning to dull olive gray (5Y 5/1 and 5Y 4/1) downcore. Texturally the unit consists of structureless mud to sandy mud, with minimal bedding. Thirteen cores contained clasts (Fig. 9) within the reddish brown mud matrix indicating proximity to Unit M's base. We found virtually no macrofossils and did not find any articulated fossils in life positions and therefore did not date any samples. Pore water content generally ranged from 60% to 25% from the surface to the bottom of the cores. LOI values ranged from 3.5% to 10%.
4.1.7 Natural gas (NG) 
Acoustically opaque wipe-out curtains occur throughout Holocene sediments in all three estuaries. This unit obscures deeper reflections and is interpreted to be due to gas in the sediments. Although gascharged sediments challenge resolution of deeper units, in Belfast Bay and Passamaquoddy Bay such acoustically opaque curtains are interspersedwith unblanked reflections (Fig. 7). Thesewindows of transmission, on the order of tens of meters, typically coincide with pockmarks, and allowed us tomap sediment thickness in these two estuaries.Within Belfast and Passamaquoddy Bays gas typically coincides with Holocene sediments >16 m in thickness (Fig. 5). Other indications of gas in sediments, such as acoustically disrupted columns, or ※chimneys§ (sensu Cathles et al., 2010), were not observed in any of the estuaries. 
While collecting core PBVC-08-11 we witnessed bubbling at the sea surface. This core sampled the center of a pockmark underlain by gas (Fig. 5 for location). The deepest segment of the core shows extensive cracking. We interpret the cracks as evidence of gas expansion following Barnhardt et al. (1997, their Figure 10), who describe similar habit in cores from Belfast Bay.
4.2 Pockmarks spatial distribution and subsurface characteristics 
Pockmarks in this study occur between ?10 and ?108 m water depths. Their frequency and size varied among embayments (Table 3). Because pockmarks in Belfast Bay have a consistent depth-to-diameter ratio (Andrews et al., 2010) we use the depth of a pockmark's base below the background seafloor as an indication of its size. In Passamaquoddy Bay pockmarks are the most numerous (4515), smallest (mean depth below seafloor=3.6 m), and vary the least in size (standard deviation ㊣2.6 m). Blue Hill Bay hosted the largest pockmarks (mean depth below seafloor=12 m) with the greatest standard deviation (㊣8 m). Belfast Bay has the greatest concentration of pockmarks per area (46.5 pm/km2), and has size characteristics in between the other two bays. In all three embayments pockmarks are circular close to shore, but become elongated farther offshore, or in thalwegs (Fig. 2b). In one example, the seafloor in southern Belfast Bay terminates in flute-like structures, 50每150 m wide, 2每4 km long and 15每60 m deep (Fig. 1). Nearest neighbor analysis, the ratio of where features are expected to occur relative to where they actually occur, indicates that pockmarks are non-randomly distributed in all three fields (Table 4). This means that the pockmark's spatial distribution departs significantly from complete spatial randomness, implying that pockmark occurrence depends upon some factor(s).
Where sediment thickness could be mapped, pockmarks were identified only in areas with Holocene sediment thickness >2 m. Holocene sediment thickness strongly relates to pockmark size (Fig. 10). Larger pockmarks occur in thicker deposits while smaller pockmarks are in thinner deposits. Point-density analysis reveals that in Belfast and Passamaquoddy Bays the densest clustering of pockmarks occurs in Holocene sediments b15 m thick. Pockmark chains in Belfast and Passamaquoddy Bays consisting of large pockmarks (>100 mdiameter) coincidewith abrupt changes in depth of the unconformity (Figs. 11 and 12). Chains of small pockmarks outline glacial deposits (Fig. 13).
5.DISCUSSION 
5.1 Depositional and Erosional History 
The study area's nearshore stratigraphy reflects the regional glacial and local sea-level history (Barnhardt et al., 1997; Belknap et al., 2002; Kelley et al., 2010). Late Wisconsinan glacial retreat from the Maine coast, between 15 and 13 ka (Dorion et al., 2001), was accompanied by marine incursion. During retreat, glaciers left heterogeneous deposits of till and glacial outwash (Unit T) across the seafloor (Belknap and Shipp, 1991; Kelley and Belknap, 1991). Glacial-marine muddy sediments (Unit GM) then blanketed these coarse-grained deposits. By 12.5 cal. ka further ice retreat and associated isostatic uplift caused relative sea-level fall to the lowstand of approximately 60 m below present-day level (Barnhardt et al., 1995). After the lowstand, sea level rose at varying rates (Kelley et al., 2010) leaving an eroded surface (U) along most of the region (Shipp et al., 1991). Since submergence, estuarine mud has accumulated over the transgressive surface (Unit M).
Within this study we identified acoustic Unit B only in Belfast Bay. Further, the unit has not been identified elsewhere in regional seismic surveys. This could reflect the unit's origins, or, less likely, could relate to the different seismic sources used (Table 2). Unit B's placement above the unconformity indicates that the unit was deposited during the Holocene. The unit's occurrence in the periphery of Belfast Bay suggests that Unit B formed around the estuary during lower sea level, perhaps as a marsh or eroded bluff toe deposit (e.g., Belknap et al., 2002). Due to the unit's depth within the sediment column we were unable to sample it using vibracores. Unit B's patchy distribution along the edge of Belfast Bay and absence in Blue Hill and Passamaquoddy Bays suggest that it is not related to pockmarks.
5.2 Pockmark Field Paleogeography since the Late Pleistocene and Shallow gas origins 
Though mottled glacial-marine sediments were logged previously in the region (e.g., Ostericher, 1965; Shipp, 1989; Barnhardt, 1994; Gontz, 2005), their significancewas uncertain because of the paucity of samples. We consistently recoveredmottled features at the basal unconformity in Belfast Bay's periphery (Fig. 8). Based on color, composition and texture, we identify these induratedmudclasts and softmasses as redoximorphic features. Diagnostic of hydric (wetland) soils, the development of redoximorphic features requires saturation, respiring bacteria, and an adequate supply of decomposable organic carbon (Vepraskas, 2001). Redoximorphic features form by reduction, translocation, and oxidation of Fe and Mn oxides (Vepraskas, 1994; New England Hydric Soils Technical Committee (NEHSTC), 1998). Within a soil profile, redoximorphic features occur directly under the uppermost soil horizons, O or A, richest in organic material (NEHSTC, 1998) (Fig. 14). Our cores did not sample any organic-rich (>10% LOI) horizons.
The distribution of redoximorphic features confirms that wetlands occurred throughout Belfast Bay at a period of lower relative sea level. The features' coincidence with the Holocene/Pleistocene unconformity and the absence of the upper organic horizons in core samples strongly suggest that the upper organic horizons of the paleosol eroded during transgression (Fig. 14). The consequent unconformity was then buried with Holocene-age nearshore detrital sediments (Unit M).
Rogers et al. (2006) suggested that a buried, terrestrial, organic-rich layer, such as a peat (LOI ~60%), evolved into methane, and, by inference, formed regional pockmarks.We found no evidence for such a layer. Further, a concentrated organic deposit is not necessary to generatemethane in this setting. According to Rice (1992), total organic content (TOC) in excess of 0.5% is sufficient to induce methanogenesis in sediments. LOI values, an estimate of TOC, measured in Unit M are at least an order of magnitude greater than that threshold. Because there is no evidence for a buried, organic-rich Pleistocene-age deposit, and the organic content of UnitMis sufficient to fostermethane generation, we conclude that Holocene estuarine sediments are the stratigraphic source of shallow gas in Belfast Bay, and likely the other study sites.
5.3 Pockmarks' relation to stratigraphy 
Pockmarks occur solely in the Holocene sediment package, with the bases of pockmarks not extending below the Holocene/Pleistocene unconformity. We interpret this confinement as evidence that pockmarks formed during the Holocene from the expulsion of gas generated in estuarine sediments. Our spatial analysis indicates that variation in Holocene sediment thickness, reflecting the region's irregular underlying topography, influences pockmark size and distribution (Figs. 10 and 15). Deep pockmarks coincide with areas of thick Holocene sediment, while shallow pockmarks occur in areas of minimal Holocene sediment accumulation. Based on this relationship we hypothesize that size differences between pockmarks in the three embayments (Table 3) reflect Holocene sediment thickness (i.e., Blue Hill Bay has the thickest sediments and thus the largest pockmarks, Passamaquoddy Bay has the thinnest sediments and thus the smallest pockmarks). The absence of shallow pockmarks in thicker sediments may reflect the propensity for pockmark collapse, amalgamation, or erosion in thick deposits; or alternatively, such size sorting may indicate different consequences of fluid flow in thin and thick sediment packages, such as the development of more integrated pathways of gas flow in thicker units.
Chains of pockmarks also relate to the Holocene sediment package and are quasi-normal to the steepest gradient in Holocene sediment thickness. Variation in Holocene sediment thickness likely creates a pressure gradient, enabling gas migration from zones where the overburden pressure is greatest (thickest sediment) to zones of minimal overburden pressure (thin sediments). In Belfast Bay and Passamaquoddy Bay, pockmark chains occur at the abrupt thinning of Holocene sediment related to antecedent geology, glacial deposits and paleochannels. Hovland et al. (2002) noted that pockmark chains often nucleate along zones of weakness where fluid flow focuses. The tens of meters of relief over a relatively short horizontal difference (e.g., 200 m) that typifies the regions' underlying topography may set up conditions for focused fluid expulsion capable of forming pockmarks. Pinet et al. (2010), and Lavoie et al. (2010) have also noted a relationship between pockmarks and the underlying bedrock and topography of the St. Lawrence estuary.
We note that not all muddy, gassy estuaries underlain by irregular topography have pockmarks. For example, paleochannels similar in relief to the underlying topography of north temperate estuaries underlie Chesapeake Bay, yet the bay lacks pockmarks. Differences in seafloor morphology may result from the permeability of sediments that infill topographic lows. In Chesapeake Bay the lower channel-fill unit of each paleochannel is a fluvial deposit, typically made of coarse sand and fine gravel (Coleman et al., 1990). This is in stark contrast to the homogenous, cohesive fine-grained estuarine sediment that fills the nearshore basins in the north temperate pockmark fields. Coarser-grained channel fill could promote fluidmigration and diffusion along the channel, rather than vertical migration of fluids that occurs in fine-grained sediment (e.g., Jain and Juanes, 2009; Scandella et al., 2011), resulting in gas spatially coincident with paleochannels and minimal seafloor disturbance (Hill et al., 1992). 
Within the three temperate embayments, pockmarks do not persist seaward of the 70 meter isobath. In seaward portions of all three embayments pockmark morphology alters from circular to comet-like features terminating in elongate scours related to increased nearbed uni-directional flow (Fig. 2b) (e.g., Brothers et al., 2011b). In Belfast and Blue Hill Bays, nearbed currents within thalwegs are likely 1.2每1.7 times faster than the velocities in the upper bay, using continuity of water volume where Uc is channel velocity, Ub is basin velocity, Dc is channel depth, Db is basin depth, Wc is channel width, and Wb is basin width.
In semi-enclosed bodies of seawater, it is possible that the protection of bedrock, or till headlands, is required to preserve pockmark morphology. Enhanced offshore nearbed currents may limit the seaward extent of the estuarine pockmark field. In addition, the pockmarks may not continue offshore due to the diminution in Holocene sediment thickness, but more extensive seismic coverage is necessary to confirm this. The factors that appear to control pockmark distribution in the examined temperate estuaries may also explain pockmark distribution in other structurally-controlled coastal settings such as fjords, or Rias (Iglesias and Garcia-Gil, 2007;Webb et al., 2009).
6.CONCLUSION 
We conclude that a terrestrial deposit high in organic content is not the source for gas responsible for pockmarks. Instead, our data analysis suggests that sediments deposited in the last 11,000 years fostered the gas related to the pockmarks. Holocene sediment thickness, dictated by available accommodation space, exhibits control on pockmark size and distribution. Our findings indicate that variation in Holocene sediment thickness made possible by irregular underlying topography may ultimately play a more important role in temperate estuarine pockmark distribution than drowned terrestrial deposits. Supplementary data to this article can be found online at http:// dx.doi.org/10.1016/j.margeo.2012.09.006.
ACKNOWLEDGEMENTS
Graduate support for Brothers came from a Maine Economic Improvement Fund Dissertation Fellowship. We thank Wayne Baldwin, Emile Bergeron, Steve Brucker, Doug Cartwright, Bill Danforth, Dave Foster, Travis Hamilton, Barry Irwin, Jim Leslie and James Muggah, for technical expertise. Randy Flood and the crew of the RV Argo Maine, Steve Dickson, Bob Johnson, Ashley Simpson, and Britt Gillman were central in vibracore collection and laboratory analysis. David Twichell, Jason Chaytor, Brian Todd and an anonymous reviewer provided constructive reviews of earlier versions of this manuscript. Any use of trade names is only for descriptive purposes and does not imply endorsement by the U.S. Government.
Abstract

Digital Twins are core enablers of smart and autonomous manufacturing systems. Although they strive to represent their physical counterpart as accurately as possible, slight model or data errors will remain. We present an algorithm to compensate for those residual errors through Reinforcement Learning (RL) and data fed back from the manufacturing system. When learning, the Digital Twin acts as teacher and safety policy to ensure minimal performance. We test the algorithm in a sheet metal assembly context, in which locators of the .xture are optimally adjusted for individual assemblies. Our results show a fast adaption and improved performance of the autonomous system. 

INTRODUCTION 

The emergence of Internet of Things, Big Data and cloud computing has given rise to the concept of the Digital Twin in manufacturing. The Digital Twin is seen as a core enabler for smart and autonomous manufacturing systems [1]. In essence, it is an ultra-realistic digital model of the product or system with bidirectional automated data exchange used for simulation, optimization and control. Irregardless of its accuracy, slight model or data errors will remain, due to limitations in modelling and data capturing. These resid-ual errors create an improvement potential. As repeatedly pointed out in the literature on Digital Twins, machine learning and Arti.cial Intelligence (AI) could realize those improvements through learning expressive nonlinear models. However, to the best of our knowledge only few publications address how that could be achieved. Jaensch et al. [2] present a generic concept for incorporating learning methods into Digital Twins. Wang et al. [3] and Sapronov et al. [4] tune parameters of the Digital Twin through machine learning. We are interested in generic algorithms that leverage and adapt Digital Twins in the context of control, though. 
In this context, we study requirements on an AI solution to compensate for remaining model or data errors. We propose an algorithm based on Reinforcement Learning to adapt the Digital Twin¡¯s control policy derived from erroneous models. To that end, we require minimal performance of the learning algorithm to address safety and quality concerns. We show its application to a case of industrial size. Our results indicate a fast realization of the improvement potentials, and overall an increased performance (see Figure 1). The main contributions of this paper are: the discussion and demonstration of using


Fig. 1. The performance of the proposed EDiT Reinforcement Learning algorithm applied to an industrial problem. Two sheet metal parts of varying geometry need to be joined through spot welding. The 12 locators of the .xture can be adjusted for each individual assembly to improve the resulting part geometry. The performance of the Digital Twin¡¯s default control policy is based on optimizing a Finite Element model and used as a baseline (black) in this plot. Our EDiT agent can not perform worse than 0.5% of the default control policy at any time, depicted as the exploration constraint (red). After an initial exploration phase, our EDiT agent improves upon the Digital Twin¡¯s default policy, leading to an enhanced overall performance. 
deep learning to enhance Digital Twins; and a novel deep Reinforcement Learning algorithm for enhancing Digital Twins. 
The paper is structured as follows: Section II elaborates on the concept of Digital Twins and the identi.ed research gap; furthermore, Reinforcement Learning and its reduced formulation of Contextual Bandits are discussed; Section III presents necessary considerations on, and our solution to, enhancing Digital Twins through RL; Section IV gives results of our experimental test case; and Section V concludes the paper. 
II. PRELIMINARIES 

A. Digital Twins 
The term Digital Twin originates from a NASA technology report, in which a Digital Twin is de.ned as: ¡°an integrated multiphysics, mulitscale simulation of a vehicle or system that uses the best available physical models, sensor updates, .eet history, etc. to mirror the life of its corresponding .ying twin [5].¡± As such, the Digital Twin is both a collection of algorithms and a data structure of high .delity. In the manufacturing context, researchers are yet to agree on a common de.nition of the concept [6]. This may be due to most publications being either conceptual work or case studies [6]. Another reason may be the broad application area of Digital Twins to all phases of the product life cycle: design, engineering, manufacturing, operation and service [7], [8]. In manufacturing, the application to production planning and control dominates the available literature [6]. We, too, focus on production planning and control in this paper. 
A number of challenges present themselves in realizing a Digital Twin. One is understanding and modelling all relevant physical phenomena, as well as exploring their potentially massive state space to uncover any undesired behaviors [9]. Another is the automatic and continuous update of models and model parameters needed over the life cycle of the system [1], [10]. A third is to identify and utilize incon-sistencies between model and real system [10]. Lastly, how deep learning methods can be employed and continuously improved in the context of Digital Twins is a key challenge that needs to be addressed [8]. 
For the remainder of this paper, we assume the Digital Twin to be a high-.delity, but ¨C due to above¡¯s challenges 
¨C ultimately inaccurate digital model, used for simulation, control and optimization, with a bidirectional, automated data exchange to the physical counterpart. We hypothesize what types of data are exchanged. If the Digital Twin contains algorithms for optimization, decision making and control, data that in.uence the state of the physical system represent the sole sensible type of data sent from the Digital Twin to the physical system. We refer to this type of data as control actions. In the opposite direction, sent data are either state updates or feedback signals. Since the Digital Twin tracks all information about the physical system, changes in the state of the system must be transmitted to the twin to keep them synchronized. Feedback signals that re.ect the goodness of control actions may be viewed as a sub-type of state updates. 
Due to the characteristics of the data link, any control method would suit the concept of Digital Twins. However, a method, capable of leveraging the comprehensive informa-tion available in the Digital Twin and capable of handling potentially nonlinear behavior of the system, is preferred. Reinforcement Learning lends itself as such method and also as a deep learning method that could address the key challenge of enhancing Digital Twins through learning. 
B. Reinforcement Learning 
Reinforcement Learning is inspired by the way humans learn. The learning agent observes the state xt ¡ÊX of the environment, decides on a control action ut ¡ÊU that alters the state of the environment, possibly receives a reward r according to some reward function R(x,u), and observes the new state xt+1 ¡ÊX of the environment. Over time, it will learn to distinguish good from bad actions. 
More formally, the underlying model is a Markov Decision Process, or MDP in short. An MDP is de.ned through the tuple X,U,R,T,¦Ã, where X is the space of states, U is the space of actions, Ris the function of rewards rt = R(xt,ut), T is the transition probability function T(xt,ut,xt+1)= p(xt+1|xt,ut), and ¦Ã ¡Ê[0,1) is the discount factor. An important property of the state is the Markov Property, meaning the state must contain all information necessary to predict the associated reward and next state. 

C. Contextual Bandits 
For many learning tasks in manufacturing, the full Rein-forcement Learning setting does not apply. When considering tasks per part basis, the problem formulation may be free of dynamics. In other words, if the system state depends on the particular part that is being processed, but, irregardless of the selected action for that part, the next state is already decided -for instance by the production schedule -, the transition probability function T(xt,ut,xt+1) is obsolete. And with that, the discount factor ¦Ã is also no longer required. This setting is referred to as a Contextual Bandit in literature. 
A Contextual Bandit is de.ned by the tuple X,U,R, where X is the space of states, U is the space of actions, R is the function of rewards rt = R(xt,ut). At each round t, the environment prepares a state xt (also called context), the learner selects and action ut and receives a reward rt. The next state xt+1 prepared by the environment is unrelated to xt and ut. The agent aims to learn learn a policy ¦Ð : X

that satis.es the optimal value function: 


In continuous control problems the policy ¦Ð maps states to probability distributions of actions: ¦Ð : X,U
¡úp(U). The value function of a particular action ut in state xt is de.ned by 
Q(xt,ut)= E [R(xt,ut)] . (2) 

D. Function Approximation 
We consider problems of industrial size with high-dimensional continuous state and action spaces. In such cases, the number of states and actions is intractable and too large for a discrete, tabular representation of the value functions V and Q, and the policy ¦Ð. To handle that problem, function approximation has been proven successful [11]. More speci.cally, we consider arti.cial neural networks for our context, due to their expressiveness. 
Neural networks a class of nonlinear function ap-proximators, consisting of M linear combinations of input variables ¦Õ1,...,¦ÕD (e.g. parameters of the state xt) and weights ¦È1,1,...,¦ÈM,D. The linear combinations are com-plemented by a nonlinear, differentiable activation function h(¡¤) 


where j =1,...,M, and ¦Èj,0 is the bias [12]. The outputs y1,...,yM of the network layer can be in turn the inputs to a subsequent layer. The neural network learns via minimizing a loss function L(¦È). The gradients of the loss function are back propagated through the network to tune its weights. 
Digital Twin 

Fig. 2. The architecture of our EDiT algorithm. The Digital Twin observes a state xt and decides on a control action dt based on its default policy ¦Ðd. Our RL algorithm EDiT observes, both, xt and dt. It decides then whether to apply dt or ut = ¦Ða(xt) to the physical system G. The system then generates a feedback signal (reward) rt and a next state xt+1 that is observed by the Digital Twin. rt is used to improve the EDiT policy ¦Ða. 
III. ALGORITHM 
Based on the reduced Reinforcement Learning problem formulation of Contextual Bandits, we propose the deep learning algorithm EDiT for Enhancing Digital Twins. While introducing the algorithm, we elaborate on key re-quirements on an AI solution aiming at compensating for model inaccuracies of the Digital Twin in control and op-timization. For ease of understanding, Figure 2 depicts the schema of the algorithm¡¯s architecture. 
A. Policy Function 
Deep Reinforcement Learning allows for continuous con-trol policies1, needed for our application case. Assuming a normally distributed policy distribution, we propose a neural network to approximate mean ¦Ì and standard deviation ¦Ò of the policy function ¦Ða, such that 

where Nis a multivariate Normal distribution in U. This can be interpreted as the network outputting deterministic values of the action (¦Ì) and adding some normally distributed exploration noise ¦Ò to it. 
For better training stability and sample ef.ciency, we use a trust region policy optimization method [13], [14]. In particular, we apply Proximal Policy Optimization [15] and update ¦Ða by minimizing the loss 
where A.t = Q.(xt,ut) .V.(xt) is the advantage function estimate, E is a small trust region parameter, and ft(¦È) is the fraction 

ft is used as a replacement of the logprobability of the policy that is used in policy gradient methods. Clipping 
1Although we present only the continuous case here, the EDiT algorithm can easily be adapted to the discrete case, in which the policy would be a discrete probability distribution. 

ft at a lower or higher bound has the effect of bounding the loss. In the minimization step, that bound limits the gradients, resulting in smaller policy updates such that the policy remains proximally in the trust region. Intuitively, this method mitigates the risk of destructive policy updates that move ft too far away from 1. For further details on this method, we refer the reader to [15]. 
B. Value Functions 
To estimate the advantage of an action under the current policy that is required for updating the policy, we need to approximate its value function V.(xt). As loss function for the value function network, we choose the mean square error 


where N is the batch size of training data, and c a small regularization constant for the L2-norm of the weights ¦È. Similarly, the loss of the advantage function A.¦È(xt,ut) is 


(8) Since the network V¦ÈV is, both, being updated as well as being used to compute the target values for A¦ÈA ,we 

maintain a copy V¦ÈVÂ¬ of the network V¦ÈV used for the target 
computations. The weights ¦ÈVÂ¬ of the copy are updated using 


a soft target update ¦ÈV¡û¦Ó¦ÈV +(1 .¦Ó)¦ÈV, where ¦Ó is a small value. This has been shown to stabilize the convergence during training [16], [17] -an important property when learning online. 
C. Safe Exploration 
In a manufacturing context, performance of the control policy at any time is paramount. When learning, the agent must explore the action space to improve its potentially sub-optimal policy. This process may lead to catastrophic errors. While this is undesirable in any real-world context, in manufacturing it causes an additional economic cost. We are thus interested in safe exploration strategies that maintain on average with high probability a given level of performance during learning. 
Garcia and Fernandez [18] identi.ed Teacher Advice as common approach to incorporate external knowledge in the exploration process to make the same safer. With the availability of the Digital Twin, have to 
default policy ¦Ðd that can be regarded as such teacher advice. The default policy ¦Ðd is the original control policy of the Digital Twin, before we apply deep learning to compensate for model inaccuracies. This default policy may be sub-optimal, but arguably superior to the agent¡¯s policy ¦Ða in the initial learning period. A problem formulation similar to Wu et al. [19] then suits our manufacturing case. Accordingly, 
de.ne a cumulative performance constraint for safe exploration: For all rounds t, the sum of rewards ru of the agent¡¯s policy ¦Ða can at most be worse by a fraction ¦Áof the sum of rewards rd of the default policy ¦Ðd, formally written as

..
D. Bayesian Neural Networks 
By themselves, neural networks do not compute con.-dence bounds on their predictions. However, Gal and Ghahra-mani [20] have shown that dropout can be used as a Bayesian approximation. Dropout refers to the technique of randomly disabling units of a neural network layer. Originally used as a method to mitigate over-.tting, it can be used to approximate a Gaussian Process. Con.dence bounds can then be derived from the statistics of a Monte Carlo sample. The intuition behind using dropout as a Bayesian approximation is that the variance ¦Ò.2 of the sample will be high in regions of low data density and lower in regions with an abundance of available data. 
We propose to apply this technique to the advantage network A to compute the required values of Q.UCB and Q.LCB. For that purpose, we sample S times from A(xt,ut) 
and compute 
Q.LCB/UCB(xt,ut)= V.(xt) .3.¦ÒS A.(xt,ut), (15) 

where ¦Ò.S is the standard deviation of the drawn samples S. 
E. Sample Ef.ciency 
In the context of manufacturing, each data sample may represent a particular physical part. Given that the num-ber of samples is limited, we wish to be ef.cient with the data and learn quickly. We store all collected data samples xt,ut,rt,¦Ða(ut|xt)in a data structure D and repeatedly retrain a random subset of D to improve sampleef.ciency.Intonhat, we take inspiration from [21] and keep demonstrations of the Digital Twin¡¯s default policy ¦Ðd separate. When sampling a minibatch from D, we ensure that a small percentage of the samples originates from those demonstrations. Through that mechanism, the default policy is constantly kept present in the learner, helping with training stability. 
F. EDiT Algorithm 
Drawing on the previous subsections, the full EDiT al-gorithm presents itself as listed in Algorithm 1. In eachroundt,EDiTobservenows the state xt, and the action dt proposed by the default policy. We then sample K-times from the policy distribution ¦Ð to build up a set of possible actions ut,1,...,ut,K for which we compute the mean action u¡¥t. This results in a sample bias for u¡¥t that allows for exploration. u¡¥t will be applied to the system as long as the lower bound on the expected reward Q.LCB(xt,u¡¥t) is within the exploration budget. Otherwise, dt will be applied. We store the resulting data and update our neural networks on a minibatch sample of the replay buffer D. 

EXPERIMENTAL RESULTS 


computes adjustments of the .xture locators based on a Finite Element Analysis; and the adjustments are applied to the physical system to improve the geometrical quality of the .nal assembly. A pre-study to using Reinforcement Learning for adapting the Digital Twin policy to the real system in this context can be found in [23]. 
Our particular test case consists of two sheet metal parts of a car body shown in Figure 3. The geometry of the parts is given by their point clouds of ¡«2.5k points each and represent our state xt. We evaluate our algorithm on 250 part instances. Each of the .xture¡¯s 12 locators are adjustable along their axis in the range [.2,2] and represent our action space U. As reward function R, we de.ne the relative perfor-mance of the agent compared to the default policy in terms of the root mean square error of the resulting assembly from the nominal assembly: rt ¡«RMSE(xt,ut)/RMSE(xt,dt). The default policy of the Digital Twin is derived from the outcome of a Genetic Algorithm on the nominal model [24]. To emulate the real system, we use a slightly perturbed version of the nominal model. 
A. Implementation Details 
We .rst transform the point cloud of the parts through a Principal Component Analysis, and choose the 128 points as new state representation that explain most of the point cloud¡¯s variance. The size of the neural networks and related parameters are given in Table 1. To enable distributed pro-cessing for Digital Twin, EDiT and system emulation, we use Apache Kafka as asynchronous publish/subscribe messaging bus. 


Fig. 3. The test case of our algorithm. Two sheet metal parts (green and yellow) are clamped into position for a subsequent spot welding operation. The 12 locators of the .xture, shown in red here, can be adjusted along their axis to improve geometry of the welded assembly. Our EDiT agent must .nd adjustments for all 12 locators simultaneously that improve upon the optimal adjustments calculated by the Digital Twin¡¯s default policy ¦Ðd for the nominal model. 

B. Results 
We test the proposed EDiT algorithm over 10k rounds and track the cumulative improvement over the twin¡¯s default policy ¦Ðd in terms of accumulated rewards. Figure 1 depicts the outcome of 10 repetitions of the experiment. Overall, we see an improvement over the default policy. In the best case, this improvement is realized just after a few rounds. In the worst case, EDiT requires up to 2k rounds of exploration until improving upon the default policy. In average, though, we see an improvement after a few hundred rounds. 
The mean reward of EDiT over all 10k rounds and 10 repetitions is about 1.0025 as listed in Table II. This corresponds to an average improvement of 0.25%.Wedo not expect EDiT¡¯s learned policy to be the optimal one, yet. The state-action space is rather large in our case, due to the 
12-dimensional actions. We expect EDiT to take many more rounds in this case, before the optimal policy is fully learned. 
We further notice a number of violations of the safety constraints (see Table II). Most notably, the per part con-straint (Eq. 13) is violated 207 times in average over 10k rounds, meaning the received reward ru was below 95% of the Digital Twin¡¯s expected performance. This is due to two inaccuracies in Q: the estimation error of inter-/extrapolating, and the approximation error of the lower con.dence bound (LCB) through dropout (refer to Subsection III-D). An al-ternative implementation of Bayesian neural networks (e.g. ensembles) might reduce the number of per part violations. The cumulative performance constraint (Eq. 9), on the other hand, is in average only twice violated and often not at all. 
V. CONCLUSIONS 
This paper has shown that Digital Twins, used for con-trol of manufacturing processes, can be adapted through Reinforcement Learning. It has also been demonstrated that Contextual Bandits, a reduced formulation of Reinforcement Learning, are suitable for applications within the manufactur-ing context. Based on that, we have introduced the learning algorithm EDiT for enhancing the control policy of Digital Twins in continuous domains. It utilizes the Digital Twin as safety policy to maintain a constraint imposed on the learners performance. EDiT combines recent safe RL and deep learning methods, such as Bayesian neural networks and Proximal Policy Optimization. 
Future research directions include extensions of the algo-rithm to improve safety and sample-ef.ciency. The behaviour of deep neural network estimates can be unpredictable while learning. Although we employ a performance constraint and Bayesian neural networks to estimate uncertainty, we see further safety guarantees needed for the application of deep Reinforcement Learning in industry. 
Abstract 

Because of their cross-functional nature in the company, enhancing Production Planning and Control (PPC) functions can lead to a global improvement of manufacturing systems. With the advent of the Industry 4.0 (I4.0), copious availability of data, high-computing power and large storage capacity have made of Machine Learning (ML) approaches an appealing solution to tackle manufacturing challenges. As such, this paper presents a state-of-the-art of ML-aided PPC (ML-PPC) done through a systematic literature review analyzing 93 recent research application articles. This study has two main objec-tives: contribute to the definition of a methodology to implement ML-PPC and propose a mapping to classify the scientific literature to identify further research perspectives. To achieve the first objective, ML techniques, tools, activities, and data sources which are required to implement a ML-PPC are reviewed. The second objective is developed through the analysis of the use cases and the addressed characteristics of the I4.0. Results suggest that 75% of the possible research domains in ML-PPC are barely explored or not addressed at all. This lack of research originates from two possible causes: firstly, scientific literature rarely considers customer, environmental, and human-in-the-loop aspects when linking ML to PPC. Secondly, recent applications seldom couple PPC to logistics as well as to design of products and processes. Finally, two key pitfalls are identified in the implementation of ML-PPC models: the complexity of using Internet of Things technologies to collect data and the difficulty of updating the ML model to adapt it to the manufacturing system changes. 
Keywords Machine learning.¡¤ Industry 4.0.¡¤ Smart manufacturing.¡¤ Production planning and control.¡¤ State-of-the-art.¡¤

Introduction 

The current manufacturing environment is characterized by high complexity, dynamic production conditions and volatile markets. Additionally, companies must offer cus-tomized products while engaging low costs and reducing the time-to-market if they want to remain competitive in a globalized world (Schuh et.al. 2017b; Carvajal Soto et.al. 2019). This situation poses tremendous challenges for manufacturers who seek to implement new technologies to meet their objectives while expecting a return on invest-ment. Several countries have developed projects that aim to help companies adapt their industries to new production technologies. For instance, Germany created Industry 4.0 (I4.0), the United States proposed the Smart Manufacturing Leadership Coalition, and China introduced the plan called China Manufacturing 2025 (Wang et.al. 2018a). This has led to significant financial support for manufacturing research; 
iFAKT France SAS, Toulouse, France 
for example in the European Union around €7 billion will be invested by 2020 in Factories of the Future (Kusiak 2017). 
Among the Industry 4.0 groups of technologies (Ruess-mann et.al. 2015), Big Data and Analytics (BDA) allows the constantly growing mass of produced data to be harnessed to generate added value. In fact, data generation in modern manufacturing has undergone explosive growth, reaching around 1000 Exabytes per year (Tao et.al. 2018). However, the potential of this data has been found to be insufficiently exploited by companies (Manns et.al. 2015; Moeuf et.al. 2018). As BDA enables the exploitation of data, the scope of this review will focus on this technology, and more spe-cifically ML applied in Production Planning and Control. 
In the context of I4.0, Production Planning and Control (PPC) can be defined as the function determining the global quantities to be produced (production plan) to satisfy the commercial plan and to meet the profitability, productivity and delivery time objectives. It also encompasses the con-trol of production process, allowing real-time synchroniza-tion of resources as well as product customization (Tony Arnold et.al. 2012; Moeuf et.al. 2018). In this review, I4.0 is considered a synonym of Smart Manufacturing, as they both refer to technological advances that value data to draw improvements in production. For example, Ruessmann et.al. (2015) proposed nine technologies for I4.0 while Kusiak (2019) suggested six, but for Smart Manufacturing. Both proposals tend to refer to similar technologies and variations depend on the authors¡¯ focus. Hence, as the PPC is a core function of manufacturing, this paper regards its improve-ment through I4.0 technologies, namely ML, which concerns BDA. Regarding ML, the definition that will be retained is the one of a computer program capable of learning from experience to improve a performance measure at a given task (Mitchell 1997). 
Classical approaches to performing PPC include analyti-cal methods and precise simulations, providing solutions that may rapidly become unfeasible in the execution phase due to the stochastic nature of the production system and uncertain-ties such as machine breakdowns, scrap rate, delayed deliv-eries, etc. Moreover, Enterprise Resource Planning (ERP) systems perform poorly at the operative level (Gyulai et.al. 2015). To tackle this issue, ML can endow the PPC with the capacity of learning from historical or real-time data to react to predictable and unpredictable events. Even though this may suggest that organizations must invest in data ware-housing to handle the mass amount of collected data, stud-ies have reported that enterprises successfully implementing data-driven solutions have experienced a payback of 10¨C70 times their investment in data warehousing (Rainer 2013). 
Having introduced the synergism between ML and PPC, this study aims to provide an analysis of its state-of-the-art through a systematic literature review. This will contribute to the definition of a methodology to implement a ML-PPC and to the proposal of a map to classify scientific literature. This paper analyzes research produced in the context of the I4.0 and is guided by five research questions: 
1. 
Which are the activities employed to perform a ML-PPC? 

2. 
Which are the techniques and tools used to implement a ML-PPC? 

3. 
Which are the currently harnessed data sources to imple-ment a ML-PPC? 

4. 
Which are the addressed use cases by the recent scien-tific literature in ML-PPC? 

5. 
Which are the characteristics of the I4.0 targeted by the recent scientific literature in ML-PPC? 


The first three questions are related to the first objective of this research. They will contribute to the definition of a methodology to implement a ML-PPC. The last two ques-tions address the second objective, as they will provide the basis to create a classification map. 
The reminder of this paper is organized as follows: sec-tion ¡°Research methodology and contribution¡± will explain the systematic literature review methodology employed to search and choose the sample of scientific articles. Addition-ally, the contribution of this paper with respect to similar studies will be briefly highlighted and a short bibliometric analysis is presented to assess the keywords used as string chains. The ¡°Analytical framework¡± section will explain the four axes encompassed by the analytical framework. After-wards, the ¡°Results¡± section will focus on the results of the systematic literature review and an analysis of it. Finally, the ¡°Conclusion and further research perspectives¡± sec-tion will conclude this study and provide further research perspectives. 


Research methodology and.contribution 

To meet the two objectives of this study, a systematic litera-ture review was carried out following the method proposed by Tranfield et.al. (2003) who extended research methods from the medical sector to the management sciences. This method has been successfully employed by other authors to draw insights from the scientific literature (Garengo et.al. 2005; Moeuf et.al. 2018). This literature review focuses exclusively on applications of ML in PPC in the context of I4.0. 
In another domain, Zhong et.al. (2016), proposed a bibli-ometric analysis of big data applications on different sectors such as healthcare, supply chain, finance, etc. but its focus on manufacturing was limited. (Kusiak 2017; Tao et.al. 2018; Wang et.al. 2018a) provided a literature analysis of data-driven smart manufacturing, citing representative references. 
However, these references were not chosen through a sys-tematic literature review. Finally, (Sharp et.al. 2018) could be considered as a study close to this paper as the authors used a pre-defined methodology to select the articles to ana-lyze. Nevertheless, they employed Natural Language Pro-cessing (NLP) to analyze around 4000 unique articles and provide insights about the scientific literature of ML applied in I4.0. The use of NLP can be useful to identify important trends, but it does not allow the authors to analyze the detail of the reviewed papers, where it is likely to find interesting research gaps and insights. On the other hand, a systematic review allows the authors to both follow a rigorous method-ology and perform a detailed study of each chosen article. 
Even though the PPC is closely related to the domain of supply chain, the latter is not included in the scope of this review as its vastness would increase the risk of stray-ing from the focus on PPC. Therefore, to learn about recent trends on this topic, the authors invite readers to refer to Hosseini et.al. (2019), who performed a comprehensive review of quantitative methods, technologies, definitions, and key drivers of supply chain resilience. In fact, supply chain resilience is a growing research area that examines the ability of a supply chain to respond to disruptive events (Hosseini et.al. 2019). Applications of this topic have been done by Hosseini and Barker (2016), who applied Bayesian networks to perform supplier selection based on primary, green, and resilience criteria; and Hosseini and Ivanov (2019), who proposed a method using Bayesian networks to assess the resilience of suppliers in identifying critical links in a supply network. 
The queries were performed between 10/10/2018 and 24/03/2019 in two scientific databases: ScienceDirect and SCOPUS. The following keywords conducted the research: 
. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction scheduling¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction planning¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Pro-duction control¡±) 

. 
(¡°Deep Learning¡± OR ¡°Machine Learning¡±) AND (¡°Line balancing¡±) 


To consider the context of I4.0, only papers published since 2011 were considered because this year corresponds to the formal introduction of I4.0 at the Hannover Fair. Addition-ally, only communications labeled as ¡°Research Articles¡± in ScienceDirect and ¡°Conference paper¡± OR ¡°Article¡± in SCOPUS were included to solely capture articles present-ing application models. Subsequently, a review of titles and abstracts allowed for the exclusion of articles not related to ML-PPC. After the removal of duplicates, a full text analysis allowed a final selection that excluded papers that did not fit with research questions. The sample size obtained encom-passes 93 scientific papers. The article selection methodol-ogy with its Restrictions (R) is described in Fig..1. 

A brief focus on.the.query keywords 

The used string chains represent a core strategic choice for review. Therefore, this sub section aims to provide an analy-sis of the employed keywords. 
Concerning the keywords used in the first parenthesis of the string chains, the use of ¡°Deep Learning¡± and ¡°Machine Learning¡± was done for two reasons: firstly, they are rela-tively new terms, which eases the identification of recent trends in the literature; and secondly, they are directly related to one of the two core subjects in this study, which is ML. Other terms such as ¡°Data Mining¡± or ¡°Statistical Learn-ing¡± could have been sensible choices too, as they are often used interchangeably with ¡°Machine Learning¡± and ¡°Deep Learning¡±. Nevertheless, using these two terms might have deviated this study from its core topic. In fact, a recent study suggests that the differences between ML and Data Min-ing are not consistently defined in the literature. Thus, Data Mining is mostly considered to be the process of generat-ing useful knowledge from data (Schuh et.al. 2019). To do so, it draws from other fields such as Artificial Intelligence, Statistics, ML, and Data Analytics. Therefore, Data Mining can be a vast topic, and does not exclusively concern ML, which could potentially affect the focus of this study. As there seems to be no clear boundary between these terms, a short bibliometric analysis was performed to assess the chosen keywords. The analysis was done using VOSviewer, software developed by the University of Leiden to draw insights from scientific literature. Furthermore, using key-words related to specific ML techniques such as ¡°Random Forest¡± or ¡°k-means¡± did not seem appropriate due to the risk of introducing a bias when answering the second research question. In fact, this could have artificially boosted the results of the queried techniques. 
The bibliometric analysis followed a similar methodology to that used to choose the final article sample (cf. Fig..1). The objective was to briefly assess the influence of differ-ent keywords on the queries¡¯ results. For the analysis, three different string chains were considered: ¡°Deep Learning¡± OR ¡°Machine Learning¡±, ¡°Data Mining¡±, and ¡°Statistical Learning¡±. The queries were performed on 06/10/2019 and the detail of the search strategy can be found in ¡°Appendix I¡±. Finally, as the aim is to analyze the available literature when querying with a certain string chain, no Title and abstract review was done as this could introduce a bias into the results due to the authors¡¯ influence. 
The bibliometric analysis focused on the keywords defined by the authors for all of the papers of each of the three samples. To represent the results, the network visualization from VOSviewer was employed. In such a net-work, the nodes represent the keywords or items, their sizes represent the keyword importance determined by the number of occurrences, and the links between the nodes represent their co-occurrence. Furthermore, the relatedness between two terms is represented through their spatial distance in the network: two keywords closely related will be spatially closer. For this review, the obtained networks were displayed under the ¡°overlay visualization,¡± which shows the average publication year for each of the keywords through a color scale. For clarity reasons, a filter was applied on the mini-mum number of occurrences to display; at most, 50 items per graph. Also, the queried keywords were highlighted with a red frame to assist in their identification. The networks are presented on Figs..2, 3, 4. 


Results from the bibliometric study suggest that ¡°Statisti-cal Learning¡± may not be a common keyword to find in ML-PPC research because the size of the obtained article sample (241 articles) is far below the results obtained with the other two queries. In fact, ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± and ¡°Data Mining¡± provided 2862 and 2166 articles, respectively (cf. ¡°Appendix I¡±). This is also stated in all of the networks, in which the item ¡°Statistical Learning¡± does not appear, probably due to the filter excluding keywords with a low number of occurrences. 
Analyzing the relatedness between ¡°Data Mining¡± and ¡°Machine Learning¡± by their spatial distance on the net-works provides an idea of how these concepts are associ-ated: they are spatially closer on the ¡°Data Mining¡± Network (Fig..3) than on the ¡°Deep Learning¡± OR ¡°Machine Learn-ing¡± network (Fig..2). This suggests that Data Mining tends to relate more often to ML, rather than ML to Data Mining. Such a relation may support what is said in (Schuh et.al. 2019), in which Data Mining is considered a field drawing from ML, Artificial Intelligence, Statistics, etc. to produce useful insights. 
Findings from the network visualizations show that the item ¡°Machine Learning¡± is always associated with a more recent average publication year than ¡°Data Mining¡±. This supports the idea that ¡°Machine Learning¡± is a relatively new term, which can lead to the identification of recent trends in literature. Furthermore, querying with ¡°Deep 


Fig. 2 Network visualization with the average publication year for ¡°Deep Learning¡± OR ¡°Machine Learning¡± 

Fig. 3 Network visualization with the average publication year for ¡°Data Mining¡± Fig. 4 Network visualization with the average publication year for ¡°Statistical Learning¡± 

Learning¡± OR ¡°Machine Learning¡± provides a more recent average publication year (2017.06) for the item ¡°Machine Learning¡± than with the other two queries: 2016.95 when querying with ¡°Data Mining¡± and 2016.41 when querying with ¡°Statistical Learning¡±. Finally, ¡°Deep Learning¡± OR ¡°Machine Learning¡± was the only query enabling the inclu-sion of the item ¡°Deep Learning¡± with enough occurrences 
(25) to pass the filter, which is a recent research topic with an average publication year of 2018.28. 
From the bibliometric analysis, it could be concluded that using ¡°Deep Learning¡± OR ¡°Machine Learning¡± as part of the query keywords is appropriate enough, as this allows for the identification of a big sample of recent papers, enabling the identification of new trends. It seems that ¡°Statistical Learning¡± does not provide enough recent results to be con-sidered. Finally, even if ¡°Data Mining¡± is closely related to ¡°Machine Learning,¡± it covers a vast domain that can deviate from the focus of this review. 
Regarding the keywords employed in the second paren-thesis of the string chains, the objective was to represent the main functions of the PPC under the definition provided in the introduction section. Consequently, a determination of the global production quantities was represented by ¡°Produc-tion Planning¡± and the aspect of the main objectives (i.e. profitability, productivity, and delivery time) was depicted by ¡°Production Control¡±. Finally, the real time synchroni-zation of resources as well as product customizations were represented by both ¡°Production Scheduling¡± and ¡°Line Balancing,¡± given the fact that companies should be able to perform balanced scheduling even when facing customized client orders. 
As the PPC is a transverse topic tangled with other func-tions such as maintenance, quality control, logistics, etc., the challenge was to decide whether or not these related subjects should be included as explicit keywords for the queries. The final choice was to not include them through keywords, as this would broaden the perimeter of the research too much, losing a focus on PPC. Nevertheless, it was decided to include, in the final article sample, the studies dealing with other functions only if they were related to the PPC. 

METHOD

Analytical framework 
This section presents the four axes that build the analytical framework that will be employed to harness knowledge and insight from the final sample of 93 scientific articles. 
First axis of.the.analytical framework: the.elements of.a.method 
This axis concerns the first and second research questions: the activities, techniques, and tools to implement a ML-PPC model. To link these three elements, the concept of ¡°Man-datory Elements of a Method¡± (MEM) proposed by Zellner (2011) is used. In fact, this concept has been successfully employed by other authors to propose methodologies in research domains such as product development (Lemieux et.al. 2015) and lean in hospitals (Curatolo et.al. 2014). Moreover, (Talhi et.al. 2017) suggested its use to develop a methodology in the context of cloud manufacturing applied to product lifecycle management. Thus, the MEM suits the first objective of this study, which concerns the definition of a methodology to implement a ML-PPC. There are five elements in the MEM: 
1. 
A procedure: order of activities to be followed when the method is employed. 

2. 
Techniques: the means to generate the results. Activities from the procedure are supported by techniques, while the latter is supported by tools. 

3. 
Results: they correspond to the output of an activity. 

4. 
Role: the point of view adopted by the person who per-forms the activity and is responsible for it. 

5. 
Information model: this refers to the relation between the first four mandatory elements. 


In the scope of this study, the first two elements are the con-cern. Firstly, to evaluate the procedure, the activities used to perform a ML-PPC implementation will be recognized and their use will be measured. By activities, this research refers to tasks such as ¡°model comparison and selection¡± or ¡°data cleaning¡±. Secondly, to address the techniques, ML models and tools will be identified, and their use will be measured. ML models point to elements such as Support Vector Machines or Neural Networks, while tools relate to programming languages or software used to implement these ML models. 
To provide further insight concerning the ML techniques, the learning types will also be measured. This will be used to summarize the information regarding the techniques as well as to ease the identification of trends and research perspec-tives. Additionally, the learning types will serve as a bridge between the first and second objectives of this study, as they will be used in the mapping to classify scientific literature. Based on the work of Jordan and Mitchell (2015), three main learning types can be identified: 
1. Supervised Learning (SL), which concerns ML tech-niques approximating a function f (X)= Y by learning the relationship between the inputs X and the outputs 
Y. For instance, learning the mapping between the Red Green, and Blue (RGB) codes (input X) in an image and the objects in it (output Y) to determine if a certain picture contains a misplaced product in a stock rack. 
2. Unsupervised Learning (UL), which encompasses tech-niques allowing data exploration to find patterns and hidden structures in a given dataset X. For instance, finding categories in maintenance reports by using the description of the problem and the duration of the main-tenance intervention. 
3. Reinforcement Learning (RL), which are techniques allowing the learning of actions to be performed by an agent interacting with a certain environment to maxi-mize a reward. For example, teaching an Automated Guided Vehicle (AGV) in a warehouse how to avoid obstacles to maximize the number of delivered pack-ages. 

Second axis of.the.analytical framework: employed data sources 
This axis addresses the third research question: the har-nessed data sources. Identifying which are the data sources used to perform a ML-PPC is capital. In fact, data could be considered as the raw material allowing ML models to develop autonomous computer knowledge gain (Sharp et.al. 2018). Moreover, the quality of the final model will depend to a great extent on the quality and appropriateness of the used data. Therefore, the choice of the data source is an important decision when training a ML model. To address this axis of the analytical framework, the data source types proposed by Tao et.al. (2018) will be used. They mention that there are five main data sources used in the data-driven smart manufacturing: 
1. 
Management data (M): historical data coming from com-pany¡¯s information systems such as the ERP, Manufac-turing Execution System (MES), Customer Relationship Management system (CRM), etc. M data will concern production planning, maintenance, logistics, customer information, etc. 

2. 
Equipment data (E): data coming from Internet of Things (IoT) technologies implemented in the factory. It refers to sensors installed in physical resources such as machines, places such as workstations or human resources such as workers. In the case of workers, data is collected passively, such as by RFID sensors installed on helmets. 

3. 
User data (U): consumer information collected from e-commerce platforms, social media, etc. It also encom-passes feedback given by workers or experts that will be used to train the ML-PPC model. User data coming from workers is collected actively, for example through interviews or questionnaires. 

4. 
Product data (P): data originating from products or ser-vices either during the production process or from the final consumer. 

5. 
Public data (Pb): data available in public databases from universities, governments or from other researchers. 


The analysis of the 93 shortlisted articles suggested that some of them did not fit into the five data sources proposed by Tao et.al. (2018): these communications used artificially generated data through computer simulations. Therefore, a sixth data source is proposed, which corresponds to the first contribution of this paper to the scientific literature: 
6. Artificial data (A): data generated by computers (e.g. simulations) to assess ML-PPC implementations. 

Third axis of.the.analytical framework: the.use cases of.the.ML.PPC in.the.I4.0 
This axis concerns the fourth question: it aims to show which applications can be achieved when applying a ML-PPC. Moreover, identifying the use cases and quantifying their use frequency is important to detect trends as well as further research gaps. By use cases, this study refers to the different possible applications in a certain domain, such as maintenance, quality control, distribution, etc. In fact, as the PPC is entwined with several manufacturing subjects, is difficult to perform a complete review on PPC if these topics are ignored. For example, if there were a predictive maintenance study meant to enable a more robust produc-tion scheduling, such application would be directly related to the PPC through maintenance. To start this analysis, the use cases of I4.0 initially proposed by Tao et.al. (2018) were considered. They identified six of them: 
1. 
Smart Maintenance: harnessing data to perform preven-tive and predictive maintenance. For instance, monitor-ing machine components to estimate the best date to perform a maintenance intervention. 

2. 
Quality Control: applying BDA to supervise the manu-facturing process or products, seeking for possible qual-ity problems and/or allowing the identification of root causes. 

3. 
Process Control and Monitoring: constantly analyzing data coming from the shop floor to perform a smart adjustment of the functioning parameters of physical resources (machines, AGVs, etc.). The objective is to automatically control these physical resources and/or optimize their parameters with respect to the working conditions. 

4. 
Inventory and Distribution Control: stock management, parts and tools tracking, and distribution control with the use real-time and/or historical data. 

5. 
Smart Planning and Scheduling: considering produc-tion uncertainties to perform a production planning and scheduling closer to the current state of the production system. For instance, considering unexpected mainte-nance problems to reschedule a production order and minimize the delay. 


6. 
Smart Design of Products and Processes: using BDA to support new products and processes development. For instance, using NLP to analyze the technical require-ments of a new product and then to propose the poten-tially suitable manufacturing process. 

The analysis of the 93 scientific articles suggests that these six use cases are not enough to fully characterize the recent publications. Additionally, papers not fitting in the initially proposed use cases shared the same application: time esti-mation (cycle time, operation time, etc.). Consequently, a seventh use case is proposed: 

7. 
Time estimation: adaptation of different manufactur-ing related times to current working conditions. For instance, estimating the operation times to the actual work rate of each employee instead of using the data from the Method Time Measurement (MTM) approach. 



Fourth axis of.the.analytical framework: the.characteristics of.I4.0 
The I4.0 aims to transform the collected data during the product¡¯s lifecycle into ¡°intelligence¡± to enhance the manu-facturing process (Tao et.al. 2018). With this transforma-tion, the objective is to reduce costs while improving the quality, productivity, sustainability of the production system (Wang et.al. 2018a). However, what specific benefits could be expected when embracing the I4.0? To answer this ques-tion, the characteristics of I4.0 need to be identified. Tao et.al. (2018) argue that I4.0 enables the following paradigms: 
1. 
Customer-Centric Product Development: production systems in the I4.0 should be able to adjust their param-eters by considering variables coming from customers such as their behavior, their needs, the way they use the products, inter alia. It is the case of manufacturing per-sonalized products, designing processes from the cus-tomer requirements or proposing a target manufacturing cost for each consumer profile. 

2. 
Self-Organization of Resources: I4.0 should endow pro-duction systems with the capacity of considering data coming from the manufacturing process to better engage the available resources. Additionally, this data should also be used to plan capital and operational expendi-tures. For example, updating the scheduling of machines the shop floor after new urgent order is released. 

3. 
Self-Execution of Resources and Processes: in the I4.0, resources should become ¡°smart¡± by providing them a real-time awareness and interaction capacity with the manufacturing environment (Huang et.al. 2019). There-fore, the self-execution of resources concerns their fac-ulty of making decisions depending on the received 


information or measured data. It is the case of machines automatically adapting their functioning parameters to work optimally or trolleys automatically replenishing workstations when these reach a certain level of security stock. 
4. 
Self-Regulation of the Production Process: unexpected events should be effectively handled in the I4.0. Thus, this characteristic concerns the capability to perform the required adjustments to respond to unpredicted prob-lems. For example, relaunching the scheduling process for a certain production line when one of the machines experienced a breakdown. 

5. 
Self-Learning of the Production Process: this charac-teristic follows a similar logic as the self-regulation of processes in terms of adjustability. However, it relates to the capacity of the production system to adapt to pre-dicted events. It is the case of predictive maintenance, which uses BDA to estimate the remaining useful life of machine¡¯s components. Afterwards, the manufacturing system can adapt to the results of this prediction. 

After concluding the analysis of the 93 articles, three char-acteristics seem to be overlooked: the environmental dimen-sion, the knowledge generation, and the inclusion of the human being. To consider these dimensions that seem to not be explicitly raised in the work of Tao et.al. (2018), three new characteristics are proposed: 

6. 
Environment-Centric Processes: estimations suggest that the electronics and home appliances industry scrapped around 100 million goods in China in 2012 (Tian et.al. 2013). As exemplified, the environmental impact of industry is far from being negligible, which is the reason why industrialized countries have started to tighten regu-lations and engage environmentally friendly practices in manufacturing (Tuncel et.al. 2014). Research done in the context of I4.0 must not overlook this aspect. Therefore, this characteristic concerns the use of new technologies to create environment-centric processes. For example, optimizing the disassembly scheduling process to maxi-mize the number of components that can be recycled. 

7. 
Knowledge Discovery and Generation: most of the com-panies have been computerized for a long time, which has eased the collection of data. Despite the access to a plethora of information systems, generating knowledge from raw data still supposes a major industrial and aca-demic challenge. Besides, the generation of knowledge is a mandatory step to improve the adoption of BDA by companies (Grabot 2018). In fact, knowledge could be considered as one of the most valuable assets in manu-facturing (Harding et.al. 2006), the reason why generat-
ing it represents an important gain behind the adoption of BDA. Therefore, as I4.0 is characterized by allowing 


knowledge creation, research efforts must include it to generate value. One example of this is harnessing data from maintenance reports to provide the production of responsible real-time information about the root causes of machine breakdowns. 
8. Smart Human Interaction: even with the advent of multi-ple I4.0 technologies, its adoption would be significantly hindered by not keeping humans in the loop or not con-sidering their interaction with the proposed solutions. For instance, Thomas et.al. (2018a) experienced the case of a company that was not willing to introduce an improved version of a quality control system because it somehow excluded the person from the process. There-fore, this characteristic concerns the consideration and/ or inclusion of a human being when implementing new technologies. Examples of this would be a worker behavior recognition system based on computer vision or software interacting with operators through NLP. 
Figure 5 summarizes this section. It also presents the relationship between the Research Questions (RQ), the analytical framework axes, the research objectives, and the expected outputs of this study. 



RESULTS

First research question: activities employed in.ML.PPC 
To identify the activities, the tasks used to implement a ML-PPC in each of the 93 communications were identified. Afterwards, these tasks were grouped into categories to ease the information analysis. These groups of activities were analyzed by two experts to keep the most meaningful ones. Results suggest eleven standard and recurrent activities: 
1. 
Data Acquisition system design and integration (DA): design and implementation of IoT systems to collect data. This activity also encompasses the data storage and communication protocols. 

2. 
Data Exploration (DE): use of data visualization tech-niques, inferential statistics, and others to derive initial insights and conclusions about the dataset. 

3. 
Data Cleaning and formatting (DC): data preparation from the raw data to make it exploitable by the ML-PPC model. It concerns tasks such as outlier removal or missing values handling. 

4. 
Feature Selection (FS): choice of the most suitable inputs to the ML-PPC model. It can be done through statistical techniques, e.g. stepwise regression or by means of expert insight. 




Fig. 5 Relationship between the building blocks, research objectives and expected outputs of this study 
5. 
Feature Extraction (FE): use of variables from the ini-tial dataset to calculate more meaningful features. 

6. 
Feature Transformation (FT): representation of the initial features into different spaces or scales using techniques such as normalization, standardization or kernel transformations. 

7. 
Hyperparameter Tuning and architecture design (HT): definition of the ML model architecture and adjustment of its hyperparameters to improve the performance. For instance, optimizing the learning rate and defining the activation function in a neural network. 

8. 
Model Training, validation, testing, and assessment (MT): using the data to perform the training, validation and testing process. It can be done through techniques such as k-fold cross-validation. It also encompasses the choice of the training/validation/testing set split and the model¡¯s performance assessment. 

9. 
Model Comparison and selection (MC): several ML techniques can be used to achieve a certain task. This activity concerns the comparison of multiple ML mod-els to choose the one that better suits the needs. 

10. 
Contextualized Analysis or application (CA): going further than just assessing the model¡¯s performance. It concerns the actual implementation of the ML-PPC model or the analysis of its results in the context of the problem that is addressed by the study. 


11. Model Update (MU): data used to train ML models represents the context of the studied environment at a given moment. However, this context is dynamic, hence the ML-PPC model must be adapted. Therefore, this task concerns the model update through new data. 
To address this research question, the percentage of papers using each activity was measured. These results are summa-rized in Fig..6. Findings suggests that four groups of activi-ties can be proposed following their usage: 


These groups show that a considerable amount of research papers only focus on the architecture design, training, and assessment of ML-PPC models (CUAs cluster), while not employing or documenting the use of other activities. Con-sidering OUAs, it is surprising to find that only half of the communications used the CA, which corresponds to an actual implementation of the proposed model in the context of the study. This suggests that half of the studies go no further than just training and evaluating the performance of the model. 
MUAs group encompasses data pre-processing tasks, which are capital to any ML implementation. Even if these activities are frequently employed in practice, their low usage is probably because researchers do not mention them, implying a lack of documentation. Moreover, as one of the characteristics of big data is the variety (in type, nature, for-mat, etc.) (Zhou et.al. 2017), it is crucial to employ data pre-processing activities to ensure the quality of the final models. Consequently, this lack of documentation can repre-sent a pitfall to practitioners willing to apply ML-PPC based on research papers. 
Finally, SUAs cluster highlights the most important research gaps in scientific literature. Three key findings can be inferred from activities in this group: firstly, the low usage of DA highlights the challenge of coupling IoT technolo-gies with ML-PPC. This is a major pitfall to deploy ML-PPC in companies, as they normally need real-time data or statuses from their manufacturing systems. Secondly, the lack of DE utilization could mean that ML-PPC applica-tions tend to jump directly to activities in the CUAs cluster while overlooking descriptive and basic inferential statis-tics techniques. This represents an obstacle to generating knowledge from data, as DE can draw conclusions easily interpretable by non-ML specialists. Finally, the rare use of MU implies that adapting the ML-PPC model to a dynamic manufacturing context is seldom addressed. This unpredict-able change of the statistical properties and relationships between variables over time is known as concept drift (Ham-mami et.al. 2017). Not addressing this issue can be harmful for the model reliability in the long term. 


Second research question: techniques and.tools used in.ML.PPC 
Concerning the techniques, results present the number of times a given ML model is used. In the case of communica-tions comparing several techniques, only the one chosen by the authors because of its better performance was consid-ered. If this best-performing model employs several tech-niques, each of them is counted as used once. 
There are numerous ML techniques in scientific litera-ture. Therefore, to ease the analysis of results, a grouping of techniques in families is proposed is Table.1. These families were determined with the help of a ML expert. It is impor-tant to mention that the column ¡°Concerned techniques¡± in Table 1 is not an exhaustive list, it is limited to techniques found in the systematic literature review. 
Results are presented in Fig..7. They suggest that NN, Q-Learning, and DT are the most used techniques in ML-PPC. The extensive use of NN is probably due to their abil-ity to learn complex non-linear relationships between vari-ables, often delivering good performance when compared to other techniques. Even if Q-Learning remains, by far, the most used RL technique, other RL models such as Sarsa or R-Learning are used, which points an interest in agent-based modeling in ML-PPC. Finally, the attention drawn by DT techniques is probably linked to their excellent trade-off between accuracy and interpretability, allowing knowledge generation. 
The high use of Clustering techniques could be explained by the fact that data in manufacturing systems is normally unlabeled and can contain meaningful unknown patterns. Therefore, clustering can be employed to discover groups as well as hidden structures in datasets. 
The usage evolution of the six most used technique fam-ilies was also measured. Figures representing this can be found in ¡°Appendix II¡±. Due to an imbalance in the amount of articles over the different years, results are presented as relative frequencies. For example, if the NN achieved a usage of 27% in 2018, it means that 27% of all the techniques used in that year corresponded to such a model. Results sug-gest that there is a strong growth in the use of NN since 2015, this is possibly due to the growing computing power, recent findings in terms of architectures such as CNNs or LSTMs, and the development of specialized frameworks like PyTorch, TensorFlow, Keras, etc. which ease the task of implementing such models. Moreover, results show a grow-ing interest on Ensemble learning techniques which evolved from not being used between 2011 and 2013 to accounting for 14% of applications in 2018. This can possibly explain the loss of interest on DT since 2017, as Random forests (a type of Ensemble learning) can achieve better performance by using committees of decision trees. 
Table 1 Technique families with their respective ML models 
Family Concerned techniques 
As NN and Ensemble learning families seem to be recently attracting the research community, a detailed view of their encompassed techniques is presented in ¡°Appen-
dix III¡±. Concerning NN, the most used technique is the Multi-layer perceptron, which is the classic architecture of a NN. However, more specialized architectures belonging to deep learning are starting to appear in PPC research. Such is the case of the CNNs, LSTMs, and Deep Belief Networks. 
These techniques have presented good performance when dealing with specific problems, such as image recognition for CNNs, time series analysis for LSTMs or feature extrac-tion for Deep Belief Networks. In the case of Ensemble learning, the most used technique is, by far, the Random forests. They seem to provide excellent results while ena-bling knowledge generation. In fact, they allow the most meaningful variables to be easily identified in the SL task, which is the reason why researchers tend to use them to both attain accuracy and model interpretability. 
To measure the utilization of the learning types, each paper was analyzed, and the learning types used were iden-tified and counted. As a given model can use several ML techniques, it can refer to several learning types at the same time. Hence, the different synergies between these were also considered. Results are presented in Fig..8. 
Findings show that the most used learning type is SL. This is probably because SL addresses two recurrent needs in applied research: classification and regression. In fact, SL can be used to learn the relationship between an input X and an output Y that can be either discrete in the case of classification or continuous for regression. Furthermore, it was found that RL techniques are extensively used, which confirms the interest behind agent-based models. 
Concerning UL, it seems to be especially used with SL (SL-UL), which suggests a strong synergy between these 
Learning type 

52 Number of uses 
Fig. 8 Number of uses by learning type 
C++ WEKA Java Python RapidMiner R Others MATLAB Not Mentioned 

41 Number of uses 
Fig. 9 Number of uses by tool 
two learning types. The reason behind this could be that UL techniques are normally used to perform data pre-process-ing, as with Principal Component Analysis, or discovery of hidden patterns in datasets, e.g. with Clustering. There are 6 papers using just UL, however, this learning type seems to unlock all of its potential when used in synergies, allowing for the design of more complex models. 
Even if there are some SL-RL synergies, they are not very common. This is probably because SL is normally coupled with RL when there is a need of performing rapid estimations of functions to save computing time. However, it seems that most of the applications do not reach a scale that needs this kind of configuration. Finally, it was found that using UL-RL and SL-UL-RL is rare in the scientific litera-ture. This does not mean that their synergy does not provide advantages, it is just that there may not be a current need for it. Also, it could be that coupling these learning types over-complexifies the model design, which prevents its use. 
Concerning the tools, only programming languages or software used to implement the ML model were considered. Therefore, other tools such as discrete event simulation soft-ware are out of the scope of this research. Results are pre-sented in Fig..9. 
For clarity sake, tools being used only once were grouped in the category denominated as ¡°Others¡±. These tools were: ACE Datamining System, C#, Clementine, GeNIe Modeler, Hugin 8.1, NetLogo, Neural-SIM, Visual C++, and Xelopes Library. Additionally, it is important to mention that most of the researchers do not mention the tool the use to imple-ment the models. 
It could be said that MATLAB is, by far, the most used tool to perform ML-PPC in research. Besides its robust cal-culation capacity, the reason behind this could be that uni-versities often invest in licenses for this software; therefore, they expect their researchers to use this tool. R is the second most used tool, which may be because it is a free software targeting statistical applications, including ML. Finally, the third most used tools are both RapidMiner and Python. The former eases the implementation of ML models thanks to its visual programming logic, while the latter is a multipur-pose programming language recently characterized by its ML libraries and frameworks such as Scikit-learn, PyToch, Keras, etc. 


Third research question: used data sources to.implement a.ML.PPC 
To answer this question, the data sources used by each of the analyzed papers were identified. These results are sum-marized in Table.2. The column ¡°Identification¡± (ID) will assign a number to each communication. This will be used later to establish a mapping of the scientific literature. 
Results show that ¡°Artificial data¡± is the most used data source in recent scientific literature. This probably highlights the difficulty of accessing data coming from companies. Additionally, it is important to remember the extensive use of RL techniques. These models normally require constant access to data concerning the real-time status of the produc-tion system, which can be difficult to find in real factories. Therefore, researchers normally use Artificial or Public data to test their models. This issue could be addressed by creating digital twins, but this still represents a research challenge. 
The extensive use of artificial data suggests that there are data availability issues. This poses two main challenges: firstly, dealing with highly unbalanced datasets when train-ing, for instance, SL algorithms for classification, and sec-ondly, accessing enough data to enable good generalization capacity, especially in deep learning models. 
The first challenge is common when training ML mod-els to identify disruptions. In fact, disruptive events in PPC such as machine breakdowns or quality problems tend to be scarce when compared to the total size of the dataset. Thus, ML techniques struggle to learn these events. To tackle this issue, some authors have proposed solutions such as data augmentation, a common practice in computer vision that consists of artificially creating new training examples by modifying existent observations (Perez and Wang 2017; Miko.ajczyk and Grochowski 2018). Another approach is to use crafted algorithms adapted to class-imbalance. Bi and Zhang (2018) performed a comprehensive comparison of 
Table 2 Data sources used by 



Fourth research question: addressed use cases by.recent scientific literature 
To answer this question, each analyzed article was allo-cated to one of the seven proposed types of use cases. This allows to measure their importance in the scientific literature (Fig..10). 
Results point out that Smart Planning and Scheduling is the most addressed use case in recent scientific literature, with nearly half of the communications discussing it. This result may come from two main reasons: firstly, the string chains used in the methodology are closely related to this use case; secondly, it normally uses structured data relatively easy to get from information systems, which eases the task of implementing a data-driven approach. The strong use of Time Estimation in ML-PPC (14% of the papers) suggests that classical time measurement methods are not compliant with the growing complexity of the manufacturing systems, which may represent a pitfall to perform a reliable planning. Therefore, ML models considering more diverse variables


as inputs are being adopted. Moreover, some researchers have addressed the coupling of Smart Maintenance, Process Control and Monitoring, and Quality Control with the PPC. However, there is still effort to be made, as the share of these use cases was no higher than 10%. 
Finally, two use cases are targeted as critical: The Inven-tory and Distribution Control (6%) and the Smart Design of Products and Processes (4%). These findings suggest two things: first, a lack of integration of the logistic functions into the ML-PPC, and secondly, a difficulty for harnessing insights from data to serve product and process design. This difficulty is probably because data employed in design is highly unstructured (text data, image data, etc.) and greatly depends on people¡¯s experience. 


Fifth research question: the.characteristics of.I4.0 
To quantify their usage, the addressed characteristics in each of the 93 analyzed papers were identified and counted. Results are summarized in Fig..11. In this figure, the sum of all the totals is higher than 93 as one ML-PPC model can satisfy several characteristics. 
Findings show that the Self-Organization of Resources is, by far, the most addressed characteristic (56 uses) in ML-PPC applications. This result was expected, as this charac-teristic can be achieved through production planning and scheduling, two functions directly related to the PPC and found to be extensively employed in the use cases. There-fore, it can be concluded that the ML-PPC based models effectively enable this characteristic. 

Smart Human Interaction 
Knowledge Discovery and Generation Environment-Centric Processes Self-Learning of the Production Process 
Self-Regulation of the Production Process Self-Execution of Resources and Processes Self-Organization of Resources Customer-Centric Product Development 
Number of papers 

papers), as well as the Knowledge Discovery and Genera-tion (26 papers) appear to be moderately boarded. This leads to two main conclusions: first, ML-PPC models effectively endow manufacturing systems with the capacity of adapting to unexpected events and predicting production problems. This is suitable to handle the stochastic nature of production environments. Secondly, ML is suitable to generate knowl-edge from PPC data, which is crucial in I4.0, where data is abundant, and it can provide useful guidelines to improve the company¡¯s know-how. 
Four characteristics were rarely satisfied: The Customer-Centric Product Development (3 papers), the Self-Execution of Resources and Processes (4 papers), the Smart Human Interaction (7 papers), and the Environment-Centric Pro-cesses (8 papers), which points to strong research perspec-tives of ML-PPC applications enabling these features. Con-cerning the Customer-Centric Product Development, it was rare to find papers including customer-related variables into their PPC. This can be due to the difficulty to access data from customers or end users. For instance, as observed in the data sources section, user data was seldom employed. 
The low number of papers dealing with Self-Execution of Resources and Processes suggests that it is unusual to couple the PPC with autonomous physical resources. This can be due to the complexity of such systems as they require important capital investments as well as multi-disciplinary knowledge in production systems, mechatronics, and control theory. 
It was very surprising to find that the Smart Human Interaction (7 papers) and the Environment-Centric Pro-cesses (8 papers) are rarely addressed. Indeed, manufactur-ing systems can be human based in several steps such as during the execution in the shop floor or during the tactical planning definition. Not considering the interaction of the proposed ML-PPC models with humans can be harmful for the deployment of the proposed system, as it may worsen the working conditions. Therefore, thinking about this human-ML interaction is the cornerstone for a successful adoption. Concerning the Environment-Centric Processes, scarce applications tried to minimize the environmental impact of production processes through ML-PPC. In a world where natural resources are becoming rare, this is a non-negligible aspect that must be considered, not only because of the tightening of environmental laws by gov-ernments but also because of the ethical responsibility of companies. 


Cross.axes analysis: mapping the.scientific literature through.use cases, I4.0 characteristics, and.learning types 
To address the second objective of this study, a mapping of the scientific literature in ML-PPC is proposed. This is achieved through a cross-analysis employing the use cases, characteristics of I4.0, and learning types. Results are repre-sented via a cross-matrix having the use cases in the vertical axis and the characteristics of I4.0 in the horizontal axis. This matrix also allows the maturity of a given use case to be assessed. For instance, a mature use case in the scientific literature will tend to satisfy more I4.0 characteristics. From this point of view, the crossing between a characteristic of I4.0 and a use case will be referred as a domain. 
The ID numbers defined on Table.2 are employed to place the analyzed articles in the matrix. Additionally, the learn-ing types employed by each communication are represented using a color code. Figure.12 provides a summarized view of this matrix, allowing for a high-level analysis that will help to identify research gaps and trends in ML-PPC. Figure.13 is 



a detailed view of the matrix indicating the scientific articles with their respective learning types found in each domain. 
Figure.12 shows that among the 56 possible domains, 18 (32%) were not addressed at all. Furthermore, 24 (43%) domains lie in the range of 1 to 3 papers. This means that nearly half of the domains are in an exploration phase. These two remarks lead to conclude that ML-PPC in the I4.0 is still an active research topic with strong perspectives. 
From Fig. 13, it can be said that there is a strong trend of using multiple synergies between learning types across all of the different use cases. However, there are no applications of RL in Time Estimation and in Smart Design of Products and Processes. The reason for this may be that these use cases have strong strategic impacts. Therefore, current ML implementations in such applications aim to support deci-sions rather than automating them such as with agent-based systems driven by RL. 
There are two use cases achieving a high maturity: Smart Planning and Scheduling and Process Control and Moni-toring. They both cover all but one of the characteristics of I4.0. In the case of Smart Planning and Scheduling, it fails to address the Self-Execution of Resources and Pro-cesses, which suggests that there are research perspectives in coupling the production planning and scheduling with autonomous physical resources. For the Process Control and Monitoring, there is a lack of applications satisfying the Customer-Centric Product Development, which would be an automatic optimization of physical resources from the analysis of customer-related variables. 
Knowledge Discovery and Generation is the only char-acteristic addressed by all the use cases, which denotes an intense interest in knowledge creation from data. Further-more, there is a strong presence of SL, UL, and SL-UL in this characteristic. This implies an important affinity between these learning types and the generation of useful information from raw data. Following a similar trend, there seems to be a generalized interest in Environment-Centric Processes, a characteristic that is addressed by almost all of the use cases. However, its low number of papers implies that there are strong research avenues to be explored. 
Communications addressing the Self-Execution of Resources and Processes focused exclusively on Process Control and Monitoring applications. This shows that the dynamic optimization of working parameters of the machines allows data-driven intelligent resources to be created. However, this characteristic has further potential to be explored in PPC research with other use cases, such as in Inventory and Distribution Control with autonomous AGVs to serve logistic needs or in quality, by automating processes. 

CONCLUSIONS

Conclusion and.further research perspectives 
This state-of-the-art analysis studied 93 research articles chosen through the logic of a systematic literature review. These papers were analyzed by means of an analytical framework composed of four axes. First, the elements of a method were reviewed, which enabled an analysis of activi-ties, techniques, and tools to perform a ML-PPC. Secondly, the data sources employed to implement a ML-PPC model were recognized and assessed. Thirdly, an analysis of the use cases enabled the recognition of the applications of data-driven models in the 4.0. Fourthly, the characteristics of I4.0 were identified and assessed through their usage. Additionally, a mapping of the scientific literature was pro-posed by means of the use cases, characteristics of I4.0 and ML learning types. 
Results concerning the activities allowed the recogni-tion of eleven recurrent tasks that are employed to create a ML-PPC model. They were grouped in four clusters follow-ing their use percentage: CUAs (Commonly Used Activi-ties), OUAs (Often Used Activities), MUAs (Medium Use Activities), and SUAs (Seldom Use Activities). From these clusters, it can be concluded that activities belonging to the CUAs and OUAs clusters are well documented in the scien-tific literature. MUAs activities mainly contain data pre-pro-cessing tasks, which are necessary but not commonly docu-mented by researchers. Finally, the SUAs cluster suggests that there are three activities rarely addressed in literature: the design and implementation of data acquisition methods from the manufacturing system, the exploration of data to get insights, and the constant adaptation of the proposed ML-PPC model to the environment dynamics. 
An extensive review of the techniques identified the most used families in scientific literature. These were found to be the NN, Q-Learning, DT, Clustering, Regression, and Ensemble learning. From these results, a temporal evolu-tion analysis of the top 6 most used families was performed. Findings suggested a growing interest in NN and Ensemble learning, which motivated a focused study on the detailed techniques encompassed by these families. Concerning the NN, the Multi-layer perceptron was the most used technique. Nevertheless, more specialized deep learning techniques such as CNNs, LSTMs, and Deep Belief Networks are starting to be employed. With respect to Ensemble learn-ing, the most used technique was Random forests. 
The ML learning types were also reviewed. Findings showed that scientific literature mainly focused on the individual use of SL and RL. However, synergies between learning types are also employed. For instance, the most used synergy was SL-UL, which allows to explore and pre-process the data through UL to improve the SL training. The UL-RL and SL-UL-RL synergies had only one use each, which could be considered as a research gap, advising improvements in its integration. In fact, each learning type has its advantages and limitations. Hence, it is important to explore more synergy possibilities, as they may help over-come individual limits. 
Other than increasing data availability, one option to encourage the utilization of UL-RL and SL-UL-RL is to boost the development of specialized libraries to build complex models coupling several learning types. Examples of this are deep learning frameworks such as TensorFlow, Keras, PyTorch, etc. which have eased the implementation of deep learning applications. This has allowed researchers to spend more time on the addressed problem than on the coding stage. 
Results concerning the tools showed that MATLAB, R, Python, and RapidMiner are the most used tools in develop-ing ML-PPC models in research. However, most authors did not mention the tool used, which is a limit of this study. Fur-thermore, it is important to mention that these results come from a sample of scientific articles, meaning that results are mainly valid in an academic context. If there are practition-ers willing to implement ML-PPC models in companies, other aspects need to be analyzed such as the cost of the software, its scalability, skill availability in the labor market, compatibility with existing information systems, etc. 
The current horizon of data sources used is dominated by Artificial and Management data. The former points to a difficulty in collecting all of the data required to implement ML-PPC models, while the latter suggest that companies are interested in valuing their data stored in information sys-tems. Data coming from IoT sources such as Equipment and Product data was moderately used, nevertheless showing an interest in these technologies to collect data. Finally, ML-PPC models failed to integrate User data, probably because it is complex to collect and it engages an important respon-sibility concerning data privacy. 
The most addressed use cases were Smart Planning and Scheduling and Time Estimation, probably because they are directly concerned by the PPC, which may lead to its high utilization. The fact that there are research articles in all of the use cases suggests that the PPC is a transversal func-tion that benefits from several applications. Therefore, when designing a ML-PPC system for a company, the impact on all of the use cases must be assessed. Finally, it was found that Inventory and Distribution Control, as well as Smart Design of Products and Processes, are seldom addressed. This suggests that there is still a lot of progress to be made when coupling the PPC to logistics as well as product and process design through ML. 
Concerning the characteristics of I4.0, results suggest that scientific literature in ML-PPC is extremely focused on satisfying the Self-Organization of Resources, which was expected, as one of the main goals of the PPC is resource management to satisfy the commercial plan. At a second level, the Self-Regulation of the Production Process, the Self-Learning of the Production Process, and the Knowl-edge Discovery and Generation seem to be more frequently addressed. However, Fig..13 showed that they are mainly employed for Smart Planning and Scheduling, implying a lack of research in the other applications. Finally, there are three characteristics that are partially overlooked by researchers: Environment-Centric Processes, Smart Human Interaction, and Customer-Centric Product Development. The first two are essential characteristics of building more responsible production systems as they aim to include human beings and reduce the environmental impact of manufactur-ing processes. The latter relates to the alignment of the PPC to the customer¡¯s needs. Hence, it appears that recent ML-PPC research ignores the influence of the customer in the manufacturing process. 
As illustrated in the proposed cross-matrix, 75% of the possible research domains are barely addressed or were not explored at all. This means that the ML-PPC is still a key topic for the enablement of I4.0, which presents strong research avenues. The main future research perspectives could be summarized in the following three key items: 
1. Reinforce the role of IoT in ML-PPC: this would allow an improvement to the data acquisition system¡¯s design and would provide a means to perform a model update to tackle the concept drift issue. To do so, the ML mindset and workflow should be shifted from a linear to a cir-cular process, considering the need to constantly retrain through new data. This way of thinking would enable the identification, from an early development stage, of the retraining policy and the necessary variables that could be measured again at a sensitive cost. By defining these two aspects, the data acquisition system design will be less complex to conceive, as the needs will be clearer. 
This would avoid investment in sensors and resources and architecture that would not be exploited. Concern-ing the retraining policy, a review in the context of PPC reporting common practices, advantages and pitfalls seems to be missing in the scientific literature. 
2. 
Improve the integration between the PPC, logistics, and design: it was stated that the PPC benefits from different use cases. However, recent literature seems to overlook logistics as well as product and process design applica-tions coupled with the PPC. To tackle this challenge, it is necessary enable data availability, continuity and sharing over the design, logistics, and production depart-ments. This could be achieved through interoperability as well as communication of intra-organizational sys-tems such as the PLM, ERP, and MES. Even if projects that are meant to couple such systems are costly, they are necessary to ensure data availability and quality. One way to achieve this is the use of data lakes, which have been recognized as suitable to handle big data reposito-ries of a structured and unstructured nature (Llave 2018; Lo Giudice et.al. 2019). For instance, Llave (2018) con-cluded, through expert interviews, that one of the key purposes of data lakes is to serve as experimentation platforms for data scientists. 

3. 
Set human interaction and environmental aspect as pri-orities to ensure the development of ethical manufactur-ing in I4.0: exploring the interaction of humans with the proposed ML-PPC models is paramount to build-ing inclusive technologies at the service of society. To achieve this, the short-and long-term impact of ML-PPC systems on employees¡¯ working conditions must be assessed. If the system degrades them, it must be redesigned. Concerning the second aspect, seeking a reduction in the environmental impact of manufactur-ing through ML could provide important developments. This can be addressed from a purely PPC approach by optimizing, for instance, the scheduling of disassembly processes or by improving the prediction of production times to avoid energy waste. Other approaches could be the optimization of the supply chain. Even though the supply chain was not covered in this review, it is an appropriate domain for researchers to implement ML applications. For instance, by considering environmental criteria when choosing suppliers, as in (Hosseini and Barker 2016). 


Some of the research gaps indicated in this review could motivate future work. Future work will be focused on the following aspects: 
1. 
The proposed activities will be reviewed to determine an order between them, creating a procedure: this would help shift from a linear to a circular workflow when implementing ML-PPC models. 

2. 
The most suitable techniques and tools will be linked to each of the activities with sectorial information: linking techniques, tools, and activities is the key to creating good practices that could be helpful to new practitioners, both in research and industry. Furthermore, according to Kusiak (2017, 2019), there are profound differences in the volume of data generation and usage across different industries. Therefore, future work will aim to identify trends categorized by sectorial information. 

3. 
The current state of data availability solutions and work-arounds will be explored: as data availability was found to be a main issue, a review of techniques to tackle the class-imbalance problem and the use of transfer learning in the context of PPC will be performed. Additionally, the utilization of data lakes for ML-PPC will also be explored. 

4. 
Future research avenues will be proposed through an NLP analysis: NLP may enable the discovery of non-trivial trends present in the corpus of the 93 sampled articles. This will complement the results of the system-atic literature review. 

Introduction 

Global recession over the last years changed the overview on the industrial sector, now looking at the real value-added that it creates. Companies that followed the trend to relocate activities by looking for low cost labor, are now committed to recover their competitiveness. 
German manufacturing strategy played a key role on this shift-ing, launching initiatives to maintaining and promoting its impor-tance as a ＆＆forerunner§ in the industrial sector [1]. The buzz word ＆＆Industry 4.0§ has been presented and with it big promises arose to face the latest challenges in manufacturing systems. The impeller Industry 4.0 (I4.0) is enabling and reinforcing this trend using its technologies, changing the way of living, creating new business models and new ways of manufacturing, renewing the industry for the so-called digital transformation. 
In 2011, the German government have brought into the world a new heading called Industrie 4.0 (I4.0), assumed as the fourth indus-trial revolution [2每6]. I4.0 aim is to work with a higher level of automatization achieving a higher level of operational productivity and ef.ciency [3,7],connecting the physical to the virtual world[8每 9].It will bring computerization and inter-connection into the tra-ditional industry [3]. According to several authors [3,5每6], I4.0 can be assumed as Cyber-Physical Systems (CPS) production, based on heterogeneous data and knowledge integration and it can be summed up as an interoperable manufacturing process, integrated, adapted, optimized, service-oriented which is correlated with algo-rithms, Big Data (BD) and high technologies such as the Internet of Things (IoT) and Services (IoS), Industrial Automation, Cybersecu-rity (CS), Cloud Computing (CC) or Intelligent Robotics [3,7,9]. From the production approach, Martin and Sch.ffer [8] de.ne I4.0 as the intelligent.owofthe workpieces machine-by-machineinafactory, ona real-time communication between machines.On this environ-ment, I4.0 will make manufacturing become smart and adaptive using .exible and collaborative systems to solve problems and make the best decisions [7].It bringsa good development for the industrial scenario focusing on creating smart products, smart pro-cesses and smart procedures [5]. Companies expected to increase the level of digitalization, working together in digital ecosystems with customers and suppliers [10]. 
Since I4.0 boom, the research community has experienced differ-entapproachestoI4.0 concept;however,thegeneralsocietymaybe confused basedonthelackof understandingonthis area. Thereisa needfor clari.cationofI4.0 related conceptsand technologies. 
This paper deals with the research of I4.0 in manufacturing environments on a literature review over the enabling technolo-gies, focusing on the state-of-the-art and future trends. The approach of I4.0 for manufacturing systems in this paper is based on the Smart Factory (SF) concept. The SF concept makes use of components such as IoT, IoS, the systems integration and Cyber-Physical Production System (CPPS) that is formed by several linked CPS (CPS may use up until nine key enabling technologies, widely assumed by research community). 
The paper is structured as follows: section 2 presents the Refence Architecture Model Industrie 4.0 (RAMI4.0) as the guid-ance for the I4.0 technologies implementation, section3 presents key enabling technologies of I4.0, section4reviews the Smart Fac-tory (SF) concept of the I4.0 structured with its components, and the .nal remarks are in section5 which introduces the summary and gives future outlooks. 
2. 
Reference Model of I4.0 
Several German associations and institutions cooperated on the creation of the reference model for I4.0. This 3D model in Fig.1 is the development of a shared language and a structured framework [11每12] that describes the fundamental bases of I4.0. It is intended to assist on the I4.0 technologies implementation [13]. 
Unlabelled image
The Reference Architecture Model Industrie 4.0 (RAMI4.0) should enable to identify the existing standards and among it, identify and close the gaps, loopholes and identify the overlaps [14]. 
On the left horizontal axis from the IEC 62,890 standard, facili-ties and product lifecycle with the correspondent value stream are showed [15]. RAMI4.0 clearly describes the difference between instance and type. When the design and prototyping is completed, the type becomes an instance, ready for production [14]. 
The hierarchy levels from the IEC 62264 standard are showed in the right axis, representing the different grouped entities by func-tional properties, de.ned to represent all hierarchical levels of the enterprise, from the ＆＆Product§ (e.g., a workpiece) to the ＆＆Connect World§ level. The ＆＆Connect World§ is the last stage of the I4.0 development enterprise environment using IoT and IoS to connect enterprises, customers and suppliers [13每14]. The hierarchy levels are discussed further insidetheSFin section4throughthe Fig.25. 
The layers on the vertical axis represent a reminder to integrate all aspects on the enterprise digitalization [11]. The functionallay-ers of the organized vertical axis describe: 
. 
＆＆Asset Layer§ represents reality, for instance, physical compo-nents including linear axes, robots, conveyor belts, PLC＊s, metal parts, documents, archives also persons that form a part of con-nection to the virtual world via the ＆＆Integration Layer§ [12,14每 15]. Also, non-physical objects such as software or ideas; 

. 
＆＆Integration Layer§ provides processed information for the dig-itization of the assets. Elements connect to Information Tech-nologies (IT) such as sensors, Radio Frequency IDenti.cation (RFID) readers, integration of Human-Machine Interface (HMI) and computer-aided controls the technical processes [12,14]. Persons via HMI also participate on this layer. In the virtual domain, each signi.cant event is mirrored through the enabler [12]; 

. 
＆＆Communication Layer§ with the function of communication standardization. It makes use of uniform data format and prede-.ned protocols, providing services for the ＆＆Integration Layer§ [12,14每15]; 

. 
＆＆Information Layer§ to process and integrate consistently the different available data into useful information [14]. Also receives and transforms events to match the data which are available for the next layer [15]; 

. 
＆＆Functional Layer§ to enable formal descriptions of functions. It creates an horizontal integration platform of several functions that can be with remote access, resulting of the necessity of data 

. ＆＆Business Layer§ enables mapping of the business model and links between different business models. It ensures, within the value stream, the integrity of the functions [14每15]. 
It＊s possible to map all crucial aspects of I4.0, allowing the clas-si.cation accordingto the model,of objects such as machines. This model allows the step-by-step migration from the actual to the future manufacturing environments [13]. 
The I4.0 essential technological elements are compiled at the .rst time as RAMI4.0 and it is registered in Germany in the DIN SPEC 91345 standard [14]. 
3. The Key Technologies of I4.0 
I4.0 is characterized on manufacturing and services by highly developed automation and digitalization processes, electronics and IT [3]. From the production and service management perspec-tive, I4.0 focus on establish intelligent and communicative systems such as Machine-to-Machine and Human-Machine Interaction, dealing with the data .ow from intelligent and distributed system interaction [16]. Among other features, I4.0 promotes autonomous interoperability, agility, .exibility, decision-making, ef.ciency or cost reductions [17]. 
The I4.0 implementation should be interdisciplinary in a closely between different key areas. Several authors [5,18每19] described nine pillars (also called the building blocks) of the I4.0 framework as follows in the subsections.A fundamental key point to achieve the integration of I4.0 framework is the human contribution that will be improved with the development of professional skills of the stakeholders. 
3.1. The Industrial Internet of Things 
On the IT, the IoT is the connection oftwo words i.e. ＆＆internet§ and ＆＆things§. ＆＆Internet§ as the network of the networks. A global system serving users worldwide with interconnected computer networks using Standard Internet Protocol suit (TCP/IP). As individ-ually distinguishable by the real world, the ＆＆things§ can be any-thing like an object or a person [20]. Today, IoT is widely used for instance, in transportation, healthcare or utilities [21]. Thing-to-Thing, Thing-to-Human and Human-to-Human form a network inside IoT, connected to the internet. Individually identi.able objects exchange information inside this network. [22每23]. 
IoT has been increase with the advancement of mobile devices. IoT can be achieved with connected RFID, Wireless Sensor Net-works (WSN), middleware, CC, IoT application software and Soft-ware De.ned Networking (SDN) as the key enabling technologies [23]. Fig.2 presents the associated technologiesin IoT. 
One simple de.nition of IoT described by Sezer et al. [21] is: ＆＆IoT allows people and things to be connected anytime, anyplace, with 
Unlabelled image
anything and anyone, ideally using any path/network and any ser-vice§. In other words, Bortolini et al. [24] de.ned IoT as an ubiqui-tous presence for a common purpose of various things or objects interacting and cooperating each other, digitalizing all physical systems. For different aims, the digitalized information can be used to adjust production patterns, with the use of a virtual copy of the physical world and using sensor data [7]. The entire production systems such as machinery and related resources can be the ＆＆things§ managed and virtualized by I4.0 [4,7]. In addition, the IoT nature as to be decentralized and heterogeneous [25]. 
Regarding to IoT design architecture, Trappey et al. [26] estab-lished a logical framework by layers to classify IoT technology and used to characterize and identify CPS. According to several authors [25,27每28], IoT architecture most common layering in a typical network, includes four main layers as represented in the Fig.3 as follows: 
1) ＆＆Sensing Layer§ to sense the ＆＆things§ status with a unique identity and to integrate, e.g., actuators, sensors, RFID tags as several types of ＆＆things§; 
2) ＆＆Network Layer§ to support the transferred information through wired or wireless network from the ＆＆Sensing Layer§ to ＆＆Service Layer§, being the support＊s infrastructure. This layer determines and maps ＆＆things§ automatically in the network enabling to connect all ＆＆things§ for sharing and exchange data; 
3) ＆＆Service Layer§ makes use of a middleware technology sup-porting services and applications, required by the users or applications. The interoperability among the heterogeneous devices is ensured by this layer, performing useful services, e.g., information search engines and communication, data storage, exchanging and management of data as well as the ontology database; 
4) ＆＆Interface Layer§ to make the interconnection and manage-ment of the ＆＆things§ easier and to display information allow-ing a clear and comprehensible interaction of the user with the system. 
Differing from IoT based users, regarding to industrialenviron-ments needing real-time data availability and high reliability [29], the Industrial InternetofThings(IIoT)isthe connectionof industrial products such as components and/or machines to the internet. For instance, linkingthe collected sensingdatainafactorywithIoTplat-form, IIoT increases production ef.ciency with the BD analysis [22]. 
AtypicalIIoTis showedinFig.4,withwireand wireless connec-tions, increasing value with additional monitoring, analysis and optimization. 
As a natural evolution of IoT, the IoS can be seen as the connec-tivity and interaction of the things creating valuable services and is one of the fundamental basis of the SF. IoS is discussed further in section 4. 

3.2. Cloud Computing 
Cloud Computing (CC) is an alternative technology for compa-nies who intent to invest in IT outsourcing resources [30]. Assante et al. [31] characterized CC for Small and Medium Enterprises (SMEs) as a resource pooling with rapid elasticity and measured service, on-demand self-service and broad network access. The adoption of CC has several advantages related to cost reduction, e.g., the direct and indirect costs on the removal of IT infrastructure in the organization, the resource rationalization service by the dynamically scalable users consuming only the computing resources they actually use or portability when using any type of device connected to the internet such as mobile phones or tablets accessing from any world location [30].Bythis, the cloud can have 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 3. Generic Service-oriented Architecture (SoA) for IoT [25]. 
Unlabelled image
Unlabelled image
Fig. 4. Typical IIoT network [137]. 
any of the four types of access: public (usually on a data center location, managed by vendors and available for all public [32]), pri-vate (same organization location and offering special bene.ts [32]), hybrid (combination of public and private clouds [32])and com-munity (shared by multi organizations and supported by a speci.c sharing of interests and concerns community [33]). Everything is treated as a service in CC. These services de.ne a layered system or types of service models structured for CC as in Fig. 5 and the management overviewis shownin Fig.6,as follows [31,33每34]: 
. Infrastructure as a Service (IaaS) is where cloud service provi-ders supply users with fundamentalcomputingresources, with virtual infrastructures, e.g., virtual servers, networks or storage and where users into the cloud can deploy and run arbitrary software, which can include, for instance, operating systems applications; 
. 
Platform as a Service (PaaS) is where users develop and run applications using programming languages on the cloud infras-tructures. Therefore, it can be achieved scalability, high speed server and storage. Users can build, run and deploy their own applications with the use of remote IT platforms. On this layer, there is no concern on the resource＊s availability and mainte-nance [35]; 

. 
Software as a Service (SaaS) is where applications reside and runs in a cloud infrastructure [34]. Accessible from various cli-ent devices through an interface such as a web browser and programs. The focus is to eliminate the service applications on local devices of individual user, achieving an high ef.ciency and performance for the users. This category enables software applications such as Computer-Aided-Design (CAD) software and Enterprise Resource Planning (ERP) software, with a lower total cost of ownership [35]. 



V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

All underlying Everything as a Service (XaaS) layers allows direct interactions with the user interface layer at the top. 
On manufacturing environments, Cloud Manufacturing (CMfg) concept was proposed to make use of CC technology, in order to improve the current manufacturing systems [36]. Cloud-related manufacturing has two approaches: 
1) CC in manufacturing industry as a manufacturing version of CC -using cloud applications in manufacturing industry directly, web-based manufacturing applications or computer-aided are examples of possible deployments in the CC system. These applications are implemented at two service levels of CC system, matching SaaS and PaaS levels [37]; 
2) CMfg systems as an entire new type of cloud service, based on Service-orientedArchitecture (SoA) in the cloud environ-ment that provides manufacturing capabilities [36].It re.ects the IaaS level on CC system [37]. 
With the combination of advanced technologies, it arises a new computing and service-oriented manufacturing mode as CMfg [38]. A solution such as CMfg enables users to request services fromall stagesofa product lifecycle rangingfrom design, manufac-turing, management and so on [38每39].Bythis meaning, the main characteristics of CMfg is the service-oriented approach [40] and its trend on shifting manufacturing approach from production-oriented to service-oriented [33,41].Abrief CMfg model is shown in Fig.7, consisting on three categories of stakeholders: providers, operators and consumers, with their cooperation to maintain sus-tainable operation of a CMfg system [42每43]: 
. 
Providers 每 own and provide the abilities and the manufactur-ing resources [43].Within the entire product lifecycle, for shar-ing purposes, providers publish manufacturing resources to the CMfg platform and also receive manufacturing tasks from the cloud platform. Everything is transformed into services, under the exclusive management ofthe operator [42]; 

. 
Operator/s -to operate CMfg platform and to deliver services to providers, consumers and even third parties [43]. In an on-demand manner, consumers from the cloud platform can achieve high-quality and sustainable manufacturing services. Providers have permission to publish their resources and capa-bilities with the use of tools provided by the cloud platform [42]; 

. 
Consumers -to subscribe the manufacturing computing ser-vices availability in a CMfg service platform [43]. Under the exclusive management of the operator, consumers, including enterprises consumers and individual consumers, submit their 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
Unlabelled image
Fig. 7. CMfg model [42]. 
requirement tasks to the CMfg platform, e.g., design, manufac-turing, test or simulation tasks and also receive the execution results of their orders [42]. 
CMfg is a manufacturing paradigm based on Knowledge. In the running process, the knowledge plays as the central role [44], e.g., models, standards, protocols, rules and algorithms as knowledge, indispensable in many process and activities within entire lifecycle services as service generation, service management and service applications [42]. 
The concept of CMfg makes use of CC, BD, IoT, CPS, the net-worked manufacturing, service-oriented manufacturing, virtual manufacturing and virtual enterprise [45每46]. Cooperation can be enabled and supported by CMfg, sharing and management of manufacturing resources such as fabrication capabilities, equip-ment, applications, software tools, know-how, etc., of companies [47每48] and these companies can be included into the cloud, becoming accessible to potential consumers, in a pay-as-you-go manner [39]. CMfg enables the recommendation and execution, intelligent mapping and search of a service [33]. CMfg can provide in a form of service scalable, .exible and cost-effective solutions with lower maintenance costs and supports. Manufacturing tasks can be obtained also as services into the CMfg service platform [41]. Cloud data center owns the computational resources and the different organizations, e.g., manufacturing enterprises, owns the manufacturing resources [45]. There is no need for manufac-turers and users to invest in high-tech computers, computer licenses or worrying about software updates or upgrades [48]. Mai et al. [46] in Fig. 8 discussed a CMfg platform integrating resources and services related to 3D printing, including, e.g., design, 3D printers, assembly, simulation, models, software, etc. It is important to consider model library management and the online-device integration on the construction of the 3D printing service CMfg platform, due to the close relation between 3D print-ing and 3D models. 
Usually with a short budget for the initial investment, lack of experience and related technical support, SMEs are seeking novel technologies such as cloud technologies. According to Wang et al. [49], SMEs needs high level of safety and security regarding their customer＊s requirements, i.e., all data and results as to be maintain within the boundaries of the own company. These facts indicate that public or community cloud services probably are not suitable in this scenario. To ful.ll this need, Wang et al. [49] proposed a CMfg system tailored to meet the requirements of SMEs, consider-ing a hybrid cloud structure. Within this, the sensitive data stays inside a private cloud, with integrated and managed hardware and software. Moreover, the data interoperability presence of the public and private clouds is identi.ed on the multiple levels in the CMfg. 
3.3. Big Data 
Huge amount of generated data from different types, can come from interconnected heterogeneous objects [24]. This huge amount of structured, semi-structured and unstructured data can describe Big Data (BD). In order to obtain the correspondent value, these data would need too much time and money to be store and to be analyzed [50]. Bringing value opportunities to industries in the era of Internet of Everything can be achieving with the connection of more physical devices to the internet and with the use of a gen-eration of novel technologies. 
Data collection or storage characterize BD, but the core charac-teristic of BD is the data analysis and without it, BD has no much value [51]. Systematic guidance can be provided by BD for related production activities within entire product lifecycle [52], achieving cost-ef.cient running of the process and fault-free [53], and help managers on decision-making and/or to solve problems related to operation [52]. The use of BD provides a business advantage through the opportunity of generated of value-added [54]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 8. Various services in CMfg [46]. 
Cemernek et al. [55] presentedBDde.nition of the TechAmer-ica Foundation, as ＆＆a ＆＆term§ describing large volumes of high velocity, complex and variable data requiring advanced techniques and tech-niques to enable the capture, storage, distribution, management and analysis of the information§. BD demands a cost-effective, innova-tive forms of information processing for enhanced insights. Accord-ing to the researched de.nitions of BD, differing from the traditional data processing [21], the .rst suggestion to characterize BD was related in terms of Volume, Variety, and Velocity, also named as the Three V＊s. These was the three dimensions that emerged as a common framework of challenges in data manage-ment [56].To 
process continuously large amounts of unstructured heteroge-neous data collected in formats such as video, audio, text, or others [51], additionally, other dimensions have also been attempted to assign for a better characterization such as: Veracity, Vision, Volatility, Veri.cation, Validation, Variability and Value [56]. According to several authors [21,51,56每57], the description of the dimensions as follows: 
. 
Volume 每 great data volume size consuming large storage or consist of enormous number of collections. BD sizes are men-tioned in multiple terabytes and petabytes; 

. 
Variety 每 various types of data, generated from a large sources and formats variety, and multi-dimensional data .elds contents. It refers to the structural heterogeneity in a dataset; 

. 
Velocity 每 rapid production. Generation, analysis, delivery, and data creation measured by its frequency. It refers to the data generation rate and the speed for analyzing and acting upon; 

. 
Veracity 每 represents the unreliability in some data sources. Some data requires BD analysis to gain reliable prediction; 

. 
Vision 每 only a purposeful process should send data generation. The likelihood of data generation process is addressed in this dimension; 


. 
Volatility 每 a limited useful life can characterize data generated. The data lifecycle concept is addressed by this dimension. It ensures the replenishment ofthe outdated data with new data; 

. 
Veri.cation 每 conformity of the data generated by a speci.ca-tion set. It ensures the conformity of the engineering measurements; 

. 
Validation 每 the vision conformity of the data generated. Behind the process, the transparency of assumptions and connections are ensured; 

. 
Variability 每data.ow rates measuredbyits variation. Variability and Complexity was added as two additional dimensions of BD; 

. 
Value 每 through extraction and transformation, de.nes how far BD generates economicallyworthy insights and bene.ts. Value as a de.ning BD attribute. 


On manufacturing domain and at the BD process comprehen-sion, it is the engineering aspects that give value to the BD analysis using its dimensions [51]. These dimensions are dependent from each other, related with the relativity of BD volumes applied to all dimensions [56]. 
To explore data, advanced data analysis is required. Using CC through the advanced analytics, methods and tools, off-line and real-time data are analyzed and mined, e.g., machine learning, forecasting models, among others. Knowledge is extracted from the huge data number enabling manufacturers on understanding the product lifecycle various stages [50]. Moreover, the advanced analytics of BD can be used as a facilitator, identifying and over-coming bottlenecks created by IoT generated data [58]. 
The mutation opportunity from today＊s manufacturing para-digm to smart manufacturing is offered by BD [59]. Therefore, BD can help manufacturers on more rational, informed and responsive decision-making way. Manufacturing competitiveness in the global market is enhanced by these BD characteristics. Various stages in data lifecycle where manufacturing data is exploited are depicted in Fig.9 consisting on the complete manufacturing data journey. 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Fig. 9. Manufacturing data lifecycle [59]. 
According to Mourtzis et al. [58],ina framework structuredby levelsofa manufacturing enterprise,the lower level generatesdata directly from machine tools and operators. For an enterprise, this data is very important, providing precious information when used and analyzed enabling adaptivity and .exibility on the higher levels of the enterprise. 
BD analytics is an essential key to digital manufacturing, play-ing as an enabler for technologies. Moreover, the scope of mass customization focusing on the needs of individualized markets, use BD analytics as foundation [58]. 
As mentioned above, IoT data converges to BD in order to ana-lyze it and take conclusions from collected datasets. In other words, IoT data will be a part of BD [21] and BD cannot be explored further without the IoT [57]. Furthermore, CC and BD are consid-ered as a coin with its two faces: BD is seen as the absorbent appli-cation of CC, while CC provides the IT infrastructure of BD [57]. 
3.4. 
Simulation 
For the successfulimplementation of the digital manufacturing [60], an indispensable and powerful tool, the computer simulation, is becoming a technology to better understand the dynamics of business systems [61]. Manufacturing industry current challenges can be approached by this technology [62], dealing with the com-plexity of the systems, with elements of uncertain problems that cannotbe resolvedwith usual mathematical models [63].Ona cus-tomized product manufacturing environment, the value of simula-tion is remarkable and evident. Simulation allows experiments for the validation of products, processes or systems design and con.g-uration [60]. Simulation modeling helps on cost reduction, decrease development cycles and increase product quality [61]. In order to analyze their operations and support decision-making, manufacturers have been using modeling and simulation [64]. Simulation technologies already proved its effectiveness in the approach of several practical real-world problems in manufac-turing sector [65]. Mourtzis et al. [60] presented on their research, the domain areas of simulation as shown in Fig. 10 with the focus on simulation methods and tools. Simulation is de.ned as an oper-ation imitation, over time, of a system or a real-world process. It uses a system＊s arti.cial history and its observation, drawing infer-ences over the operational features of the representation of the real system. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

Simulation modeling is the method that makes use of a real models or imagined system models or imagined process models. It helps ona better estimating and understanding the modeled sys-tems or process through its behavioural analysis [61]. A model is an entity (generally a simpli.ed abstraction) used to represent other entity with a particular de.ned purpose [66]. Simulation modeling allows to gain insights into complex systems by the development ofcomplex and versatile products and make possible to test new concepts or systems, resource policies and new operat-ing before its real implementation, allowing to gather information and knowledge with no interference on the actual running system [60]. The Fig. 11 shows types of simulation models discussed by Mourtzis et al. [60] regarding to the classi.cation, dimensions, and differences. 
Choose and develop the best suitable type of simulation model to represent the real system is a multiparameter decision, e.g., sta-tic models for modelling a structure without activity and dynamic models for investigating the behaviour of a system evolving through time [67]. 
Simulation have been playing a spotlight role in design evalua-tion (referred to as off-line) and operational process performance (referred to as on-line) during a manufacturing system [65,67]. 
Its usual the existence of making long-term decisions on the design process [67] in, e.g., facility layouts, system capacity con.g-urations, material handling systems, .exible manufacturing sys-tems and cellular manufacturing systems [65]. Simulation runtimein off-lineisnot signi.cantonthe simulation process, offer-ing the advantages to study and analyze the what-if scenarios [67]. 
On the operational process of the manufacturing system, e.g., manufacturing operations planning and scheduling, real-time con-trol, operation policies and maintenance operations [65], the decision-making is short-term, making the simulation runtime a very important aspect. On-line simulation relates the number of entities belonging to the production system, the number of its gen-erated events, the activities complexity and simulation time hori-zon. If the IT system is integrated with the on-line simulation, for instance, it＊s possible to own the capacity to estimate the future shop .oor behaviour and to emulate and/or determinate the man-ufacturing system logic control [67]. 
Optimal or near-optimal system design is the goal for decision makers. This optimization is possible due a systematically search on a wide decision space without restrictions or pre-speci.ed requirements. This simulation optimization tool will search for the optimal design within a given system, according to the com-


V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
puter simulation model. On dynamic and uncertain environments, this tool has the potential on optimizing control decisions and on supporting real-time decision-making. This can be possible when the required computational ef.ciency is reached [68]. Compared to conventional simulation, real-time simulation, on-line, can ana-lyze the behaviour of user and system in milliseconds, allowing the user to develop and produce ＆＆virtually§a prototype for the product or service [69]. According to Cede.o et al. [69], a real-time simula-tion is when a computer runs at the same rate as the physical sys-tem, so the simulation model needs to be feed with real-time data that can be reached using IoT. 
Ahigh-.delity simulation of a manufacturing factory is de.ned as Virtual Factory (VF). An industrial collaboration environment focusing on Virtual Reality (VR) representation of a factory [70] or an emulationfacility [71] canbe consideredaVF.TheVF vision con-siders validated real factories simulation models to generate data andtobe workedin formatsofreal conditionsinareal factory [64]. 
The new simulation modeling paradigm is based on the concept of Digital Twin (DT) [61].An ultra-high-.delity simulationis pro-vided by the DT concept and it plays an important role in I4.0. It extends simulation to all product lifecycle phases, combining real-life data with simulation models for better performances in productivity and maintenance based on realistic data [61]. 
Technologies based on simulation are the core role in the digital factory approach, allowing experiments and validation upon differ-ent manufacturing system patterns, processes and products [72]. 
3.5. 
Augmented Reality 
New challenges are coming with Augmented Reality (AR) usage in everyday [73]. Increase human performances is the aim of AR, supplying the needed information toa given speci.c task [74]. This novel technology provides powerful tools, acting as an HMI [75].AR technology can be found on a wide range of sectors, e.g., entertain-ments, marketing, tourism, surgery, logistics, manufacturing, main-tenance, etc. [76].Asa growing evolving technology, recently,AR usage is spreading to different manufacturing .elds [77]. The use of AR on manufacturing processes regarding to simulation, assis-tance and guidance has been proven to be an ef.cient technology helping on problems [78]. AR technology increase reality operator＊s perception by making use of arti.cial information about the environment,wheretherealworldis ful.lledbyitsobjects [79每 80].As long as it interacts with human senses, AR can make use of any kind of hardware [74]. Using AR can help on closing some gaps, e.g., between product development and manufacturing operation, due to the ability to reproduce and reuse digital information and knowledge at the same time that supports assembly operations [78]. Fig. 12 shows the most relevant tasks related to industrial environments and manufacturing .elds where the AR brings value. 
The principle of AR is the combination of two scenarios: 1) dig-itally processed reality with2) digitally added arti.cial objects that could be 2D .at objects, or by other de.nitions that only considers 3D objects within the scene [73]. The authors [79每80] de.ned AR system features as: 1) the ability on combining real and virtual objects on a real environment, 2) the ability on align each other the real and the virtual objects, and 3) the ability on running inter-actively, in 3D, and on real-time. 
Making use of conventional hardware, the use of AR has a big advantage that can be minimal or even zero purchase expense. Some cases, the see-through glasses component can be more expensive [73]. On industrial environment, other key advantage was pointed by Blanco-Novoa et al. [81] about the assets: AR pro-vides dynamic real-time information,soitcan suppress mostofthe paperwork. 
The AR system software might be selected based on environ-ment＊s considerations, which obviosity differ among them, e.g., on the military environment the proper use is zero-connectivity to ensure CS, differing from commercial environment that requires providing remote assistance＊s connectivity [74]. 
The essential parts of an AR system make use of electronic devices to directly or indirectly view a real-world combination with virtual elements. According to Fraga-Lamas et al. [75], these elements can be: 
. 
Image capture element 每 web camera is suf.cient [73]; 

. 
Display 每 for projection of the virtual information on the images acquired by the image capture element. Basically, three device types with optical options can be used [80,82]: 1) hand-held (video and optical), 2) head-worn (video, optical, and retinal), and 3) spatial (projector and hologram); 

. 
Processing unit 每 to generate virtual information to be projected; 

. 
Activating elements 每 to trigger the display of virtual informa-tion, e.g., sensors,QR markers, GPS positions,images, etc. 



In order the user to visualize information, these AR devices use types of optics as follows [82]: 
. Video 每 merged worlds (real and virtual) into the same digital view; 


V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 



. 
Optical 每 real world with virtual objects overlaid directly on the view; 

. 
Retinal 每 direct projection of virtual objects onto the retina with the use of low-power laser light; 

. 
Hologram 每 real world mix with virtual objects using a photo-metric emulsion; 

. 
Projection 每 projection of virtual objects directly on real-world objects with the use of a digital projector. 


Related to the quality of products, Segovia et al. [83] proposed an AR system solution to production monitoring, based on Statisti-cal Process Control (SPC) and Six Sigma methodology. It uses AR in realtime reportsto assist qualitydata reportingby monitoringCpk indexes to support the decision-making process. The AR system was linked to a Computer-Aided-Quality (CAQ) to receive data. The CAQ used was Quality Data Analysis (QDA) software that allows the user to verify quality goals. The used measurement device was wireless connected to QDA software. The QDA software generatedreports and exported them automatically in a .le to the AR application. The mobile device used to run the AR application was a tablet. Fig. 13 shows the AR technology with the inside of the facilities and the displayed Key Performance Indicators (KPI) of each workstation. According to Segovia et al. [83], one of the big-gest bene.ts of this tool is the reduction on audit times. 
Maintenance is one of the most promising .elds of AR. It enhances human performances in technical maintenance tasks execution as also supports on maintenance decision-making [76]. One example of AR in maintenance is shown in Fig. 14 on a step-by-step assembly procedure of a consumer device, using Hand-Held Display (HHD) to carry out maintenance tasks. The AR appli-cation has text description of the task on the bottom, right and left arrows to go forward and backward on the procedure. 
Other example in the use of AR technology is on the diagnostics .eld.Ameaningful example is shown in Fig. 15, also with the use of an HHD. The defects inspection and mapping on the pipe was madewitha3Dimage.The defects positionis indicatedonthepipe anditcanbeseenaclearerimageofthe natureandscaleof defects. At the end, the operator can detect, locate and mark defects using a tablet and a marker [84]. 

3.6. Additive Manufacturing 
Products and services innovations needs hard and long research work and development that I4.0 with the novel technologies such as simulation via virtual reality are enabling it. However, on the next step, there is a manufacturing process with its related costs that can be a barrier to competitiveness. Additionally, at the end, there is a dilation of product or service lead time for markets. 
The Additive Manufacturing (AM) paradigm is being increas-ingly developed and it brings into real industry, high feasible appli-cations [85]. Jian et al. [86] discussed the potential of AM on the replacement of many conventional manufacturing processes. AM is an enabling technology helping on new products, new business 

V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

models and new supply chains.A set of technologies that enables ＆＆3D printing§ of physical objects form the collective term AM [87]. Products such as one-of-a-kind, can be manufactured without the conventional surpluses, so it is a big advantage. AM technolo-gies can be referred also with other synonyms such as rapid proto-typing, solid freeform manufacturing, layer manufacturing, digital manufacturing or 3D printing [88]. With AM it＊s possible to create prototypes to allow value chain elements independence, and therefore, achieving time reduction on design and manufacturing process. 
As follows in Fig. 16,AMprocesses are classi.ed into seven cat-egories according to the standard of the International Organization for Standardization (ISO)/American Society for Testing and Materi-als (ASTM) 52900:2015 (ASTM standard F2792). 
AM technology is de.ned by Kim [85] as a process of creating a 3D object-based on the deposition of materials on layer-by-layer or drop-by-drop undera computer-controlled system. Some potential bene.ts of AM can be summarized as follows [89]: 
. 
Manufactured parts directly from CAD data .les (.nal or near .nal parts with minimal to no additional processing); 

. 
Greater customization without extra tooling or manufacturing cost; 

. 
Manufacturing of complex geometries (some geometries cannot be achieved on conventional processes, otherwise, it is achieved by splitting it into several parts); 

. 
Manufacturing of hollow parts (achieving less weight) or lattice structures; 

. 
Maximization of the material utilization for the ＆＆zero waste§ approach; 

. 
Smaller operational foot-print towards manufacturing a large variety of parts; 

. 
On-demand manufacturing and excellent scalability. 



According to Shin et al. [90],AMwork.ow includes the geome-try design, computational tools and interfaces development, mate-rial design, process modeling and control tools, and it was also discussed the AM applications .elds such as nano-scale (bio-fabrication), micro-scale (electronics), macro-scale (personal prod-ucts, automotive), and large-scale (architecture and construction, aerospace and defense). 
For the next generation of AM processes, Chang et al. [91] dis-cussed novel processes such as micro/nano scale 3D printing, bio-printing (AM of biomaterials), and 4D printing (combination of AM with smart materials (stimulus-responsive that change their shape or functional properties)) to fabricate within high resolution a complex 3D features, in multi-materials, or multi-functionalities. 
On a near future, AM technology will expand eventually to super-advanced technology areas and substitute current technolo-gies [85]. 
3.7. Horizontal and Vertical Systems Integration 
Engineering, production, marketing, suppliers, and supply chain operations, everything connected must create a collaborative sce-nario of systems integration, according to the information .ow and considering the levels of automation [18].In general, the sys-tems integration of I4.0 has two approaches: horizontal and verti-cal integrations [10,92]. Real-time data sharing is enabled by these two types of integration [16]. 
Horizontal integration is the inter-company integration [92] and is the foundation for a close and high-level collaboration between several companies, using information systems to enrich product lifecycle [16], creating an inter-connected ecosystem within the same value creation network [10,92]. It is necessary an independent platform to achieve interoperability on the devel-opment of these systems, based on industrial standards, enabling exchanging data or information [92]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 
Vertical integration is a networked manufacturing system [93], the intra-company integration [92] and is the foundation for exchanging information and collaboration among the different levels of the enterprise＊s hierarchy such as corporate planning, pro-duction scheduling or management [10,93]. Vertical integration ＆＆digitizes§ all the process within entire organization, considering all data from the manufacturing processes, e.g., quality manage-ment, process ef.ciency or operations planning that are available on real-time. By this, in a high level and .exible way, providing the small lot sizes production and customized products, the verti-cal integration enables the transformation to SF [16]. It＊s important to refer that standards must be the bases of the vertical integration [92]. 
According to several authors [16,93每96], the paradigm of I4.0 in manufacturing systems has another dimension between horizontal and vertical integration considering the entire product lifecycle. This kind of integration is based on vertical and horizontal integra-tions[93].Ina visionof holistic digital engineering,asthe natural .ow of a persistent and interactive digital model, the scope of the end-to-end digital integration is on closing gaps between product design and manufacturing and the customer [94], e.g., from the acquisition of raw material for the manufacturing system, product use and its end-of-life. The phase of end-of-life product contains reusing, remanufacturing, recovery and disposal, recycling, and the transport between all phases [95]. Fig. 17 shows the relation-ship between the three types of integration on a manufacturing system, considering vertical integration as the corporation(s), hor-izontal integration between corporations, and end-to-end integra-tion linking design, production and logistics as an example. 
3.8. Autonomous Robots 
Manufacturing paradigm is shifting rapidly production from mass towards customized production, requiring robots, for instance, as a recon.gurable automation technology. The impact on the production systems of the manufacturing companies is that this trend leads to the production adaptation for a wider product variation, focusing ideally on batch size one. Nowadays, to reach the .exibility demanded level, robots are essential on production systems [97]. Towards that, abilities on computing, communica-tion, control, autonomy and sociality are achieved terms when combining microprocessors and Arti.cial Intelligence (AI) with products, services, and machines to make them become smarter. Robots with AI, adaptive and .exible, can facilitate different prod-ucts manufacturing and consequently providing decreasing pro-duction costs [16].In addition,a robot also canbe seen as oneof the forms of AI [98]. 
Processes such as product development, manufacturing and assembling phases, are processes that adaptive robots are very use-ful on manufacturing systems [16]. It is important to refer that fully autonomous robots make their own decisions to perform tacks on a constantly changeable environments without operator＊s interaction [99]. Fig. 18 shows an overview, not strict, on the autonomous robot characterizations, considering industrial and non-industrial environments. 

Dirty or hazardous industrial applications on unstructured envi-ronments can be improved by an Autonomous Industrial Robot (AIR) or multiple in a close collaboration. Hassan et al. [100] pre-sented a multiple autonomous robot＊s collaboration approach in Fig. 19, consisting on robots with different capabilities performing grit-blasting and spray painting. 
According to Hassan et al. [100], with the deployment of multi-ple autonomous industrial robots working as a team, it＊s possible to have a larger range of manufacturing applications. Other approach in multi-robot systems can be seen in Fig. 20 during a 

Fig.19.Autonomous industrial robots performing grit-blasting or spray painting [100]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

sequence of collaborative assembly operations, dealing with robot con.gurations to grasp assembly parts and build complex struc-tures such as a chair [101]. 
Collaborative robots concept also introduces the proximity of robotswith humans [102].Onthe visionofSF, collaborative robots (cobots) and humans will work closely together. Cobots are a cat-egory of robots specially designed to interact directly and physi-cally with humans, in a close cooperation [103每104]. This is possible due to the safety existing limits on speed and forces that automatically restarts the cobot allowing to guide the cobot by hand [103]. By this, for manufacturing companies, human-robot barrier is break down offering bigger affordability and .exibility on solutions [104]. 
3.9. 
Cybersecurity 
Every year, increasingly, devices are connected to the global network: the internet. In a close future, the main source of data will be inanimate objects [105].Bythis, IoT, virtual environments, remote access, stored data on cloud systems, etc., are many open opportunities that represents increasing new vulnerabilities lead-ing to a compromised information for people and enterprises. The risk scenario becomes reality because the enterprise bound-aries are unclear and are vanishing [106]. Kannus and Ilvonen 
[107] de.ned Cybersecurity (CS) as a new term on a high level of information security, and through the word ＆＆cyber§ it spreads to apply also on industrial environments and IoT. CS is a technology laying on protecting, detecting and responding to attacks [108]. 
IoT has to be built based on safety communications on each point of the manufacturing process and safety interoperability has to be assured between facilities as basic elements of the supply chain value. I4.0 technologies must allow the creation of a safety cyber environment, bene.ting on CS. 
Direct attacks from evil persons and/or software can be hard jeopardies to Industrial Control Systems (ICS). These ICS of the industrial sectors are basically control such as Supervisory Control and Data Acquisition (SCADA), process control systems, distributed control systems, CPS or Programmable Logic Controllers (PLC) [109]. The increasing of connected devices means more possibili-ties of cyber-attacks. Benias and Markopoulos [110] discussed why industrial devices get hacked, the main reasons as follows: 
. 
Devices running for too much time (weeks or months) without updating security or anti-virus tools; 

. 
Considerable number of old controllers used in ICS networks, designed when CS was not a concern; 

. 
CS threats can enter bypassing CS measures due to the existence of multiple pathways from several ICS networks; 


. Quick spread of malware due to several ICS networks that still remains implemented as a .at network without physical or vir-tual isolation among other unrelated networks. 

I4.0 creates valuable information that needs to be protected. Information and data security are critical for the industry success. It is important that data is available just for authorized persons. Integrity and information sources must be ascertained. I4.0 has raised two demands for CS in order to secure smart manufacturing systems: Security Architecture and Security by Design. Hence, attacks, threats and malware must be automatically detected with zero-installation by the systems [106]. Manufacturing operations can be shut down by a cyber-attack, therefore, companies have money losses, but the main issue are cyber-attacks targeting sys-tems requiring safety operations and representing a serious risk for the safety of the operators [111]. Elhabashy et al. [112] dis-cussed other approach on manufacturing environments regarding to some potential attacks such as modifying product designs (related to CAD .les, tolerances), modifying manufacturing pro-cesses (Computer-Aided-Manufacturing (CAM) .les, machine parameters, used tools, tool paths) or manipulating process/pro-duct data (inspection results, indicators of machine maintenance). These attacks can delay a product＊s launch, cause the production of modi.ed products, can ruin customer trust or increase warranty costs. 
The cyber-attack could be internal and/or external source. According to Khalid et al. [113],in Fig. 21, a cyber-attack can come from an internal source such as an operator that physically access to a data port or an external source such as an outside communi-cation channel or also a wireless transmission. 
The ICS safety is time-sensitive so an automatic incident response is need it. For a variety of industrial attacks, Software-De.ned Networks (SDN) and Network-Function Virtualization (NFV) can facilitate automatic incident response. The incident response in ICS can be achieved using a private-cloud architecture (cost-effective investment). SDN and NFV makes automatic inci-dent response possible to rapidly detect and temporarily replace the failing systems with virtual implementations of those systems. SDN and NFV are technologies to improve the following aspects:1) network visibility, 2) network capabilities (enables network traf.c .ows with better management), and 3) network functions deploy-ment and control using software, instead of speci.c hardware mid-dleboxes [108]. However, the combination of SDN with NFV shows a capable approach in new defense solutions in depth for ICS [114]. 
The concept of defense-in-depth, as showed in Fig. 22, was dis-cussed by Jasen et al. [115], according to the international standard IEC/ISA-62433 with the incorporation of three measures as techno-logical, organizational, and human-centered, as multilayer approach for security ICS. Security controls at system level, net-work and plant must exist on this concept. 
Updating the implemented security controls continuously is obligatory, keeping the protection up-to-date [115], such as fol-lows on: 
. 
Device level -with the installation of new security patches; 

. 
Network level -with the .rewall signatures of new threats updated; 

. 
Plant/factory level -with the analysis and monitoring of the actual log sources. 



4. The Smart Factory of the I4.0 
According to several authors [2,4每8,116], the framework of the I4.0 is the development of the Smart Factory (SF). In conceptual terms, the SF is the heart of I4.0 [117]. CPS, IoT and IoS were assumed as the main components of I4.0 [1]. 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 


These components have very closely linked each other, enabling the SF and built on the concept of decentralized production system with a social network connecting persons, machines and resources [1]. Using cloud-based manufacturing in SF, both IoT and CPS tech-nologies converges to IoS to create, publish and share the manufac-turing processes, represented in services that could be supply by virtual enterprises [118]. 
Comparedto humans living in two worlds such as the physical and the cyber world, SF will work on the physical and on the DT, in the cyberspace. The DT will collect generated data from manual inputs and sensor networks, will process data on cyberspace and take the corrective actions on real-time to handle the physical world [29]. 
Based on the manufacturing process digitalization, I4.0 is the development of a new generation of SF＊s [24]. According to several authors [2,4,8,10],in this new generation of SF, the main key tech-nology is CPS. SF is the key feature of I4.0 and the core concept component, where vertical integration occurs, the horizontal inte-gration occurs in the SF value network and across different SF＊s, enabling end-to-end engineering integration across the entire value chain [119]. Fig. 23 identi.es the transformation technolo-giesof the current industrial productioninaSF framework. 

4.1. Cyber-Physical Systems 
Cyber-Physical Systems (CPS) has the potential to change our life with concepts that already emerged, e.g., robotic surgery, autonomous cars, intelligent buildings, smart manufacturing, smart electric grid, and implanted medical devices [120] (e.g., a pace maker in a smaller scale [121]). CPS represents the latest and signi.cative developments of Information and Communication Technologies (ICT) and computer science [120]. 
CPS is the merger of ＆＆cyber§ as electric and electronic systems with ＆＆physical§ things. The ＆＆cyber component§ allows the ＆＆physi-cal component§ (such as mechanical systems) to interact with the physical worldby creatinga virtualcopyofit.This virtualcopywill include the ＆＆physical component§ of the CPS (i.e., a cyber-representation) through the digitalization of data and information. By this, CPS can be assumed as a range of transformative technolo-gies to manage interconnected computational and physical capa-bilities [122]. CPS embraces smart elements or machines who has the augmented intelligence and ability to communicate each other to make part of planning, unique or non-repetitive tasks. These smart elements, for instance, can control the needs of workpieces, alter the manufacturing strategies for the optimal production, choose (if already exists) or .nd a new strategy all by themselves. These elements will build their own network [123].In other words, the CPS core is the embedded system to process information about the physical environment. This embedded system will perform tasks that were processed by dedicated computers. CPS model can be described as a control unit with one or more microcon-trollers, controlling sensors and actuators that interacts with the real world and processes the collected data [124每125].A commu-nication interface will enable this embedded system to exchange data with the cloud or with other embeddedsystems. CPS is asso-ciated with the IoT concept [126]. According to Humayed et al. [127], CPS mainly consists of three components such as: 1) com-munication; 2) computation and control and; 3) handling and monitoring. The CPS communication can be both wired or wireless 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 


and connects CPS to a higher level such as control systems, or lower-levels such as physical world components. The intelligence is embedded on the computation and control component with the exchange of control commands and received measures. CPS is connected to the physical world by the handling and monitoring component, using actuators to handle physical components and using sensors to monitor them [127]. 
Referring a manufacturing system and according to Keil [128], Fig. 24 shows a schematic representation of a CPS, an embedded system integrated in physical systems such as production lots or machines. The sensors collect physical data and the electronic hardware and software will save and analyze it. The interaction between data processing and other physical or digital systems are the CPS bases. it＊s also possible to identify an HMI in this CPS schematics for supervision and exchange information. 
Several CPS linked within digital networks formaCyber-Physical Production System (CPPS) [128],based on sub-systemsand autono-mous and cooperative elements linked across all levels of produc-tion [120]. According to Rojas et al. [129], CPS are the building blocksfortheSF, structuredasCPPS.The collecteddatawillbesent to BD and become accessible via CC. The CPPS interaction with the virtual world enables IoTin manufacturing [13,118].As the system are getting intelligence regarding to the so-called smart objects, the IoT creates the connect environment with smart objects to the glo-bal internet. Several authors [3,6,10,19,58,94,113,121,124,130] dis-cuss the level of cooperation and communication of CPPS in manufacturing. 
The implementation of CPPS in the SF leads to a fundamental design principle as the real-time management in industrial pro-duction scenarios. CPPS will make the automation pyramid approach on a different manner. The traditional automation pyra-mid,as showsthe Fig.25,is partly breakatthe PLC＊s level.The .eld level and control remain including closest PLC＊s of the technical processes to improve critical control loops, and the highest levels of the hierarchy will be decentralized [131]. 
In the CPS-based Automation of the Fig. 25, the squares repre-sent inputs/outputs devices, the lines represent service interac-tions and the blue, yellow, grey and black points represent the 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

corresponding functionalities of the .ve-layer architecture of the traditional automation pyramid [126]. 
Some researchers are developing a .ve C＊s structure for better analyzing I4.0. This .ve C＊s architecture can guide the development of I4.0 and it is dependent of CPS attributes. These .ve levels are: Connection Level (main attribute is self-con.gurable), Conversion Level (main attribute is early-aware), Cyber Level (main attribute is controllable), Cognition Level (main attribute is informational) and Con.guration Level (main attribute is communicable) [116,132每133]. 
4.2. Internet of Services 
Replacing physical things by services, the Internet of Services (IoS) is based on the concept that services are available through the internet so that private users and/or companies can create, combine and offer new kind of value-added services [1]. IoS can enable service vendors to offer their services on the internet. Thus, the manufacturing industry of product-oriented trend is rapidly shifting for service-oriented to enable gaining revenue through all lifecycle of a product service system. By this, high quality on products can be enable by SoA, and side-by-side, gives a strong competitive position for companies through the value-added ser-vices. IoS enables collecting product information, e.g., during its operation, for updates and for the development of new services, increasing the perceived product quality [29]. IoS is consider by Andulkar et al. [29] as the technology to monitor the product lifecycle. 

CONCLUSIONS

5. Conclusions and Outlooks 
As aforementioned, the foundations of the I4.0 are the advanced technologies ofautomation, and the ICT present across this review. Key challenge of I4.0 is to make the production systems more .ex-ible and collaborative. For this purpose, the use of enabling tech-nologies is the strategy that is behind of I4.0 paradigm. On an industrial context, each implemented technology in an individual manner will present a lower impact. On the other hand, when implemented together, it offers new possibilities to embrace the future. For instance, one of the I4.0 impact will be the elimination of monotonous work as well as physically demanding jobs. 
IoT is an in.nite world of possibilities on innovation and opti-mization, due to the combination of many advanced systems and technologies such as BD and analytics, AI, networks, clouds, intel-ligent objects, robotics, middleware, people, among others. 
The development of a CMfg service integration platform is pro-posed by Mai et al. [46] as a promising concept. It is an online tool consistingon builda processwith several sub-taskswitha seriesof modules sequentially connected each sub-task. This concept allows consumers to have customized products or even make products in the cloud. Even more, through CMfg, producers can create smart solutions to save costs and improve pro.ts.Acrucial note is the improvement of the safety and security regarding to online services that was mentioned at all examples. The develop-ment of CS technology deserves maximum efforts from all actors, since individual, professional users, and organizations that need to be safe and secured to face these rapid technological advances. 
The Systems integration of I4.0 has two major characteristics relying on vertical and horizontal integration. The vertical integra-tion of the manufacturing processes, breaks the traditional automation pyramid, focusing on distributed and collaborative architectures. The horizontal integration allows the creation of a new kind of value-added [129]. By this, there is an unavoidable surrounding of customers and suppliers that are involved just from the beginning of the product life cycle. 
A challenging scenario with the deployment of I4.0 will be the extinction of the centralized applications used in common manu-facturing environments, that leads to decentralized systems as one of the main I4.0 goals. By this meaning, distributed computing systems also plays a key role on I4.0 paradigm. It allows to save time on computing runtimes, allows working with more accurate details on smaller systems and for the overall system, and decreases the fail reaction time, e.g., if one computing system fails the others can continuing on computing. 
Providing a guideline for the interdisciplinary I4.0 technologies, the RAMI4.0 was developed, describingthe connection between IT, manufactures/plants and product lifecycle througha3Dspace. The integration of RAMI4.0 and I4.0 component (component as, e.g., a production system, an individual machine or an assembly inside the machine) close the gap between standards and I4.0 technolo-gies at the production level, leading to the emerge of CPPS [130]. 
Interoperability is one of the I4.0 design principles and can be found between BD and simulation as discussed by Shao and Jain [64]; BD on its analytics supports simulation by estimating the unknown input parameters and performing data calibration for simulation and its validation results. The return is the support of simulationfor BD analytics on various roles. Data analytics applica-tion can summarize and report production trends (e.g., product 
V. Alc芍cer, V. Cruz-Machado / Engineering Science and Technology, an International Journal 22 (2019) 899每919 

variation cycle time or throughput average). Diagnostic data anal-ysis can respond to what has happened and what is happening, identifying causes. Diagnostic analysis can take advantage using of manufacturing system＊ simulation model that emulates the cur-rent operation. Predictive analytics estimates performance based on planned inputs, e. g., product cycle time and throughput estima-tion for several products based on current policies. It will take advantage from simulation models to execute the what-if scenar-ios. Prescriptive analytics can respond to how can we make it hap-pen and what will be the consequences.It uses simulation models to improve the production performancein future periodsby emu-lating operations under paralleled realities and these plans can be improved with the arrangement of simulation and optimization models. 
In the VF level, simulation can be seen as data generator allow-ing VF to generate for instance, streams of production data and resource utilization, and feed data to analytics applications. Can be seen also as supporting evaluation and validation giving an advantage to the real factory. 
Simulation technology on I4.0, using VR, is an integral process to simulate all industrial processes, from planning, design, manu-facturing, providing services, maintenance, try-outs or even quality controls. All processes can be simulated as modular [132]. It＊s pos-sible to simulate and virtual verifya factory manufacturing process before being realized. After approved, all physicals can be done. For instance, if it is considering the combination within simulation and AM, after product simulation, the production of prototypes allows the time reduction on design and production process, by reducing the value-added dependencies. These time reductions are particu-larly relevant on customized markets. 
Grieco et al. [4] presented an interesting case study in fashion manufacturing where a decision support system as a software is developed under the I4.0 concept, aiming the minimization of: 1) orders delivered later than due date, and 2) resource overload cases. 
Many researchers discuss that the data is the raw material of the XXI century and the real world will be a huge information sys-tem. According to this, Lu [3] discussed one of the major chal-lenges in I4.0 that will be the development of algorithms for dealing with data. 
According to Salkin et al. [16], there is no speci.c I4.0 de.nition, and therefore, there is no de.nitive utilization of the enabling tech-nologies to initiate the I4.0 transformation. 
But the fact that this fourth revolution is been announced before it takes place, opens several opportunities for co-working environments between academic researchers and industrial practi-tioners, shaping on the manufacturing future [134]. 


5.1. Looking Forward 
As mentioned by Rojas et al. [129], I4.0 is on its infancy and to make it a reality, several challenges and gaps must be addressed. By this, the roadmap for the I4.0 ful.llment is still not clear to date in both academia and industry [132]. Considering .ve fundamental manufacturing systems to conceive I4.0, Fig. 26 can represent the research gaps between the current manufacturing and the I4.0 requirements [132]. These .ve manufacturing systems are systems where is hard to achieve intelligent concepts, that are the goal of I4.0 development, neither I4.0 lower or upper levels. The closest to I4.0 is the Recon.gurable Manufacturing System. 
5.2. Executing I4.0 in SMEs 
Looking at European Union, SMEs represents the backbone of the economy and the key to competitivity. Inside this enterprise dimension, special approaches must be developed to introduce and apply I4.0 technologies [129]. The enabling technologies of I4.0 are the foundation for the integration of intelligent machines, humans, physical objects, production lines and processes to form a new kind of value chain across organizational boundaries, featur-ing intelligent, networked, and agile. By this, due to the increase level of complexity, manufacturing SMEs has doubts on the required .nancial effort for the transformation technologies and its impact on their business model [135]. 
The implementation of I4.0 in SMEs can be facilitated, for instance, on a SaaS approach, enabling technology acquisition for digital services with appealing investments. A clear example can be an SME integration on the supply chain of a product, allowing collaborative of project development, collaborative working on product＊s launch and time to market reduction, shared innovation, and consequently, minimizing the related risks. 
Acknowledgments 
Authors would like to acknowledge to the reviewers for their valu-able feedback. Special thanks to Freepik for providing vectors/icons for some .gures, available at www..aticon.com. 
Declaration of con.ict interests. 

The authors declare no potential con.ict of interests at all in this paper. 
Keywords:
Wind energy
Wind turbine monitoring Wind farm monitoring Control chart
SCADA monitoring Statistical inertia

Abstract

A method for monitoring wind turbine generators (WTG) using data provided by the SCADA system is proposed. This method relies mainly upon comparing one WTG with the average of all remaining WTGs on a wind farm. Because environmental conditions on a wind farm are roughly the same over its entirety, the difference between each WTG and the average of the remaining WTGs on the wind farm is constant over time. The statistical inertia of averaged conditions for the entire farm provides a good yardstick for WTG monitoring. The results of monitoring four aspects of a WTG are presented here: these are electrical energy produced; tower vibration; nacelle yaw; and gearbox temperature. Control charts are used to detect abnormal behaviour. With regard to the electrical energy produced, one accidental activation of a curtailment algorithm was found. For tower vibration, we describe an application for the detection of rotor imbalance. For yaw, an example showing detection of nacelle drift is covered. Lastly, for gearbox temperature, the proposed methodology succeeded in detecting an issue two months prior to failure. We have included limitations as to the minimum wind farm size required in order to use the wind farm average. A centralized control chart is also proposed.

1. Introduction

During the life time of a Wind Turbine Generator (WTG), many events can affect its performance. These events can be classi?ed according to the time scale on which they occur. Faults such as blade-angle asymmetry or generator over-speed generally affect the produced electrical energy for hours. Downtime resulting from changing or repairing a principal component such as a main bearing or a gearbox can last for days, even weeks [1]. Other events such as blade erosion build up over months and years. The effect of such events on power output can also be divided in two categories: events that partially reduce the production (e.g.: icing, blade erosion) and others that stop the WTG (e.g.: faults, downtime for repairs). Preventing these events or limiting their duration is an important aspect of wind farm operation and maintenance (O&M). The ageing of WTGs is now a timely topic for the wind industry, since many wind farms in Canada and around the world have been in service for decades. With time, failure of components is more frequent and underperformances can appear. Some authors are reporting a performance reduction rate of approximately 1.5% per year [2]. Also, some operators are even considering the option of repowering, as their farms are getting closer to the end of their planned lifetimes, or as new WTG models, signi?cantly larger than the ones built decades ago, become available [3]. Thus, various monitoring method are used to improve availability of WTGs and to achieve condition based maintenance. The ageing of the wind farms also motivates interest in the great amount of data available for the development of monitoring tools.
The objective of this paper is to propose a data-driven method to monitor wind farm WTGs base on the long term, which is also robust and suitable for the industry. Here, control charts are used for the generation of alarms. Unlike most other monitoring methods, the proposed methodology can be used to monitor a wide range of WTG components or aspects. It is also simple to under- stand and use: no advanced knowledge in data mining or modeling is required. Thus, the proposed methodology is suitable for indus- trial applications. Furthermore, this method allows for the moni- toring of various aspects of a WTG simultaneously, with the help of a centralized control chart. This method is suitable for medium and large wind farms (more than 25 WTGs). Since the number of units per wind farm is constantly rising, this method is can be applicable to most cases.
First, previous work on the monitoring of WTGs will be reviewed. Then the proposed method will be detailed, followed by the results of its application to industrial wind farms. The method used here for monitoring a wind farm is based on comparing a single WTG with the average of the other WTGs on the same wind farm. The effectiveness of this approach will be demonstrated by means of four separate industrial study cases.

2. Literature review

One way of increasing the reliability and availability of wind energy is by monitoring WTGs. With proper monitoring, failures can be avoided and their consequent down-times limited, all of which increases availability. Maintenance can be planned ef?- ciently, and replacement parts can be ordered before failure occurs. There are various ways of performing the monitoring of a WTG. These can be classi?ed as follows: condition monitoring systems (CMS); Supervisory Control and Data Acquisition (SCADA) moni- toring; power curve monitoring; and fault prediction.

2.1. Condition monitoring systems

Condition monitoring systems (CMS) involve the use of addi- tional sensors that evaluate the health of WTG components. They are often based on vibration analyses and use methods such as wavelet analysis or Fourier transformations. They can achieve great precision by predicting the failure of a component months before it happens [4] [5] [6]. However, installing additional sensors can be costly in large wind farms [7]. CMS may also include oil analysis, thermography, shock pulse methods, acoustic emissions and ul- trasonic techniques, as reviewed in Ref. [5].

2.2. SCADA monitoring

SCADA monitoring is the monitoring of a WTG using the data provided by its SCADA system and does not need the use of addi- tional sensors [7]. This monitoring method is limited by the data SCADA provides. Often it is component temperature that is analyzed via SCADA monitoring [8]. In some cases, models are used in order to predict component behaviour [4] [9] and in other cases, the monitoring can be based on the signal itself (mean values, standard deviation, slopes, root mean square, spectrum, etc.) Power curve monitoring can be viewed as a subset of SCADA monitoring. Power curve monitoring is based on the relationship  between wind speed and power output. A change in behaviour of a WTG may be re?ected by its power curve [10] [11] [12] [13]. This method of monitoring can be powerful for the detection of small, progres-
sive underperformances [14].
However, power curve monitoring relies upon measurement of wind speed. According to IEC (International Electrotechnical Commission) 64100-12-1 [15], wind speed shall be provided by a met mast in order to assess the performance of a WTG as a function of freestream speed. But since wind farms have only a few met masts, nacelle wind speed is used instead in power curve moni- toring. In fact, the important point is to obtain a reliable, repeatable and representative wind speed measurement. Therefore, the power curves, using nacelle wind speeds, can be used for monitoring. However, a major ?aw in power curve monitoring is that any change in nacelle anemometry can create a considerable shift in the power curve. Fig. 1 illustrates a change in nacelle power curve following various changes in nacelle anemometry. These power curves were obtained with the bin method described in IEC's 64100-12-1 standard, while using nacelle anemonetry [15].  A noteworthy dif?culty while monitoring a WTG's components using power curve monitoring, is that often, their failure will be seen in the power output after a critical point is reached.

2.3. Fault monitoring

Another type of monitoring is the prediction of faulty behav- iours in a WTG. As de?ned by Ref. [16], a fault occurs when a parameter of a system deviates from standard conditions, such as blade angle asymmetry, component over-temperature or generator over-speed. Operational data are analyzed by means of complex algorithms in order to predict or even avoid the shutdown of the WTG [17] [18] [19] [20] [21] [22]. Faults monitoring methods can be classi?ed into two categories: model-based and signal-based [16]. In the ?rst case, a model is used to predict the value of a parameter and the predicted value is compared to the observed value in order to ?nd abnormal behaviours. For the signal based approach, fea- tures of the signal are studied. These features may be in the time domain (mean, root mean squarre, gradient) or frequency domain (spectrum) [16] [23]. Fault monitoring also includes fault-tolerant control, where a system analyses the severity of the fault  and takes appropriate action (compensation, controller recon?guration, etc) [16].
Fault prediction is especially important for offshore wind farms, where access to the WTG is more dif?cult than onshore wind farms. High frequency data (~1 Hz) must be available for use in predicting faults; often, additional sensors or complex models, or algorithms are used as well. The types of conditions the control system of a WTG uses in order to detect faulty behaviour requires input to remain above a certain threshold for a few seconds or minutes. Some work has also been done on the development of a WTG controller that can be optimized according to the conditions of the WTGs subset in order to avoid faults. Used components or component subsets may act differently from their nominal behav- iour and thus, the optimal control strategy should be revised [22]. For more on fault monitoring and diagnosis, see Refs. [16] and [23]. The monitoring methods detailed above can all be useful in the O&M of a wind farm. Operators might consider using a combina- tion of these methods, as each evaluate the condition of a different
aspect of their WTG, based on different criteria.

3. Data source
The data used to develop and test the proposed method was taken from ?ve industrial wind farms located in Canada. All the WTGs were the same model and were MW class and pitch regu- lated. Each wind farm contained over 50 WTGs. While the data was recorded at a frequency of 1 Hz, ten-minute averages are used. This averaging limits the noise in the signals and is the norm in the wind power industry, as suggested by the IEC standard [15]. The database has been in service since 2009, which is of interest for long-term monitoring. No CMS were installed on the WTGs. Available values included (online, repair, maintenance, curtailment, etc.) Because there was no mea- surement of principal-component vibration, monitoring methods using these values could not be used. Extreme values correspond- ing to obvious instrumentation malfunction have been removed from the database. Fig. 2 illustrates the data available and the data acquisition process. Here the acquisition and archiving system is the PI system from OSISoft. There is a local server in each wind farm, linked with an Internet connection to a main server. This redundancy allows to archive data even in the eventuality of connection losses with the main server and the on-site server. The
Fig. 1. Example of shifts in power curve caused by instrumentation. On the left, the effect of a faulty anemometer and on the right, the effect of the anemometer type.
Fig. 2. Available data and data acquisition process.

4. Method

The proposed method for monitoring the WTGs on a wind farm compares each WTG's behaviour with the mean of the other WTGs' behaviour on the wind farm. This method is data-driven and empirical based. No models are used.
The behaviour of a WTG constantly ?uctuates due to the various productive WTGs. However, this difference should be constant in time, the same assumptions are made in Ref. [24], where a moni- toring method using wind farm power curves is presented. Thus, the difference between a speci?c WTG and the average of the remaining WTGs on the wind farm should be statistically constant in time.
the difference with the mean of the others,, can be
expressed by the following relation in its environment (wind speed, wind direction, air density,

ambient temperature, etc.). However, all WTGs on a wind farm are
relatively affected by the same environmental variations, since weather conditions are roughly the same over the entire wind farm. Some WTG will perform better, depending on the con?guration of the wind farm. Because physical quantities such as the components and structural vibrations and component temperatures are pro- portional to power output, their values will be higher in the most

This calculation is made at 10 min intervals for which the WTG is available. The rest of the ?eet's average, or the average of all other WTGs on the wind farm, is in fact the average of all the other available WTGs (N(t)). Units under maintenance, curtailment, repair or other production limiting state should not be included in calculating the average for the remaining WTGs on the wind farm, as these are not in operation at that time.
In the case of industrial wind farms, since there are many WTGs,
the ?eet average is a robust value. Small, instantaneous variations

of statistical hypotheses, this implies the need to test two hy- potheses. The ?rst test is on the expected value for Dxi,j(t). The null hypothesis (H1) is that the expected value, E  Dxi;j  t   , is a constant
(m?0?). The alternate hypothesis (H1) is that the expected value is not or change in behaviour of any one WTG will not affect the moni- toring of the other WTGs. Fig. 3 illustrates the calculation method
for the Dxi,j(t). The minimal size required by a wind farm will be constant. discussed in Section 6.
A change in Dxi,j(t) implies a change in the behaviour of the physical quantity j of the WTG i. It is easier to detect an abnormal wind  farm's  environment  or  by any seasonal  effect  are removed
while calculating the difference between the WTG and the average of all others WTGs. Due to the stochastic nature of wind, the out-

These two parameters are not functions of time since, in the

There are several ways of testing these two hypotheses. For the hypothesis, tests on the mean of a distribution (such as the Student t-test) may be used, and for the second, a test on the normality of distribution (such as the Kolmogorov-Smirnov test) might be appropriate. Here, however, rather than using statistical absence of abnormal behaviour,

As mentioned

tests to detect a change in Dxi,j(t) at any time, we will rely upon before and illustrated on Fig. 4 for the case of tower vibration, this distribution is normal. and, as mentioned above, this bias is constant in time. The standard devi- ation is a random noise caused by the ergodicity of the signal.
The detection of a change in behaviour of the physical quantity j of a WTG i in fact detects a change in distribution f(mi,j,si,j). In terms

control charts: these are more appropriate for continuous moni- toring of a quantity. The control chart methodology will be described in the following section.
Lebranchu et al. have presented in Ref. [8] a method for the monitoring of a main bearing using the temperature difference between it and the remaining bearings on its wind farm. Here we will present its application to a wider range of components of a WTG: we will also discuss this method's limitations.

Fig. 3. Illustration of the calculation of the difference between measurement of a single WTG and average of all other WTGs on the wind farm.
Fig. 4. Example of the distribution of the Dxi,j(t). Here the case of the Difference in tower vibration between a WTG and the wind farm's remaining WTGs.


4.1. Control charts

In order to decide whether or not the WTGs have sustained changes in their behaviour, an Exponentially Weighted Moving Average (EWMA) control chart was used. Control charts are used in Statistical Process Control (SPC). A time series of the data or a sta- tistic of a sample of this data (such as the mean, standard deviation, minimum, maximum) is plotted together with control limits [25]. Control limits are de?ned by the process's history or requirements, to 0.7 provided satisfactory results. However, l 0.3 seems to be the optimal value, and therefore, all our analyses were made using this value.
 The lower and upper control limits, LCL and UCL, were calculated based on the standard deviation of the process within a reference period. Here the reference period used was the ?rst year of avail- able data,  covering  any  seasonal  variations.  These  were  are expressed as follow expressions: and whenever the data steps outside of these limits, the process is viewed as out of control and thus, statistically different from its reference. There are a few rules that can be used to determine if the process is out of control or not.


In the present case, prior to using the control chart,



e main objective of this step was to reduce noise in the data. Here, data was taken at ten minute intervals, and the objective was to detect events building up on a daily to monthly time-scale. The use of a moving average as a high-frequency ?lter helped reduce the noise and improved detection performance. The period of the moving average depended on the quantity monitored and the time-scale over which problems could occur.
After the moving average was taken, the EWMA control chart was applied [26]. The expression for the EWMA Zi,j(t), for Dxi,j(t) was expressed as follows:
with Zi,j(0) is the target value for the process or its historical mean. The EWMA acted as a high-frequency ?lter with l as a smoothing value. l is from 0 to 1 and the smaller the l, the higher the smoothing. Depending on the level of variation in the variable of interest, the optimal lambda changes. For a large, sudden change, l must be relatively high, near 0.6e0.8 and for a small, progressive variation, l the optimal l falls somewhere between 0.2 and 0.4 [27]. Various values for this parameter have been studied and values up
with s, being the variance of the quantity monitored over a refer- ence period where the process has not experienced any abnormal issue. Usually, k 3 correspond to a three sigma or 99.9% con?- dence level. The EWMA control must be applied on data that is normally distributed. This requirement was satis?ed for each application presented.

5. Results

To illustrate the effectiveness of the proposed methodology, four study cases are presented here. In each of these cases, a change in behaviour of a WTG was observed and the use of a monitoring method could have reduced production losses caused by under- performances or prolonged downtime. The produced electrical energy is not directly impacted but nonetheless present. The study cases are from four different WTGs, each on a different wind farm.

5.1. Electrical energy produced

In this case, the monitored quantity is the electrical energy produced by a WTG. This kind of monitoring could re?ect problems such as underperformances or blade erosion. For each ten minute intervals where the WTG was available, the difference in produc- tion between it and the remaining available WTGs on the wind farm was calculated. This difference was then averaged on a monthly basis. Fig. 5 illustrates the application of a EWMA control chart to the difference in electrical energy production between one WTG and the rest of the ?eet.
 Following the application of the proposed methodology, it was found that between April 2012 and October 2012, on this WTG, a curtailment algorithm was mistakenly activated following major maintenance. If the proposed methodology had been used, the control chart would have detected a change in behaviour almost immediately. Instead, this step-type shift caused production losses of approximately 800 MWh per day over the whole period.

5.2. Tower vibration

Tower vibration can provide useful information on the health of the structure or highlight rotor or yaw problems. The tower vi- bration is measured directly under the drivetrain. In this applica- tion, the difference between this WTG and the ?eet average vibration level was calculated for each ten minute average while the WTG is available. Fig. 6 illustrates a case of a sudden change in vibration level: following the replacement of one of the blades on this WTG, the vibration level suddenly increased. This was caused by a mechanical imbalance of the rotor. A couple of weeks after the change, the rotor was rebalanced, and vibrations returned to their prior level. Rotor imbalance is a serious issue that can damage the drive-train's components if not treated rapidly.
For another WTG, a progressive increase in vibrations following preventive maintenance has been investigated (Fig. 7). During the maintenance of this turbine, the yaw system was over-torqued, causing excessive vibrations each time the WTG changed its orientation.
Analyzing the tower vibration can provide useful indications of damage the WTGs sustained in terms of fatigue analysis. This can

help plan preventive maintenance on wind farms and, over the long term perspective, prolong the lifetimes of WTGs.

5.3. Yaw

 Even though a WTG is designed to face the wind at all times, in some cases, like in imposed directional curtailment, yaw or nacelle azimuth is an important value to monitor. While it is assumed that the WTG is always facing the wind correctly, the value measured is not always accurate. In some cases, for two neighbouring WTGs, the SCADA will measure and archive major differences in yaw values. In the present case, since there is a discontinuity in the yaw (passing from 360¡ð to 0¡ð), the average and the difference must be calculated vectorially. Fig. 8 illustrates this issue, where the layout of one section of a wind farm at a certain time is shown. The position of each wind turbine is shown by a dot. Line color and length is an indicator of the turbine production level (blue is the lower and red is the higher), and line direction represents nacelle azimuth, or yaw, as provided by the SCADA system. As seen in Fig. 8, one of the WTGs appears to be drifting in a direction opposite to its neighbours. This situation is caused by an error in the measurement of the yaw itself: this value is measured using a proximity switch which counts the
number of metal teeth. If the teeth are worn, or if the grease con- tains metal particles, the tooth count can be erroneous, resulting in an apparent drift in yaw. Fig. 9 presents the case of an apparent drift of a WTG's nacelle. Since 2009, the difference between the yaw of this particular WTG and the mean yaw of the wind farm's remaining WTGs has been constantly increasing.
In general, this will not be an issue, because even in cases where measured yaw is incorrect, the WTG is, in reality, still facing the wind. However, when directional curtailment is applied to a WTG, if this curtailment is intended for a certain wind direction, yaw measurement must be reliable. The WTG's performance will be reduced in the wrong direction and the WTG will be unprotected in the proper direction.
As mentioned above, since the yaw is an angle, one must be careful with the mathematical operations. Here, sums and

Fig. 5. Case study of the application of a control chart to the difference between the production of a single WTG and the mean production of the remaining WTGs on the wind farm. This example shows the detection of a badly activated curtailment.

Fig. 6. Case study of the application of a control chart comparing a WTG's tower vibration of a WTG a with the mean vibration of the wind farm's remaining WTGs. This example shows detection of a rotor imbalance.
Fig. 7. Case study of the application of a control chart showing one WTG's the tower vibration as compared to the mean vibration of the wind farm remaining WTGs. This example shows the detection of a progressively rising tower vibration. subtractions must be done vectorially. Since yaw is an input parameter for a WTG, model monitoring cannot be used here and thus, proposed methodology of particular interest for monitoring yaw.

5.4. Gearbox temperature

It has been demonstrated that some failures in gearboxes or bearings can be detected using temperature analysis [8]. In these

cases, friction becomes greater and thus transfers energy by heat. Therefore, monitoring the temperature of gearboxes and bearings can possibly warn the operator of upcoming failures.
 Here, two different case studies will be presented on the application of the differences in gearbox temperature between a single WTG and the average of the wind farm's remaining WTGs. The gearbox temperature of the studied WTG is controlled in order to maintain its temperature around 55 ¡ðC. However, if the gearbox is near failure, its cooling system capacity may prove to be

Fig. 8. Partial layout of a wind farm at a certain time. Line color indicates the wind speed and line direction indicates wind direction. (For interpretation of the references to colour in this ?gure legend, the reader is referred to the web version of this article.)

Fig. 9. Case study of the application of a control chart to the yaw difference of WTG as compared to the mean yaw of wind farm's remaining WTGs. This example shows of slow drift.

insuf?cient. In February 2012, this gearbox failed and was replaced with a new one. In the weeks before the failure, the temperature had risen and the methodology proposed here would have raised an alarm in December 2011, two months before the failure occurred, as reported in Fig. 10. For wind farms in cold climates, as the ones studied here, it is important to avoid major repairs in wintertime.
A second issue occurred during winter 2015. Each winter, since, December 2013, the temperature of the gearbox oil of another WTG was found to be lower than the average temperature of the remaining WTGs on the wind farm, as reported by the control chart in Fig. 11. It was found that the oil temperature control system of the gearbox was faulty. Although, this situation is not as important as a gearbox failure, if the gearbox is not suf?ciently cooled or is overcooled, it will suffer damage. In winters of cold climates such as in Canada, the oil must be heated. If control over



6.1. Required wind farm size

 To use the mean of the wind farm as a reference, a certain minimum number of WTGs in the wind farm is required. The mean of the wind farm must be consistent. In the case of a wind farm that has too few WTGs if an important shift occurs on one of them, the average over the wind farm will be affected signi?cantly by that shift. Thus, the statistical inertia of the mean of a population (all WTGs of the wind farm) in this speci?c case must be evaluated.
 The effect on an important shift in the behaviour of one WTG on the average of a quantity is expressed by the following: overcooled gearbox.
 The temperature control of the gearbox makes the use of physical model-based monitoring dif?cult. A model that can pre- dict the temperature of the gearbox must take into account all parameters that could affect oil temperature. However, the power
   
Let's add a shift of dxi to one of the xi. We de?ne ¦Åx as the maximum acceptable impact on the average. Thus, we can re-write eq. (11) in this inequality:

of the cooling or heating system is not measured. Thus, it is now possible with the proposed methodology to provide a model that will adequately monitor a gearbox, based on available measurements.

which can simplify to:

Fig. 10.   Case study of the application of a control chart to the difference between one WTG's gearbox temperature and the mean temperature of the wind farm's remaining WTGs.   This example shows early detection of a gearbox failure.

Fig. 11. Case study of the application of a control chart to the gearbox temperature difference of one WTG as compared with the mean temperature of  the wind farm's  remaining WTGs. This example shows oil temperature control failure.

(13)
   
We choose to accept a maximum shift in the average of 1% of x, which sets ¦Å to 0.01. As for the maximum shift size to be observed that has no in?uence on the average, we choose 25% of x. This lead

In the present case, since all WTGs are in the same operating conditions and thus, each xi are similar, we can af?rm that x and xi are in the same order (x xi ). Thus, the precedent inequality becomes: to minimal wind farm size of around 25 WTGs. This limit is a guideline and the values for ¦Å and d could be different. However, ¦Å should be small, since the aim of this calculation is to limit the variation of x. As for d, smaller values would conclude to smaller wind farm size. However, a greater d would not be appropriate since we have made the assumption that x is similar to xi.
Here, the wind farms studied have each more than 50 WTGs.

Fig. 12. Centralized control chart for a WTG.

Thus, the proposed methodology can be used for all of them.

6.2. Centralized and normalized control chart

Since this methodology is optimal for use in large wind farms, and because several physical quantities are measured on each WTG, the number of control charts or ?gures needed for analysis can be great. One way to reduce the number of control charts is to group  all  the  physical  quantities  of  a  single  WTG  onto  a singlegure. In order to do this, Dxi,j(t) must be nondimensional. This means  reshaping  distribution  to  distribution  f(0,1). average of the remaining WTGs on its wind farm. Following this comparison, control charts are used in order to determine whether or not the WTG is behaving abnormally. Various cases based on the data available have been presented in order to illustrate the effec- tiveness of this monitoring approach.
 In comparison with model-based monitoring, the use of the wind farm as a reference is a simpler method that can be used by O&M engineers. While model-based monitoring can provide better results, it uses complex algorithm such as arti?cial neural networks, random forests, principal components analyses or other data- mining methods. The method we are presenting here is suitablefor industrial applications as it is simple and robust. It could easily be implemented online to continuously monitor WTGs. Moreover, it can be used for a wide range of turbine aspects and components. However, the proposed method can only be used on large wind farms (minimum size of around 25 WTGs) and if an issue is affecting all the WTGs in a wind farms, this method will not be able to generate an alarm.

One important aspect of a monitoring method is the manage- ment of alarms. A good method must maximize the detection rate and speed of changes in the behaviour of a WTG while minimizing the rate of false alarms. This kind of robustness required can be
are the average and the standard deviation of

provided by Control Charts such as the EWMA control chart.

Fig. 12 shows the centralized control chart for a WTG, with applications described in Section 5 (nacelle yaw; tower vibration; and gearbox temperature). For the case of this control chart, the control limits are set to 3 and 3, corresponding to three standard deviation limits. For the case of this particular WTG, the control charts detect a change in yaw in autumn 2010, while the behaviour of the other physical quantities remains in control.




Keywords:
Wind turbine monitoring Wind farm monitoring SCADA data
Fault detection Condition monitoring Performance evaluation
Abstract

The monitoring of wind turbines using SCADA data has received lately a growing interest from the fault diagnosis community because of the very low cost of these data, which are available in number without the need for any additional sensor. Yet, these data are highly variable due to the turbine constantly changing its operating conditions and to the rapid fluctuations of the environmental conditions (wind speed and direction, air density, turbulence, . . . ). This makes the occurrence of a fault difficult to detect. To address this problem, we propose a multi-level (turbine and farm level) strategy combining a mono- and a multi-turbine approach to create fault indicators insensitive to both operating and environmental conditions. At the turbine level, mono-turbine residuals (i.e. a difference between an actual monitored value and the predicted one) obtained with a normal behavior model expressing the causal relations between variables from the same single turbine and learnt during a normal condition period are calculated for each turbine, so as to get rid of the influence of the operating conditions. At the farm level, the residuals are then compared to a wind farm reference in a multi-turbine approach to obtain fault indicators insensitive to environmental conditions. Indicators for the objective performance evaluation are also proposed to compare wind turbine fault detection methods, which aim at evaluating the cost/benefit of the methods from a production manager＊s point of view. The performance of the proposed combined mono- and multi-turbine method is evaluated and compared to more classical methods proposed in the literature on a large real data set made of SCADA data recorded on a French wind farm during four years
: it is shown than it can improve the fault detection performance when compared to a residual analysis
limited at the turbine level only.

Introduction

Renewable energy has been a growing sector for several years, because of the necessity to reduce CO2 emission in the near fu- ture. The electrical power produced by wind turbines has been multiplied by 10 in the last 10 years. A significant amount of the electricity consumed in the world now relies on the electric power produced by wind farms, which have to be operational all along the year. Failures may cause important production losses, mainly due to the damages they cause and the time it takes to repair, which are no longer acceptable. This calls for a drastic change  in maintenance solutions, which must switch from periodic and corrective to condition-based. One of the motivations, and a per- spective, for the work presented in this paper is to develop a fault detection procedure that can deliver information on a developing fault early enough so that it can be used for condition-based or predictive maintenance decision making: indeed an in-advance detection allows the maintenance decision-maker to better plan the maintenance operations.
From the sensor technology point of view, 3 technologies are possible to monitor a wind turbine, [1]:
Using high rate data from a Condition Monitoring System (CMS) monitoring e.g. vibration: the use of these data  for fault detection usually relies on signal processing methods aiming at identifying the signature of a fault, e.g. in the signal spectrum;
Medium rate 1-second SCADA data: these data can be used for fault detection using for example model-based fault de- tection and isolation (FDI) methods, [2];
Slow rate 10-minutes averaged SCADA data: in this case, the measured quantities are averaged on 10-minutes length windows.
Several condition monitoring systems (CMS) are available on the market [1,3每5]. They are based on vibration analysis for the most part. They require additional sensors to be placed on each nacelle to measure the vibrations of several components of the drive train. The highly sampled acceleration signals they produce must be analyzed by an expert using signal processing methods, so as to detect faults in progress. Indeed, the automatic decision systems developed still generate a high number of false alarms. These make them a costly solution to implement [6,7].
On the opposite, SCADA systems have been integrated in wind farms since the emergence of wind energy. They generate loads of cheap data containing useful information on the turbines state but the data low sampling rate, one average measure every 10 min, is not adapted to an accurate in-depth monitoring of wind turbines. However, their availability for free and the breadth of SCADA data scope have encouraged researchers to propose solutions to create SCADA based fault indicators [1,8]. The work presented in this paper is developed for 10-minutes averaged SCADA, classically recorded for production supervision purposes.
SCADA data monitoring relies on the numerous variables mea- sured mainly for production supervision purposes. Fault moni- toring approaches based on SCADA data differ in the way the data can be merged to synthesize a fault indicator residual, and can be categorized in ＆＆mono-turbine methods＊＊ or ＆＆multi-turbine methods＊＊, as proposed in [9]. Mono-turbine methods combine variables recorded on a unique wind turbine while multi-turbine methods combine variables coming from all the turbines within a wind farm or subset of a wind farm. In this work, it is proposed to use 10-minutes SCADA data in an innovative way, combining a mono-turbine (or turbine-level) and a multi-turbine (or farm- level) information processing.
Wind turbines SCADA variables are highly non-stationary be- cause of the frequent changes in operational conditions and of the variations in external conditions. Following a classical diagnosis approach [2], mono-turbine methods aim at explaining a SCADA variable evolution as a function of other SCADA variables recorded on the same turbine and thus generate residuals (unexplained part) for diagnosis purposes. Variables may be linked by causal relations
- a change in some variables induces a change in other variables-  or by similarity relations two variables evolve in the same way because they are submitted to the same excitations, [9]. The most common causal model in wind energy is the power curve [1], which expresses the link between the wind speed and the active power produced by the turbine. It can be used as a visualization tool by comparing the power curve built with data gathered during the current period with the reference curve provided by the construc- tor [10]. It can also be used to create fault indicators by measuring the difference between the produced power and its value predicted by a model using the wind speed. In order to reduce the dispersion of the power data, additional input variables may be added such as the air density, the wind direction [8,11,12], the rotational speed or the pitch angle [13]. The model may be a simple polynomial approximation, a Gaussian process model or an artificial neural network [14]. The fault indicators may be the difference at each sampling time between the measured power and the expected power or it can be a distance between a reference power curve and an on line curve built with the most current measures [8]. The limitation of this approach comes mainly from (i) the length of the training period, which has to be large in order to cover  all the wind speed and temperature ranges, and (ii) the difficulty to localize the fault once it is detected with the power curve. To address the localization issue, one solution is to split the wind turbine into smaller independent systems, such as the rotor, the gearbox, the generator, the transformer and the convertor and to build models of these reduced systems, [15,16]. One popular variable to be modeled is the temperature of some components, whose variations can be explained by changes in the operational conditions or in the outside temperature. Models explaining the temperature variations use at least the produced power, the na- celle temperature and the train rotation speed as input variables. They are learnt on data measured when the turbine operates in normal conditions. The fault indicator is defined as the difference between the actual measured temperature and the expected tem- perature, named residual. Models may be static, i.e. they use data measured at one sampling time [17,18] or dynamic, such as ARX models [19,20]. They can be simple linear regression models or more complex artificial neural networks [21每25]. Let note at this point that several published works follow a classical model-based fault detection and isolation approach (model-based FDI) to build fault indicators sensitive to faults, but robust to disturbances, for wind turbine monitoring, [26每30]. In these works, the residuals are built using state observers or parity equations, [2]. The proposed methods are usually implemented on wind turbine benchmarks, emulated by differential equations models where different faults can be simulated. They use high frequency SCADA data, recorded every second. Though the results obtained are very interesting, the problem addressed in these works is very different in nature from the one addressed in the present paper where 10-minutes SCADA data are used and the methods are validated on real data.
Another approach is to model the temperature of one com- ponent as a function of the temperature of some other reference components that should evolve in a similar way, such as the tem- peratures of two bearings but also the bearing temperature and the stator temperature [18] or the temperature of the hydraulic break and the bearing temperature [31]. Such models using similarity may be simpler than models using causal relations but they rely on two temperatures evolving in the same way.
Mono-turbine approaches merge variables from the same wind turbine to generate fault indicator residuals that are insensitive to changes in its operational conditions. However, these residuals remain sensitive to the variations in the external environment such as the wind direction, air humidity and so on. On the op- posite, multi-turbines approaches merge variables recorded from different turbines of the same wind farm in order to reduce the influence of the environmental conditions. Indeed, turbines of the same farm are submitted to the same weather conditions so variables should evolve in a similar way, somehow. [32] compares the behavior of different turbines in the same farm using curves displaying the temperature of a drive train bearing as a function of the produced power. The temperatures and powers are measured during a period of time and the curves from all the turbines are plot on the same graph. Faults can be identified visually when one of the curves deviates from the others. [10] measures the difference between the power curve given by the manufacturer and an actual power curve built using data gathered during a current period of time. The difference between the two curves can exhibit a loss in performance. The differences measured for all the turbines in the farm are compared one with each other to detect a turbine with a larger loss of performance, which can be due to a fault. [33] compares the evolution in time of the temperature of a component normalized by the external temperature for turbines from the same farm. The deviation of the temperature of one turbine from the others can be the symptom of a fault. [17] builds residuals from the differences in the normalized temperatures and concludes that changes in operational conditions can create themselves fluctua- tions that are too large to allow for reliable fault detection.
The literature review shows that most authors proposed meth- ods to build fault indicators using mono-turbine approaches and so get rid of the influence of the operating conditions on the fault indicators. Few authors adopt a multi-turbine approach that allows getting rid of the influence of the external conditions, and mostly as a visualization tool. A method to synthesize a fault indicator for each turbine in a farm by comparing the temperature measured on a turbine to a farm reference (average or median of temper- atures measured on all the turbines within the farm) has been proposed in [34] and used in [35] on other types of measured SCADA variables. It has been shown that such an indicator remains sensitive to the operating conditions, which can be different from one turbine to another. Hence, no solution able to get rid of the influence of both the operating conditions and external environ- ment has been proposed thus far. Moreover, as stated by [36],  in their extensive review on wind turbines condition monitoring using SCADA data, there is a lack of published performance metrics to properly evaluate the advantage of one method from the others in terms of false alarm, true failure prediction and normal behavior prediction. To address these issues and fill this gap, we propose in this paper a hybrid multi-level synthesis method to take benefits of both approaches 每 mono- and multi-turbine 每 and to build fault indicators combining the two approaches. At the turbine level, residuals obtained with a mono-turbine model learnt during a normal condition period are first calculated for each turbine. At the farm level, these residuals are compared to a wind farm residual reference, in a multi-turbine approach. The use of mono- turbine residuals enables the influence of the operating conditions to be reduced while the use of a wind farm reference enables the changes in the environmental conditions to be accounted for. The performance of the method proposed is evaluated and compared to methods proposed in the literature on a large data set made of SCADA data recorded on a French wind farm during four years. Objective performance evaluation metrics are also proposed to compare the methods, which aim at evaluating the cost/benefit of the methods from a production manager＊s point of view. Fault indicators should warn of a progressing fault early enough for a maintenance operation to be scheduled. However, they should not generate false alarms with the extra cost of sending a maintenance operator team on a remote site for no reason.
The contribution of the work presented in this paper is then
twofold:
The first and main contribution of the hybrid multi-level approach proposed in this paper lies in the online real-time comparison of the prediction residual computed on one tur- bine (i.e. at the turbine level, using a turbine normal behavior model) to a farm-level reference prediction residual, com- puted using the prediction residuals from the neighbor wind turbines. This approach is said ＆＆hybrid multi-level＊＊ because

Second, the other contributions of the work presented in this paper are (i) the definition of objective performance metrics for the fault detection consistent with the applicative requirements of a wind farm manager and (ii) using these performance metrics, the evaluation of the proposed fault detection approach on a 4 years real data base from wind farm with 6 turbines, which shows that it has better performance than a residual analysis limited at the turbine level only.
The outline of this paper is as follows. Section 2 first presents different possible methods for the generation of fault indicators and focuses on the proposed combined mono- and multi-turbine approach developed in this work. The objective performance met- rics considered for the evaluation of the fault detection perfor- mance are also detailed in this section. The data set used to analyze the performances is described in Section 3 followed by a presenta- tion and a discussion of the results obtained.

2. Method
This section presents the proposed two-levels methodology to synthesize fault indicators by combining both a mono-turbine and a multi-turbine approach. Each of the two levels is described: synthesis of the fleet reference at the farm-level, and elaboration of a relevant turbine-level variable for comparison to the fleet reference. Finally, the performance evaluation metrics, which are used latter in Section 3 to compare the obtained fault detection results with different fault indicators, are introduced.
2.1. Fault indicators synthesis

2.1.1. Farm-level: comparison of the turbine variables to a fleet refer- ence
In wind farms, turbines are part of a fleet: they are of the same make and are subject to the same environmental conditions (wind speed, external temperatures, . . . ). Thus, the SCADA variables monitored on turbines from the same farm should evolve in a similar way when the turbines operate under normal conditions. The rationale for the method proposed in this paper relies on this assumption. SCADA variables from different turbines are compared on line. A deviation between a variable computed from a turbine and a farm reference is an indicator of an abnormal behavior. Its general concept is presented in Fig. 1.
 SCADA variables or model residuals computed from SCADA variables are recorded on line, averaged over a given time period and compared to a fleet reference. The distance between the vari- able monitored on a wind turbine and the fleet reference serves as a fault indicator for the turbine.
Let V be a variable measured on a wind turbine or a model residual synthesized on a wind turbine. V is assumed to carry in- formation on the turbine deterioration, and several ways to choose this variable V are considered in Section 2.1.2. Vj(k) is the value of the variable V from turbine j at time k. Let NT be the number of turbines in the wind farm. Let Win be an analysis window of size W . For each turbine j, with j varying from 1 to NT , a fault indicator Fj(k) is built at time k, as follows:
1. For each turbine j in the farm, an averaged value of the turbine-level variable Vj is calculated over the NWj samples present in the analysis window Win:
k

it combines a mono-turbine (or turbine-level) step with a multi-turbine (or farm-level) step. To the best of the authors＊ knowledge, no other existing and published method proposes to compare online in real-time the prediction residual gener- ated on a given turbine to a farm reference synthesized from the prediction residuals generated on the neighbor turbines.

Fig. 1. Multi-turbine approach: generic principle for the synthesis of fault indicator Fj for wind turbine j. The choice of the variable V as a mono-turbine residual corresponds to the combined mono- and multi-turbine method proposed in this paper.




the number of samples NWj is smaller than NWL, V j(k) is not computed as it would not be representative enough.
2. The fleet reference V fleet (k) is calculated using the averaged values V j(k) at the turbine level, as follows:

The fleet reference is calculated using the median and not the mean, so as to make it insensitive to abnormal values generated by faulty turbines. For the practical implemen- tation, the fleet reference is computed if the number of averaged values V j(k) computed at time k is higher than a limit number NTL (i.e. information is available from enough turbines) and the reference should be representative of the farm normal conditions as long as more than half of the turbines used to calculate the reference operate in normal conditions.
3. A fault indicator Fj(k) is then calculated for each turbine j as the distance between the monitored variable Vj(k) and the

In practice, NWL  is set to  W  + 1, which means that the variable  V j(k) should be available during at least half the analysis window W . NTL is set to NT  1 , which means that V j(k) should be available for more than half the turbines of the farm for the fleet reference   to be computed.
2.1.2. Synthesis of a turbine-level variable for comparison to the fleet reference
At the turbine level, the variable V used to create the fault indicator may be of two kinds: it can be either a residual generated as the difference between a measured value of SCADA variable and its prediction by a normal behavior model, or in a more straight- forward way, it can be directly a measured SCADA variable, Fig. 1.
The variable V may be a residual, computed as the difference between a SCADA variable and its prediction by a normal behavior model. In this paper, in the same logic as using a component temperature as a fault indicator, the difference between the measured temperature of a component and its temperature predicted by a physics-related model is used. In the generator, the active power produced by the turbine, the rotation speed of the drive shaft, and the nacelle temper- ature can account for a change in a component temperature.

So a linear model relating these variables to the component temperature is proposed, Eq. (4)

with Tj(k) the component predicted temperature, Pj(k) the active power produced, ?j(k) the shaft rotation speed, Tnacellej
(k) the nacelle temperature, recorded by the SCADA system
and a, b, c and d constant parameters. To built a normal behavior model, the parameters a, b, c and d of the model are regression coefficients learnt using a classical least squares algorithm [2], on data gathered during periods when the turbine is operating under normal conditions. The variable V is then taken equal to the temperature residual Rs taken as the difference between the actual measured temperature and its predicted value, i.e. for turbine j at time k:

The normal behavior model (4) expresses normal tempera- ture variations due to the turbine producing electric power. Thus, the model is valid only when the turbine is in operat- ing conditions and, for the practical implementation, all the samples gathered when the power produced by the turbine is below a production threshold Thp , i.e. when the turbine is stopped, are removed from the analysis window Win.
This hybrid ＆＆mono-multi-turbine＊＊ approach consisting in generating a fault indicator by comparing the residual gen- erated for each (＆＆mono＊＊) turbine to a (＆＆multi-turbine＊＊) farm reference is the original fault indicator synthesis proposed and defended in this paper for its good performance and lower sensitivity to both internal (operating conditions) and external (environmental variations) influences. The main steps for the implementation of this fault indicator synthesis are sketched in Algorithm 1.
In a more direct way, the variable V may be a SCADA variable, relevant in itself as a fault indicator, such as a component temperature. As seen in the literature -e.g. [18,31]-, compo- nent temperatures are commonly used as fault indicators. An overheating component can be the sign of a mechanical prob- lem or of a cooling system failure. In this case, the variable V is taken as the monitored temperature T of a considered component, i.e. for turbine j at time k:

Reading or sensors errors may affect SCADA measures. Thus, in the practical implementation of this fault indicator, any temperature value outside an acceptable range is removed from the analysis window Win.
This approach based on the direct comparison of SCADA data of each individual turbine to a farm reference has already been proposed and assessed in [34] for temperature mea- surements, and extended to other kinds of measurements in [35]: it has been proved too sensitive to the influence of possibly different operating conditions between each indi- vidual turbine. It is considered in this paper for comparison purposes with the proposed combined mono- and multi- turbine method.


Algorihm 1: General algorithm for the combined mono- and multi-turbine fault indicator generation


Phase 1 - Offline learning phase

Phase 2 - Online implementation phase 

2.1.3. Fault detection based on the generated fault indicators
The occurrence of a fault on wind turbine j leads to a change in the corresponding fault indicator Fj. Ideally, a fault indicator should be sensitive to a fault occurrence, but insensitive to the vari- ations of the farm environment and of the wind turbine operating conditions: building such a fault indicator was the motivation to propose a combined ＆＆mono-multi-turbine＊＊ approach. Under these assumptions, the statistical properties, in particular the expected value, of the fault indicator Fj changes with a fault occurrence, and the fault detection can be performed using statistical hypothesis testing tools (e.g. testing  hypothesis H0	E(Fj)	?0j vs. H1 E(Fj)	?0j), and can be implemented by setting a threshold on Fj. In theory, in this classical setting, when no information is available on the properties of the fault indicator Fj in presence of a fault, the value of this threshold can be determined using the estimated or assumed statistical properties of Fj when no fault is present in order
to guarantee, for example, a false alarm rate. In our approach, we want to avoid any additional hypothesis on the distribution of Fj under the ＆＆no fault ＊＊ assumption, which could be different for each turbine and could possibly lead to a different threshold for each turbine. Consequently, in the following, our approach is to set the detection threshold for the whole wind farm, on the basis of the whole available historical 4-years database for the farm in order to get a given false alarm level.

2.2. Performance evaluation indicators

Fault indicators are to be used in wind farms to assist mainte- nance operators. They should allow the detection of faults occur- ring on a wind turbine without generating too many false alarms. However, in the literature on wind turbine fault indicators, no paper addresses the issue of the performance evaluation of the indicators proposed. Most of them merely report the ability of their indicator to detect a particular fault. In this paper, we chose to evaluate the fault indicators proposed from a wind farm produc- tion managers point of view. Fault indicators should definitely be able to detect faults on wind turbines, to avoid a major degradation and then a costly repair. However, they should be able to do it early enough for a maintenance operation to be scheduled. Another major issue is the cost of false alarms. A false alarm results in a useless maintenance action: a maintenance team has to be sent to the production site to inspect the supposedly faulty turbine. Because wind turbines are usually located in remote sites, the inspection takes time and consequently costs a lot of money. So, in this paper, we propose 3 performance evaluation indicators, able to make a fair and objective assessment of the indicator usefulness: the detection time before failure, the useless maintenance action number and the indicator persistence, which gives to the operator some indications about the relevance and performance of the fault detection process based on the proposed indicator.
Performance evaluation indicators are computed from a data base gathered on a wind farm during a period of length D, where faults occurred. The data base is previously split into normal con- ditions periods and faulty periods. A normal conditions period is a period of time where no fault occurred on a wind turbine. A faulty period is a period of time where a fault was continuously present on a turbine. It ends when the failure occurs, i.e. when the turbine stops functioning.
Let Th be a detection threshold, set on the fault indicators Fj(k). When the indicator exceeds Th, an alarm is raised. Th is set for the whole wind farm, i.e. the same value is set for all the turbines fault indicators, Fj(k) .

2.2.1. Detection time before failure
The detection time before failure measures the time separating the first time the fault indicator exceeded the detection threshold Th during a faulty period from the failure time, i.e. the end of the faulty period. It gives an estimation of the time left to the maintenance team to repair the fault before the failure.

2.2.2. Number of useless maintenance actions
A false alarm occurs when the fault indicator exceeds the de- tection threshold Th during a normal conditions period. The false alarm rate is converted into an equivalent ＆＆number of useless maintenance actions＊＊ so as to better consider its cost. A ＆＆false alarm day＊＊ is a day when at least one false alarm occurred on at least one of the turbines. A ＆＆false alarm period＊＊ is a period made of consecutive false alarm days. The shorter possible length for a false alarm period is one day. During a false alarm period, a maintenance team has to be sent once to the wind farm location. So each false alarm period results in a useless maintenance action. The useless maintenance action number is the number of times a maintenance team has to be sent to the wind farm during the period D; it is equal to the number of false alarm periods.

2.2.3. Indicator persistence
The persistence measures the percentage of time during which the fault indicator remains above the detection threshold Th during a faulty period. A persisting indicator makes the maintenance operator more confident in the occurrence of the fault than an indicator constantly being set on and off.

3. Results and discussion

3.1. Data base

The methods presented in Section 2 are applied on a set of real data, gathered on a French wind farm located in the south of France. The 6 wind turbines forming the farm are identical. They are of the same make, conceived to produce 2 MW, with a horizontal axis. SCADA data were recorded every 10 min during 4 years, from November 2009 till December 2013.
During this 4 years, 6 single faults affecting the generator oc- curred on different turbines at different times, some of them gen- erating a major failure and, consequently, the machine shutdown for several weeks, see Fig. 2:
Fault on two bearings: 2 bearings broke down on 2 genera- tors, because of a lack of lubrification. One of the generators had to be replaced. These 2 episodes are named ＆＆faulty bear- ing WT6 (FB_WT6)＊＊ and ＆＆faulty bearing WT9 (FB_WT9)＊＊, for faulty bearing on wind turbines 6 and 9.
? Faults on two stator windings: two generators were stopped

The residual Rsj(k) is generated using Eq. (5), with Tj the predicted bearing or stator temperature using the normal behavior model in Eq. (4). The temperature variations due to changes in the power production are accounted for by the model. The model is learnt for each turbine, during a normal conditions period of two months at the beginning of the 4 years recording. Whenever a fault occurs on a turbine, the turbine model is learnt just after the necessary repair is made, on a fixed period of 2 months.

This model residual Rsj(k) is then averaged to obtained Rsj(k), using Eqs. (1) with Vj Rsj ;
The average residual Rsj(k) is finally compared to the fleet reference Rsfleet (k) computed using Eq. (2) with the averaged residuals from all the turbines, to obtain the fault indicator RESmultij (k).
3.2.2. Implementation of classical fault detection indicators for com- parison
   For each turbine j, three other fault indicators are considered: two of them are generated using a mono-turbine approach, the third one is generated by a simple multi-turbine comparison of the raw measurements (no residual generation at the turbine level).
The two indicators Tmonoj (directly based on raw measure- ments) and RESmonoj (based on residuals) are implemented following a classical mono-turbine approach, i.e. using SCADA data recorded from the turbine j alone:


an over-heating period which damaged the winding insu- lating material. The generators had to be replaced. These episodes are named ＆＆faulty winding WT9 (FW_WT9)＊＊ and ＆＆faulty winding WT11 (FW_WT11)＊＊.
? Faults on two cooling systems: a fault occurred on the cooling

either the bearing temperature or the stator tempera- ture.
In this case, the SCADA temperature is averaged over
24 h and thresholded, in a way very similar to the traditional SCADA detection system.
loose. These episodes are named ＆＆faulty cooling system WT7 (FCS_WT7)＊＊ and ＆＆faulty cooling system WT10 (FCS_WT10)＊＊.
The SCADA data used to implement and compute the fault indicators are the bearing temperatures, the stator temperatures, the nacelle temperatures, the active power produced and the shaft rotational speed on the 6 turbines.

3.2. Implementation of fault indicators
Four fault indicators are implemented and compared using the performance evaluation indicators presented in Section 2.2 calculated on the 4 years of data: the first considered fault indicator corresponds to the proposed combined mono- and multi-turbine approach, the three others are more classical and are implemented for comparison purposes. Each fault indicator is calculated every 10 min, using a sliding window of size W 144 samples, i.e.
24 h. NWL  is thus set to 72 and NTL  to 4. When relevant, the    limit production threshold Thp, above which the temperature can be predicted, is set to 50 kW, mainly because production levels below 50 kW correspond to starting or shutting down transient behaviors. In the following, depending on the considered fault to monitor, Tj can be either the bearing temperature or the stator temperature.

3.2.1. Implementation of the proposed combined mono- and multi- turbine approach	     	     
The indicator Rsfleet (k) is computed for each turbine j, following the proposed combined mono- and multi- turbine approach, see Algorithm 1:

Tj(k) Tj(k) where Tj can be either the bearing temper- ature or the stator temperature.
In this case, the residual generated using Eq. (5) and the model in Eq. (4) is averaged and thresholded. The temperature variations due to changes in the power production are accounted for by the model. The model is learnt for each turbine, during a normal conditions period of two months at the beginning of the 4 years recording. Whenever a fault occurs on a turbine, the turbine model is learnt just after the necessary repair is made, on a fixed period of 2 months.
An indicator Tmultij (directly based on measurements) is im- plemented using a multi-turbine approach, using a fleet ref- erence

	
每 Tmultij (k)   T j(k)   T fleet (k), using Eqs. (1) and (2) with Vj Tj, where Tj is the bearing temperature or the stator temperature. The component temperature is averaged and compared to the fleet averaged temperature refer- ence.

3.3. Performance results

Fig. 3 shows the evolution of the 4 fault indicators during the four years when the SCADA data were recorded on the 6 turbines of the wind farm. The upper figures display the evolution of the mono-turbine indicators (temperatures on the left, residuals on the right), the lower figures the multi-turbine indicators (temper- atures on the left, residuals on the right). The detection thresholds corresponding to a useless maintenance action number of 10, 20 and 30 (i.e. a total of 10, 20 or 30 useless maintenance actions

Fig. 2. Occurrence of failures and ＆＆failure periods＊＊ on the 6 wind turbines of the considered farm during the 4 years recorded database - Bearing failures in red, cooling system failures in green and windings failures in blue. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)

Fig. 3. Evolution of the 4 indicators for the 6 wind turbines over the 4-years period. The colored shaded areas correspond to the failure periods shown in Fig. 2. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article. will be generated on the whole farm during 4 years) are also displayed. The faulty periods are delimited by vertical lines. The five faulty periods correspond to the following faults, in chronolog- ical order: FB_WT6, FB_WT9, FCS_WT7, FW_WT11 and FCS_WT10 and FW_WT9 during the same faulty period. One can see that the seasonal variations inducing a change in the external temperatures are visible on the mono-turbine fault indicators. Both the tempera- tures and residuals are higher in summer and lower in winter. The fault indicators monitoring the 6 turbines follow the same pattern. During faulty periods, abnormal increases in temperatures can be seen on the faulty turbines. However, the temperature increases are somehow buried in the variations observed in the temper- atures during normal conditions periods. The faulty indicator to normal indicator contrast ratio (defined in a similar way as a signal to noise ratio: the energy of the indicator during a faulty period on the energy of the indicator during a normal condition period) is rather low, which makes the detection of the fault with a minimum amount of false alarms using Tmonoj quite difficult. The evolution of RESmonoj is rather similar. Since it is a residual, its evolution varies around 0. Its standard deviation is reduced compared to Tmonoj since the variations in temperature due to the change in power production are accounted for by the model and thus removed but its mean remains affected by seasonal temperature variations. Abnormal changes due to faults are more visible on the indicators but the fault detection using a fixed detection threshold seems still difficult to do.
The use of a multi-turbine approach makes it possible to remove the external temperature variations (seasonal effects) affecting the mono turbine indicators since the temperature variations are also present in the fleet reference and  consequently  subtracted  from the residuals. The faulty indicator to normal indicator contrast ratio seems to be increased and it is all the more so when RESmultij  is used, since the residual is rid of variations due to external temper- ature changes and variations due to changes in power production. Whenever a fault occurs on a turbine, it is preceded by an increase in the corresponding turbine residual, which clearly stands out from the other residuals. A fault detection strategy using a fixed detection threshold seems much more feasible.

Fig. 4. Performance characteristics curves: advance detection time as a function of the number of useless maintenance interventions for 6 different faults: FB_WT6, FB_WT9, FCS_WT7, FW_WT11, FCS_WT10, FW_WT9.

To compare the four indicators performances, two performance curves plotting the persistence or the detection time before failure as a function of the useless maintenance action numbers are built for each of the 6 faults. The curves are similar to traditional ROC curves, [37]. For a given value of the detection threshold, the detec- tion time before failure (advance detection time), the persistence and the useless maintenance action number are calculated and form a point in the performance curves.
The two performances curves are displayed in Figs. 4 and 5. The performance curves of multi-turbine indicators based detection are displayed in plain lines and mono-turbine ones in dotted lines. One can see that for the faults FB_WT6, FB_WT9, FW_WT11 and FW_WT9, the detection implemented with multi-turbine indica- tors outperforms the detection based on the mono-turbine ones, as could be expected from Fig. 3. Using these multi-turbine indicators allows detecting the faults earlier with a reduced number of useless maintenance actions. If the detection threshold is set at its highest value, i.e. set to obtain zero useless maintenance actions, with RESmultij  it is still possible to detect all the faults at least 500 min
ahead of time i.e. about 4 days. If the threshold is decreased to allow
3 useless maintenance actions, the faults can be detected as early as 1200 min ahead of time i.e. about 8 days, which is sufficient to plan a maintenance intervention. When mono-turbine indicators are used, FB_WT6 and FW_WT11, two major faults, cannot be detected. The detection time before failure remains close to zero whatever the value of the detection threshold. FB_WT9 and FW_WT9 can be detected two weeks ahead of time, with RESmono or Tmono, but at the very high cost of more than 15 useless maintenance actions. The two faults where the mono-turbine indicators performances equal the multi-turbine indicators are the faults on the cooling systems, FCS_WT7 and FCS_WT10, which occurred suddenly and generated a sudden and large increase in the recorded temperature.
The detection time before failure as a function of the useless maintenance action number curves show that RESmultij performs slightly better than Tmultij . The performances are comparable on

FB_WT6, FW_WT9, FCS_WT11, FCS_WT7 and FCS_WT10 but improved with RESmultij on FB_WT9. However, the persistence as a function of the useless maintenance action numbers curves shows
the superiority of RESmultij . Indeed, even if the detection time be- fore failure is equivalent, the persistence is globally higher when RESmultij is used. This is an important criterion because the higher the persistence is, the higher the confidence the operator may have in the indicator. Based on this criterion, RESmultij clearly outper- forms all the other fault indicators on the considered real data set, especially the mono-turbine ones. This is illustrated in Fig. 6 where a zoom in time is made on the period preceding FB_WT9. Indicators are presented in the same order as in Fig. 3. The colored lines are the detection thresholds which set the number of useless maintenance actions at 10, 20 or 30. Their value is adapted to each indicator. One can see that, though a slight increase in Tmonoj can be visually observed during the period preceding the fault, this increase cannot be detected with a fixed threshold. RESmonoj can detect the fault but only because it exceeds the detection threshold for a very short period of time. This could be figured out from the persistence curve whose value remained close to zero. On the contrary, the multi-turbines indicators show an obvious increase, which can be easily detected. The fault symptom appears earlier in RESmultij and the indicator remains more often above the detection threshold, which results in a higher persistence.

4. Conclusion

In this paper, we propose a new hybrid multi-level strategy to build fault indicators to monitor turbines within a wind farm, following a combined mono- and multi-turbine approach. The approach proposed reduces the effect of changes in the operational and environmental conditions on the fault indicators. Residuals computed from a mono-turbine model are compared to a fleet reference residual to form a multi-turbine fault indicator. The mono-turbine residuals are less sensitive to the internal variations because the SCADA temperatures are first processed using a model explaining the variations of the temperature in function of the operational conditions. Rises in the temperature due to increases in the power produced or in the rotational speed are accounted for by the model and removed from the residuals. The mono-turbine residuals are further post processed in a multi-turbine approach, which enables the impact of the variations in the environmental conditions to be reduced. Indeed, turbines in a same wind farm are part of a fleet. They are of the same make and model, and their variables evolve in a similar way since they are submitted to the same environmental conditions. New performance evaluation criteria were proposed to analyze the indicator performances for fault detection purpose, keeping in mind the cost of any useless maintenance action. The performance of the combined mono- and multi-turbine fault indicators were compared to more tradi- tional mono-turbine indicators on a data base made of SCADA data recorded every ten minutes from a real wind farm on a four year period. As far as we know, this is the first time a paper presents such an extensive analysis of fault indicators performance for fault detection, on real data with a wind farm production manager point of view. The results clearly showed that the effects of changes in both environmental and operational conditions on the combined mono- and multi-turbine residuals were reduced during the fault- free periods. This results in the combined mono- and multi-turbine fault indicators being able to detect all the faults occurring on the farm with a limited number of useless maintenance interventions and with an increased persistence.

Optimal Time Interval Between Periodic Inspections for a Two-Component Cold Standby Multistate System
Abstract¡ªThe establishment of the optimal time interval be- tween inspections for multistate redundant systems considering availability and costs related to maintenance and production losses is a challenging issue. This paper extends previous research for redundant multistate systems where the time-to-repair cannot be neglected. Discrete-time Markov chains are used to define transition probabilities between the different system states and the costs related to each transition. To optimize the time interval between inspections, the total cost is minimized utilizing the Markov chains properties followed by a numerical search technique. Two models are analyzed and numerical examples are presented. System I is a binary system with cold standby redundancy and component repair, while system II is a multistate system with cold standby redundancy and component repair. The main contribution of the method presented in this paper is the establishment of the optimal time interval between inspections for cold standby systems comprised of components that have different levels of degradation and where the component state can be determined only through periodic inspections. Systems with these characteristics are widely applied in industry, but are still not fully modeled in the literature.
I.INTRODUCTION
    Although time-based preventive maintenance is a strategy widely applied in industry, it is known that this maintenance approach leads to repair and replacement of components before the end of its useful life, increasing maintenance costs. This fact contributes to foster the interest of organizations and research in the development of models that allows for the use of preventive maintenance based on condition, where different states of system degradation can be observed and the best state for repair and replacement can be established. These systems are called multistate systems (MSSs).
    MSSs with different degradation states allow for more accurate component aging prediction, being more realistic than approaches that consider binary states (operating or failed) components. MSS can have several degradation levels, from perfect operation to complete failure. Some examples of MSS are hydraulic bombs in petroleum refineries, software systems, power generation systems, and train engine bearings. MSS may consist of a single component or many components. As the number of states and number of components increases, reliability analyses become much more complex [1].
    There are situations where MSSs have one or more components in a cold standby position. In these cases, if continuous monitoring is not possible, periodic inspections are recommended. These inspections verify the state of each component at predetermined times and initiate component repair if a high level of degradation is detected. The time between inspections should be optimized to maximize availability and to minimize costs [2]. Frequent inspections can serve to improve the availability, but involve high costs of preventive maintenance. On the other hand, lengthy times between inspections reduce total costs of inspection, but increase the costs of corrective maintenance and downtime, since there can be a long period when the system is unavailable [3], [4]. The establishment of the optimal time between inspections is essential to ensure the required availability with the lowest cost possible.
    Despite recent growth in the number of papers addressing this topic, redundant MSSs have been studied since the 1970s. In one of the first works about this theme, El-Neveihi et al. [5] developed a basic theory for the study of systems with finite number of states. Even though Karpinski [6] established the optimal time interval between periodic inspections for an MSS of a single component using Laplace transforms, most research about MSS consider continuous monitoring, as can be seen in Zhang et al. [7], Hsieh and Chiu [8], and Sheu and Zhang [9]. Also, these approaches study two main problems: 1) the optimal redundancy allocation for system reliability [10], [11], and 2) the optimal time interval for component replacement [9], [12], [13].
    Some authors analyzed the reliability of systems with different levels of degradation, but do not classify them specifically as multistate. These authors based their studies on Markov chains methods [14], [15]. However, it was only after the study presented by Levitin and Lisnianski [10], [16] that many studies on redundant MSS emerged utilizing the universal generating function (UGF) method.
    At the first moment, these authors developed a method for the joint optimization of redundancies and replacement intervals for MSS using UGF and genetic algorithms [10]. Later, the authors improve the method considering imperfect maintenance [16]. This method has been used along with Markov processes in recent studies concerned with MSS [2], [9], [11], [12], [17], [18].
    In addition to Markov process and UGF, some recent studies are applying recursive methods and Lz-transforms method to analyze the reliability and optimize parameters of MSS [19], [9], [20], [21]. The focus of Sheu and Zhang [9] and Lisnianski et al. [21] is optimization of age replacement time and Sheu et al. [19] focus in optimization of preventive maintenance schedule. Most of the studies on MSS analyze complex systems subject to continuous monitoring assuming that repair starts immediately after the failure occurrence. These papers aim to optimize the acquisition, allocation, and redundancy level along with the establishment of the optimal time interval to replacement. Just a few studies consider MSS subject to periodic inspections where the components and system state are verified only at inspections. However, this is a situation observed in many industrial scenarios, especially when the analysis of some component requires local inspection and the access is difficult. In these cases, the components are only monitored periodically.
    Le and Tan [22] optimized sequential inspections and continuous monitoring for an MSS assuming instantaneous repair. In this system, inspections are performed after a warning given by a continuous monitoring system, and repair or replacements are performed for the whole system and not for individual components. Lu et al. [20] utilized non-homogeneous continuous-time Markov chains, z transform, and particle swarm optimization to optimize the maintenance threshold and the inspection intervals of an MSS with five maintenance effects. Ruiz-Castro [23] analyzed the performance of an MSS with and without preventive maintenance using Markov process and reward processes algorithmically. This author considers a system subject to internal failures and external shocks with random inspections. Using semi-Markov process and Monte Carlo simulation, Koutras et al. [24] established inspection time and maintenance policies that optimize the dependability and/or performance of an MSS with deterioration.
    Considering a binary redundant system with instantaneous repair and constant failure rate, Alebrant Mendes et al. [25] established the optimal time interval between periodic inspections using Markov chain and search technique.
    As shown in the previous paragraphs, the interest for studies that address MSS subject to inspections has increased in the last few years. Nevertheless, none of them address the optimization of the time interval between periodic inspections of a system in which the components are only monitored periodically. Also, the application of discrete-time Markov chains to solve such problems is a challenging novelty.
    The main contribution of the method proposed in this paper is the analysis of redundant MSS comprised of components that have different levels of degradation that can be determined only through periodic inspections. This configuration is common in systems that cannot be continuously monitored and are difficult to access, such as, internal components of heavy equipment where substantial disassembly effort is necessary to access and verify components. The establishment of the optimal time interval among inspections, considering inspection, failure, and unavailability costs is important to ensure the desired reliability with the lowest cost possible.
    The complexity of problems involving redundancies increase when the time-to-repair is considered as a random variable distributed according to some defined parametric distribution. Usually, the authors suppose that repair times are exponentially distributed or fit a phase-type distribution, making the problem solution easier using Markov processes, as seen in Tian et al. [17], Mine and Kawai [26], Liu et al. [11], Lu et al. [20], and Montoro-Cazorla and Perez-Ocon [27].
    The study of systems having different levels of degradation using Markov processes also becomes complex as more components and degradation states are added. Soro et al. [28] and Sheu and Zhang [9] took into consideration only the degradation levels of the whole system, reducing the number of variables in the model and allowing the inclusion of many degradation states. In those studies that focused on the degradation states at the component level, as seen in Mokaddis et al. [15], Montoro-Cazorla and Perez-Ocon [27], Jia et al. [29], and Guilani et al. [30], only a few states and components were considered, since increasing the number of states and components substantially increases the number of equations needed to describe the problem.
    This paper presents a model for systems with two components, in which one of them is in the cold standby position, and having three levels of degradation besides the failure state. The addition of components into the system increases the number of equations and the space required to describe them, but the model can be generalized and utilized to determine the optimal time interval between inspections of redundant MSS with n components.
    The maintenance of a redundant MSS is a stochastic process since it has n components and each component can be in any level of degradation or failure at any time. Each combination of component states represents a likelihood in the state space of a Markov chain process. When periodic inspections are performed, the system state (different levels of degradation or failure) is verified at discrete times (only during the inspections). This scenario justifies the application of discrete-time Markov chains to study such problems.
    This paper is organized as follows. Section II describes the system, lists the assumptions and notation used, and ex- plains the methodology applied for modeling the problem. In Section III, the problem is solved defining the transition prob- abilities and modeling the costs of the system. In Section IV, numerical examples are presented and analyzed. Section V summarizes the paper and includes concluding remarks.
II.METHODOLOGY
    This section describes the system modeled and the assumptions accepted.
A. System Description
    This paper builds upon and extends the method developed by Alebrant Mendes et al. [25] to establish the optimal time interval between periodic inspections. Since the time-to-repair cannot be neglected in many situations, this study first extends the proposed method to incorporate this aspect. Through the inclusion of the time-to-repair in the model, its effect in the system reliability and costs can be analyzed. Next, an additional aspect was modeled. Given that many redundant systems suffer degradation during operation, and that, in many cases, the degradation level can be verified through inspections, the method was extended to accommodate MSS and inspection. The systems analyzed in this study are presented next.
    These systems reflect real systems installed in a petrochemical company that operates in southern Brazil. This is a large company comprising several operational systems, many of them 1) in a difficult-to-inspect position (demanding some disassembling for inspection) and 2) protected by redundancy. The operating condition of some of these difficult-to-inspect systems cannot be properly evaluated and, as far as detection is concerned, they change directly from operational to failed state (as system I described next). The operating condition of other difficult-to- inspect systems, however, may be evaluated and a maintenance decision may be taken based on that evaluation (as system II de- scribed next). Regarding the numerical example (presented in Section IV), in order to protect confidential information, the real numbers concerning costs, mean time between failures (MTBF), and mean time-to-failure (MTTF) were modified, but the aver- age proportion [between MTBF/mean time-to-repair (MTTR) and among costs] observed in the petrochemical company was preserved.
    a) System I ? Binary system with cold standby redundancy and component repair
    In this system, one component is active and the other is in a cold standby position. When the active component fails, the cold standby component is instantaneously activated. During the periodic inspection, the binary state (operational, failed) of each component is verified and component repairs are per- formed when necessary. Time-to-repair is a random variable and the system operates unprotected (without redundancy) until the repair is completed. The repaired components return to the system in the cold standby position. The system fails when both components fail between two consecutive inspections or if one component fails while the other is under repair.
    b) System II ? Multistate system with cold standby redundancy and component repair
    In this system, one component is also active and the other is in a cold standby position. Each component has, besides the failure state, three different operational states related to its degradation level:
    1) excellent (e) ? component is operating in a perfect state;
    2) good (g) ? component is operating in a good state, but it is possible to verify some level of degradation; and
    3) poor (p) ? component is still operating, but in an advanced state of degradation, being recommended performing a repair to avoid failure.
    Components move from excellent to good and from good to poor, and if not repaired, from poor to failure state. Components never fail before passing through all levels of degradation. However, the system can pass through more than one state and fail between two consecutive inspections. When the active component fails, the cold standby component is instantaneously activated and experiences the same degradation levels of the previous component. A component is sent to repair if a poor or failure state is detected during inspection.
C. Assumptions
    This study has the following assumptions for system I:
    1) components have only two states (operating or failed);
    2) perfect and instantaneous switching;
    3) times-to-failure follow an exponential probability distribution;
    4) components have different parameters for the probability distribution of times-to-failure and equal parameters for the probability distribution of times-to-repair;
    5) components in cold standby position do not degrade or fail;
    6) each inspection is perfectly and instantaneously performed; and
    7) perfect repair.
    For System II, the assumptions are:
    1) components degrade incrementally, experiencing all degradation states before fail;
    2) perfect and instantaneous switching;
    3) times-to-transition between degradation states and times-to-repair fit an exponential probability distribution;
    4) components have different parameters for the probability distribution of times-to-transition and equal parameters for the probability distribution of times-to-repair;
    5) components in cold standby position do not degrade or fail;
    6) each inspection is perfectly and instantaneously performed; and
    7) perfect repair.
    This study analyzes systems where failures are neither detected nor repaired until an inspection is performed. If the system failure occurs before the inspection, the downtime results in an increase of costs. These costs increase until the next inspection. When the system is still working and a component failure is detected, the component is repaired (with an associated cost and time) to preserve the reliability level. In system II, if inspection detects a poor condition, the component is also repaired (with an associated cost and time) to prevent system failure.
    Markov chains are an attractive model for this class of problem because complex behavior can be represented efficiently, and an appropriate mathematical model developed and used to optimize design or maintenance. However, it should also be clearly stated that use of Markov chains requires a series of assumptions that are mostly appropriate and reasonably accurate. There may be cases where the assumptions simply are not valid and the models proposed in this paper should not be applied. For example, when a repair is ongoing and an inspection occurs, it is assumed that the random repair time begins again.
    Exponential probability distributions were used to facilitate the use of Markov chains. The exponential distribution is appropriate and convenient to model times-to-failure, times-to-repair, and times-to-state transition for many components. However, if it is not the case, other approaches, such as Monte Carlo simulation, should be utilized.
III. RELIABILITY AND COST ANALYSIS OF REDUNDANT SYSTEMS SUBJECT TO PERIODIC INSPECTIONS
    In this section, the transition probabilities for the two systems are developed using Markov chains. Next, the costs related to system maintenance are determined and a cost function is developed and minimized to find the optimal time interval between inspections.
A. Definition of Transition Probabilities Using Discrete-Time Markov Chains
    A Markov chain is a process consisting of a finite number of states and known transition probabilities pij , where pij is the probability of transitioning from state i to state j. The probabilities pij depend only on states i and j, not depending on the time or previous number of transitions n or the previous states visited. The set of states in a Markov chain process is called state space [31].
    Considering that each different configuration possible to be found during an inspection is a state and that transition times from one to another state are random variables, it is possible to define a state space diagram for both systems being studied, as presented next in this paper.
    Every time a periodic inspection is performed, the system state is verified. Once in a failure state, the system does not change to another state anymore. Each state of system failure is an absorbing state of the Markov chain. The state space diagram and the transition matrix of each system are presented as follows. 
    The model corresponds to two-component systems. In theory, a corresponding Markov chain could be developed for larger systems with more components, but practically, it would become inefficient and cumbersome. Therefore, for larger systems, alternative models might be more practical. Stochastic Petri nets and Monte Carlo simulation have demonstrated much promise for problems of this type
    a) System I ? Binary system with cold standby redundancy and component repair: The state space diagram for system I is presented in Fig. 1. The circles Sn represent each state from the state space. Each letter inside the circles represents the com- ponent state: operating (O), standby (S), repair (R), or failure (F). Each number associated with each letter inside the circles represents one component (1 and 2). For example: the state S0 has the component 1 in operating state O1 and the component 2 in cold standby state S2. The states S6, S7, and S8 represent system failure and are absorbing states for the purpose of this study.
    The matrix P1, in Appendix B, shows the transition prob- abilities for system I. For example, the probability p30 is the probability of transitioning from state 3 to state 0. This prob- ability is the probability of component 2 being repaired and component 1 being operative during the time interval between inspections.
    Since the time-to-failure (TTF) of the active component, the time when the standby component starts to operate, and the time- to-repair are random variables, standard probability theory was used to determine these probabilities. For example, the probability p30 of transitioning from state 3 to state 0 can be determined as the probability of the time-to-repair of component 2 being smaller than the time interval between inspections combined with the probability of the TTF of component 1 being higher than the time interval between inspections: Pr R2 < ¦Ó T1 > ¦Ó  . These probabilities are expressed by 15 equations. Equations (1)?(7) are presented next. The other equations can be checked in Appendix A.
    1) Probability of component 1 remaining operational during the time interval between inspections:
    2) Probability of component 1 having failed and component 2 starting to operate but not failing during the time interval between inspections:
    3) Probability of components 1 and 2 failing during the time interval between inspections:
    4) Probability of component 2 remaining operational during the time interval between inspections:
    5) Probability of component 2 having failed and component 1 starting to operate but not failing during the time interval between inspections:
    6) Probability of component 1 being repaired and component 2 remaining operational during the time interval between inspections:
    7) Probability of component 1 being repaired before component 2 fails, given that the component 2 fails and component 1 starts to operate during the time interval between inspections:
    Considering (1)?(7) presented here and (32)?(39) presented in Appendix A, P1 can be rewritten. The matrix P1 can be checked in Appendix B.
    b) System II ? Multistate system with cold standby redundancy and component repair: The state space diagram for system II is presented in Fig. 2. The circles Sn represent each state from the state space. Each letter inside the circles represents the component state: operating in an excellent state (Oe), operating in a good state (Og), operating in a poor state (Op), cold standby (S), repair (R), or failure (F). Each number associated with each letter inside the circles represents one component (1 and 2). For example, the state S0 has component 1 operating in excellent state O1e and component 2 in cold standby state S2.
    Fig. 2 shows that there are 21 possible states (S0 to S20) in the state space of system II. The state of the components of each system state is presented in Fig. 3.
    The system starts with component 1 operating in an excellent state and component 2 in a cold standby position. The system can stay in the same state or move to another state. Repair of failed components starts just after the next periodic inspection. Components in a poor state of operation are also sent to repair after the periodic inspection, if the other component is available to operate. If the other component is being repaired, no action is taken during the periodic inspection. However, if the other component is in a failure state, the failed component is sent to repair during the periodic inspection. In both cases, the system stays operating with the component in a poor state.
    Times-to-repair of each component vary according to their probability distribution. The system fails when all components fail during the time interval between inspections or if a component fails while the other component is being repaired. After failure, the system is not repaired. Thus, states S18, S19, and S20 are absorbing states.
    Matrix P2 (see Appendix B) presents the transition probabilities of the system. The probabilities in matrix P2 were solved using probability theory. For example, the probability p4,15 is the probability of component 1 being repaired before component 2 having failed and, in the sequence, component 2 failing and component 1 degrading until the poor state of operation, during the time interval between inspections. This probability can be calculated through the combination of three probabilities:
    1) probability of the time-to-repair of component 1 being smaller than the sum of the times-to-degradation and failure of component 2;
    2) probability of the sum of times-to-degradation and failure of component 2, plus the time-to-degradation of component 1 until the good state of operation, being smaller than the time interval between inspections; and
    3) probability of the sum of times-to-degradation and failure of component 2, plus the times to component 1 degrading until the poor state of operation, being higher than the time interval between inspections:
    The calculation of these probabilities follows the same methodology used to solve the probabilities of system I. Some probabilities and their solution in terms of integral equations are presented in the following. The probabilities were solved using the software MATLAB 7.9 R2009b.
    The system has a total of 79 equations that represent the states transition probabilities. Two examples of these probabilities and their solutions are presented next:
    1) Probability of component 2 degrading and failing after component 1 has been repaired and component 1 starting to operate and degrading from the excellent state to the poor state of operation, during the time interval between inspections:
    2) Probability of component 2 degrading from the excellent state to the poor state of operation and component 1 remaining under repair, during the time interval between inspections:
B. Cost Models
    Optimizing the measure of performance per unit of time is equivalent to optimizing the measure of performance over a long period. Thus, the cost models in this paper are based in the establishment of the maintenance total cost per cycle. In this application, a cycle is the period of time elapsed from the beginning of the system operation until its failure:
    Four elements were considered to determine the costs associated with the redundant system subject to periodic inspections:
    1) cost of periodic inspection (Ci);
    2) cost of component repair (Cr);
    3) cost of downtime per time (Cp); and
    4) cost of system repair (Cs).
    The cost of periodic inspections comprises costs of manpower, tools and materials required to perform the inspection, even if there are no components in the failure state. The cost of component repair includes costs of manpower, tools, replacement of parts, and materials utilized to repair failed component. The cost of system repair is related to costs incurred to reactivate the system to its completely operational condition after a system failure. Finally, the cost of downtime refers to the production losses during the time that the system is down. This cost includes the loss of sales opportunity and monetary fees for delivery delay.
    The costs of periodic inspection, component repair, and system repair are assumed to be constants and must be determined for each system individually. On the other hand, the cost of downtime is a function of time, because losses increase along with the length of system unavailability. To establish this cost, it is necessary to determine the expected downtime.
    For a binary system (system I) comprised of two components (1 and 2) in parallel, in which one component is in cold standby position and both components have times-to-failure that follow an exponential probability distribution, the mean time to system failure (MTTFij ), given that the system is in state i and goes to state j that is a failure state, is calculated as:
    For an MSS (system II) comprised of two components (1 and 2), in which one component is in cold standby position and both components have times-to-transition that follow an exponential probability distribution, the mean time to system failure (MTTFij ), given that the system is in state i and goes to state j that is a failure state, is calculated as following. The letters associated with the parameter ¦Ë represent the component operation state:
    Consider a system having only one component with times-to-transition following an exponential distribution. Supposing that this component does fail during the time interval between inspections (¦Ó) and that this interval is relatively small when compared with the expected TTF, the expected downtime would be:
    Consider a system with two components where times-to- transition follow an exponential distribution. Supposing that these components both fail during the time interval between inspections (¦Ó) and that this interval is relatively small when compared with the expected TTF, the approximated expected downtime would be:
    Equations (25) and (26) are approximations that are appropriate when the time interval between inspections is smaller than the expected time to system failure, as usual in industry. Exact relations could be derived based on a truncated conditional distribution. However, this approximation is useful for the purpose of this study, given the complexity of the cost rate model.
    The approximations in (25) and (26) are based on three observations:
    1) all transitions related to downtime are associated with a system failure, so the expected downtime is conditional on the occurrence of a system failure (all components in operation in the beginning of the interval fail),
    2) the occurrence of failures in a cold standby system is modeled as a homogeneous Poisson process with rate ¦Ëi until the last component fails, and
    3) the probability of more than one failure for a Poisson process with rate ¦Ëi is small for an interval shorter than the expected time to system failure.
    Combining 2) and 3) it is possible to approximate the occurrence of exactly n failures during the interval, where n is the number of components working in the beginning of the time interval between inspections. For a Poisson process, the expected occurrence of a particular number of failures is distributed uniformly in a fixed interval. So, for n = 1, the expected TTF is ¦Ó /2 and downtime is estimated as ¦Ó /2 + MTTR. For n = 2, the expected times-to-failure are ¦Ó /3 and 2¦Ó /3, where the second fail represents the system fail and downtime is estimated as ¦Ó /3+ MTTR. When the time intervals between inspections become longer than the expected TTF, observation 3) does not hold and this approximation is not appropriate.
    As the time interval between inspections increases, the expected downtime becomes longer than the system MTTF and the approximations used previously are no longer appropriate. For ¦Ó >MTTFij, the expected downtime can be approximated using (¦Ó MTTFij) + MTTR. As a result, the expected downtime for all cases can be approximated by:
    where n is the number of components that are in an operational condition at the beginning of the interval between inspections.
    When the inspection interval becomes much larger or much smaller than the expected failure time, the approximation in (27) converges to the exact solution, and for other cases it provides a reasonable approximation that is practical and efficient. If a decision maker desires an exact solution, a more rigorous probability model could potentially be developed [31]. However, for most applications, there is unlikely to be a detectable difference in the optimal inspection intervals selected.
    The cost matrix of each system is presented next. The costs showed in the matrices represent the costs incurred for each transition between two states in the respective system state space. For both systems, for each inspection, cost of inspection is computed. Whenever a component fails, cost of repair is added to the model. Whenever the whole system fails, the cost of inspection, cost of system repair, and cost of downtime are summed. The matrices C1 and C2 show the costs for systems I and II.
    After determining the costs involved in each redundant system subject to periodic inspections, it is necessary to combine these costs to establish the total cycle cost. This total cost can be calculated utilizing the discrete-time Markov chain properties. Accordingly to these properties, the expected number of times that the process passes in the transient state j, given that it started in the transient state i, is given by the matrix N [see (28) at the bottom of the page], where Q is the transient part of matrix P [31].
    Since the sum of each line in N reveals the expected number of discrete time steps before absorption, given that the chain began in the ith non-absorbing state, and using the probabilities and costs for each transition, the expected cost in a cycle can be calculated by:
    The length of the cycle depends on how many times the system goes from state 0 to the other states and how long the time between inspections is. Then, it is possible to calculate the expected length of the cycle by:
    Based on (29) and (30), the total cost as a function of ¦Ó can be rewritten as:
    The total cost in (31) can be minimized and the optimal time interval between inspections can be established setting values for the costs of inspection, component repair, downtime, and system repair. Since the time interval between inspections is the only variable in the equations, it is possible to determine the minimum total cost using numerical search techniques.
    Genetic algorithms as other meta-heuristic search algorithms could be utilized to obtain solutions for this problem, but this approach was not necessary. This problem has a relatively complex objective function of costs, but its optimization is performed for only one decision variable: time interval between inspections. Consequently, a simple one-dimensional numerical search is sufficient. As this model is extended, there will be more advanced multivariable problems requiring methods such as genetic algorithms.
NUMERICAL EXAMPLES
    Numerical examples for each system studied are presented and analyzed next. For comparison purposes, identical parameters of costs and time-to-repair were utilized in both systems. Equivalent parameters of MTTF were considered. The MTTF of each component in system I (1¦Ë1 and 1¦Ë2), which is a binary system, is equivalent to the sum of mean times-to-transition between states excellent to good, good to poor, and poor to failure of each component in system II (1¦Ë1e + 1¦Ë1g + 1¦Ë1p and 1¦Ë2e + 1¦Ë2g + 1¦Ë2p), assuming that the system is not inspected and repaired.
    The parameters values were determined to represent real systems scenarios. Despite being similar, components often have failure rates that are different. The cost parameters utilized are selected based on real scenarios where the cost of inspection and component repair is smaller than the cost of system repair and downtime. Figs. 4 and 5 show the total cost per cycle per month and the optimal time interval between inspections in days for systems I and II, respectively. The parameters utilized were:
    1) System I?Different parameters of exponential distribution for times-to-failure for each component: ¦Ë1= 0.005, ¦Ë2=0.006 (these parameters represent MTTFs of 200 and 166.67 days, respectively).
    2) System II ? Different parameters of exponential distribution for times-to-transition between different states of degradation for each component: ¦Ë1e=0.02, ¦Ë1g=0.01, ¦Ë1p=0.021, ¦Ë2e=0.025, ¦Ë2g=0.015, and ¦Ë2p=0.026 (these parameters represent MTTFs of 200 and 166.67 days).
    3) Equal parameters of exponential distribution for times-to- repair: ¦Á1 and ¦Á2=0.05.
    4) Ci=10 000 and Cr=30 000: cost of component repair is higher than cost of inspection.
    5) Cs=100 000 and Cp=100 000/units per day: downtime cost is higher than the other costs. It means that unavailability incurs higher costs than those associated with inspection and repair. See the equation shown at the top of the next page.
    Fig. 4 shows that the time interval between inspections that minimize the total cost for system I is 14 days. In the same way, Fig. 5 shows that the optimal time interval between inspections for system II is 28 days. Even with equivalent MTTFs, the optimal value for the time interval between inspections is quite different between the two systems presented. The binary system has a higher minimum total cost and requires a time interval be- tween inspections smaller than the MSS. This happens because in system II it is possible to identify an advanced level of degradation and perform the repair before the failure has happened. This reduces the costs related to system failure and downtime.
    Moreover, results show that the model developed for the optimization of binary systems is not effective for MSSs. The analysis of MSSs assuming that they have a similar behavior to binary systems can lead to an unnecessary reduction of times interval between inspection and a consequently increase of the total costs.
    A sensitivity analysis was conducted to analyze the effect of different parameters (¦Ë1e, ¦Ë1g, ¦Ë1p, ¦Ë2e, ¦Ë2g, ¦Ë2p, ¦Á1, ¦Á2, Ci, Cr, Cs, and Cp) in the total cost per cycle of system II. Fig. 6 presents six graphs. Fig. 6(a) and (b) shows the effect of ¦Á in system with different parameters of ¦Ë1e, ¦Ë1g, ¦Ë1p, ¦Ë2e, ¦Ë2g, ¦Ë2p. Fig. 6(c), (d), and (f) shows the effect of Ci, Cr, Cs, and Cp, on the total cost and the optimal time interval between inspections.
    Accordingly to Fig. 6(a), higher repair rates (¦Á) increase the minimum CT while the optimal times interval between inspections remain approximately the same. When Fig. 6(b) is com- pared with Fig. 6(a), it shows that smaller rates of degradation and failure reduce the system total cost. Fig. 6(c) shows that higher costs of inspection (Ci) cause higher minimum CTs and times interval between inspections. Fig. 6(d) demonstrates that costs of repair (Cr) have a small effect on CT and that this effect does not change over the range of ¦Ó. Fig. 6(e) demonstrates that costs of system repair (Cs) also have a small effect on CT. Finally, Fig. 6(f) shows the effect of costs of downtime on the minimum CT. Costs of downtime have a higher effect on minimum CT and this effect increases when ¦Ó increases.
    There are two ways to generalize the solution of the problem presented in this paper to consider systems consisting of a larger number of components:
    1) the exact solution can be obtained via Markov chains by implementing in software the same reasoning presented in this paper for the two-component systems. Software support would enable to cope with systems constituted by a larger number of components, since the software could handle the hundreds or thousands of transition states that would be established in cases involving a larger number of components.
    2) An approximate solution could be obtained using Monte Carlo simulation. The Monte Carlo simulation would have an important advantage related to the possibility of assuming any probability distribution for the variables involved. In any case, it is worth mentioning that safety systems usually have a backup (as the problem presented in this paper), but a relatively small number of backups, usually two, three, or four components in parallel. This makes it feasible, though laborious, to develop a software or to structure a Monte Carlo simulation.
    Aiming to solve the generalization problem by simulation, discrete event simulation approach could be used to develop the simulation framework along with Monte Carlo simulation to generate the random numbers that represent the probabilities that are characteristic of stochastic processes. The distributions parameters and the costs variables would be the simulation input parameters. As output, we would have the systems TTF, the numbers of inspections and repairs, and the total cost per cycle. The variables of the simulation would be:
    1) the time variable t (referring to the amount of time that has elapsed until the end of the simulation) would be represented by the system TTF;
    2) the counter variables would be the number of inspections, number of repairs, and time of operation; and
    3) system state variable (that describes the system state at t) would be all the system states possible to be found during a periodic inspection.
    The periodic inspections would be the events when the values of the variables would be changed or updated and relevant out- put would be collected. The random variables generated through Monte Carlo simulation would be used to represent the probabilities of the distributions of times-to-transition and times-to- repair. Having the probability, it would be possible to determine the distribution matching time that represents a random time-to- transition or time-to-repair. Every time, when we run the simulation, we would get as output an independent random variable that would have always the same probability distribution. The average of these values would be then used as an approximator of our variable of interest. In order to have a quality approximator and an acceptable error, it would be necessary to run the simulation many times and collect an appropriate number of random variables. The logic that would be needed to build the simulation is similar to the logic developed to solve the problem using Markov Chains. However, the equations that would be needed to be solved are much simpler, because it would not have integral or differential calculations.
V. CONCLUSION
    This paper presented a model to establish the optimal time interval between periodic inspections for MSS with redundancy in cold standby position utilizing discrete-time Markov chains.
    MSS in which different states of degradation are identified captures in detail the component aging, representing in a better perspective many of real situations than other approaches that consider systems as binary (operating or failure state). When continuous monitoring is not possible, periodic inspections are required. During these inspections, component states are verified and repair is started when a failure state or an advanced state of degradation has been detected. The time interval between inspections must be optimized to minimize costs. Frequent inspections improve the system availability, but require higher costs of preventive maintenance. On the other hand, long periods be- tween inspections reduce the costs associated with inspections, but also increase the costs of corrective maintenance, downtime, and costs related to safety accidents. The establishment of an optimal time interval between inspections is important to ensure satisfactory system availability along with the lowest cost possible.
    The method presented in this paper uses discrete-time Markov chains to define both the transition probabilities between system states and the costs related to each transition. To optimize the time interval between inspections, the total cost of maintenance was modeled and minimized using the Markov chains properties and a numerical search technique. The minimal cost and the optimal time interval between inspections were obtained taking into consideration the costs of unavailability (downtime) and maintenance (inspection, component repair, and system repair).
    Two redundant systems in cold standby position were modeled and numerical examples for systems comprised of two components were presented: 1) System I ? Binary system with cold standby redundancy and component repair; 2) System II?MSS with cold standby redundancy and component repair. Results reveal that the binary system has a minimum total cost higher and requires a time interval between inspections smaller than the MSS. This happens because in MSS it is possible to identify advanced levels of degradation and perform the repair before system failure. This reduces the dominant costs related to downtime. Results also show that the model developed for the optimization of binary systems is not effective for MSS. The analysis of MSS assuming they have the same behavior of binary systems conducts to an unnecessary reduction of time intervals between inspection and a consequently increase of the total costs.
    This model pertained to a system with cold standby redundancy. An analogous model could also be developed for active redundancy, or in an extended research effort, a system using both active and cold standby together.
    For future research directions, one promising option would be the establishment of the optimal number of redundant components for an MSS comprised of identical components, aiming to achieve predetermined reliability requirements. Also, a model with imperfect maintenance or immediate detection of failures would be interesting and practical for some applications. Furthermore, an interesting improvement in the model developed could be the assumption of the cost of repair as a function of time. This would represent the inclusion of another random variable into the model.
    
Reliability optimization of series-parallel systems with K-mixed redundancy strategy
    This paper revisits the redundancy allocation problem, a well-known problem in reliability optimization, and implements a novel redundancy strategy, called K-mixed, to improve system reliability. The K-mixed strategy is a general form of a mixed strategy. The mixed strategy is a combination of active and standby redundancy strategies which was introduced in 2014. Initially, the mathematical formulation for calculating the reliability of the K-mixed strategy is investigated and then its power and efficiency are evaluated against different test problems and a famous benchmark problem. In order to solve the proposed benchmark problem, an efficient genetic algorithm is developed and the results are compared with those reported elsewhere. It is found that the proposed K-mixed strategy results in higher reliability in most situations than would otherwise be achieved by its mixed counterpart. It is concluded that by using the K-mixed strategy in an optimization model, system designers have the opportunity to select the best strategy for each subsystem from among all the different strategies available to design systems with maximum reliability.
1. Introduction
    High technology systems are mostly complex and expensive, but also need to be highly reliable because they are required to have important roles in modern industry. Reliability and mean time to failure (MTTF) are two fundamental measures of such systems. Reliability is de?ned as the probability that a system (or an entity) sufficiently performs its specific functions over a specific period of time under specific environmental conditions [1]. In order to improve the reliability of a specific system, one can use highly reliable components and/or a well- designed configuration to meet functional requirements and performance specifications. Finding the best configuration for a system by adding parallel components is called the redundancy allocation problem (RAP). RAP has been proved to belong to the NP-hard class of optimization problems [1]. In RAP, there are discrete component choices with known characteristics such as reliability, cost, and weight, where the objective is to find the optimal number of redundant components in each subsystem to maximize the overall system reliability subject to some constraints.
    The redundancy strategy is used to determine how the redundant components are used in the system (or its subsystems). Traditionally, there are two redundancy strategies: active and standby. The active redundancy strategy is based on the assumption that all redundant components start operation simultaneously from time zero although only one is required for the whole system to operate. The standby strategy comes in three variants of cold, warm, and hot [2]. In the cold- standby strategy, the redundant components are protected from the operational stress associated with system operation so that no component fails before its activation. Components in the warm-standby strategy are somehow affected by the operational stresses. Finally, in the hot-standby strategy, component failure does not depend on whether the components are idle or in operation. The mathematical formulation for the hot-standby strategy is the same as that for the active one [3]. In all three variants of the standby strategy, redundant components are used sequentially based on a predetermined order at failure times of operating components by using a switching system. Recently, a new redundancy strategy, called mixed strategy, has been introduced which combines the active and standby strategies and is shown to outperform both [2]. In other words, the mixed strategy is the general form while the active and standby ones are its special forms.
    Either of two scenarios may be envisioned for a switching system. In the first scenario (S1), the failure detection system/mechanism continually monitors system performance to detect a failure in which a redundant component is then activated. In addition to component failure, a switching system malfunction may occur at any time. In this scenario, switch reliability is a non-increasing function of time (老(t)). In the second scenario (S2), the switching system is used only at the occurrence of failure of a component. Failure of the switching system, therefore, happens only when it is used, and the probability of switch failure is considered as a constant value (老) [2?4].
    Over the past decade, many optimization methods have been proposed for solving the RAP with active strategy [5?9], standby strategy [4,10?14], and a combination of active and standby strategies [9,15,16]. Underlying all these studies is the assumption that the redundancy strategy for a specific subsystem can only be one of the active or standby strategies. In most cases, the redundancy strategy is considered to be a predetermined decision while it is considered as a decision variable in some recent studies [3,14?16]. Recently, a new redundancy strategy, called the ※mixed redundancy strategy§, has been introduced [2]. It potentially uses both active and standby strategies in a subsystem simultaneously. Therefore, the number of active and standby components in each subsystem is a decision variable and must be determined by a mathematical model. Ardakan et al. [17] developed the mixed strategy for a bi-objective RAP problem and demonstrated its advantages over the active and standby counterparts. More specifically, results have shown that the mixed strategy leads to more than 10 structures with superior reliability to those obtained in all previous cases.
    Feizabadi and Jahromi [18] developed the mixed strategy for a RAP problem with non-homogeneous components. Put more clearly, they considered the case in which standby components could be different from active ones but that all active and standby components had to be identical. In another study, they extended their model to a bi-objective RAP [19]. Recently, Gholinezhad and Hamadani [20] developed a mixed strategy for situations in which all the active and standby components were different. They developed a mathematical model for this non-homogeneous RAP problem and used a genetic algorithm (GA) to solve it. Ardakan et al. [21] investigated the capability of the mixed strategy applied to a reliability-redundancy allocation problem (RRAP) which is more complicated than the RAP. Results demonstrated that the mixed strategy led to a higher reliability in all the benchmark problems than did either active or standby strategies. The above mentioned studies con?rm the ability of the newly introduced mixed strategy to create highly reliable systems. In a recent study, Peiravi et al. [22] proposed a new redundancy strategy called K-mixed strategy which is a general form for the mixed strategy. Because of the complex nature of K-mixed strategy, they implemented the strategy only on one specific subsystem with 4 components.
    In this paper, a famous series-parallel system with 14 subsystems is investigated when all four different redundancy strategies, i.e., active, standby, mixed and K-mixed can be used in any subsystem. For this purpose, a new mathematical model is developed where the redundancy strategy and the number of components in each subsystem are considered as the decision variables. The proposed mathematical model must select the best strategy with the best redundancy level to maximize the total system reliability. The results of implementing the K-mixed strategy to the benchmark problem are compared with those reported elsewhere to evaluate its applicability and efficiency. By solving this new mathematical model, the capabilities and advantages of the K-mixed strategy will be revealed, especially in comparison with the mixed one.
    The rest of the paper is organized as follows. In Section 2, the mixed and K-mixed redundancy strategies are described in detail. Also, the mathematical formulation for calculating the reliability of these strategies are investigated in this section. In Section 3, the modeling of the problem is presented and in Section 4, a GA is developed for solving the proposed model. A well-known benchmark problem is adopted and solved using the new strategy in Section 5 and different problems are created and the results are compared to demonstrate the superiority and advantages of the new K-mixed strategy over the previously developed active, standby, and mixed strategies. Conclusions are finally presented in the end of the paper.
2. Mixed and K-mixed redundancy strategies
2.1. Operating mechanism
    In both mixed and K-mixed strategies, the numbers of active and standby components in each subsystem are considered as decision variables which must be optimally determined. This is a common feature of these strategies. The differences between the K-mixed strategy and the mixed one may be illustrated by considering a subsystem with four components. For this specific subsystem, all the different redundancy strategies (i.e., active, standby, mixed, and K-mixed) as outlined in Table 1 below are implemented.
    In order to disclose the distinctive features of these two strategies, they are applied to the proposed subsystem with four components. As seen in Table 1, the proposed subsystem might have two different structures depending on whether the mixed or the K-mixed strategy is applied. These two structures are: I) Three components in the active mode and one in the standby; II) Two active and two standby components.
    Fig. 1 illustrates these two different structures within the framework of a mixed strategy. Clearly, the subsystem starts its operation with 3 active components in structure I or with 2 in structure II. When the first active component fails, nothing happens for either structure and the subsystem continues working with the rest of its active components. When the second component fails in structure I, the subsystem continues operation with the last active component. In structure II, how- ever, the first standby component is activated by the switching system to replace the failed component because the second failing component is the last active one. From this point onwards, only one component is active in both structures and one standby component is replaced in a predetermined order upon the failure of this only component. This procedure continues as long as there is at least one standby component. When the last standby component is activated, the subsystem fails when this component also fails. It is worth noting that the standby components begin to operate in the mixed strategy when the last active component fails; this procedure (i.e., activating a redundant standby component to replace a failed active one) is the same as that in the standby strategy. Numerical results have shown that, for a switch re- liability equal to 0.99, the mixed strategy yields a higher system re- liability than those achieved under either the active or standby strategy [2,17,21].
    Both the mixed and standby strategies require a switching mechanism/system to substitute the redundant component into the sub- system, and the switching system can fail itself. If the switching system fails at the time of component replacement, the subsystem (and thereby the entire system) fails while there may still exist redundant standby components that have not yet been used. Accordingly, the reliability of the switching system is considered to be a weakness of the standby and mixed strategies, as it has an important and decisive role in their performance.
    The main contribution that the K-mixed strategy intends to make is to introduce a new strategy that is less sensitive to switch reliability. This strategy can then be expected to improve system reliability. In the proposed K-mixed strategy as a general form of the mixed strategy, all the active components start operating at time zero. When the first active component fails, the switching system immediately replaces the failing component with the first standby one available. This is where the mixed and the K-mixed strategies show their main difference. While the mixed strategy in this same situation (i.e., when the first active component fails) activates no standby component and the system continues working using the remaining active components, the K-mixed strategy replaces the failed component with the first standby one available. When the next active component fails, it is replaced with the next standby component available, and so on. The procedure continues until all the redundant components are activated and added to the subsystem [2]. In other words, the K-mixed strategy strives to keep a specific number of components in the active mode (i.e., the initial number of active components) as long as possible. From this point onward, the subsystem works with the remaining active components. Subsequently, the subsystem fails when the last active component fails.
    Fig. 2 illustrates the proposed subsystem with the K-mixed redundancy strategy in both structures I and II (compared to Fig. 1). It is seen that the number of active components is considered in the K-mixed strategy to be the same as that in the mixed one. It should be noted that only one component in this structure is required at any given time for the subsystem to operate.
    In structure II, the system starts its operation with two active and two standby components. When the first active component fails as detected by the switching system, the failed active component is replaced with the first standby component. This results in the subsystem still operating with two active components. When the second failure occurs, the failing component is replaced with the last standby component and the subsystem continues again with two active components. When the third failure arrives, no standby component is available anymore and the subsystem must, therefore, operate with only one active component. The subsystem then fails with the next component failure.
    In redundancy strategies with standby components (i.e., standby, mixed, and K-mixed strategies), a switching mechanism or failure detection system is required to detect the failure of a component and to replace it with a standby one. However, the switching mechanism is itself subject to failure. It is, therefore, more realistic and reasonable to consider the reliability of system regarding the switching failure. Coit [4] described two distinct scenarios for an imperfect switching system. In scenario I, the failure detection is continually monitored to detect a failing component and to replace it with a redundant one. In this scenario, the switch can fail at any time and switch reliability is a non- increasing function of time (老(t)) and switch reliability does not depend on the number of required switches. In scenario II, the switching system can only occur in response to a failure. Failure of the switching system occurs with a constant probability (老) when the switch is required.
    There is a significant difference between the K-mixed strategy and the mixed strategy with respect to the switching system. In the mixed strategy, the switching system is activated when the last active component fails and switching failure, therefore, results in subsystem/ system failure. In the K-mixed strategy, however, the switching system is used to substitute a redundant component when the first active one fails. In all the possible combinations, it is assumed that the switching system follows the second scenario and that it is triggered only in response to component failure. If the switch fails upon triggering, then it does not switch on the standby component even if it is repaired until the next failure triggers the switch back into operation. This procedure has two advantages. First, the subsystem/system is still operating when the switching system fails. Second, there is enough time to repair or replace the switching system. This is while the repair time or the time for replacing the switching system is considered to be short. Regarding the new proposed strategy, there is a decrease in the probability that the system fails due to switching failure. This study is an attempt to minimize the required number of switching operations which may result in direct subsystem failure.
2.2. Reliability model
    As explained in Section 2.1, there are 2 different forms of the K-mixed strategy for a subsystem with four components; they include: I) three components in the active mode and one in the standby mode; and II) two components in the active and two in the standby mode. Regarding the complexity of the K-mixed strategy formulation, this specific subsystem is considered and the formulation is described in more detail in order to provide a better demonstration of the formulation. The K-mixed formulations for the proposed subsystem with imperfect switching in structures I and II are defined by Eqs. (1) and (2), respectively. The times-to-failure of all the components follow an exponential distribution. Detailed procedures of obtaining these equations are presented in Peiravi et al. [22].
    In the K-mixed strategy, the reliability formulation for each subsystem depends entirely on the number of active and standby components. The following formula describes a subsystem of structure I with four components (3 active and 1 standby).
    Formula for a subsystem with 4components
    The reliability of a subsystem of structure II with four (2 active and 2 standby) components is given by Eq. (7):
    where, 老is the failure-detection/switching reliability for scenario 2 which is a constant value, (1 ? 老) is the probability that the failure-detection/switching does not work, and ri(t) is the reliability of the component used in subsystem i at time t. Since the component time-to-failure follows an exponential distribution, ri(t) is calculated as follows:
    pdf for the minimum failure time of active components which is calculated as follow:
    In Eq. (4), f(t)is the probability density function of time-to-failure for the components and F(t)is the cumulative distribution function. In Eqs. (1) and (2), subsystem reliability is the sum of probabilities associated with mutually exclusive events that lead to successful subsystem operation during the mission time t.
    Changes in the number of components or the combination of active and standby components lead to changes in the formulation of the K-mixed strategy. It is clear from the example that when the number of active components is changed from three to two, the reliability formulation changes correspondingly and the number of probabilities increases from six to eight segments. Therefore, there exists no fixed number of terms for the reliability formulation in each subsystem so that the formula must be developed and extended for each subsystem based on its own redundancy level.
    Active components have different starting times in some cases; this is important for calculating f(u) that shows the minimum failure time of the active components. In these conditions, if the component time-to-failure follows an exponential distribution, it can be assumed that all the components start working at the same time because of the memoryless property of this distribution; hence, Eq. (4) is justified. For other distributions, calculation of f(u) is more complicated.
    The formulation of mixed strategy in the second switching scenario is given by Eq. (5) [2]:
    where, f(t) is the pdf for the maximum failure times of number of failures of a component in the subsystem and is calculated as follows:
    RAP belongs to the NP-Hard class of problems, so it is hard to obtain an exact solution [1]. It is even more difficult to solve the problem using the proposed mathematical model for the K-mixed strategy. A GA as an efficient meta-heuristic algorithm is, therefore, developed in this paper for the optimization of the problems.
3. Mathematical model for RAP
    Significant research has been directed toward well-known bench- mark problems, including series system, series-parallel system, parallel- series system, and complex system ones. In this paper, a series-parallel system made up of 14 subsystems is considered from the literature. In such a system, each subsystem, i, can have its own unique number of active and standby redundancies considered as a decision variable. There are multiple choices of components available in each subsystem, but only one type of component with an unlimited supply is allowed to be used. Each type has its own levels of such parameters as reliability, weight, and cost. The objective is to maximize system reliability by determining the best strategy and the number of active and standby components in each subsystem based on weight and cost limitations.
    The assumptions of the mathematical model are as follows:
    Components are non-repairable.
    The components＊ time-to-failure follow an exponential distribution.
    The components and the system are two-state (good or bad) ones. 
    Each component's failure is independent of others and does not cause damage in them.
    All the components in each subsystem are identical. 
    The subsystem has an imperfect switching system.
    The switching system is replaceable or repairable and the switch works as perfectly as a new one after each replacement or repair. 
    The second scenario of switching (detection and switching only at the time of failure) is considered for the switching system.
    The mathematical formulation for the proposed problem runs as follows:
    Eq. (7) specifies the objective function which contains the component type, redundancy level, and the best redundancy strategy for each subsystem to achieve maximum system reliability. Constraints on cost and weight are given by Eqs. (8) and (9), respectively. System reliability is calculated using Eq. (11) in which the second scenario of switching (i.e., switch activation only in response to a failure) is considered.
    where, RMixed and RK?Mixed are equations for calculating the reliability of the subsystems with mixed and K-mixed strategies.
4. Genetic algorithm
    GA belongs to the family of meta-heuristic algorithms successfully employed for the optimization of combinatorial problems due to its simplicity and practicality. In this paper, a powerful GA with an efficient encoding procedure is applied for solving the proposed model when all kinds of strategies with different type and number of components can be selected for individual subsystem. GA has different characteristics such as solution encoding, objective function, crossover and mutation which are described in the following subsections.
4.1. Solution encoding
    A new efficient chromosome is chosen to represent the possible solutions which consist of selected component and redundancy level for each subsystem. This solution encoding is represented in a 2 ℅ s matrix. The first and second rows represent the type and number of selected components, respectively. An example of encoding solution for this problem with s = 14 is shown in Fig. 3.
    The main point about this encoding is that all possible strategies which can be applied to the subsystem by considering the redundancy level are considered. Then, the reliability of subsystems by using different strategies is calculated. The strategy which leads to a higher reliability for the subsystem is selected. For example, in subsystem number five, 4 parallel components are considered. All redundancy strategies include active, standby, mixed and K-mixed can be applied to this subsystem. Therefore, the best strategy with the higher reliability must be determined by the algorithm. In previous studies, the redundancy strategy has been considered in the solution encoding and it makes the solution space more complicated.
4.2. Objective function
    After the solutions are generated, their fitness functions are calculated by considering the penalty for constraint violations. In order to transform an infeasible solution to a feasible one in the next iterations of the algorithm, a dynamic penalty function is implemented. In this method, infeasible solutions are penalized by reducing their fitness value regarding to their violation level. Also this penalty function encourages the population to seek the feasible region and near the border of feasible region. The penalty function used here is based on research in [2].
4.3. Crossover and mutation
    In this section, double-point crossover and a modified version of max?min crossover proposed by [16] is applied. The mutation operator is used to increase diversity and prevent premature convergence into a local optimization solution. In this paper, besides the simple mutation, a max-min mutation operator is performed [2,16].
5. Numerical results
    In order to demonstrate the efficiency of the proposed K-mixed strategy, it is initially applied to three different subsystems and then to a well-known benchmark problem. The three different subsystems are considered to have 2, 3, and 4 components and the reliability values for the components and the switching system are assumed to vary over a wide range. The benchmark problem is a series-parallel system with 14 subsystems originally due to Fyffe et al. [23] which was later modified by Coit [4] and subsequently used by many researchers [2?4,16?20,24].
5.1. Three different subsystems
    The reliability of a specific system depends on different parameters as component reliability, switch reliability, redundancy level, and redundancy strategy. The present section investigates the effects of these parameters on the reliability of a subsystem. In these analyses, three different subsystems with 2, 3, and 4 components are considered and the reliability of different strategies in each subsystem is calculated based on different combinations of component and switch reliabilities. The objective is to derive the relationships among the different para- meters of the system and the best strategy in different situations. The following ranges are considered for component and switch reliabilities: 
    Component reliability: [0.95, 0.93, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.50, 0.40, 0.30]
    Switch reliability: [0.99, 0.95, 0.90, 0.85, 0.80, 0.75, 0.70, 0.65, 0.60, 0.50, 0.40]
    In previous studies of RAP in which this benchmark problem was considered, the optimal structure always used 2, 3, or 4 components in each subsystem and component reliability ranged over [0.80, 0.95]. In this paper, these three different levels of components are, therefore, considered as three different subsystems and the efficiency of each strategy is evaluated. The results are, finally, compared with those obtained from implementing the K-mixed strategy on the benchmark problem.
    All the combinations of component and switch reliabilities are considered and the reliability of each of the four strategies is calculated for each combination and compared. In each combination, the reliability values of different strategies are calculated and sorted in a descending order. The problem is solved with 11 different values for switch reliability and 12 different values for component reliability. This yields 132 test problems. All the different combinations are presented in three 12 ℅ 11 tables, in which the rows represent switch reliability and columns represent component reliability. The results obtained for the subsystem with 4, 3, and 2 components are presented in Figs. 4, 5, and 6, respectively. Each cell in these tables represents the order of different strategies from the best to the worst one. The abbreviations used in these tables are presented in Table 2.
    For more clarification, Figs. 4?6 are presented in different colors, with green representing the range in which standby strategy leads to better results, blue denoting the combinations in which the mixed strategy leads to better result, yellow representing the ranges in which the new strategy (K-mixed) results work better than others, and finally brown indicating the active strategy as the best one.
    As shown in Figs. 4 and 5, the active and standby strategies exhibit better performances at the corners of the matrixes (i.e., highly reliable components with a weak switch, or a highly reliable switch with weak components). In most of the combinations, however, the proposed K-mixed strategy outperforms others. It is worth mentioning that the mixed and K-mixed strategies can be de?ned for subsystems with more than 2 components while in a subsystem with two components, only the active and standby strategies can be de?ned.
    Figs. 7?9 present the effects of the switching system on the different strategies for all the three proposed subsystems. These figures also demonstrate the reliability values of different strategies for a switch reliability reduced from 0.99 to 0.40 and a component reliability fixed at 0.65. Clearly, the standby strategy is the one most sensitive to switch reliability followed by the mixed strategy (in structure II). The active strategy is not sensitive to switch reliability because it does not use any switching system. It is interesting to note that the K-mixed strategy in its first structure is somehow unaffected by switch reliability and may be considered as a reliable strategy. Therefore, if the switching system is not reliable, then the proposed K-mixed strategy is beneficial for the system.
5.2. Series-parallel system
    In this subsection, the famous series?parallel system in RAP is investigated and the K-mixed strategy is implemented on this benchmark problem. The series?parallel system consists of 14 subsystems in each of which three or four component choices with predetermined cost, weight, and reliability are available. The input data for this benchmark is presented in Table 3. The purpose is to maximize system reliability at a given 100 units of time by considering constraints on system cost (C = 130) and system weight (W = 170). It is assumed that the failure detection and switching hardware activate the redundant component only in response to a failure.
    In order to analyze the efficiency of different strategies, particularly the proposed K-mixed one, considering changes in switch reliability, the benchmark problem is solved for the different switch reliability values of 0.99, 0.95, 0.90, and 0.85. The main goal is to investigate the capability of the new strategy to cope with different situations. A second goal is to evaluate the performance of the different strategies studied by changing the switch reliability value. More specifically, it is interesting to observe how the best strategy for a subsystem might change with changes in switch reliability values.
    In order to make a clear comparison of the proposed and the conventional strategies, for each value of switch reliability, the problem is solved in two situations:
    Situation I: The subsystems can use only the conventional redundancy strategies; i.e., standby, active, and mixed,
    Situation II: In addition to the conventional strategies, the subsystems are allowed to use the K-mixed strategy.
    The proposed GA is used to solve the benchmark problem in each situation. The problem is solved then in five trials and the best solution is taken as the final solution. The best solution with maximum reliability is subsequently chosen for comparisons between the two different situations. Four different values are considered for switch reliability. The results obtained for each value are presented below.
    We begin with a switch reliability of 0.99 that was also used in all previous studies of the benchmark problem in question [2?4,16?20,24]. The problem is solved in the two situations (i.e., with and without the K-mixed strategy) and the results are reported in Table 4.
    Ardakan and Hamadani [2] considered the same benchmark problem and employed a mixed strategy to find an optimal solution with a reliability of 0.992328. They assumed that the component time-to- failure (TTF) followed an Erlang distribution. For the K-mixed strategy, we assume that all the component TTFs follow an exponential distribution. Therefore, in this paper, the mixed strategy is also considered with an exponential TTF so that exhaustive comparisons are possible to make. For other problems, the same procedure is applied and the best solutions are obtained. The benchmark problem is formulated and solved by considering the mixed strategy with an exponential distribution. The structure of the best solution obtained in this paper for the benchmark problem with an exponential TTF is the same as that obtained by Ardakan and Hamadani [2] and the reliability of the best solution with an exponential TTF is equal to 0.98194689. Thus, the same best structure is obtained for this problem with either an Erlang or an exponential TTF.
    The benchmark problem is then solved with the proposed K-mixed strategy applied to the system. The optimal structure is observed to be almost the same as the previous one and the three conventional strategies, but the K-mixed one, are used in all the subsystems. This result was predictable from Figs. 4?6. However, the GA developed in this paper finds a solution with a better reliability (i.e., 0.982022) compared to the best solution obtained in previous studies [2]. When component TTF in this new solution is changed to an Erlang distribution, the system reliability is equal to 0.992335, which is greater than the one (0.992328) obtained by Ardakan and Hamadani [2]. Table 4 shows the results of the comparisons between the two situations when a switch reliability of 0.99 is adopted.
    According to Table 4, the mixed strategy outperforms the K-mixed one while this strategy is also used in all the subsystems with more than 3 components when a switching system with reliability of 0.99 is employed. Moreover, the proposed GA is found to be superior to that presented in [2] as it was able to find a better solution. Table 5 reports the results obtained for the proposed benchmark with and without the K-mixed strategy when the switch reliability is equal to 0.95. Because none of the previous studies considered this value for switch reliability, the results of both situations are presented here only with an exponential distribution.
    As seen in Table 5, the final solution uses four subsystems with the K-mixed strategy when it is applied to the benchmark problem. This solution is distinctly different from the best structure with a switch reliability of 0.99. Comparison of the results for Situations I and II reveals that the K-mixed strategy improved the overall reliability of the system. These results are in complete agreement with those shown in Figs. 4?6. For example, the second subsystem uses two components with the active strategy while it is shown in Table 6 that the best strategy for this value of switch reliability (0.95) and two components is the active one.
    These results are also in complete agreement with our findings reported in Figs. 4?6 where it is seen that the best redundancy strategy for each subsystem depends on component reliability when the switch reliability is fixed. For example, in the first subsystem, three components of type 3 are chosen with a component reliability equal to 0.91. It is clear from Fig. 5 that the best strategy in this situation is KM1 (i.e., K-Mixed redundancy strategy in structure I). For other subsystems, the results can be compared with those in Figs. 4?6. Table 6 reveals the differences between the solutions obtained for the two Situations I and II. 
    Tables 7 and 9 report the results obtained for the benchmark problem with switch reliabilities of 0.90 and 0.85, respectively. Comparisons of the two Situations (i.e., using conventional or the K-mixed strategies) are reported in Tables 8 and 10 for each value of switch reliability. The interesting point in these two final test problems is that, as also shown in the previous section, different strategies exhibit different degrees of sensitivity to switch reliability. It was shown that the K-mixed strategy with the structure I had the minimum sensitivity to switch reliability. As a result of this fact, the final solutions in Tables 7 and 9 employ more subsystems with the K-mixed strategy in Structure I (i.e., KM1).
    Fig. 10 presents the reliability values for the different subsystems in the two situations I and II. Fig. 11 presents a complete comparison of Situations I and II for different values of switch reliability. Clearly, the proposed K-mixed strategy helps the system remain highly reliable despite the reduced reliability of the switching system. When switch reliability is lower, the overall system reliability in both Situations I and II get closer because the system prefers to use an active strategy which is not sensitive to the switching system. Fig. 12 presents the structure of the series-parallel system considering different switch reliability values. It shows how each subsystem uses the best redundancy strategy to gain the highest reliability.
6. Conclusion
    In this paper, a recently introduced redundancy strategy called the K-mixed strategy was investigated as a general form of the mixed strategy. This strategy can be used in all systems with redundant components. The mathematical formulation of this strategy was initially presented and a well-known series-parallel system was considered and a novel mathematical model was developed to evaluate the efficiency of the proposed strategy. The problem was formulated as a non-linear integer programming model subject to a number of constraints. The reliability of the system was then calculated in two different situations: with and without the K-mixed strategy used in the subsystems. Moreover, different values of switch reliability were considered and the problem was solved. Finally, the efficiency of the proposed strategy was investigated against each of these values. Numerical results revealed that the proposed K-Mixed strategy outperformed the previously used mixed, standby, and active strategies for most combinations of components and switch reliability. For future studies, the proposed strategy is suggested to be implemented in other reliability problems such as RRAPs.
    
The evolution of system reliability optimization
    System reliability optimization is a living problem, with solutions methodologies that have evolved with the advancements of mathematics, development of new engineering technology, and changes in management perspectives. In this paper, we consider the different types of system reliability optimization problems, including as examples, the redundancy allocation problem (RAP), the reliability allocation problem and the reliability-redundancy allocation problem (RRAP), and provide a flow of discussion and analysis on the evolution of the approaches for their solutions. We consider the development and advancement in the fields of operations research and optimization theory, which have allowed the formalization and continuous improvement of the methods and techniques to address reliability design problems of even very complex systems in different technological domains. Technological advances have naturally brought changes of perspectives in response to the needs, interests and priorities of the practical engineering world. The flow is organized in a structure of successive ¡°Eras of Evolution,¡± namely the Era of Mathematical Programming, the Era of Pragmatism, the Era of Active Reliability Improvement. Insights, challenges and opportunities are highlighted.
1. Introduction
    ¡°Success is walking from failure to failure with no loss of enthusiasm¡± ? Winston Churchill
    Reliability engineering is a formal engineering discipline, founded on mathematical principles, particularly of probability theory and statistics, for systematically and rigorously analyzing functional problems in components and systems with the aim to produce a reliable design. As an engineering discipline, reliability aims at analyzing and evaluating the ability of products and services to perform the functions that they are intended to provide by design.
    While technology improves and advances, the complexity of modern engineered systems also increases. At the same time, consumers¡¯ expectations for high functionality, high performance and high reliability increase, leading to challenges and opportunities. Then, although system reliability optimization problems have been studied, analyzed, dissected and reanalyzed, the continuous rise of new challenging problems demonstrates that this general research area will never be devoid of interesting problems to be solved.
    On one side, the development and advancement in the fields of operations research and optimization theory have allowed the formalization and continuous improvement of the methods and techniques to address reliability design problems of even very complex systems in different technological domains. On another side, the evolution of technology, the advancement of research ideas and theories, have naturally brought changes of perspectives, in response to the needs, interests and priorities of the developing practical engineering world. So, the development and application of formal optimization methods to the practical goal of achieving maximum reliability under various physical and economic constraints, has remained an ongoing topic of scientific development.
    In formal terms, the task of optimization involves formally conceptualizing the decision variables, the constraints and the single or multiple objective functions that describe the performance of the engineering design problem, and searching for the combination of values of the decision variables that achieve the desired goals with respect to the objective functions. Whether expressed explicitly in mathematical terms or not, every engineering design problem has design objectives which should be maximized or minimized or designed to achieve some acceptable requirement. When there is a single predominant objective that can be expressed with a series of explicit mathematical equations, then the problem can potentially be solved using mathematical programming methods or useful heuristics. Researchers working within the broader mathematical programming community are continually developing new methods and algorithms to solve broader classes of problems, larger and more difficult problems, and to do so more efficiently than before.
1.1 Eras of research evolution
    The research in complex system reliability optimization has evolved as a continuum of ideas and techniques. This evolution can be loosely and chronologically classified into the following three eras:
    Era of Mathematical Programming
    Era of Pragmatism
    Era of Active Reliability Improvement
    The era of mathematical programming is preceded by the original development of innovative groundbreaking methods, such as dynamic programming and the simplex algorithm (or linear programming), for which the reliability optimization problem has served as a very interesting and practical vehicle to demonstrate the methods and apply them. Yet, practicing reliability analysts recognized the limitations of these methods in practical applications, mostly because only problems that could be formulated to strictly meet the assumptions required by the methods could be solved, and this is rarely the case in practice. Furthermore, only small and/or unrealistic problems could be solved, because of computation limitations at the time.
    Driven by the desire to apply reliability optimization in practice, the era of pragmatism evolved and became increasingly important. New problems were solved and new methods developed in response to the pressing needs to consider and integrate into the problem some critical issues that could not be readily accommodated by the rigorous mathematical methods. For example, actual engineering systems problems could fall outside the assumptions required by the methods. Analysts were interested in complex forms of redundancy perhaps mixing functionally equivalent, but different components, whose failure behavior may not be simply described as a transition from one functioning state to a failure state, but rather as a process of transition across multiple states. To address a broader range of problems, compromises could be accepted, thus expanding the practical usefulness and applicability of the optimization methods.
    In these previous eras, reliability optimization had been mostly considered as singular, static analyses to be conducted and implemented to yield the desired design reliability of the system. Under this view, common assumptions were made on the existence of populations of homogeneous components and systems, sharing the same failure behavior but with failure occurrences being independent from one another. Furthermore, the implied assumption was that the conditions defined or considered when conducting the analysis are static and remain unchanged throughout the horizon of the analysis (the often-called mission time).
    When changes occur during the system lifetime, then the results of the analysis are simply no longer valid and applicable. We are currently experiencing another industrial revolution, particularly driven by the increase in information sharing, data availability and computational capabilities. In particular, with the proliferation of sensors, environ- mental stresses, usage stresses, failure data, etc., can be collected and processed at regular time intervals, and the advancements in information knowledge that these can bring on the states of the systems, offer new opportunities of development for the analysis and assessment of reliability. The era of active reliability improvement is, then, ongoing and it is visionary in recognizing that components and system conditions change throughout their lives, and system reliability optimization methods, to be practically useful, need to dynamically respond to these changes.
    Within each of the three macro-eras discussed above, further sub- classification can be introduced, in a specific and unique way. In the era of mathematical programming, further sub-classification can be done based on the specific mathematical algorithms developed to solve the reliability design problem. For the era of pragmatism, sub-classification can refer to which practical consideration in the design problem, which previously could not be analyzed, was now readily being addressed by the available optimization models. Within the era of active reliability improvement, sub-classification can be based with reference to the available data and new models for real reliability improvement.
2. Reliability optimization problems
    System reliability design problems have multiple, and often competing objectives. However, there are some universal ones, including reliability (to be maximized) and cost (to be minimized). Often, the approach taken is to follow a prioritization of the objectives by the decision-makers, select the most important objective as the objective function and constrain the other objectives within acceptable limits.
    For each formulation to be studied or solved, system reliability optimization problems must have three elements: decision variables, constraints and an objective function or functions. The decision variables are those variables that can be changed or decisions that can be made to improve performance, with respect to the objective function or functions. Examples of decision variables include component type (with its intrinsic characteristic of failure behavior and reliability), redundancy configuration in the system and others. Constraints are mathematical expressions of practical limitations, such as monetary budget or acceptable reliability, which limit the choice of decision variables in relation to their feasibility of respecting the constraints.  The objective function measures the performance of the system for given values of the decision variables, and thus, enables the decision on the optimal combination of variables values for the optimal solution. The objective function can often be the system reliability to be maximized, or the system cost to be minimized.
    Different forms exist of the system reliability optimization problem. Three typical ones are the redundancy allocation problem (RAP), the reliability allocation problem and the reliability-redundancy allocation problem (RRAP). The solution methods for each problem are obviously different, because of the assumptions and structure of the problem. A thorough review of system reliability optimization is given by Kuo et al. [1?3].
2.1. Redundancy allocation problem
    The most widely studied reliability optimization problem is the RAP. For many systems composed of discrete component types, with fixed cost, reliability and weight, in mathematical terms system design becomes a combinatorial optimization problem. For providing the demanded system functions, there may often be alternative component types available, at different cost, reliability, weight and other proper- ties. The practical problem is to select the optimal combination of components types (decision variables) to collectively meet reliability, weight, etc. (constraints) at a minimum cost (objective function), or alternatively, to maximize reliability (objective function) while achieving given minimum (or maximum) values of other system properties (constraints).
    For the mathematical formulation of this problem, we can consider that there are mi discrete component type choices available for each subsystem (i = 1, ÿ, s), which the system is formed of. Fig. 1 shows a typical example of a system with a number s of k-out-of-n logic sub- systems. If k is equal to 1 for each subsystem, then this is a simple series-parallel system. For each subsystem, ki components must be selected from the mi available choices (e.g., assuming an unlimited amount available for each of the components). The optimal decision is driven by the possibility of placing additional components in parallel in any of the subsystems and/or adding levels of redundancy (> ki) of lower reliability components as an alternative to using more reliable (and expensive) ones. In other words, there is a large number of possible solutions to test, even for relatively small problems (small number of subsystems, small number of components, small number of components types, etc.).
    The RAP for a series-parallel system, as the one shown in Fig. 1, can be formulated as to maximize reliability or minimize cost, under various constraints. Often the RAP is solved for series-parallel systems, but other system structures have been considered, as well as more advanced forms of series-parallel systems, including those with several failure modes, phase mission types, systems with uncovered failures and systems with imperfect fault coverage.
    RAP has proven to be quite difficult to solve. Chern [4] showed that the problem is NP-hard and many different optimization approaches have been used to determine optimal or ¡°very good¡± (i.e., near-optimal) solutions, including (1) dynamic programming, (2) integer programming, (3) mixed integer and nonlinear programming, or (4) evolutionary algorithms.
2.2. Reliability allocation problem
    The reliability allocation problem and RAP are fundamentally different problems. For the reliability allocation problem, the system structure is fixed and the component reliability values are continuous decision variables. For this problem, there is no general restriction on the system structure. An example of a general system is depicted in Fig. 2. Component cost and other parameters are defined as mathematical functions of the component reliability. Increasing the component reliability (and thus, the system reliability) increases the cost, weight and other factors, which may be included as part of the constraints or the objective function. Here, also, the goal of the optimization is typically to maximize system reliability or minimize system cost, and since the decision variables are continuous, different forms of non- linear programming can be used to determine the optimal solutions. To assure that the constraints are satisfied, Lagrangian multipliers are often introduced as part of the objective function.
2.3. Reliability-redundancy allocation problem (RRAP)
    The reliability-redundancy allocation problem is the most general problem formulation. The system is composed of one or more ¡°sub- systems,¡± i.e., collections of logically connected sets of components. Each subsystem has xi components with reliability of ri as decision variables. The problem is then to optimally allocate redundancy and reliability to the components of each subsystem with an objective to maximize the overall system reliability. The system and subsystems can be in any arrangement, as depicted in Fig. 2. Again, typically the objective of the optimization is to maximize system reliability or minimize system cost.
    RAP is often considered for series-parallel systems, but other system structures can be considered as well. The reliability allocation problem and RRAP has been applied to many different system structures, including common structures (series, parallel, etc.), but also consecutively connected systems, sliding window systems, flow transmission systems, common bus systems and others.
2.4. Component assignment and sequencing problems
    Two other related system reliability problems are assignment or sequencing of components within a system. For assignment problems, there is typically a defined system structure, but the available components are assigned to specific locations with the system. Sequencing is particularly interesting and important for systems with standby redundancy, where the components are activated as needed in accordance with a defined sequence. Sequencing problems are solved for optimization for systems with consecutively connected systems and/or standby components with cold or warm redundancy required a defined activation sequence.
    The problem of assignment of components to positions within a system to maximize reliability was originally presented by Derman et al. [5,6]. An important early research effort [7] defined optimal assignments for different system structures. The related problem involves sequencing of redundant components. More recently, sequencing of standby redundant components has been considered by researchers to maximize system reliability [8,9].
2.5. Other optimization problems
    There have been many other related system reliability optimization problems as well. There have been extensions of the original problems, as well as related problems including spares provisioning, optimization of series-parallel topology, optimal load distribution, optimal mission abort policy, test plan allocation, preventive maintenance optimization and others [2].
3. Era of mathematical programming
    Methods initially developed to solve complex system reliability optimization problems can be referred to as belonging to the era of mathematical programming. The emphasis was on applying advanced mathematics to obtain mathematically provable optimal solutions. The priority was on solving problems to optimality, but in doing so, often the structure and size of the problems were limited to be able to apply the rigorous mathematical model. The problems being solved in the end were rarely realistic or indicative of actual design problems. Assumptions were often introduced for mathematical convenience, and problems that did not meet those conditions were avoided. However, these original methods are very important and influential, and they have served as the foundation for much of the research and development work in quantitative reliability engineering that followed.
    Solution of complex system optimization problems was possible because of advancements in operations research theory and the development of new algorithms. A key mathematical challenge was to find an efficient way to at least approximate solutions to problems otherwise unsolvable with classic analytical methods. Numerical and mathematical approaches were introduced to successfully solve such problems, with the turning point having been the realization of the first computer, which provided the possibility to perform sets of operations for hand- ling large numbers of data in a time much shorter than previously possible.
    The newborn field of computer science sparked the mind of several mathematicians that tried to formulate methods to use computers to help solving practical problems involving high computational efforts. The field of mathematical optimization was then born. Several pioneers from the first half of the 20th century contributed to develop formulations and algorithms to be implemented in computing machines to solve difficult optimization problems, including from the fathers of  linear programming, Leonid Kantorovich and George Dantzig, Richard Bellman, originator of dynamic programming, to the founders of evolutionary algorithms, such as Lawrence Fogel, John Holland, Ingo Rechenberg and Hans-Paul Schwefel. Their ideas were the milestones from which other variants of the methods developed until today.
    When mathematical programming methods associated with the field of operations research were being developed and formalized, but still in their infancy, researchers were searching for interesting applications. Maximization of system reliability was considered an attractive application. Indeed, it is a difficult and challenging problem, yet practical and useful to demonstrate the utility of new mathematical programming algorithms. Typical formulations of the problem are challenging, with a highly nonlinear objective function and often integer decision variables.
    Dynamic programming was applied to the system reliability problem as one of the very first applications explored. Considering the initial formulations of RAP, dynamic programming could almost be directly applied to efficiently obtain optimal solutions.  The problem was that it was difficult to extend it to solve more realistic or actual design problems. Linear programming, or the simplex algorithm, is a very powerful advancement, which allowed for very large linear problems to be solved. However, system reliability is a highly nonlinear objective function, so researchers had to be creative to exploit the power of the simplex algorithm to solve reliability problems. RRAP is nonlinear with both continuous and integer decision variables creating another very challenging problem, that was often solved using some variation of nonlinear programming.
3.1. Dynamic programming
    Dynamic programming was originated in 1954 by Richard Bellman [10], and system reliability optimization was among the first problems studied. The aim was to treat mathematical problems arising from the study of multi-stage decision processes. The key advancement, as compared to previous methods, relies on the fact that when analyzing these problems, not all the possible sequences of the present and following stages are needed, i.e., it is possible, instead, to state general conditions to determine for each stage the most suitable decision ac- cording to the current state only, whereas classical approaches gathered information about all the sequences, making the calculation cumber- some and unpractical [10].
    A problem solvable by dynamic programming can be performed as a system, described by a set of quantities, the state parameters, that undergo a state variation caused at a certain time t by a decision made by the user. The solution aims at taking an initial decision for guiding the future ones so that it is possible to maximize a given objective function of the state parameters. In many cases the number of parameters considered to make the decision is very large, especially when considering stochastic processes in which decisions determine a distribution of outcome states. In these cases, the approach allows reducing the dimension of the problem by focusing on the current time. To perform the dynamic programming optimization, the state parameters and the sequence of decisions to analyze, i.e., a policy, are needed. The optimal policy is, then, the one that determines the decision required at each time with respect to the current state of the system.
    Bellman [11] and Bellman and Dreyfuss [12,13] demonstrated that an optimal solution to the RAP could be found using dynamic programming. In their problem, there was only one component choice for each subsystem, and the objective was to maximize reliability with a single cost constraint. For each subsystem, the problem was to identify the optimal levels of redundancy. A well-known disadvantage of dynamic programming formulations is the difficulty of efficiently solving problems with multiple constraints.
    Fyffe et al. [14] used a dynamic programming approach to solve a more difficult design problem. Their problem involved a system with 14 subsystems and cost and weight constraints. For each subsystem in the Fyffe formulation, there are three or four different component choices each with different reliability, cost and weight. However, several of these component choices are dominated by other competing choices. Similar to Bellman, only 1-out-of-n redundancy was considered. To accommodate multiple constraints within a dynamic programming formulation, they used a Lagrangian multiplier for the weight constraint within the objective function.
    Instead of using Lagrangian multipliers, Nakagawa and Miyazaki [15] used a surrogate constraint combining the cost and weight constraints into one. They then solved a series of problem iterations with different surrogate multipliers, with a heuristic to successively update the surrogate multipliers. Stopping criteria was provided to identify cases when their algorithm would not lead to an optimal solution. The algorithm was demonstrated by solving 33 variations of the Fyffe problem with different weight constraints. Of the 33 problems, they found optimal solutions to 30 of the problems. Otherwise, the final solution was not feasible (although there are feasible solutions to the problem).
3.2. Linear programming/integer programming
    Linear programming (LP) and Integer Programming (IP) are powerful methods to find the maximum or minimum of a linear function describing the performance under assessment, which is called the objective function. A standard mathematical definition is the following: max{cx; Ax b,0}or min{cx; Ax	b, x	0}
    For this formulation y = cx is the objective function to be maximized or minimized, x is the vector of non-negative variables to be found, b and c are vectors of known coefficients, and A is a matrix of known coefficients that when multiplied by x have to satisfy the constraints expressed by the vector of coefficients b. The problem is defined within a convex polyhedron-shaped feasible region, intersection of finitely half-spaces represented by linear equalities/inequalities.
    This optimization framework was initially used by the Soviet economist Leonid Kantorovich who was trying to organize the actions of soldiers to decrease expedition costs and increase enemy losses. At the same time another economist, T.C. Koopmans was working on the applicability of linear programming to solve classical problems. Their work was recognized by the Nobel prize in economics in 1975. Following that, mathematician George Dantzig developed an LP methodology to solve optimization problems, providing a formal proof of the solution [16]. One of the most important achievements was the reduction of the possible solutions, and therefore, the advantage of the method in terms of computing power needed.
    If the objective is to maximize reliability or minimize cost given multiple nonlinear but separable constraints, many variations of the problem can be transformed into an equivalent integer programming problem using 0?1 decision variables. This was originally demonstrated by Ghare and Taylor [17] who used a branch-and-bound approach to solve many randomly generated RAPs with 30 subsystems with 15 constraints, and 99 subsystems with 10 constraints. Ghare and Taylor assumed that there was only one component choice for each subsystem and redundancy was active always 1-out-of-n redundancy.
    Bulfin and Liu [18] also used an IP approach to solve the RAP. They developed one heuristic and two exact algorithms to be applied de- pending on the problem structure. They formulated the problem as a knapsack problem and used a surrogate constraints approach, similar to Nakagawa and Miyazaki [15]. The surrogate multipliers were approximated as the optimal Lagrangian multipliers as found by sub- gradient optimization. Bulfin and Liu formulated the Fyffe problem and its variations as integer programs and solved the 33 problems previously investigated by Nakagawa and Miyazaki, and other examples as well. They also considered only subsystems with 1-out-of-n redundancy. Coit and Liu [19] extended their approach to k-out-of-n redundancy subsystems if no mixing of component types is allowed within the subsystem parallel structure.
    Other examples of IP solutions to the redundancy allocation problem were presented by Misra and Sharma [20], Gen et al. [21], and Gen et al. [22]. Misra and Sharma presented a very fast and useful algorithm to solve integer programming problems formulated like those of Ghare and Taylor [17]. Gen, Ida, Tsujimura and Kim and Gen, Ida and Lee formulated the problem as a multi-objective decision-making problem with distinct goals for reliability, cost and weight.
    For other system reliability applications, LP has been proved useful in the context of structural system reliability by Corotis and Nafday [23]. They used LP to identify the most critical failure mode for a structural system. More recent papers [24] demonstrate that LP is particularly useful in structural system reliability analysis. The LP bounds can be applied for any type of system and for different component probabilities. These bounds are the narrowest possible bounds that one can be obtained for a system, for any specified information for the component failure probabilities.
    The main drawback of using LP or IP is that the size of the problem to be solved, and its computational cost, increases exponentially with the number of components, questioning its efficiency when it comes to realistic, complex systems. An approach has been proposed to overcome this issue and extend the applicability of LP. Decomposing the entire system into subsystems based on failure modes can be applied to identify component state probabilities and joint probabilities of the states of a small number of components. It can also provide bounds for the failure probability of large systems. This is particularly useful when other methods are not applicable. This approach has been presented by Chang and Mori [25]. The idea is the development of a relaxed LP (RLP) bounds method to reduce the number of design variables using the universal generating function (UGF) [26].
    RLP bounds method can be applied to a single series or parallel system, but it is not applicable to a general system that consists of both series subsystems and parallel subsystems. For this reason, an additional assumption can be made to obtain the Strategic Relaxed Linear Programming (SRLP). After decomposing the system according to different failure modes, each critical failure mode is also considered as a system (or subsystem) itself. The bounds on the system failure prob- ability can be computed by the RLP bounds method if it is a series or parallel system, and the bounds on its joint failure probability can also be computed by the RLP bounds method. The bounds estimated by the RLP bounds method are, then, used as constraints in solving the LP problem for estimating the failure probability of the entire system.
3.3. Nonlinear programming
    Nonlinear programming (NLP) refers to a collection of optimization methods defined by the same main principles of linear programming, with the difference that the objective function and/or its constraints, and therefore the feasible region of the problem, are defined with at least one nonlinear equation. The addition of nonlinear equations makes the optimization problem much more difficult to be solved, for example:
    In a nonlinear function it is hard to assess whether a maximum is local or global, and unlike linear functions where a max/min location is restricted to the borders of the feasible region, for a nonlinear function it can be in the interior of the feasible region.
    If the objective or any constraints are non-convex, the problem may have multiple disconnected feasible regions and multiple locally optimal points within such regions.
    The numerical method chosen to get to the solution may cause two different starting points to lead to two different solutions.
    It is difficult to ensure that the constraints applied to the problem meet the requirements of the feasible region.
    A tolerance region for the solution has to be considered with a proper uncertainty.
    NLP solvers generally attempt to solve the problem by computing gradient values at various trial solutions, and moving in the direction of the negative gradient (when minimizing, positive gradient when maximizing). They usually also exploit second derivative information to follow the curvature as well as the direction of the problem functions. To solve constrained problems, NLP solvers must take into account feasibility and the direction and curvature of the constraints as well as the objective. A review of nonlinear programming optimization methods is provided by Floudas [27].
    Mixed integer and nonlinear programming have been effectively used to solve the redundancy allocation problem. Considering reliability optimization, important research contributions were provided by Tillman et al. [28,29]. In these problems, component reliability is a continuous decision variable and component cost is expressed as a function of reliability and other parameters.
3.4. Evolutionary algorithms
    Evolutionary algorithms (EA) are a group of optimization methods that perform their task with a built-in ability to evolve. EA have the three following common features:
    1. Population-based, i.e., they handle a group of solutions, the population, manipulated in different ways to optimize the problem;
    2. Fitness-oriented, meaning that EAs favor individuals (a solution belonging to a population) which are fitter than others according to established criteria. Each individual has a gene representation which is its code together with a performance evaluation, i.e., its fitness value. Choosing fitter individuals drives the optimization and the convergence of the algorithm;
    3.Variation-driven: In order to simulate evolution steps, individuals are subject to random variations, necessary to explore the problem's solution space.
    The basic intent of EAs is to implement the Darwinian concept of survival of the fittest, applying it to functions to optimize. Through each generation, the solutions considered weak in terms of the specific criteria adopted for the optimization face extinction, whereas the best ones combine to produce new individuals that potentially can improve the convergence to an optimal solution.
    The first attempts to mimic evolution by simulating genetic processes date back to Fraser [30] and Bremermann [31]. The main contributor, however, is John Holland, who in 1975 published ¡°Adaptation in Natural and Artificial Systems [32]¡± in which he introduced the main fundamental concepts of genetic algorithms (GA). In GA, each in- dividual of the population has two characteristics: a chromosome and a fitness value representing its quality. The chromosome is composed of genes; in the original formulation each gene was considered as a bit, therefore either 1 or 0, and the chromosome was a string of zeros and ones. In the following years, several researchers developed new forms of GAs.
    A chromosome can be viewed as a sorted string or vector. The evolutionary process starts when all fitness values of the initial population have been assigned. Afterwards, the selection process begins, in which some individuals are selected in order to be included in the mating pool. The fittest individuals are more likely to be selected and spread their properties to the offsprings: individuals in the mating pool are combined to produce new hybrids whose finesses are evaluated to decide whether or not to pass onto the next population, replacing other individuals. It is common practice to keep a constant number of individuals inside a population at each stage.
    GA have not been applied practically for system reliability problems until the 1990s, when researchers such as Coit and Smith [33,34] implemented it in a combinatorial reliability design problem. The evolutionary optimization proved very efficient in terms of cost-effectiveness of the selection of the parts and allocation redundancies for system reliability. Several authors then tackled optimization problems by GA. For example, Painton and Campbell [35] presented a model based on such methods, highlighting again their robustness and capability of finding the optimum over a high dimensional nonlinear space in a considerably shorter time than the required one for enumeration. In order to improve the reliability of a personal computer, they identified the main components and their failure modes in order to determine some possible improvement levels.
    With regards to applications, reliability allocation to minimize total operating costs, subject to an overall plant safety goal, was presented by Yang et al. [36]. System optimization was used to enhance the design, operation and safety of new and/or existing nuclear power plants. They determined the reliability characteristics of reactor systems, sub- systems, major components and plant procedures in accordance with a set of top-level performance goals. The cost for improving and/or de- grading the reliability of the system was also included in the reliability allocation process as a multi-objective problem formulation. GA was demonstrated to determine effective solutions for a typical pressurized water reactor.
    Konak et al. [37] presented general guidelines regarding the implementation of GA for multi-objective reliability optimization, pro- posing a list of techniques and highlighting the advantages and difficulties of each of them. The reliable network design problem has been studied using multi-objective GA. Kumar et al. [38] presented a multi- objective GA to optimize telecommunication networks while simultaneously improving network performance and design costs given a system reliability constraint. Kim and Gen [39] studied bicriteria spanning tree networks considering the objectives of cost and reliability, while Marseguerra et al. [40] determined optimal surveillance test intervals using a multi-objective GA to improve reliability and availability.
    Problems studied by Martorell et al. [41,42] involved the selection of technical specifications and maintenance activities at nuclear power plants to increase reliability, availability and maintainability for safety- related equipment. They also considered the optimal allocation of more reliable equipment, testing and maintenance activities to assure high reliability, availability and maintainability levels for safety-related systems. Additional limited resources (e.g., budget and workforce) were required, to form another a multi-objective problem. Solutions were obtained by using both single-objective GA and multi-objective GA, to solve the problem of testing and maintenance optimization with the objective functions of unavailability and cost.
    Various other meta-heuristics have been used for reliability optimization. For example, Ant Colony Optimization (ACO) is a population- based, general search technique for the solution of difficult combinatorial problems [43]. The method is inspired by the pheromone trail laying behavior of real ant colonies. In ACO, artificial ants probabilistically build solutions by taking into account pheromone trails, which change dynamically at run-time, to reflect the agents acquired search experience and heuristic information on the problem instance. ACO algorithms have been applied for the reliability optimization of series- parallel systems [44], also including quantity discounts on the redundant components [45], and network optimization by embedding a Cellular Automata approach combined with Monte Carlo simulation for network availability assessment [46], within a multi-objective ACO search engine [47]. ACO has also been applied in hybrid form with Simulated Annealing (SA), called ACO SA, for the design of communication networks [48], where the design problem is to find the optimal network topology for which the total cost is a minimum and the all- terminal reliability is not less than a given level of reliability.
    SA is another type of meta-heuristics introduced by Kirkpatrick et al. [49] and Cerny [50] as a general probabilistic method for solving combinatorial optimization problems. SA searches the global optimal solution avoiding entrapment in poor local optima by allowing a (probabilistically) occasional uphill move to worse solutions. A SA algorithm for communication network reliability optimization has been proposed [51], which selects the optimal set of links that maximizes the overall reliability of the network subject to a cost constraint, given the allowable node-link incidences, the link costs and the link reliabilities. The algorithm employs a variation of the SA approach coupled with a hierarchical strategy to achieve the global optimum. SA has also been applied to search the optimal solution of system reliability-redundancy allocation problems [52] also considering nonlinear resource constraints [53]. Different SA strategies have been applied to solve multi- objective system reliability optimization problems [54].
    Particle Swarm Optimization (PSO) is another algorithm conceptually based on the social behavior of biological organisms that move in groups, such as birds and fishes [55]. The basic element of PSO is a particle, which can fly throughout the search space toward an optimum by using its own information and that provided by other particles within its neighborhood. As in GA, the performance of a particle is determined by its fitness that is assessed by calculating the objective functions of the problem to be solved. Then, PSO has certainly some similarities to evolutionary algorithms such as GAs, but it also in- corporates a cooperative approach. Indeed, all individuals (particles) which are allowed to survive change their positions over time and one particle's successful adaptation is shared and reflected in the performance of its neighbors. Originally developed for the optimization of continuous unconstrained functions, PSO did not attract much attention from the reliability community, because most reliability optimization problems are of discrete nature and have constraints. However, it has been shown that properly adapted PSO can be an effective tool for solving some discrete constrained reliability optimization problems [56]. PSO has, then, been applied to solve reliability optimization and RAP of complex systems [57].
    Several optimization meta-heuristics have been designed for various optimization applications in reliability engineering, with varying degrees of success. As no meta-heuristic is so versatile to always outperform the other meta-heuristics in all kinds of reliability optimization problems, developing new, good optimization approaches can be very helpful in some specific applications and benefit practitioners providing more options. Overall, some preferences in practice is given to the use of GAs, as they are able to solve both integer reliability problems and mixed-integer reliability problems. Furthermore, their applicability is not limited to series-parallel systems. In many reliability optimization problems, the optimal solutions found by GAs have turned out to be superior to those of the other meta-heuristic methods for both integer reliability problems (in which component reliabilities are given and redundancy allocation is to be decided) and mixed integer reliability problems (in which both the component reliabilities and redundancy allocation are to be decided simultaneously). Therefore, GAs are very competitive and attractive meta-heuristic methods, especially appropriate for design of nonstandard series-parallel systems. In addition, the multiple solutions found by the GA sometimes vary significantly in the
    component reliabilities and/or redundancy allocation for systems. This offers the design engineer a variety of options from which to choose with only small differences in the system reliability.
4. Era of pragmatism
    After exhausting much of the inventory of reliability optimization problems that could be solved to optimality by mathematically rigorous methods, researchers entered into an era of pragmatism. The driver for this was the need to expand the types of problems to treat, considering more complex systems and more realistic reliability behaviors of the components, without necessarily being able to mathematically prove the optimality of the solutions (although this remains highly desirable). 
    Original problem formulations that were solved to optimality often adhered to some common assumptions, although not always, including (i) active redundancy, (ii) perfect switching of redundant components, (iii) limitations on mixing functionally equivalent components within a parallel structure, (iv) binary behavior of components and systems, and others. These assumptions simplified the problems and optimal solutions could be found, but artificially constraining the problem spaces far from real conditions. Therefore, the usefulness of these methods was limited, and there was a need to analyze systems with more realistic behaviors, including multi-state systems, uncertain systems, realistic forms of redundancy, etc.
    For the more realistic and complex problems, the assumptions or model forms required for mathematical programming algorithms could generally not be satisfied. At the same time, more advanced computers and computer processing provided for exhaustive forms of heuristic search. GA and other forms of meta-heuristics were now used pre- dominantly.
4.1. Multi-state systems
    For components and systems used in practice, often a binary state description (functioning or failed) may not be a proper representation of the reliability behavior, because the component and system reliability performance has a range of different levels (Barlow and Wu [58], Hudson and Kapur [59]). However, evaluation of multi-state system (MSS) reliability is more difficult, and potentially mathematically cumbersome.
    Levitin et al. [60], and Levitin and Lisnianski [61] presented pioneering and influential research models to optimize system design for multi-state systems. Levitin et al. [60] determined an optimal system structure, and Levitin and Lisnianski [61] optimized imperfect preventive maintenance intervals. They used a universal generating function (UGF) approach to evaluate multi-state reliability and a GA to search the solution space to determine the best solution, although not guaranteed to be the optimal solution. UGF is a convenient function based on a z-transform that is useful to systematically and efficiently compute multi-state reliability.
    Levitin and his colleagues continued to extend their innovative work to analyze additional applications of multi-state systems. The first formulation of joint structure and maintenance optimization problem for multi-state systems was presented in Levitin and Lisnianski [62], and the optimization approach was extended to systems with common cause failures by Levitin [63]. Later, Levitin and Xing [64] analyzed systems with propagating failures. Each of these research efforts re- presented fundamental advancements. System reliability optimization could, then, be applied to an entirely new class of systems design problems.
    In recent years multi-state models for system reliability assessment have become increasingly popular. In particular, significant research efforts have been devoted to the solution of RAPs for series-parallel multi-state systems (MSSPS) [3,65?67], which was first introduced in [68]. Series-parallel structures are typically considered because they are quite common in practice. Due to the difficulty of the problem, meta-heuristics are often used to solve MSSPS RAP, even though they can become time-consuming, especially on large systems.
    On the other hand, theoretical analysis of meta-heuristics for MSSPS RAP has been generally lacking. Exact/approximated algorithms or guidance for meta-heuristics design have not yet been proposed in the MSSPS RAP literature, while it is important because the application of RAP to multi-state models often requires exhaustive computational resources. Indeed, the difficulty of solving MSSPS RAP is not only due to the well-known problems of MSS reliability evaluation, but also to the discrete, probabilistic and nonlinear nature of RAP problems.
    Another form of a system where the components exhibit multiple states is when component failure time distributions or state prob- abilities and replaced with a stochastic degradation process. This problem can be particularly challenging when the individual component degradation processes are dependent or have interactions. Song et al. [69] determined optimal replacement intervals and inspection intervals for systems with dependent failure processes. Bian and Gebraeel [70] developed a prognostics model for a multi-component system with degradation interactions.
4.2. Uncertainty
    The optimization of system reliability relies on a model to provide a representation of the system failure behavior. The model is built on a number of hypotheses on the types of distributions which the stochastic failure processes of the components obey. The values of the parameters of these distributions need to be estimated, and there is always some level of estimation uncertainty. There is intrinsic uncertainty and in- complete knowledge of the system behavior. Uncertainty can be model or structural uncertainty, which exists on the hypotheses of the model, or parameter uncertainty, which exists on the values of its parameters. 
    In the literature, a number of aspects, factors and causes of un- certainty have been identified, as summarized by Armacost and PetEdwards [71], Zimmermann [72]:
    Lack of information or knowledge: Lack of information, knowledge and/or data is the main source of uncertainty. This type of uncertainty can be reduced by collecting more information and data.
    Approximation: Any model involves some degree of approximation, which is necessary when there is insufficient information to describe exhaustively the phenomenon of interest or when it is desirable to simplify the analysis due to computational constraints or other reasons.
    Abundance of information or knowledge: People are incapable of assimilating many pieces of data and information simultaneously. The analyst usually focuses on those parameters and those pieces of data and information that are considered to be more important, while neglecting the others. This type of uncertainty is related to biases in subjective probability assignments (see Kahneman and Tversky [73] and Aven [74]).
    Conflicting nature of pieces of information/data: When there is conflicting data, increasing the amount of available information and data would not decrease the uncertainty. More data may just increase the conflict among different pieces of information and data. Some information are affected by errors creating the conflict, although the analyst cannot identify them, or otherwise, the model used by the analyst is poor.
    Measurement errors: The measurement of a physical quantity, such as temperature, weight, length, is always affected by the precision of the measurement capability.
    Linguistic ambiguity: An expert may express that something is big, but the meaning of ¡°big¡± is ambiguous, and can be interpreted in different ways.
    Subjectivity of analyst judgments: There can be different interpretations of the same information and data, depending on cultural background and competence of the analyst.
    Uncertainty analysis involves identifying and studying the sources of uncertainty and propagating the effects onto the output of the model. Uncertainty analysis determines the uncertainty in the model output that results from uncertainty in the model inputs (Helton et al. [75]). In the practice of reliability engineering and quantitative risk analysis, it is common to distinguish between aleatory and epistemic uncertainty (Apostolakis [76], Helton and Oberkampf [77]). Aleatory uncertainty refers to phenomena occurring randomly, so probabilistic modeling is appropriate to describe such occurrences. Epistemic uncertainty involves quantifying the degree of belief of the analysts on how well it represents the actual system. It is typically expressed as subjective probabilities of the parameters of the probability models. It can be reduced by gathering information and data to improve the knowledge on the system behavior.
    For system reliability optimization, uncertainty must be properly accounted for. It is often important to consider the uncertainty in the system reliability estimation so that risky solutions with unsatisfactorily high reliability estimation uncertainty can be avoided. System de- signers and users are generally risk-averse. Decision makers would generally prefer the design of a system whose reliability is estimated with large confidence, as assured by the low uncertainty of its estimation. Thus, maximization of the system reliability and minimization of its estimation uncertainty is an important formulation, that should be emphasized.
    System reliability optimization research originally did not consider the uncertainty in the reliability estimation, although Rubinstein et al. [78] is an early example of a model to maximize the expectation of a series-parallel system reliability estimate with component uncertainty. However, maximization of the expectation of the reliability estimate may not be adequate, if it is important to avoid system designs with unacceptably high uncertainty. It is therefore desirable to use a multiple-objective optimization algorithm, which explicitly considers the component uncertainty.
    In Marseguerra et al. [79], a multi-objective GA is developed to select optimal network designs that balance the dual objectives of high system reliability and low uncertainty in its estimation. Monte Carlo simulation is used to evaluate the objective function and Pareto optimality is introduced to handle the multiple preference criteria. The decision variables are the numbers of components, xij, of a given type j to be allocated in the various sections (node pairs & links) i of a network system, i = 1, 2,ÿ, s, and j = 1, 2, ÿ, mi. The network is designed to maximize the expectation of the network reliability and minimize the variance of the estimate (by maximizing the negative variance). Introducing cost and weight constraints, the multi-objective optimization problem may be formulated as follows:
    This is an appropriate formulation for a risk-averse decision maker, as opposed to most optimization algorithms that require or assume risk- neutrality. Many decision makers may prefer a risk-averse solution, with a marginally lower expected value of reliability compared to a solution with a higher expected value, but with unacceptable un- certainty. Epistemic uncertainty has also been accounted for using interval and fuzzy multi-state models [67,80?82].
4.3. Different types of redundancy
    The original formulations of the system reliability optimization problems assumed that all redundancy was active redundancy. This is a convenient assumption because the failure time of a parallel subsystem of components is the maximum of individual component failure times, and the reliability, or probability of survival for some mission time, can be expressed using standard probability principles that are independent of any failure time distribution assumptions. However, many actual subsystem design problems, use a variety of active, cold, warm or hot standby, often within the same design, and therefore the original formulations and solution methods were not practical or applicable for many actual problems.
    System designs with active redundancy have fully activated components that can continue to provide needed design functions in the advent of failure of a primary component, until all redundant components have failed as well. Cold standby redundancy involves the use of non-activated components that can be switched-on in response to failure of a primary component. Often systems are designed with both types of redundancy within different parts of the system, and there are examples where the redundancy type is also a design variable. Cold standby redundancy requires switches to detect failure and activate redundant units. However, the switches can also fail and must be considered in the optimization model. It is assumed that components in cold-standby do not fail, while components in warm standby can fail, but at a lower rate those comparable active or hot standby components. Components in hot standby still require a switching mechanism, but fail at the same rate as active components.
    A solution methodology was developed to determine optimal design configurations for nonrepairable series-parallel systems with cold- standby redundancy by Coit [83], who considered a component with non-constant component hazard functions and imperfect switching. There were multiple component choices available for each subsystem and component failure times are distributed according to an Erlang distribution. Optimal solutions are determined using IP with 0?1 decision variables.
    There are other engineering system design projects the choice of redundancy type becomes an additional design variable. System design optimization was demonstrated in Coit [84] to maximize reliability when either active or cold-standby redundancy can be selectively chosen for individual subsystems. Formulation of the problem allowing a choice of redundancy strategies is more realistic and practical. Optimal solutions to the problem are found using IP considering imperfect switching of standby redundant components [84]. The optimal system design is distinctly different from the corresponding design obtained with only active or only cold standby redundancy. The same problem was later solved using a GA [85]. Most recent research on systems with imperfect switches has been done by Kim [8] and Levitin et al. [86].
    The problem with a choice of redundancy strategies has been ex- tended in several original ways including a mixed strategy [87] combining both active and cold-standby components within the same sub- system. Other recent meaningful system reliability optimization research considering mixed component redundancy have provided important models for more varied and practical applications [88?90]. There have also been multiple objective formulations to the problem with different redundancy types [91,92].
    More recently, other different redundancy strategies or types of problems have been considering including the standby element sequencing problem and the combined sequencing and inspection/ checkpointing/allocation policy optimization for different types of standby (hot, cold, warm, mixed). Some research efforts combining system structure optimization with optimal standby component sequencing are included in [8,9,93,94].
5. Era of active reliability improvement
    There are currently some very promising on-going research activities that can be considered collectively as an era of active reliability improvement. System reliability optimization is not a static model, but it is being conducted continuously in response to new data being collected on failures, component and system degradation, environmental stresses and usage histories. As an integral part of the optimization process, system performance can be optimized and improved dynamically as new data is collected and analyzed to provide a better under- standing of usage conditions and failure behavior, or to compensate for changing conditions. Modern sensor and communications technologies facilitate the collection and transmission of data so that the optimal system design and maintenance plans can be continually enhanced.
    Standard assumptions for most system reliability optimization models have been that component failure times form a homogeneous population, and that the failure time distributions, or reliability for a fixed time, are static or stationary. In practice, both assumptions are at best approximations of actual conditions. Although a population of components may form a homogeneous population, their corresponding failure times are influenced by specific environmental stresses and user requirements/stresses that can vary appreciably for specific sub-populations of users or applications. Also, there can be systemic trends in stresses, that can result in shifting failure time distributions over time. In these cases, there is not actually a homogeneous population of identically distributed failure times, and therefore, most optimization models cannot accommodate these realities
    The era of mathematical programming resulted in entirely new in- sights on optimizing system design, and demonstrated how advanced mathematics can be used to solve this problem. The era of pragmatism extended the more theoretical models or developed new ones to address actual conditions of fielded systems. However, in both of these eras, the optimization results were a final result. The solution to these difficult optimization problems was intended to be performed once, perhaps with associated sensitivity analyses. Of course, as new data was collected, the analysis could be repeated, but the optimization process did not directly integrate changing conditions. In the era of active reliability improvement, the changing conditions and data analyses are an integral part of the model.
5.1. Dynamic system reliability models responding to new data
    Dynamic optimization of system reliability has the potential to achieve responsive system designs, which are highly reliable with changing or diverse conditions. To achieve the highest level of reliability and minimum cost, engineering designs and maintenance plans must address changing conditions or new data that provides better estimates of model coefficients or parameters. To accomplish this, the optimization must be dynamic. The model is solved over-and-over or continually as part of the optimization in response to new data/conditions/etc. as the system is operated.
    Yildirim et al. [95,96] present two comprehensive models that involve integrating sensor-based degradation modeling and remaining life distributions with classical mathematical programming, specifically mixed integer programs. The resulting model optimizes predictive maintenance decisions for a complex system to minimize cost. The application being solved is the unit-commitment problem, a well-studied optimization problem, pertaining to power generation and trans- mission. Yildirim et al. [97] considers opportunistic maintenance scheduling again within an integrated framework that combines mixed integer programming and sensor-based degradation models.
    Hao et al. [98] addresses dynamic optimization of workload assignment to actively control the degradation and failure time for multiple-units system. Components are degrading and failing, but the rate of degradation is a function of workload assignment. Multiple units are arranged in parallel, and several identical machines may need to operate together to simultaneously produce products to meet the high production demand. This parallel configuration is designed with redundancy to compensate for unexpected events. As data is collected, there is Bayesian updating of degradation model parameters and re- optimization of workloads.
    Recent developments by Li et al. [99] have proposed providing industrial assets with a degree of agency, in order to enable real time prognostics and optimization of the asset's operation conditions. They consider the feasibility of improving system-level performance in industrial systems by integrating social networks into the IoT (Internet of Things) concept.
    Bascifti et al. [100] considers a complex system of buses, generators and transmission lines. The model considers the scenarios where un- expected failures happen based on the updated remaining life distributions. The modeling framework in this case is a stochastic optimization model with chance constraints that leverages sensor-based remaining life predictions.
5.2. System reliability optimization customized for specific subsets of users
    Data analytics can also be exploited such that the optimal system design can reflect differences within a population. There can be regional differences or fundamental differences within the user population, and by observing and quantifying specific usage conditions and failure patterns, an optimal design can simultaneously correspond to a collection of diverse users or conditions. A failure time distribution can be considered as a function of usage and environmental stresses, and specific reliability values can then naturally vary to reflect these differences.
    Ramanan et al. [101] studies an advanced distributed optimization problem. There are several interesting aspects and challenges to this problem. The first relates to the computational challenges associated with large scale decentralized optimization and the second relates to the underlying high-performance computing architecture that is would be suitable for such decentralized systems. Advanced data processing of large data sets within subnetworks (local utility companies) is required. 
    Bei et al. [102] presents a model to fully investigate the integrated redundancy allocation and maintenance planning problem with the presence of uncertain future usage stresses. Component failure time distributions are expressed as a function of environmental and usage stresses. A component system design, with component choices and redundancy levels is selected by the optimization model, but specific preventive maintenance intervals are selected for different usage and environmental stress vectors. The problem is formulated as a two-stage stochastic optimization problem with discrete scenarios defined for different usage and environmental conditions. Zhu et al. [103] extends this model by considering uncertain aperiodic changing stresses.
6. Future challenges in system reliability optimization
    The safe and productive performance of industrial systems depends on optimal designs that use equipment reliably, and on testing and maintenance activities that assure the required high level of reliability, availability and maintainability of the equipment. This is done through the efficient assignment of resources that are usually limited. A number of challenges arise in relation to the modern complex systems reliability optimization:
    Integration and response to continual streams of data proving new and updated information.
    Accounting for both aleatory and epistemic uncertainties within the decision-making framework of system reliability optimization 
    Cooperative optimization of multi-agent systems, with individual objectives to be optimized within an overall system optimization 
    Integrated optimization of reliability design, maintenance, spare parts inventory and logistics management
    Dynamic optimization of evolving systems under changing conditions
7. Conclusions
    In this paper, we have provided an organized discussion and review on the evolution of the subject of complex system reliability optimization, which is at the heart of reliability engineering. We have presented how the development of solutions to such problem, and their application, have evolved as a continuum of ideas and techniques, which we have chronologically organized into the three eras of Mathematical Programming, Pragmatism, and Active Reliability Improvement.
    In this flow of development and advancement, we have highlighted the joint pull force coming from the fields of operations research and optimization theory, and from the evolution of technology. Their combination has led to the advancement of research ideas and theories, brought new perspectives from the engineering world, and resulted in the development and continuous improvement of methods and techniques to address reliability optimization problems of increasingly complex systems.
    The underlying message that emerges from this is that system re- liability optimization is an ongoing topic of scientific development and will always be so. The research is actively pulled by the advancements in mathematics, operations research and optimization theory, and in response, researchers will continually develop new methods and algorithms to solve, more efficiently than before, broader classes of problems, and larger and more difficult problems. At the same time, the research is pushed by the changes in technology, and in the engineering and social worlds, practitioners will continually demand for new developments to cope with the practical challenges encountered on the field.
    In conclusion, today we are treating problems that involve more complex systems and more realistic reliability behaviors of the components, including multi-state, uncertain behaviors, etc. We are beginning to address them dynamically, as new data is collected and analyzed to provide a better understanding of usage conditions and failure behavior, or to compensate for changing conditions, so that the optimal system design and maintenance plans can be continually enhanced. This is possible due to the collection and transmission of data by modern sensor and communications technologies. Yet, new opportunities and challenges are always arising, and it will always be necessary to find efficient ways to solve new problems or problems previously unsolvable.
    
    

Keywords:
Fault detection Wind turbine SCADA data
Non-singleton fuzzy inference system Expanded linguistic terms and rules

Abstract

Wind power generation e?ciency has been negatively a?ected by wind turbine (WT) faults, which makes fault detection a very important task in WT maintenance. In fault detection studies, fuzzy inference is a commonly- used method. However, it can hardly detect early faults or measure fault severities due to the singleton input and the limited linguistic terms and rules. To solve this problem, this paper proposes a WT fault detection method based on expanded linguistic terms and rules using non-singleton fuzzy logic. Firstly, a generation method of non-singleton fuzzy input is proposed. Using the generated fuzzy inputs, non-singleton fuzzy inference system (FIS) can be applied in WT fault detection. Secondly, a mechanism of expanding linguistic terms and rules is presented, so that the expanded terms and rules can provide more fault information and help to detect early faults. Thirdly, the consequent of FIS is designed by the expanded consequent terms. The defuzzi?ed result, which is de?ned as the fault factor, can measure fault severities. Finally, four groups of experiments were conducted using the real WT data collected from a wind farm in northern China. Experiment results show that the proposed method is e?ective in detecting WT faults.


1. Introduction

The rapid development of wind energy has boosted the installations of wind turbines (WTs) [1]. Meanwhile, an increasing demand for higher power generation e?ciency has put more pressure on the op- eration and maintenance (O&M) of WTs [2]. One of the essential tasks of O&M is to deal with WT faults, which have negatively a?ected power generation e?ciency and caused a heavy economic loss [3]. If WT faults are detected in time, it would greatly reduce O&M costs [4]. Therefore, due to its signi?cant role in the O&M of WTs, there have been increasing studies on WT fault detection, such as the detection methods based on signal processing [5], image processing [6], machine learning [7], deep learning [8], etc.


Fault detection based on condition monitoring (CM) is one of  the most commonly-used methods. The main principle of the method is to examine whether the collected on-line data are within  the  normal range. If the data are out of the normal range, there might be  an anomaly or a fault. Furthermore, WT faults can be detected and diag- nosed according to di?erent anomaly data. Fault detection based on CM is e?ective and easy to operate. In [9], an unsupervised anomaly de- tection approach for WT condition monitoring was proposed. In [10], a temperature-based real-time aging monitoring method was  presented for power converter modules. In [11,12], CM-based fault detection methods were put forward to detect the WT faults in gearbox and generator respectively.
In fault detection methods based on CM, many types of data can be used, including Supervisory Control and Data Acquisition (SCADA) data [13], vibration data [14], strain data [15], acoustic data [16], and lu- brication oil data [17]. However, the acquisition of the vibration data and other types of data requires additional sensors, which increases the maintenance cost [18]. Fortunately, modern wind farms (WFs) usually install SCADA system, which collect hundreds types of on-line WT data at a certain interval. SCADA data are cost e?ective, as no additional sensors are needed [19]. A lot of research based on SCADA data has been put forward to realize the fault diagnosis and fault prognosis. In [20], machine learning models based on domain knowledge were proposed to realize fault detection. In [21], deep neural network (DNN)-based framework was put forward to detect WT gearbox faults. In [22], many data-mining approaches for wind power prediction were evaluated, which can be used for fault detection. In [23], a fault de- tection method based on multiclass support vector machine algorithms was proposed. Meantime, fault prognosis has gained a rapid develop- ment in recent years [24]. Based on SCADA data, an on-line fault and help to detect early faults. Moreover, the defuzzi?ed  fault  factor can tell fault severities.

The rest of this paper is organized as follows. Section 2 introduces the related work and the problem description. Section 3 describes the proposed method in detail. Experiments and comparisons are listed in Section 4. Section 5 is the conclusion of the present work.

2. Related work and problem description

2.1. Review of the FIS method in WT FAULT detection

FIS method of WT fault detection based on SCADA data generally consists of the following three steps [30,31].
Step 1: Data prediction. First, the original normal SCADA data
Dorg (x) (consisting of n variables) are collected, as shown in Eq. (1).


prognosis method [25] and a prior knowledge-based prognosis method of WT pitch faults were proposed.
Among the CM and fault detection approaches, one of the com- monly-used method is the fuzzy inference system (FIS). FIS evaluates inputs with ¡°if-then¡± rules based on fuzzy logic. There are two parts in  the rule: ¡°if¡± part gives the evaluation of the input and ¡°then¡± part returns a fuzzy output according to the rule [27]. Based on FIS, many  fault detection methods have been proposed. In [28], FIS was estab- lished   with  rare  association   rule  to  predict   the  spatiotemporal   dis-
tribution  of  energy  security  weaknesses  in  transmission  networks.   

In where, Di (x) is the ith variable in Dorg (x). The SCADA data usually consist of various variables [34,35], including wind speed (m/s), active power (kw), generator temperature (¡ãC), and bearing temperature (¡ãC), etc.
Then, fault-related variables of SCADA data are chosen. These variables are predicted by their relevant data (including current data, historical data and current data of other WTs [31]), as indicated in Eq.  (2).


[29], an adaptive neuro-fuzzy inference system and hybrid models were developed. In [30], a system  for WT condition  monitoring using where, Dk is the fault-related variable and Dd (j) is the jth relevant data adaptive neuro-fuzzy interference systems was proposed, and it has achieved good experiment results. In [31], an e?ective generalized model for WT anomaly identi?cation method based on fuzzy synthetic
evaluation was put forward. In [32], a development of a fault diagnosis of Dprd. fpredict is the prediction method, which mainly includes time series method [36], neural network method [37], etc.
Step 2: Anomaly detection by the prediction error. With the collected data Dk (x) and its predicted data Dk (x), the prediction error scheme based on identi?ed fuzzy models was presented. In [33], a  monitoring strategy of short-circuit fault between turns of the stator can be calculated by Eq. (3).

windings and open stator phases by fuzzy logic technique was proposed.

However, several problems remain in the WT fault detection based on FIS: (1) Early faults cannot be detected using conventional FIS methods due to the averaged singleton input. The widely-applied methods usually use daily averaged data as the input to reduce false alarms of WTs. However, the details of the data are omitted. (2) Fault severities cannot be measured by conventional FIS methods due to the limited number of FIS consequent terms (for example, only ¡°normal¡± and ¡°fault¡±).
In order to tackle the above problems, this paper proposes a WT   fault detection method with SCADA data based on expanded linguistic terms and rules using non-singleton fuzzy logic. First, a method of generating non-singleton input is proposed. The non-singleton  inputs can be obtained by transforming probability density functions (PDFs) of the grouped prediction errors, which enables the application of non- single FIS in WT faut detection. Second, more antecedent and con- sequent terms are expanded by the proposed method. The rules of fault detection are also expanded accordingly. Consequently, the  fuzzy system can detect the fault at an early stage and provide more in- formation about fault severities. Third, the FIS consequent  is designed by the expanded consequent terms, and the defuzzi?ed output is de- ?ned as the fault factor, which can tell the severity of the fault.
The contributions of this paper include:

(1) A generation method of non-singleton fuzzy input is proposed, and non-singleton FIS is applied to detecting WT faults. To the authors¡¯ knowledge, it is the ?rst time that non-singleton FIS has been ap- plied to detecting WT faults.
(2) A method of expanding linguistic terms and rules in FIS is proposed. The expanded terms and rules can provide more fault information,

Then, probability distribution function (PDF) of the prediction errors can be obtained. Also, the upper bound Bup and the lower bound Bdn of the PDF are calculated as follows: (1) A con?dence interval is set, and
(2) the value of the left endpoint of the con?dence interval is de?ned as the lower bound and that of the right endpoint as the upper bound. For example, if   is set as 99%, the value corresponding to 0.5% in the PDF    is the lower bound and the value corresponding to 99.5% in the PDF is  the upper bound. Therefore, if the collected on-line prediction error dr   is greater than Bup, it is marked as ¡°high¡±, whereas if dr  is less than Bdn,  it is marked as ¡°low¡± (in some researches, more bounds are introduced, such as ¡°very high¡± and ¡°very low¡± [30]).
Step 3: Fault detection based on FIS. In order to detect a certain fault, prior knowledge is extracted as rules to detect WT faults [38]. According to certain combinations of data anomalies, WT faults can be diagnosed. Fig. 1 shows an example of the WT fault detection with the conventional FIS method. Two variables of WT SCADA data (Data 1 and Data 2) are collected and predicted. Then, PDFs of the two prediction errors are calculated and the lower bounds and the upper bounds are obtained. At a certain moment, Data 1 is marked as ¡°high¡± and Data 2 is marked as ¡°low¡± (as illustrated by red points in Fig. 1). Then, it is de- termined as Fault X according to Rule 001.

2.2. Problem description

There are several problems that prevent the conventional FIS method from detecting the early fault. The ?rst problem is the limita- tion of singleton input in FIS. In order to reduce false alarms, daily averaged data, rather than the 10-min data, are often used by FIS methods, which would result in the lack of data details. Fig. 2 shows an example. In this case, the gearbox oil temperature is monitored at an interval of 10 min. The blue dot region is a time window T, in which the data points are averaged as one value. It can be found that although the averaged value in T is lower than the upper bound, many data points in T are higher than the upper bound. Such case can be regarded as a potential anomaly, which cannot be detected by the conventional sin- gleton-based FIS.
The second problem is the limited number of linguistic terms of a fault. Linguistic terms in WT fault detection are extracted from the prior knowledge and experts¡¯ experience. The number of linguistic terms is small due to the limited prior knowledge and experts¡¯ experience. In practice, the consequent of rules usually include only two terms of ¡°Normal¡± and ¡°Fault¡±. Consequently, it prevents FIS from detecting  early faults and telling fault severities.

3. Method

3.1. Architecture of the proposed method

The architecture of the proposed method is designed based on the process of conventional FIS, as shown in Fig. 3. There are six phases in this architecture (The red dotted parts are the originalities of the pro- posed method).
Phase 1: Data prediction. As discussed in [30], according to the commonly-used FIS methods, the original data are collected and pre- dicted. In this study, the current data and the historical data are se- lected to predict the target data. Many approaches can be used to predict data [39,40]. In this paper, the widely-applied neural network (NN) method is used to realize the prediction. The prediction error can be obtained by Eqs. (1)¨C(3).
   
Phase 2: Non-singleton input transformation. Di?erent from the conventional FIS method, the proposed method introduces non-sin- gleton FIS input in WT fault detection. The prediction errors are divided into groups. The PDF of each group is calculated. Then each PDF is transformed to a non-singleton input. With the non-singleton input, non-singleton FIS can be applied in WT fault detection. Details are discussed in Section 3.2.
Phase 3: Linguistic terms and rules acquisition. Similar to the con- ventional FIS method [41], linguistic terms and rules are extracted from prior knowledge and experts¡¯ experience. First, rules for detecting WT faults, such as ¡°IF A is High AND B is Low THEN Fault 01 (Fault)¡±, are acquired. Second, linguistic variables and terms are extracted from the rules, such as ¡°A¡± is extracted as a linguistic variable, whereas ¡°High¡±, ¡°Normal¡±, ¡°Low¡± are extracted as the linguistic terms of ¡°A¡±. Conse- quently, the original membership functions (MFs) can be calculated by fuzzy statistics method [42]. There are many types of MFs, such as Gaussian MF [43] and interval type-2 MF [44]. In this paper, the tri- angular/trapezoid MFs are adopted. These MFs can be easily obtained by data-driven method, and many similar methods [30,31] have achieved good results using this type of MFs in detecting WT faults.
Phase 4: Linguistic terms and rules expansion. Linguistic terms of antecedent are expanded, with the aim of enabling the FIS  to detect early faults and measuring fault severities. Then according to di?erent combinations of expanded antecedent terms, consequent terms at dif- ferent fault levels are generated. Accordingly, the expanded rules  are also obtained. Details are described in Section 3.3.
Phase 5: Fuzzy inference. In this phase, fuzzy inference engine [45]     is applied to processing fuzzy input sets into fuzzy output sets. First, ?ring levels are calculated by the non-singleton fuzzy inputs and antecedent MFs. Then, output sets are obtained by the calculation of

consequent MFs and levels.Phase 6: Fault factor. Normally, the conventional FIS method only
uses the output of the rule as the ?nal result of the fault detection. In this paper, benefitted from the expanded linguistic terms and rules, the
   
The designed process is shown in Fig. 4. First, each obtained pre- diction error Dk is regarded as a linguistic variable. Then, Dk is divided into groups by Eq. (4). fault detection can go a step further. The fuzzy output sets are de
 to a crisp output (fault factor), which is designed to measure fault severities. Details are described in Section 3.4.

3.2. Non-singleton input TRANSFORMATION

Compared with singleton FIS, fuzzy logic based on non-singleton  input proved to be a more e?ective method in theory [45]. However, in practice, it has not been applied in the fault detection ?eld. In  this  paper,  the fuzzy number  of non-singleton  input is correlated  to the PDF
of prediction  errors, which realizes  the application  of non-singleton  

where, t is the group number and P is the number of data points in each group (in this paper, P is set as 144). Next, each Dk is converted to a  fuzzy number. Di?erent from singleton FIS, which uses a single value as the input, non-singleton FIS uses fuzzy number as the basic input unit.
To qualify as a fuzzy number, a fuzzy set A must possess the following properties [41]: (1) A must be a normal fuzzy set; (2) A¦Á  must  be  a closed interval for every ¦Á ¡Ê (0, 1] and (3) the support of A must be bounded.
Normally,  the  distribution  of   is  considered  as  a  normal  distribution in this method. 

a closed interval for every ¦Á ¡Ê (0, 1]; (3) By setting an appropriate con?dence interval, the support of f k is bounded. This means fk
completely meets the requirements of a fuzzy number. Thus, it can be used in non-singleton FIS.
Therefore,   the   non-singleton   input   transformation   can   be   implemented. First, the mean ¦Ìk  and the standard deviation ¦Òk  of each Dk are calculated by Eq. (5) and Eq. (6) respectively.

Then, the fuzzy number of non-singleton input can be obtained by transforming each f k , as shown in Eq. (7).

where, t is the group number of Dk and P is the number of data points in each group. The fuzzy number of non-singleton input is brie?y marked as ¦Ìk (x) in Eq. (8).

MFs before the expansion. (b) Terms and their MFs after the expansion.

distribution of the input data set. The non-singleton input makes the calculated membership degree more accurately reflect the actual WT

condition, which could help to detect early faults that are undetected by

Therefore, the input data set ¦Ì? (x1, x2¡­xq) of the lth Rule Rl (with q
linguistic terms) can be calculated by Eq. (9).

3.3.  EXPANDING  linguistic  terms  AND  rules

where, ¡ï is the minimum t-norm operator [46].

   
In this section, a method of expanding linguistic terms and rules is proposed.
The limited linguistic terms is a problem in FIS. In order to solve this problem, Jerry M. Mendel ?rstly proposed a basic theory on linguistic

terms expansion [47]. The main idea is to extract new linguistic terms from the original ones according to their features. Inspired by this

where, ¦ÌFl is the antecedent MF. Then, ?ring levels Fll are calculated with ¦Ì? (x1, x2¡­xq) and ¦ÌFl (x1, x2¡­xq) by Eq. (11).

theory, a mechanism of expanding linguistic terms and rules for WT fault detection is designed in this paper.
PART 1: The expansion of antecedent linguistic terms. Theoretically,

the expanded terms should be generated from the existing ones.


Therefore, in this paper, every two existing adjacent terms are used to generate a new term.



where, X is the domain of the input sets.
Fig. 5 shows an illustration of the computing process of non-sin- gleton membership degrees. It can be found that the membership de- gree is no longer determined by a single input value, but by the

statistical method. Then, the crossing points of MFs and their x-values are marked (PSL and PSH in Fig. 6(b)). The crossing point is the fuzziest point (take PSL for example, it is neither ¡°Low¡± nor ¡°Normal¡±). Thus, its membership degree in the new term is de?ned as 1. Finally, the vertical


Fig. 5. Illustration of the computing process of non-singleton membership degrees. (a): The non-singleton fuzzy input and MFs of the antecedent. (b) and (c): The calculated membership degree (the red dot) of the antecedent terms. (d) Calculation results.



line through the crossing points is made and the original critical points are connected. The obtained triangles (pink dotted lines in Fig. 6(b)) are the MFs of the expanded terms.
Operator  is de?ned as the above generation process. Thus, a new
linguistic term can be obtained by Eq. (12).

where, Tmnew is the expanded new term. Tmleft and Tmright are its two adjacent terms.
PART  2:  Expanding  rules  and  consequent  linguistic  terms.  With  the expansion   of   antecedent   linguistic   terms,   rules   are  accordingly   ex- panded. Supposing that the original rule R of a fault has N antecedents, each term in the antecedent is expanded to more new terms according to the expansion method described in PART 1. Thus, each antecedent in R has two types of terms, the original term Tmorg  and the expanded term Tmexp. Through di?erent combinations of Tmorg  and Tmexp, new rules are generated. If the new rule has m original terms (accordingly, there will be  N ? m  expanded  terms),  the  consequent  of  the  rule  is  de?ned  as Error  Level  m.  Consequently,  there  is  only  one  error  result  before  the expansion,  whereas  N + 1  Error  Levels  after  the  expansion,  with  the most serious fault Error Level N and the least serious fault Error Level 0. Next, consequent linguistic terms are expanded. Each Error Level is de?ned  as  a  new  consequent  linguistic  term.  Thus,  there  would  be N + 1 consequent linguistic terms of the fault. The linguistic terms and
rules expansion algorithm is summarized in Algorithm (1).
Algorithm 1. Linguistic Terms and Rules Expansion

Fig. 7. The designed consequent and the process of calculating the fault factor.
(a) The designed consequent and the calculated ?ring levels. (b) The gravity center of the intercepted consequent.

Then, the triangular MF, which has been e?ectively used in con- sequent de?nition, is applied in all consequent terms, as shown in   Fig. 7(a).
To get the output data set of the lth rule, the lth consequent is calculated by the lth ?ring level obtained by Eq. (11), as shown in Eq. (14).

its linguistic variable), 

3.4.  Design  of  the  consequent  AND  FAULT  FACTOR

In the conventional FIS method, the process of WT fault detection ends once the fuzzy consequent (for example, ¡°fault¡± or ¡°normal¡±) is obtained. However, this result could neither help to ?nd early faults nor tell fault severities due to its limited consequent terms. In this section, defuzzi?cation is applied and the fault factor (used to measure fault severities) is designed. Fig. 7 is an illustration of the designed con- sequent and the process of calculating the fault factor.
First, original consequent terms Tmcsq and the expanded consequent terms Tmexp?csq are combined, as shown in Eq. (13).
   
In summary, the technologies of the proposed method are described as follows: NN-based data predictions and non-singleton transformation (Section 3.2) are used to obtain non-singleton inputs. Expanded lin- guistic terms and rules are obtained by prior knowledge and the ex- panding method (3.3). Then, the non-singleton FIS is applied in fault detection. The defuzzi?ed fault factor (3.4) can tell the severity of the fault. Moreover, multiple types of faults can be detected by one FIS of the proposed method with su?cient rules.

4. Experiments and discussion

4.1. Experiment settings

In order to verify the e?ectiveness of the proposed method, four groups of experiments are conducted using real WT SCADA data collected in a wind farm (WF) in northern China. There are altogether 22 WTs in this WF. All the WTs are of the same type. The capacity of each

WT is 2 MW. The SCADA data collected in two years (with a time in- terval  of  10  min)  are  used  in  the  study.  The  commonly-used back
propagation  neural  network  (BPNN)  method  is  adopted  to  make  the prediction. Moreover, similar to the previous studies [30,31,33], the number of data points in a group is set too large, the condition would be monitored at a long time interval. (2) If the number is set too small, the statistical characteristics of the data would be weakened. Therefore, the number of data points in each group is set as 144 in this paper. In the following experiments, the faults used for evaluation are all real faults, and they undergo the development from early faults to serious faults. The earlier these faults are detected, the more conducive it is for WT maintenance.
Experiments are designed as follows: the proposed non-singleton input is veri?ed to detect the early anomaly in Section 4.2. The e?ec- tiveness of the proposed method for detecting early faults is tested in Section 4.3 (with multivariate input) and Section 4.4 (with one-variable input), respectively. Finally, the robustness of the method is veri?ed in experiments in Section 4.5.


4.2. Experiment 1: electiveness of the non-singleton FIS

This experiment is conducted to verify the e?ectiveness of the proposed non-singleton input in WT fault detection.
In June 2015, the gearbox oil temperature of WT 12 started in- creasing due to the aging of the oil. The high temperature of gearbox oil is a sign of a potential fault. In order to detect this anomaly, the data of gearbox oil temperature is used in this experiment. First, BPNN method is used to predict data and prediction errors are obtained. Fig.  8(a)  shows the prediction errors of the historical data and Fig. 8(b) shows    the PDF of the prediction errors. Then, the upper bound (3.71 ¡ãC) and the lower bound (?3.28 ¡ãC) are calculated.
Then, the corresponding MFs are obtained by the fuzzy statistics method [42]. The mathematical description of the MFs are listed as follows:

where, ¦Ìlow (x), ¦Ìnml (x) and ¦Ìhigh (x) are the MFs of ¡°low¡±, ¡°normal¡± and
¡°high¡±, respectively.
 In this experiment, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method. In order to make a better comparison, the setting of the pro- posed method in this experiment is the same as that of the conventional FIS method except for the fuzzy input part. The conventional FIS method uses singleton input, while the proposed method uses the non- singleton input.
Fig. 9 shows the experiment results. Fig. 9(a) and (c) show the averaged prediction errors in one day (APE-D) and in every 10 min (APE-M) respectively. (1) Using the conventional FIS method, APE-D exceeds the upper bound on 2015-06-24, triggering an alert, as shown in Fig. 9(b). However, from 2015-06-19 to 2015-06-23, although APE-D does not exceed the upper bound due to the high data variance, many APE-M exceed the upper bound, which could be considered as a po- tential anomaly. As can be seen, the conventional FIS method cannot capture these early anomalies. (2) Using the proposed non-singleton FIS method, the anomaly detection is improved, as shown in 9(d). It can be noted that the anomaly is detected on 2015-06-19, ?ve days earlier than the conventional FIS method.
Some details of the anomaly detections are shown in Fig. 10. On 2015-06-15, both the data calculated by the conventional FIS method (DTM) (the blue point in Fig. 10), and the data calculated by the pro- posed method (DPM) (the red point in Fig. 10) are below the upper bound, which indicates that the WT is in normal condition. On 2015- 06-19, DTM is below the upper bound while DPM is above the upper bound. It can be found that due to the large data variance, DPM in- creases. As a result, the anomaly can be detected in advance by the proposed method. It is not until 2015-06-24 that DTM  exceeds  the  upper bound, which is ?ve days later. From this experiment, it can be concluded that the proposed non-singleton FIS method can e?ectively detect early WT anomalies.

4.3. Experiment 2: COMPARATIVE experiment with MULTIVARIATE input
In this section, two groups of experiments are carried out to verify the e?ectiveness of the proposed method in WT fault detection with multivariate FIS input. Similar to Experiment 1, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method.


Fig. 8. The prediction errors and their PDF of gearbox oil temperature (WT 12).
(a) Prediction errors of the historical data. (b) PDF of the prediction errors.
4.3.1. 
Experiment 2.1: detecting cooling system FAULT
In early 2015, the cooling systems of WTs were improved to en- hance their heat dissipation capability. However, several WTs were poorly updated. The converter fan was improperly installed, leading to  an increase of converter temperature. As there are no sensors that can directly measure the converter temperature,the converter choke coil temperature (CCCT) and converter controller top  temperature  (CTT)  are monitored instead. The rule of the fault from the prior knowledge is summarised as: IF (CCCT is high) AND (CTT is high)  THEN  (the  con-  verter temperature is high and there are faults in WT¡¯s cooling system), which is listed as the ?rst rule in Table 1.
First, the prediction errors and their PDFs of CCCT and CTT are calculated. Also, the corresponding MFs are obtained by the fuzzy statistics method. Second, the original MFs and rules are expanded. (1)  As has been described in Section 3, the antecedent terms are expanded
Fig. 9. The anomaly of gearbox oil temperature (WT 12) and the detection results. (a) Averaged prediction errors in one day. (b) Anomaly detection result using the conventional FIS method. (c) Averaged prediction errors in every 10 min. (d) Anomaly detection result using the improved non-singleton input.
Fig. 10. The fault detection based on non-sin- gleton fuzzy input. (a), (b), (c) and (d) are four cases in the fault detection. (a): 2015-06-15. (b): 2015-06-19. (c): 2015-06-24. (d): 2015-06-30.
The blue point indicates the data calculated by the conventional FIS method and the red point indicates the data calculated by the proposed non-singleton FIS method.







Fig. 11. The consequent of converter high-temperature fault.

from three (low, normal and high) to ?ve (low, sub-low, normal, sub- high and high). (2) Accordingly, the original rules are expanded from one to four, as shown in Table 1. As can be seen, the expanded MFs and rules enrich the outputs.
 Then, the FIS consequent  is designed according to the descriptions in Section III, as shown in Fig. 11. The designed consequent has four terms: Normal, Warning (2 sub-highs), Minor error (1 sub-high and 1 high) and Major error (2 highs). It can be found that  the  original method has a consequent of two terms whereas the proposed method has four.
 Fig. 12 shows the monitored data during the fault and the detection results using the conventional FIS method and the proposed method. It can be found that during this period, both the prediction errors of CCCT and CTT increases.
In the conventional FIS method, on 2015-05-01, CCCT exceeds its upper bound, but CTT remains normal. On 2015-05-03 CTT also ex- ceeds its upper bound, triggering an alarm (as shown in Fig. 12(a), (b) and (c)). However, it can be found that from late April to early May, there are tendencies of increasing temperatures in both the prediction errors of CCCT and CTT. The conventional FIS method fails to detect such early faults.
Di?erent from the conventional FIS method, the proposed method
uses non-singleton input, expanded terms and rules in WT fault detec- tion. Moreover, the conventional FIS method depends on upper and lower bounds, while the proposed method uses fuzzy areas to determine membership degrees. From the experiment results, it can be found that:
(1) The fault is detected 5 days earlier by the proposed method. (2) On 2015-04-28, the fault factor rises above zero and a warning is triggered. From 2015-04-28 to 2015-05-10, the fault factor keeps increasing, in- dicating that the fault is getting worse.
 From the results of the experiment, it can be concluded that com- pared with the conventional FIS method: (1) the proposed method can detect faults at an early stage, and (2) it can tell the severity of the fault.

4.3.2. Experiment 2.2: detecting BLADE ANGLE sensor FAULT
In August 2014, the output active power of WT 16 decreased. After a shutdown inspection, a fault of blade pitch angle sensor was found. The measured angle is inconsistent with the actual angle, leading to a misjudgment of the control system. Consequently, the output active power is lower than it should be. According to this fault type, a rule can be summarized: ¡°IF (wind speed is normal) AND (output active power is low) AND (pitch angle is normal) THEN (possible  fault:  pitch  angle  sensor fault)¡±. Then, (1) the conventional method (such as the method  in [33]) and (2) the proposed method are used to detect this fault.
Similar to Experiment 2.1, ?rst, the PDFs of the prediction errors of
wind speed, output active power, and pitch angle are estimated. Then, the upper bounds and the lower bounds of these PDFs are obtained, and MFs are established by the fuzzy statistics method. The prediction er- rors of wind speed, active output power and pitch angle are shown in Fig.  13(a),  Fig.  13(b),  and  Fig.  13(c)  respectively.  The  experiment

Fig. 12. The monitored data and detection results of Experiment 2. (a) The averaged prediction errors of CCCT in one day. (b) The averaged prediction errors of CTT  in one day. (c) Detection result of the conventional FIS method. (d) The averaged prediction errors of CCCT in every 10 min. (e) The averaged prediction errors of CTT in every 10 min. (f) Detection result of the proposed method.

Fig. 13. The monitored data and detection results of Experiment 2.2. (a) Prediction errors of wind speed. (b) Prediction errors of active output power. (c) Prediction errors of pitch angle. (d) Detection result of the conventional FIS method. (e) Detection result of the proposed method.
results are shown in Fig. 13(d) and Fig. 13(e).
It can be found that the proposed method detects the fault (on 2014- 08-15) two days earlier than the conventional method (on 2014-08-17). From the experiment results, it can be concluded that the proposed method is e?ective in detecting WT faults with multivariate inputs.

4.4. Experiment 3: COMPARATIVE experiment with single-input

 This experiment is designed to show the e?ectiveness of the pro- posed method in WT fault detection with one-variable FIS input. Similar to Experiment 1, the conventional FIS method, such as the method in [33], is selected to make comparison with the proposed method.
In July, 2015, due to the aging of the generator front bearing of WT 06, its temperature rose rapidly within a few days. On July 20th, WT 06 had a sudden breakdown. In this experiment, the monitored generator front bearing temperature (GFBT) is used. The same as that in the above experiments, GFBT is predicted and the prediction errors are obtained. The  upper  bound  and  the  lower  bound  of  the  predicted  errors  are

 As can be seen in Fig. 14(a) and (b), the prediction errors of GFBT increases rapidly from 2015-07-17 to 2015-07-19. The fault detection results of the conventional FIS method and the proposed method are shown in Fig. 14(c) and (d). It can  be found that: (1)  The conventional FIS method does not detect the fault until 2015-07-19, only one day before the breakdown. (2) Using the proposed method, the fault is detected on 2015-07-17, three days before the breakdown, giving more time for maintenances. (3) From 2015-07-17 to 2015-07-19, the fault factor is increasing, which indicates that the fault is getting worse.
It can be concluded that: (1) the proposed method is e?ective in
detecting early WT faults with one-variable input. (2)  The  proposed  fault factor can also tell the severity of the fault.
 Furthermore, in order to make a good comparison of the proposed method with more di?erent FIS methods. Another group of experiments is  conducted.  The experiment  setting is  the same  as that in Experiment
3. The following methods are compared with the proposed method: (1) the FIS method with a single prediction model (such as the method in [30]), (2) the FIS method with multiple prediction models (such as the method  in [31]), (3) the proposed  method  without  non-singleton input,
(4) the proposed method without rule expansion.
 Table 2 shows the experiment results. It can be found that con- ventional FIS methods cannot e?ectively detect early faults, either with

a single prediction model or with multi-prediction models. The pro- posed method can detect the fault at an early stage. Therefore, it can be concluded that: (1) The proposed method can e?ectively detect the early fault. (2) Both the proposed non-singleton input and the proposed expansion of terms and rules are e?ective in WT fault detection.



Robustness has always been an important factor for evaluating the methods in industrial applications [48,49]. In this section, experiments are conducted to verify the robustness of the proposed method.
The experiment settings are the same as those in Experiment 3. The normal data  of 300  days  are added  in the experiments. All  the data of 300 days are used to test the false alarm rate of the proposed method. In order  to  further  verify  the  robustness  of  the  proposed  method,  noises are  added  to  the  experiment  data.  First,  through  the  measurement,  it can  be  found  that  the  normal  data  themselves  have  a  signal-to-noise ratio (SNR) of 40 dB. Then, di?erent Gaussian white noises are added to the  data  to  test  the  robustness  of  the  method.  As  a  result,  the  experi- ment data have the ADDITIONAL SNR from 45 dB to 25 dB. Fig. 15 shows some examples of the experiment  data (has a SNR of 40 dB itself)  and the data with the added noise.
Table 3 shows the result of the experiments. It can be found that: (1) When the experiment data of 300 days (before adding noise) are used, there is no false alarm. Therefore, it can be concluded that the proposed method has a low false alarm rate in WT fault detection. (2) Moreover, the proposed method has no false alarm when the noise (no less  than  35 dB) is added. The experiment result shows that the proposed method has certain anti-noise capability. (3) In reality, in most cases, the noise which a?ects the data is not as intense as that in the experiments. Thus, it can be concluded that the method could keep robust and maintain a low false alarm rate in practice. (4) The missing detection rate is zero in all the experiments. Therefore, it can be concluded that the proposed method is robust in detecting WT faults.

4.6. Discussions

 Four groups of experiments have been conducted in this section, and the experiment results show that: (1) The real faults in experiments have been successfully detected at an early stage and their severities are told by the proposed method. (2) No false alarms occur using the normal data of 300 days. It can be concluded that the detection results

Fig. 14. The monitored data and detection results of Experiment 3. (a) The averaged prediction errors of GFBT in one day. (b) The averaged prediction errors of GFBT in every 10 min. (c)¨C(f): Detection results. (c) The conventional FIS method. (d) The proposed method. (e) The proposed method without non-singleton input. (f) The proposed method without terms & rules expansion.


Table 2
Experiment results of di?erent types of FIS.

Table 3
The Results of the Robustness Experiments.

Fig. 15. The experiment data (has a SNR of 40 dB itself) and the noise-added data of generator phase 3 temperature (one of the data used for predicting GFBT). (a) 40 dB. (b) 35 dB. (c) 30 dB. (d) 25 dB. are correct and the proposed method can e?ectively detect early WT faults and provide more information on fault severities.
 Similar to other FIS methods, linguistic variables and terms are used in the proposed method. In fault detection, each linguistic term is in- volved in calculating the fuzzy output according to the rules, and the detection result is obtained by combining all the fuzzy outputs. The fuzzy process makes the fault detection more ?exible.
 Moreover, the proposed method has the advantage of detecting multi-class faults. In the FIS of the proposed method, there could be many rules (for multi-class fault) in the rule base. Therefore, if more  rules are added to the rule base, more fault types could be detected. In the experiments of the paper, four di?erent types of WT faults are de- tected by only one FIS of the proposed method.
In conventional methods, if the upper bound is adjusted as a very low value, or the lower bound is adjusted as a very high value, the early fault can also be detected. However, in such case, many ¡°Normal¡± samples can be misjudged as ¡°Abnormal¡±, which could result in higher false alarm rate. In comparison, using the proposed method, early faults can be e?ectively detected without adjusting the upper bound and the lower bound.

5. Conclusion
This paper presents a WT fault detection method based on expanded linguistic terms and rules using non-singleton FIS. Di?erent from the conventional FIS methods, the proposed method is improved to detect early WT faults and to tell fault severities. There are two main con- tributions of this paper. First, this paper proposes an e?ective WT fault detection method based on FIS. For the ?rst time, a fuzzy number transformation method is introduced to convert the PDFs of the pre- diction errors of the WT SCADA data into fuzzy numbers, so that the non-singleton FIS can be applied to detecting WT faults. Second, this paper presents a method of expanding linguistic terms and rules gen- erated from the original ones. With the expansion, FIS could detect WT faults at an early stage and fault severities could be told with the de- fuzzi?ed fault factor. Four groups of experiments are conducted, using the SCADA data collected in a real wind farm. The experiment results show that the proposed method can e?ectively detect early WT faults and provide more information on fault severities.

Investigations on the material property changes of ultrasonic-vibration assisted aluminum alloy upsetting
Abstract
Numerous studies have shown the benefit of ultrasonic-vibration assisted metal forming. This benefit include a reduction in forming forces, which might be attributed to the superposition of stress, increased temperatures, the effects of interface friction, and energy absorption of dislocation. This study conducts a series of experiments and analyses to investigate the main mechanisms of a reduction in forming forces during ultrasonic-vibration assisted A6061-T6 aluminum alloy upsetting.
The findings of this research confirm that, under frictionless conditions, ultrasonic vibration still reduced forming forces, and ultrasonic vibration can increase the temperature of specimens and soften specimen surface during upsetting. From metallographic analyses and micro-hardness tests, the results reveal that energy absorption of dislocation was occurred during upsetting, which also contribute to the reduction of forming force.
This research concludes that the mechanisms of increased temperatures and energy absorption of dislocation can affect the material property and make a reduction in forming forces; however, the interface friction effect has nothing to do with a reduction in forming forces.
Highlights
The mechanisms of ultrasonic-vibration assisted upsetting were identified. Ultrasonic can reduce the material¡¯s flow stress when friction is negligible. Metal can absorb energy through heat energy, making a reduction in forming force. The grains have been refined when the ultrasonic-vibration is superimposed. The motions of dislocation were occurred, making a reduction in forming forces.
Keywords
Aluminum alloy Compression test Ultrasonic vibration Mechanism
1. Introduction
High-energy ultrasonic waves have been applied to a wide variety of uses, including ultrasonic welding, ultrasonic cutting, ultrasonic metal forming, and ultrasonic die-casting. The process of ultrasonic-vibration assisted metal forming applies ultrasonic energy to a die, which is then used to deform metal specimens. Interesting effects arise when ultrasonic vibration is applied to metal-forming processes, such as a decrease in friction between die and specimen, a reduction in forming forces, and changes in the microstructure of specimens during sheet metal forming. Blaha and Langenecker [1], [2] were the first to investigate the use of ultrasonic vibration in relation to the plasticity of metals. They superimposed high-frequency vibrations onto a static load during the tensile test of a zinc single crystal specimen. In their experiment, they observed a substantial reduction in yield stress and flow stress. In similar experiments, flow stress was clearly reduced in polycrystalline materials. The SAE1019 steel experiments showed that applying ultrasonic energy to a specimen increased its temperature, an effect related to the time and amount of ultrasonic energy applied. Microscopic observations showed that grain sizes of materials decreased when ultrasonic energy was applied. The transmission electron microscopy technique (TEM) was used to observe an increase in the density and movement of dislocations after the application of ultrasonic energy to materials.
Abramov [3] investigated the effect of the ultrasonic on the material micro-structural and mechanical properties. It showed that ultrasonically induced stress in NaCl and LiF crystals with the amplitude exceeding their yield strength enhances dislocation density. When the density of the dislocations is high enough, an alignment of dislocations occurs. Kempe [4] proposed three mechanisms by which dislocations absorb energy from vibrations to reduce flow stress: (1) a resonance mechanism, (2) a relaxation mechanism, and (3) a mechanism of simple hysteresis.
Hevill [5] attributed reductions in flow stress to a stress superposition mechanism involving the superposition of steady stress and alternating stress. Our previous study [6] proved that axial ultrasonic vibration could reduce the deformation resistance of materials during hot upsetting. We found that the effect of ultrasonic vibrations on hot upsetting could not be explained by a single simple mechanism, such as the effect of interface friction, the superposition of stress, or the absorption of ultrasonic vibration energy by dislocations.
Substantial research has been conducted on the changes in interface friction in ultrasonic-vibration assisted forming experiments and simulations [7], [8], [9], [10]. Huang et al. [11] investigated the benefits of applying the axial ultrasonic vibrations of forming tools to an upsetting process using plasticine as a model material to simulate hot metal. In their study, the application of ultrasonic vibration to die reduced the mean forming force during upsetting. The researchers concluded that the stress superposition effect and reductions in interface friction contributed to this phenomenon. Daud et al. [12], [13] performed ultrasonic-vibration assisted aluminum alloy compression and tension tests using a piezoelectric force transducer to measure the high-frequency vibration tension and compression force. Furthermore, a finite element model was constructed to describe the effects of superimposing ultrasonic vibration for compression and tension tests. The results indicated that oscillatory stress superposition and contact friction were insufficient to explain the effects of ultrasonic excitation in metal-forming processes.
As mentioned in the literatures, although ultrasonic-vibration assisted forming has been around for decades, the mechanism that induces these effects is still unclear. Explanations on the effects of ultrasonic vibration include the superposition of stress [1], [2], [5], [11], [12], [13], [14], increased temperatures [1], [2], [15], energy absorption of dislocation [3], [4], and the effects of interface friction [7], [8], [9], [10], [11], [12], [13]. These factors are usually coupled, which makes them difficult to understand. This study conducted a series of experiments and analyses to clearly investigate the main mechanisms of a reduction in forming forces during ultrasonic-vibration assisted A6061-T6 aluminum alloy upsetting. An extrapolated compression test removed the effects of interface friction between the specimens and the die, and a temperature measurement test explored the effects of increased temperatures using an IR thermometer. Finally, this study used metallographic analyses and micro-hardness tests to investigate the effects of energy absorption of dislocation during upsetting.
2. Ultrasonic-vibration assisted extrapolated compression test
To avoid the influence of friction on the measurement of the stress¨Cstrain data, Cook and Larke [16] utilized a method of the extrapolated compression test, which applied compression force to cylinder specimens with four different ratios of initial diameter to height (d0/h0) from 0.5 to 4 under identical loading conditions. If the diameter of the specimens is kept constant, the height of the specimens increases, resulting in a reduction in interface friction. When the height of specimens approaches infinity, interface friction becomes negligible. The deformation of the cylinder specimens therefore remains uniform during compressions (Fig. 1).
he compression strains obtained with different d0/h0 ratios are linear and can be extended to the origin point. When d0/h0 reaches 0, the specimen heights are infinite and the effect of interface friction in this strain data becomes negligible. Using this method, additional stain data can be obtained with different loading conditions when d0/h0 reaches 0. Based on these steps, stress¨Cstrain data under frictionless conditions can be derived.
2.1. Experimental procedure
The procedure for the high-temperature-extrapolated compression test is detailed as follows. The ultrasonic vibration system and a furnace were set up on a hot bench controlled by a microcomputer server, as shown in Fig. 2. The specimens were sprayed with MoS2 lubricant and placed between parallel dies. A 20 kg preload was applied to the specimens. The heating controller was turned on. When the designated temperature was reached, it was held constant for 10 min before the experiment began. Whenever loading reached 70 kg during an experiment, ultrasonic vibration was superimposed. After compression was complete, deformation of the cylindrical specimens was measured before the specimens were removed from the plates.
Table 1 shows the material properties and the high-temperature-extrapolated compression test conditions used in the experiment. The specimens used in this study were aluminum alloy A6061 with heights of 3 mm, 4 mm, and 6 mm (equivalent to d0/h0 of 2, 1.5, and 1, respectively) and fixed diameters of 6 mm. All specimens received T6 treatment, including solution treatment and artificial aging treatment, before the experiment. Fig. 3 shows the microstructure of the specimens with heat treatments before the experiment, and the grain size of the specimens was 22.285 ¦Ìm. Compression forces were set to 500, 800, 1100, 1100, 1500, and 1800 kg. A constant compression speed of 1 mm/min was maintained throughout the experiment. During the ultrasonic-vibration extrapolated compression test, the axial vibration frequency was maintained at 20 kHz and the amplitude was set to 5.6 ¦Ìm.
2.2. Experimental results and discussion
Fig. 4 plots the experimental strain vs. d0/h0 results of both the conventional compression (CC) test and the axial ultrasonic vibration compression (AUC) test. The environmental temperature was set to 25 ¡ãC. Fig. 4a shows the results from CC when compression loading of 500, 800, 1100, 1500, and 1800 kg were applied on specimens with d0/h0 ratios of 2, 1.5, and 1. Fig. 4a also shows compression strains under a 500 kg loading condition as 0.00187, 0.00191, and 0.00194 for specimens with d0/h0 ratios at 2, 1.5, and 1, respectively. It is therefore difficult to see the data clearly from this figure. The strain when d0/h0 = 0 can be extrapolated if the strains for specimens with different d0/h0 ratios have a linear relationship. This line can then be extended to the origin point. Fig. 4b shows the results from AUC when compression loading of 500, 800, 1100, and 1500 kg were applied to specimens with three different d0/h0 ratios. By the comparison between Fig. 4a and b, the engineering strain for AUC with the loading of 1500 kg is higher than that of CC with the loading of 1800 kg. It appears that ultrasonic vibration can significantly reduce the loading. Under these fictitious geometrical conditions with infinity of the height of specimens, no friction occurs and the stress value associated with the deformation is a function only of the material¡¯s resistance to flow.
Fig. 5, Fig. 6 plot the stress¨Cstrain curves obtained from Fig. 4 for three cylindrical specimens of different initial heights for CC and AUC. Higher loads are required for a higher d0/h0. The results show that, under the same loading conditions, flow stress decreased and compression strain increased when d0/h0 decreased. This was because interface friction was reduced when d0/h0 decreased and specimen height increased.
Fig. 7 shows a comparison of the stress¨Cstrain curves for CC and AUC under frictionless conditions. At the same loading of 1500 kg, true strain in AUC was 18.72% higher than that in CC. True stress in AUC, however, was lower by approximately 59.73 MPa. This shows that ultrasonic vibration can still effectively reduce material flow stress under frictionless conditions. Mechanisms other than friction must therefore be responsible for the reduction in flow stress caused by ultrasonic vibration.
Fig. 8 plots the stress¨Cstrain curves from extrapolated compression tests for AUC and high-temperature conventional compression (HCC). The true stress¨Cstrain curve for HCC at 150 ¡ãC is close to AUC¡¯s curve at 25 ¡ãC. The reduction in flow stress from increasing the temperature to 150 ¡ãC is comparable to the reduction caused by applying ultrasonic vibration. This result was in agreement with the literature [17], which indicated that the flow stress can be reduced when the temperature of the specimen was increased by pure heating, and also revealed that the specimen absorbed ultrasonic energy to cause the increased material temperatures and make a reduction in forming force. To clearly explore the effect of increased temperature, the temperature measurement tests were conducted.
3. Temperature measurements during ultrasonic-vibration assisted upsetting
3.1. Experimental conditions
During compression tests, materials vibrate at high speeds. Inserting a thermocouple into specimens to measure temperature may not lead to accurate measurements because of damage to the thermocouple caused by the heat generated from vibrating materials. To overcome this issue, an IR thermometer (Raytek MX4) and a film-type thermocouple (ANRITSU ST-24 K) were used to obtain mean and indirect measurements of temperature during AUC tests. Specially designed specimens with thin (0.2 mm) wings were made to allow attachment of the film-type thermocouple (Fig. 9).
3.2. Experimental results
Fig. 10 shows the measured temperatures for the AUC tests for specimens with d0/h0 ratios of 2, 1.5, and 1. The results show that ultrasonic vibration increased the temperature of the materials. Smaller specimens had higher temperatures. This shows that the ability of ultrasonic vibration to increase temperature during upsetting is related to the size of the specimen. Smaller specimens absorb more ultrasonic energy per unit of volume, which results in higher temperatures.
Temperatures measured using the film-type thermocouple exceeded those measured with the IR thermometer. This was because the ultrasonic-vibration assisted upsetting caused ultrasonic welding. The temperatures of the specimens increased both through absorption of ultrasonic energy and through the rubbing that occurred between specimens and the die. Temperatures at the interface were therefore higher than the overall temperatures of the specimens. Furthermore, the IR thermometer measured only mean temperatures at focal spots with diameters of 6 mm. The film-type thermocouple measured temperatures around the thin wings, which was near the interface areas for the specimens. The temperatures were therefore higher than the values measured by the IR thermometer. This phenomenon was in agreement with Statnikov¡¯s research [15]. The high-frequency impact on the surface is accompanied by quick surface local heating. The generated thermal energy may not heat up the whole sample, but has a significant influence on the surface and improve the surface quality. Metal can absorb energy from the ultrasonic vibration through thermoelastic energy conversion [2], [18], which could make a reduction in forming force.
Additionally, results show that temperatures increased rapidly during the initial vibration stages and then decreased with time. Fig. 10c and d shows that when d0/h0 = 1, the temperature of specimens under a compression load of 1100 kg was lower than that of specimens under 300 kg. This was because vibrating amplitude decreased as compression load increased, which reduced the output energy of the vibrations.
A discrepancy in temperature was found between indirectly measured temperatures (approximately 40¨C50 ¡ãC) and inferred temperatures (approximately 150¨C200 ¡ãC) from previous tests, which indicated that absorption of ultrasonic energy was not the only factor to increase the material temperatures. Other factors, such as stress superposition and dislocation activation, might also have been responsible. To clearly explore other mechanisms, metallographic analyses and micro-hardness tests were conducted.
4. Metallographic analyses and micro-hardness tests
A number of recent studies have explored the effects of ultrasonic energy on the microstructures of materials, which revealed that applying ultrasonic energy refined the grain sizes in these materials therefore improving their mechanical properties [19], [20], [21], [22], [23], [24]. Liu¡¯s research [24] indicated that the major approach to refine the grains of specimens for ultrasonic vibration is the motion of the dislocation.
To explore the effect of energy absorption of dislocation, metallographic analyses were conducted to investigate the grain size on specimen surface. A solution of hydrofluoric acid (10% hydrofluoric acid and 90% water) was used as an etchant. A surface area of one square inch was selected on each specimen for observation. These areas were magnified 100 times and shown on a microscope screen. Before the experiment, the grain size of the specimens was 22.285 ¦Ìm. With CC, specimens were subjected to compression forces of 500, 800, 1100, and 1500 kg, which resulted in grain sizes of 21.795, 21.496, 20.820, and 20.433 ¦Ìm, respectively. With AUC, specimens were subjected to compression forces of 500, 800, 1100, and 1500 kg, which resulted in grain sizes of 21.371, 20.523, 18.814, and 17.530 ¦Ìm, respectively. When ultrasonic vibration was superimposed on specimens, the grains on their surfaces were refined (Fig. 11, Fig. 12). The dislocation density increased during AUC, and these dislocations tangled together and the dislocation walls occurred. Finally, these dislocations walls changed into low or high angle grain boundaries, and then the original grains were refined into small grains and subgrains [3], [24]. Specimen absorbed the energy from the motion of dislocation to soften the martial and make a reduction in forming force [4], [25].
Micro-hardness tests were also conducted. Results showed that the forming process increased the specimens¡¯ surface strength because grains on the specimen surfaces were refined and the number of dislocation increased dramatically during ultrasonic-vibration superimposition (Fig. 13).
5. Conclusion
A series of experiments and analyses were conducted to explore and further understand the effects of ultrasonic vibration for the mechanical properties and microstructure changes of materials during ultrasonic-vibration assisted A6061-T6 aluminum alloy upsetting, and three mechanisms including the interface friction effect, the increased temperatures, and energy absorption of dislocation were clearly identified and analyzed.
The results of the extrapolated compression experiment showed that when friction is negligible, ultrasonic vibration can still effectively reduce material flow stress. Temperature measurement tests showed that metal can absorb energy from the ultrasonic vibration through thermoelastic energy conversion, which could make a reduction in forming force; however, heat energy was not the only factor. Furthermore, the results of metallographic analyses and micro-hardness tests indicated that energy absorption of dislocation was also occurred, which make a reduction in forming forces during upsetting.
Because of current limitations in measuring apparatuses, direct measurements of interface temperature could not be achieved. Instead of using real temperature measurements at the interface, this study relied on indirect and average local temperature measurements to understand the thermal effects of ultrasonic vibration. A more precise measuring method should be used in the future to accurately explore the relationship between ultrasonic vibration and temperature increase in specimens. Further investigation on the effects of the superposition of stress will be conducted to develop the full mechanisms of ultrasonic vibration upsetting.
A model to characterize acoustic softening during ultrasonic consolidation
Abstract
Ultrasonic consolidation (UC) is a solid state bonding process in which thin metal foils are bonded under the influence of ultrasonic vibration and pressure. Large parts can be made by placing foils side by side or by stacking layers to create thicker parts. Thermal and acoustic softening of metals during UC leads to increased plastic deformation and plays an important role in bond formation. In this work, a thermo-mechanical finite element model is developed to quantify the degree of thermal and acoustic softening occurring in Al 1100-0 foils during UC. The model uses experimentally measured temperatures and changes in the foil's geometry during UC to quantify the amount of thermal and acoustic softening. Acoustic softening is shown to reduce the yield stress of Al 1100-0 foils by up to 82%. In addition, thermal softening is found to be relatively minor, typically less than 5% of the total material softening. This method to quantify acoustic softening during UC allows for a better overall understanding of the bonding process and allows several aspects of the UC bonding process to be optimized and improved.
Keywords
Ultrasonic consolidation Acoustic softening Thermo-mechanical analysis Aluminum
1. Introduction
Ultrasonic consolidation (UC) is a bonding process in which two materials are joined under the influence of ultrasonic vibrations and pressure. A foil and a substrate or multiple foils are bonded together by a sonotrode that applies pressure and ultrasonic vibrations as shown in Fig. 1. Three adjustable process parameters influence UC weld properties; they are (i) the sonotrode's clamping force (Fc) in the negative z-direction, (ii) the sonotrode's oscillation amplitude (¦Ë) at a frequency (f) in the x-direction, and (iii) the sonotrode's speed (S) in the y-direction. A rough knurl pattern on the sonotrode prevents relative displacement at the sonotrode¨Cfoil interface and causes bonding to occur only at the foil¨Csubstrate interface.
UC is similar to the automated tape placement (ATP) process used for thermoplastic materials described in detail by Pitchumani et al. (1997). In their work, optimum ATP process conditions were identified in order to maximize the buildup of inter-layer bond strength during the process. A related work by Tierney and Gillespie (2006) investigated how the choice of process parameters in ATP leads to the development of intimate contact between parts and ultimately the final strength of the part. Similarly in UC, the choice of process parameters also influences the bond strength. UC bonds produced at low ¦Ë and Fc are generally weak. On the other hand, bonds produced at high ¦Ë and Fc can cause excessive deformations and misalignment in the part, resulting in poor bond quality. In order to create high strength UC bonds, it is essential to identify the proper weld parameters for a given material type and material geometry. This was shown by Kong et al. (2003) for Al 6061 where peel testing of UC bonds was conducted in order to determine a window of optimum UC process parameters for the material. A similar study was also done by Kong et al. (2004) for Al 3003.
UC is a low temperature solid-state bonding process in which thick parts can be fabricated by building up the thickness one layer at a time. There are many advantages in using UC over other welding techniques. Koellhoffer et al. (2011) measured the temperature of UC using an infrared camera and reported that temperatures during UC are typically less than 50% of the material's melting temperature. Yang et al. (2009) used a scanning electron microscope to examine the microstructure of UC bonds and found no evidence of melting, indicating that bonding is a solid-state process and that the materials would retain much of their original microstructure. The low temperature bonding process has the advantage of significantly reducing residual stresses in parts made with UC. The low temperatures of UC also allow thermally sensitive materials to be embedded into structures. A unique benefit to using UC over other welding techniques is that two dissimilar materials can be bonded during the process. This was shown by Obielodan et al. (2011) where the lap shear strength of commercially pure titanium and aluminum alloy 3003 dual-material structures was optimized.
The fundamental bonding mechanisms of UC are currently not fully understood; however, plastic deformation is accepted by many to be an important factor in bond formation during UC. de Vries (2004) investigated the mechanics of ultrasonic welding through mechanical modeling, a shear force sensor and an infrared camera. It was concluded that plastic deformation, heat generation, normal force and shear force all play an important role in bond formation. Kong et al. (2005) investigated the interface of UC bonds through optical microscopy and found that increased plastic deformation in Al 6061 foils leads to an increased linear weld density and therefore greater bond strength. In their work, linear weld density was determined by investigating a cross sectional cut of a UC bond and calculating the percent of the material that is bonded together. Janaki Ram et al. (2006) performed a similar study using Al 3003 foils and also found that UC process parameters that resulted in the largest amount of plastic deformation also had the greatest linear weld density. Yang et al. (2009) noted that plastic deformation at the interface during UC aids in the removal of oxide surface layers and facilitates bonding. Plastic deformation is an important part of the UC bonding process since it allows the two surfaces to come into intimate contact and aides in breaking apart oxide layers and contaminants. Only once the two materials are in intimate contact can bonding can occur. Plastic deformation during UC brings more material into intimate contact and increases the area over which bonding can take place. Thermal and acoustic material softening are an important part of the UC bonding process since they lead to increased plastic deformations in the foil and consequently could result in higher bond strengths.
Acoustic softening is the reduction in the apparent static stress necessary for plastic deformation in a material under the influence of ultrasonic energy. Langenecker (1966) performed tension tests on several materials under the influence of ultrasonic energy. It was found that acoustic softening of the material occurs immediately upon application of the ultrasonic energy and ends immediately upon removal of the ultrasonic energy. In addition, at a sufficient level of ultrasonic energy, the yield stress of a material can effectively be reduced to zero. Kirchner et al. (1984) performed quasi-static compression tests of Al 6061 and also reported a similar reduction in yield stress under the presence of ultrasonic irradiation. Izumi et al. (1966) have shown that acoustic softening occurs during compression testing of a wide range of materials including aluminum, copper, silver, steel, lead and magnesium under the influence of ultrasonic energy.
The fundamental mechanisms responsible for acoustic softening have been investigated by many researchers, but due to the complexity of the process, a clear consensus has not emerged. The following mechanisms have previously been suggested to explain the apparent reduction in yield stress due to acoustic softening: stress superimposition, volumetric energy absorption of dislocations and surface friction effects. Malygin (2000) investigated stress superimposition as an explanation of the experimentally observed acoustic softening. The stress superimposition theory states that oscillatory stresses induced by ultrasonic vibrations are added onto the static loading, making the material's yield stress appear lower during experimental measurements. Malygin (2000) concluded that the stress superimposition method adequately described the acoustic softening effect. Cai (2006) states that the stress superimposition method is not easy to verify experimentally since detailed dynamic stress profiles within acoustically softened specimens would be required. Cai (2006) investigated stress superimposition through numerical methods by modeling the dynamic stress profiles of metals under the influence of ultrasonic vibrations; however, this work did not consider the influence of energy absorption of dislocations on acoustic softening. The coefficient of friction between two surfaces is reduced under the influence of ultrasonic vibrations and can lead to a reduction in the force required for deformation to take place. Ashida and Aoyama (2007) studied the effects of ultrasonic vibration on press forming and found that the ultrasonic vibrations reduced the coefficient of friction at the surface between the sheet metal and the die.
Langenecker (1966) found that significantly less ultrasonic energy is needed to soften a material than thermal energy, suggesting that the ultrasonic energy must be absorbed primarily by the dislocations of metal grains, rather than uniformly throughout the material like thermal energy. Transmission electron microscopy (TEM) was used to observe the change in dislocation density and structure after the application of ultrasonic energy. Hansson and Tholen (1978) also used TEM to observe a change in dislocation microstructure of aluminum under the influence of ultrasonic energy. Langenecker (1966) hypothesized that localized short timescale heating may occur in a material as energy is absorbed by dislocations and that this localized heating may be responsible for the reduction in yield stress; however, Sriraman et al. (2011) monitored temperatures during UC with a thermocouple recording at 10 kHz and found no evidence of localized short timescale heating. Daud et al. (2007) compared a finite element model of acoustic softening during tension and compression tests of aluminum with experimental measurements. It was concluded that stress superimposition and interfacial surface effects do not account for the experimentally observed acoustic softening and that energy absorption at dislocations is an important mechanism of acoustic softening. Hung and Lin (2013) experimentally investigated the material property changes of aluminum during ultrasonic vibration and also concluded that energy absorption at dislocations is an important mechanism in acoustic softening.
The two process parameters that affect the amount of ultrasonic energy during UC ¨C and therefore the amount of acoustic softening ¨C are the oscillation amplitude (¦Ë) and the frequency (f). Few previous works have focused on quantifying acoustic softening over a wide range of process parameters. The previously mentioned acoustic softening study by Daud et al. (2007) was limited to only one amplitude and one frequency. Izumi et al. (1966) performed compression tests under the influence of ultrasonic irradiation and found a linear relationship between acoustic softening and the amplitude of vibrations. This relationship is not directly applicable to UC for two reasons: (i) ultrasonic vibrations in their study were parallel to the static loading direction, whereas in UC ultrasonic, vibrations are perpendicular to the static loading direction and (ii) the relative motion between the two surfaces during UC complicates the acoustic softening process since a portion of the input amplitude is absorbed by the material and the rest dissipates at the sliding surface. In addition, amplitudes only up to 20 ¦Ìm were studied by Izumi et al. (1966), whereas during UC, amplitudes over 40 ¦Ìm are often needed to create strong bonds. Yao et al. (2012) investigated the mechanism of acoustic softening in commercially pure aluminum using a model based on thermal activation theory and found good agreement with experimental measurements. Yao et al. (2012) also found a linear relationship between acoustic softening and oscillation amplitude during compression tests, but the study was limited to a narrow range of parameters: only amplitudes up to 6 ¦Ìm at a frequency of 9.6 kHz were studied.
Several previous numerical studies on the mechanics of UC have not taken into account acoustic softening. Without knowledge of the amount of acoustic softening during UC, stresses and plastic deformation cannot be accurately modeled. Doumanidis and Gao (2004) studied plastic deformation and stresses at the bond interface during ultrasonic welding through finite element modeling without including acoustic softening. Similarly, Elangovan et al. (2009) studied stress fields and volumetric heat generation due to plastic deformation during UC using a finite element model without accounting for acoustic softening during the process. Zhang and Li (2009) used thermo-mechanical modeling to study the mechanics of the bonding interface during UC. Acoustic softening was not accounted for in the model and it was concluded that only thermal softening at the interface was responsible for the material softening.
Acoustic softening during UC has been previously modeled by Siddiq and Ghassemieh (2008), but their work investigated a small range of process parameters and was validated using only a small set of experimental data by Langenecker (1966) that was not specific to UC. Pal and Stucker (2012) used a dislocation density based constitutive model in order to predict deformations of Al 3003-H18 foils during UC; however, additional experiments are required to test the validity of the model.
The vast majority of previous works on acoustic softening have investigated the mechanisms behind the experimentally observed softening in tension and compression specimens. There is limited information on a method to quantify acoustic softening during tension and compression tests and even less that is specifically related to UC. In addition, previous attempts to quantify acoustic softening have been limited to a small range of process conditions. A method to quantify acoustic softening over a wide range of process parameters is especially important for UC, since many different process parameters are used to create bonds with varying properties. The goal of this work is to evaluate in detail how various UC process parameters influence acoustic softening. A method to quantify acoustic softening under UC conditions is presented which will allow for a better overall understanding of the UC bonding process.
2. Model development
In order to model deformations in the Al 1100-0 foils used in this work during UC, the following mechanisms are included in the model: volumetric thermal softening due to bulk temperature increase, surface friction effects, strain hardening of the material, volumetric energy absorption of dislocations, stress superimposition of the ultrasonic vibrations and the clamping force.
Thermal softening during UC is caused by heat generated due to the friction at the foil¨Csubstrate interface and volumetric heat generation due to hysteresis energy loss during cyclic plastic deformation in the foil. Several previous studies have focused on frictional heat generation during UC. Elangovan et al. (2009) studied how the coefficient of friction at the foil¨Csubstrate interface influences UC process temperatures through thermal finite element modeling. Koellhoffer et al. (2011) used thermal finite element modeling and temperatures measured during UC to calculate the coefficient of friction during UC as a function of UC process parameters. In this current work, it is assumed that all work done by friction is uniformly dissipated as a heat flux over the area of contact between the surfaces. The coefficient of friction (¦Ì) during UC is a function of frictional heat generation (qfr), the horn's clamping force (Fc), the frequency (f) and the amplitude (¦Ë) as follows:
Previous works give some insight into the expected friction coefficient trends that are seen in UC. Naidu and Raman (2005) investigated the friction coefficient trends during ultrasonic fretting of aluminum and found the following trends: the coefficient of friction will decrease with increasing clamping force (Fc), increase with increasing amplitude (¦Ë), and decrease with increasing speed (S). These trends were also confirmed by Koellhoffer et al. (2011) specifically for UC.
In addition to friction, heat can be generated if the material plastically deforms during the UC process. Unlike friction, this is a volumetric phenomenon. Volumetric heat generation is typically considered to be small in comparison to frictional heat generation for typical process parameters in UC, as discussed by Koellhoffer et al. (2011). Volumetric heat generation during UC has been studied by Zhang and Li (2009), but this work did not account for the influence of acoustic softening on the yield stress of the material. In order to calculate volumetric heat generation, plastic dissipation (Wp) must first be calculated. Wp is related to the yield stress (¦Òy), the plastic strain rate (), the material volume (V) and time (t) as follows (Hodowany et al., 1999):
The expression for volumetric heat generation (qvol) is given in Eq. (3) and is related to the plastic dissipation (Wp), a material dependent heat conversion factor (¦Â) and time (t).
Hodowany et al. (1999) investigated the value of ¦Â in Eq. (3) specifically for aluminum in compression. It was found that ¦Â is a function of the amount of plastic strain and ranged from approximately ¦Â = 0.3 at low plastic strains up to ¦Â = 0.9 at higher plastic strains. Once acoustic softening during UC is quantified using the method described in this work, the contribution of volumetric heat generation to thermal softening can be investigated.
The relationship between yield stress, strain and temperature has been studied for Al 1100-0 in compression by Hockett (1967). Strain hardening behavior of Al 1100-0 is observed along with thermal softening. As reported by Hockett (1967), the stress¨Cstrain curve of Al 1100-0 at a given temperature can best be represented in the plastic region by the power law equation shown in Eq. (4). The power law model presented by Hockett (1967) and shown in Eq. (4) is modified in this work to account for acoustic softening during UC. A percent acoustic softening term (¦Î) is added to account for acoustic softening due to the volumetric energy absorption of dislocations in the material. For the purpose of this work, it is assumed that all material softening during UC is due to thermal and acoustic softening.
Due to the relatively thin foils used during UC, it is assumed that ¦Î is uniformly distributed throughout the material volume during the process. The following authors have made a similar assumption of a volume average in presenting experimental results of acoustic softening of thicker specimens (2¨C75 mm): Yao et al. (2012), Izumi et al. (1966), Kirchner et al. (1984) and Hansson and Tholen (1978). ¦Î ranges from zero to one depending on the amount of acoustic softening. At a value of ¦Î = 1, there is no acoustic softening in the material and it will deform normally. At a value of ¦Î = 0, there is sufficient ultrasonic energy to reduce the yield stress of the material to zero. For the purpose of this work, it is assumed that acoustic softening linearly affects the K term in the power law equation and that the n term remains unchanged by acoustic softening since K is primarily responsible for significant changes in ¦Òy.
In order to capture the effect of temperature on the yield stress of Al 1100-0, Hockett (1967) identified a series of constants for the power law model at different temperatures. These constants are shown in Table 1. Linear interpolation is used to calculate the values of K and n at temperatures between those listed in Table 1.
In order to investigate acoustic softening over the full range of process parameters and different foil geometries, it is useful to present results in a dimensionless form. First, a dimensionless amplitude (¦«) is defined according to Eq. (5) that is related to the peak-to-peak amplitude (¦Ë) and the initial foil thickness (H0). ¦« is related to the shear strain in the foil and in the extreme case where there is a sticking friction condition at the foil¨Csubstrate interface, ¦« would equal the shear strain in the foil.
In order to study and compare UC of foils with different initial widths (W0), it is useful to convert the clamping force (Fc) to a contact pressure (Pc). Pc is related to the clamping force (Fc), initial foil width (W0) and contact length (lc) according to Eq. (6). Here, lc is defined as the distance in the y-direction that the sonotrode and the foil are in contact during UC (see Fig. 1). The value of lc is a function of sonotrode radius and foil thickness. In this work, lc was measured to be 5.4 mm and assumed to be constant for all UC process parameters for the sonotrode and foil geometries that were investigated. It has been shown in Kelly (2012) that the assumption of a constant lc introduces minimal error to the acoustic softening (¦Î) calculation.
Finally, a dimensionless pressure (P) is defined below that is related to the contact pressure (Pc) and the yield stress of Al 1100-0 at 20 ¡ãC (¦Òy0 = 34 MPa) before its exposure to ultrasonic energy:
2.1. Thermo-mechanical finite element modeling
The amount of acoustic and thermal softening occurring during UC is quantified in this work using thermo-mechanical finite element modeling in Abaqus 6.9-2 by Dassault Systems (2009). The model accounts for volumetric thermal softening due to bulk temperature increase, surface friction effects, strain hardening of the material, volumetric energy absorption of dislocations, stress superimposition of the ultrasonic vibrations and the clamping force. The two material attributes that are needed in order to predict the temperature field and the deformation field as a function of the UC process parameters are the coefficient of friction between the foil and the substrate shown in Eq. (1) and a constitutive equation describing the material's yield stress behavior due to UC process parameters shown in Eq. (4). The friction coefficient can be determined independently by choosing a value of frictional heat (qfr) that when input into the thermal finite element model, predicts a steady state temperature that matches the experimentally measured steady state temperature. After the value of qfr that gives the best match between predicted temperatures and the experimentally measured temperatures is found, the friction coefficient between the foil and the substrate can be determined using Eq. (1). Once the friction coefficient (¦Ì) is established for a given set of UC process parameters, the acoustic softening parameter (¦Î) can be selected that results in a match between the experimentally measured change in the width of the foil (¦¤W/W0) and the predicted (¦¤W/W0) from a mechanical finite element model of the UC process. The procedure to quantify acoustic softening (¦Î) is summarized in Fig. 2.
2.1.1. Thermal finite element model
A 2D thermal finite element model in the x¨Cz plane, as shown in Fig. 1, is used to determine the frictional heat (qfr) that is required to match the predicted temperature with the experimental values measured during UC under a given set of process parameters. The thermal model, created in Abaqus 6.9-2, uses 4479 six-node quadratic triangular elements. The entire cross section of the foil and the substrate are modeled, but only a portion of the sonotrode is modeled to improve computational efficiency since the sonotrode is very large in comparison to the foil and the substrate. In order to ensure that the size of the modeled sonotrode region does not affect the temperature distribution, a sufficiently large portion of the sonotrode is modeled such that there is no heat flux through that boundary. A fixed temperature boundary condition is applied to the edges of the sonotrode and the edges of the substrate. All surfaces exposed to air have been modeled using a convection boundary condition with a convective heat transfer coefficient of h = 5 W/(m2 ¡ãC) and a T¡Þ = 20 ¡ãC. The value of h was varied in the normal range of natural convection (from 2 to 15 W/(m2 ¡ãC)) and it was found that it did not influence the temperature distribution in the foil due to the short time-scale of UC. Perfect thermal conductance is assumed at the sonotrode¨Cfoil and substrate¨Cfoil interfaces. The frictional heat (qfr) is applied at the substrate¨Cfoil interface. The thermal finite element model's boundary conditions are shown in Fig. 3.
Table 2 summarizes the material properties used in the thermal finite element analysis. The three required material properties are the thermal conductance (k), density (¦Ñ) and specific heat (c). The substrate is modeled using Al 3003-H14 properties, the sonotrode is modeled using Ti-6Al-4V properties and the foil is modeled using Al 1100-0 properties.
In this work, it is assumed that qfr is distributed evenly over the entire contact area between the foil and the substrate. Since qfr is a function of the UC process parameters, it must be calculated for each set of weld parameters. The measured steady-state temperature (Tss) during a weld is used in combination with the thermal model to determine the qfr generated during the weld. The thermal model is executed for 1.5 s (equal to the time it takes the sonotrode to travel down the length of the 60 mm weld at 40 mm/s). For each set of process parameters, an increasing value of heat input of qfr is imposed at the interface between the foil and the substrate until the error between the experimental steady state temperature and the predicted values from the thermal finite element model are minimized. This thermal model has been validated in Kelly (2012) where good agreement was found between experimentally measured and modeled temperatures in both the transient and steady-state temperature regions of the weld.
2.1.2. Mechanical finite element model
The 3D mechanical finite element model of UC used in this work was created in Abaqus 6.9-2 (Dassault Systems, 2009) and uses the explicit solver. The mechanical finite element model accounts for volumetric thermal softening of the bulk material due to temperature increase, changes in the surface friction effects during UC according to Eq. (1), strain hardening of the material, acoustic softening (¦Î) due to energy absorption of dislocations and the superimposition of the vibrational and clamping stresses. Fig. 4 summarizes the boundary conditions of the mechanical finite element model.
The sonotrode and substrate are modeled as rigid bodies since both materials have a much higher yield stress than the Al 1100-0 foil. The following boundary conditions apply to the sonotrode: ¦Ë in the x-direction at f = 20 kHz and Fc in the negative z-direction. The sonotrode is allowed zero displacement in the y-direction and is free to displace in the z-direction. Zero rotation is assigned to the sonotrode in the x-, y- and z-directions. The substrate is assigned an encastre boundary condition (zero displacement and rotation in all directions).
In Fig. 4(b), the Al 1100-0 foil is modeled with 10-node quadratic tetrahedron elements (C3D10M). In order to improve computational efficiency, only an 8 mm length of the foil is modeled rather than the full 60 mm weld length. Node-to-surface contact formulations are defined on the top and the bottom of the foil using the penalty contact method. Tangential contact between the sonotrode and the top of the foil is defined using a penalty friction formulation with ¦Ì = 1.0, preventing relative motion between the sonotrode and top of the foil. This sticking friction condition between the sonotrode and the foil is used because experimentally during UC there is no relative displacement between the two parts. The influence of ¦Ì at the sonotrode¨Cfoil interface was investigated on deformation in the x-direction. It was found that the results are not sensitive to a coefficient of friction ranging from 0.5 to 1.5 as well as a ¡°rough¡± contact formulation ¨C all yield the same result.
Tangential contact between the substrate and the bottom of the foil is also defined using a penalty friction formulation. The coefficient of friction used in the mechanical model at the foil¨Csubstrate interface is calculated using the thermal finite element model and Eq. (1). A zero y-displacement boundary condition is assigned to one edge of the foil since regions of the foil that the sonotrode has previously bonded will no longer be able to displace in the y-direction. The sonotrode speed (S) in the y-direction is ignored in the simulation since it has no effect on the amount of deformation in the x-direction in the foil.
In order to increase computational efficiency, the model is executed for a time period of only 0.001 s. This time period was chosen after running the model and observing the time required for the amount of plastic deformation in the foil to plateau. The time required for the plastic deformation to plateau varies with oscillation amplitude, clamping force and the material properties of the foil. Plastic dissipation (Wp) in the foil reaches a plateau when the applied normal and shear stresses imposed on the foil become equal to the strain-hardened yield stress of the material and has been investigated in more detail in Kelly (2012).
In order to quantify the amount of acoustic softening during a UC weld, the value of ¦Î in Eq. (4) is incrementally decreased until the error between the foil's width increase in the mechanical model and the experimental measurements for a given set of UC process parameters is minimized.
3. Equipment, materials and experiments
3.1. Equipment
3.1.1. Ultrasonic welder
The UC equipment used in this work is a seam welder custom built by AmTech. The welder's Ti-6Al-4V sonotrode is textured using an electric discharge machining (EDM) technique and has a diameter of 147 mm. Weld quality is controlled by adjusting the three process parameters shown in Fig. 1: the sonotrode's clamping force (Fc) ranging from 300 to 6000 N, peak-to-peak oscillation amplitude (¦Ë) ranging from 7 to 44 ¦Ìm and sonotrode speed (S) ranging from 0 to 300 mm/s. The available weld parameters allow a wide range of weld qualities to be produced. The frequency (f) of the welder is a constant 20 kHz, which is typical for many UC systems. The welder keeps the amplitude constant throughout the weld by automatically adjusting the power. The substrate, seen in Fig. 1, is held in place using a vise. The vise is used to reduce resonance effects in the substrate, which can occur with bolted substrates and leads to inconsistent weld quality and properties.
3.1.2. Infrared camera
An infrared (IR) camera is used to monitor temperatures during the UC process. There are several advantages in monitoring UC temperatures with an IR camera over other methods. The IR camera allows great flexibility in location and special resolution of measurements. Another method to measure UC temperatures, discussed by Sriraman et al. (2011), involves the use of thermocouples. In their work, weld temperatures were measured by placing a thermocouple and the bond interface during welding. Use of an IR camera has three distinct advantages when compared to using an thermocouple during UC: (i) one thermocouple is required for each data point, (ii) placing a thermocouple between weld materials will alter the weld properties at that location and (iii) the thermocouple cannot be reused afterwards.
The IR camera used in this work is a FLIR Thermovision Alert 194 and records images with a resolution of 320 ¡Á 240 pixels at 4 Hz. During each weld, the IR camera is placed 22 in. from the weld in the y-direction and 2 in. above the weld in the z-direction. Weld temperatures are recorded throughout the entire weld process and the data is used to identify thermally transient and steady-state regions along the length of the weld. This work focuses entirely on weld areas at steady-state temperatures.
IR camera recordings have been conducted at frequencies up to 35 kHz, but for this work it was found that a 4 Hz recording accurately captured the weld temperatures. An IR camera frequency of 4 Hz is not sufficient to capture the heat generation and temperature rise during one oscillation at 20 kHz; however, during UC each area of the foil experiences approximately 2000¨C3000 cycles (0.1¨C0.15 s) during which a steady-state temperature is reached. Fig. 5 shows that IR camera measurements of UC are independent of frequency in the range of 4 Hz¨C35 kHz.
3.2. Materials
The two materials used in this work are Al 1100-0 foils and Al 3003-H14 substrates. Since the yield stress of Al 1100-0 is much lower than the yield stress of Al 3003-H14, all plastic deformation occurs in the foil and allows finite element modeling of the process to be simplified. The materials were chosen over other aluminum alloys since both have a relatively thin oxide layer, which will reduce the possibility of inconsistent welds. Yang et al. (2009) have shown that the presence of significant oxide layers can weaken or prevent bonding via UC. Al 1100-0 was also chosen since its yield stress has been well characterized over a wide range of temperatures by Hockett (1967). The material, initial width (W0), and initial thickness (H0) of the substrates and the three foil geometries are shown in Table 3.
3.3. Experimental procedure
The following experimental data is required to quantify the amount of thermal and acoustic softening that occurs during UC using a thermo-mechanical finite element model: (i) IR camera temperature measurements and (ii) the change in the foil's width during UC processing. Temperature measurements are required to calculate thermal softening that occurs during UC and to calculate the coefficient of friction between the foil and the substrate. During the UC process, the IR camera records images at 4 Hz and the sonotrode speed (S) is 40 mm/s, so the IR camera records an image every 10 mm along the length of the weld in the y-direction. From this temperature data, a steady state weld temperature (Tss) region can be identified for any given set of UC process parameters. After UC, a digital image of each welded specimen was used to measure the average width increase in the location of Tss using Image J software (Ferreira and Rasband, 2011). The average UC processed foil width is calculated in Image J by dividing the area of the welded foil by the length of the Tss region in the y-direction. The foil's processed width is the sum of the original tape width (W0) and the increase in width after processing (¦¤W).
In this work, an array of UC process parameters was chosen that results in a wide range of bond qualities. Five amplitudes were chosen (¦Ë = 10, 18, 27, 32, 36 ¦Ìm) and five clamping pressures were chosen (Pc = 17.0, 28.3, 45.2, 62.0, 78.9 MPa). The sonotrode speed (S) is held constant at 40 mm/s for all sets of process parameters, since S will have no effect on acoustic softening. A total of 25 different combinations of UC process parameters were created using each combination of ¦Ë and Pc. Twenty-three of the 25 combinations of UC process parameters resulted in successful welding. Two sets of process parameters (36 ¦Ìm, 62.0 MPa) and (36 ¦Ìm, 78.9 MPa) did not successfully bond since the power required to produce these bonds exceeded the 3 kW maximum of the welder. Bonds made using the lowest ¦Ë could easily be separated by hand while bonds made at higher ¦Ë were sufficiently strong that they resulted in tensile failure in the base metal rather than failure at the bonded interface during peel testing. The full array of welds was produced using the three different foil cross sections shown in Table 3.
4. Results and discussion
4.1. Experimental results
A plot of the thermal development of a typical UC bond along the length of the weld (y-direction) is created according to the procedure described in Section 3.3 and is shown in Fig. 6. The maximum measured IR camera temperature for a typical weld is shown at each position along the weld length in Fig. 6 and a Tss region is identified from 30 to 60 mm along the length of the weld in the y-direction. The process is repeated for each set of UC process parameters and foil geometries and these results are shown in Fig. 7. The value of Tss in Fig. 7 ranges from 24 to 198 ¡ãC over the entire set of UC process parameters.
Fig. 8 shows an image used to calculate the average width increase (¦¤W/W0) using Image J after UC processing according to the procedure described in Section 3.3. Fig. 9 shows the experimentally measured ¦¤W/W0 for each combination of UC process parameters for the three foil geometries. All values of ¦¤W/W0 are taken for the region of Tss for each weld. The percentage increase of ¦¤W/W0 ranges from 1.3% to 24.5% over the investigated test arrays.
4.2. Model results
4.2.1. Coefficient of friction
Frictional heat generation (qfr) is calculated for each set of UC process parameters using the thermal finite element model and the Tss values shown in Fig. 7. Once qfr is determined for each combination of process parameters, the friction coefficient is calculated according to Eq. (1). The results of this calculation are shown in Fig. 10 for the 9.5 mm ¡Á 0.52 mm foil array. The trends found in this study ¨C decreasing coefficient of friction with increasing clamping force and increasing coefficient of friction with increasing amplitude ¨C agree with the previous studies by Naidu and Raman (2005) and Koellhoffer et al. (2011).
4.2.2. Acoustic softening
The coefficients of friction shown in Fig. 10 are used in the mechanical finite element model and acoustic softening (¦Î) is determined by minimizing the error between the experimentally measured ¦¤W/W0 and the predicted ¦¤W/W0 according to the procedure shown in Fig. 2. To highlight the importance of acoustic softening during UC, a few selected combinations of process parameters are shown in Fig. 11. The figure shows experimentally measured ¦¤W/W0 in addition to predicted ¦¤W/W0 from the finite element thermo mechanical model for three different amounts of acoustic softening (¦Î).
The mechanical finite element model accurately predicts the foil width increase without acoustic softening when ¦« = 0 and P = 3.40. This close agreement validates the Al 1100-0 material model and the boundary conditions employed in the mechanical model. It can clearly be seen that once ultrasonic energy is applied (¦« > 0), the material model with thermal softening and without acoustic softening (¦Î = 1) severely under predicts the foil's width increase. Fig. 11 clearly shows that the majority of material softening that occurs during UC is due to acoustic softening and can be quantified using the method described in this work. At an amplitude of ¦« = 0.062, the average acoustic softening of the five weld parameters tested is ¦Î = 0.20. All five combinations of UC process parameters fall into an acoustic softening range between ¦Î = 0.17 and ¦Î = 0.22.
Acoustic softening (¦Î) is determined for the full array of process parameters by minimizing the error between the predicted ¦¤W/W0 and experimentally measured values from the 9.5 mm ¡Á 0.52 mm foil array in Fig. 9 using the procedure shown in Fig. 2. Individual data points in Fig. 12 show the result of this acoustic softening (¦Î) calculation and result in the minimum error between experimentally measured and predicted ¦¤W/W0 for the 9.5 mm ¡Á 0.52 mm foil array. Acoustic softening (¦Î) of Al 1100-0 during UC is found to be a function of dimensionless amplitude (¦«) and dimensionless pressure (P) and can be expressed in a constitutive form as shown below:
Fig. 12 shows that ¦Î decreases with amplitude (¦«) until it approaches its minimum value of ¦Î0 = 0.177. The rate at which ¦Î approaches its minimum value is related to the dimensionless pressure (P): UC welds with a lower P approaches ¦Î0 at a lower ¦« than UC welds with higher values of P. It is also important to note that acoustic softening is a non-linear function of ¦« rather than ¦Ë; therefore, a thicker material will have a lower acoustic softening than a thinner material at a given oscillation amplitude according to Eq. (8). The acoustic softening model shown in Fig. 12 can be applied to any Al 1100-0 foil geometry during UC and this is shown in the following section where the acoustic softening model is validated using the 13.0 mm ¡Á 0.52 mm and 9.5 mm ¡Á 1.04 mm foil arrays.
In order to validate the mechanical model, it is necessary to check the modeled decrease in foil thickness (¦¤H/H0) against the experimentally measured ¦¤H/H0. This check is made once the value of ¦Î is solved for and error between the model and experimental ¦¤W/W0 is minimized. Once it is verified that the experimental and modeled deformations in the x- and z-directions match, it can be assumed that deformation in the y-direction will agree because of constant volume in plastic deformation. This comparison is shown in Fig. 13 for the 9.5 mm ¡Á 0.52 mm foil array.
4.3. Model validation
In this section, the acoustic softening model is validated using Tss and ¦¤W/W0 values from Fig. 7 and Fig. 9 for the 13.0 mm ¡Á 0.52 mm and 9.5 mm ¡Á 1.04 mm foil geometries. Acoustic softening values calculated using Eq. (8) are used as inputs to the thermo-mechanical finite element model and used to predict ¦¤W/W0 for the 13.0 mm ¡Á 0.52 mm and 9.5 mm ¡Á 1.04 mm foil arrays. Friction coefficients, shown in Fig. 10, are also input into the mechanical model. Fig. 14 compares experimental ¦¤W/W0 and predicted ¦¤W/W0 for the 13.0 mm ¡Á 0.52 mm foil array.
The average difference in ¦¤W/W0 between the experimental measurements and the model predictions in Fig. 14 is 0.5% and the maximum difference is 1.1%. Fig. 15 shows a similar comparison of experimental measurements and model predictions of ¦¤W/W0 for the 9.5 mm ¡Á 1.04 mm foil array. The average difference in ¦¤W/W0 between the experimental measurements and the model predictions in Fig. 15 is 0.7% and the maximum difference is 2.0%.
4.4. Model insights to characterize heat generation
The acoustic softening model that has been developed can be used to investigate several new aspects of UC. For example, the relative contributions of thermal and acoustic softening to the total material softening during UC can be investigated. Fig. 16 compares the relative contributions of thermal and acoustic softening on the stress¨Cstrain curve of Al 1100-0 during UC. The most significant acoustic softening is shown (¦Î0 = 0.177) in addition to the highest temperature (Tss = 198 ¡ãC) measured during the test arrays presented in this work. It can clearly be seen in Fig. 16 that acoustic softening is far more significant than thermal softening during UC. It can be seen that without the presence of acoustic softening, Al 1100-0 at 198 ¡ãC experiences a significant amount of softening (25.4%) when compared to room temperature properties. When ¦Î = 0.177, this same increase in temperature only increases material softening by 4.5%, compared to the 82.3% reduction due to acoustic softening. This shows that thermal softening plays a secondary role to acoustic softening during UC. Also, if thermal softening were ignored during the modeling process, a reasonable measurement of acoustic softening can still be made and the error should be no greater than 4.5% in the most extreme case.
The acoustic softening model also allows volumetric heating (qvol) to be calculated and compared to the heating contribution from frictional heating (qfr). qvol can be calculated using Eq. (3). In Eq. (3), Wp is calculated for each combination of UC process parameters using the mechanical finite element model and the time (t) is equal to the sonotrode speed (S = 40 mm/s) divided by the contact length (lc = 5.4 mm). In the case of ultrasonic consolidation, the exact value of ¦Â from Eq. (3) is not critical because even at a value of ¦Â = 0.9 (the upper limit for aluminum), the volumetric contribution to total heat generation is less than 5% for typical UC process parameters (¦« > 0.04). At ¦« < 0.04, little to no bonding of the materials takes place, so these processing conditions would not typically be used where strong bonding is required. Volumetric heat generation is small relative to frictional heat generation under process conditions where strong bonds are produced. In the extreme case, in which ¦« is low and P is high, the maximum contribution from volumetric heating is found and is equal to 18%. The ratio of volumetric heat generation to total heat generation for the full range of process parameters is shown in Fig. 17.
5. Summary and conclusions
Acoustic softening of Al 1100-0 foils during ultrasonic consolidation is quantified using the experimental and thermo-mechanical modeling process outlined in Fig. 2. A relationship between material thickness, UC process parameters (¦Ë and Fc) and acoustic softening has been identified. A wide range of UC process parameters were explored, ranging from parameters that create very weak bonds that can easily be peeled apart by hand to very strong welds that fail in tension in the base material during peel testing. Once acoustic softening is determined for a given material and geometry using the method described in this work, acoustic softening can be calculated for any other geometry of the same material.
For the UC process parameters investigated in this work, thermal softening alone does not account for the experimentally measured tape deformations. Acoustic softening (¦Î) during UC of Al 1100-0 foils is found to be very significant ¨C reducing the yield stress of the material up to 82%. On the other hand, thermal softening is a small contributor to the total material softening during UC (<5%). Friction is the dominant source of heat during UC. Volumetric heat generation accounts for less than 5% of all heat generated during UC process parameters that are typically chosen to produce strong bonds.
The model and experiments described in this work serve to connect the theoretical literature on acoustic softening with ultrasonic consolidation in a practical method that allows acoustic softening to be quantified during the process. Knowledge of how the yield stress of a material changes during UC will lead to a better understanding of how materials will come into intimate contact and when bonding will occur during the process.
Theoretical and experimental analyses of ultrasonic-assisted indentation forming of tube
Abstract
Indentation forming process is used for internal forming and sizing of thick-walled tubes working in high internal pressures. In this process, a mandrel with a diameter slightly larger than that of the tube is pressed and moved inside the tube, creating an internal profile. This article presents theoretically and experimentally influences of longitudinal ultrasonic vibration on this forming process. For this purpose, vibro-impact and continuous forming regimes have been investigated. Application of longitudinal ultrasonic vibration along the axis of the workpiece showed experimentally reduction of 15%¨C21% of axial forming forces and improvement of surface quality of the bore of the tube, while no effect on spring back of the formed zone was observed.
Keywords Indentation forming, ultrasonic-assisted forming, tube forming, slab analysis
Introduction
Indentation forming process is a simple way of little forming, ironing and sizing of internal surface of thick-walled tubes. The process is carried out by pressing and driving a profiled tool (mandrel) inside the tube. Since the diameter of the tool is slightly larger than that of the tube, the surface of the tube is formed and work-hardened.
In ultrasonic-assisted forming, ultrasonic vibration with a certain amplitude and frequency is applied to a part or whole of workpiece, die, tool, or combination of them in a certain direction or directions. This vibration can reduce friction, change strength of the material (if applied to workpiece) and change the loading mechanism and consequently reduce the forming force. In addition, better surface quality, tool life enhancement, more uniform dislocation distribution, internal stress concentration reduction and dimensional and geometric stability are the expected results of employment of ultrasonic vibration. Due to the likely advantages, ultrasonic-assisted forming has been investigated during the past years.
Blaha and Langenecker 1 studied the effects of applying ultrasonic vibration on the process of forming in 1955. Their research showed the softening effect of superimposed ultrasonic vibration on zinc single crystals, which were undergoing a tensile test. In 1957, Nevill and Brotzen 2 investigated the effect of mechanical vibration in the frequency range of 15¨C80?kHz on the tensile elongation of low-carbon steel wires. Their experiments showed that for a variety of amplitudes, the reduction in stress was proportional to the amplitude of vibration. In 1966, Pohlman and Lehfeldt 3 studied the influence of ultrasonic vibration on plastic forming of metals. The result of an experiment on a polycrystalline copper specimen subjected to tensile deformation, while impulses of 20-kHz ultrasound were intermittently superimposed, showed that the force drop due to ultrasonic vibration was only seen in the plastic part of stress¨Cstrain curve and not in the elastic part. They concluded that application of ultrasound under suitable conditions can considerably reduce both the external and internal forces required to overcome friction forces to form plastically metallic specimens.
Atanasiu 4 investigated tube drawing by axial ultrasonic oscillation of the plug in 1980. He found that ultrasonic affects the yield limit and causes the coefficient of friction and the coefficient of viscosity to decrease. It was also observed that the reduction in drawing force is diminished at greater drawing speeds. 4 Siegert and Ulmer 5 in 2001 investigated the superimposing of ultrasonic waves on the dies in tube and wire drawing. The results showed that longitudinal oscillation of the die at ultrasonic frequencies in the range of 20¨C22?kHz parallel to the drawing direction could reduce the friction. The reduction of the drawing force was found to be mainly a function of the ultrasonic amplitude. It was also found that increased drawing speed could decrease the drawing force reduction, perhaps due to decrease in oscillations per unit of length. Murakawa and Jin 6 in 2001 investigated the applicability of ultrasonic radial vibrated dies in the wire drawing process. They reported that radial ultrasonic vibration application is very effective in increasing the critical drawing speed, and it is approximately 10 times as fast as that for axial ultrasonic vibration application, making the radial vibration more productive than the axial vibration.
Hung and Hung 7 in 2005 studied the influence of ultrasonic vibration on hot upsetting of aluminum alloy. Their experimental results indicated that ultrasonic vibration could considerably reduce the compressive forces needed during hot upsetting. The reducing effect on compressive forces decreased while the temperature increased. The strain rate did not significantly affect the reducing effect on compressive forces.
In 2007, Mousavi et al. 8 studied the effects of applying ultrasonic vibration on the die during the extrusion process. They showed that the extrusion force and the material flow stress would be reduced by applying the ultrasonic vibration if the extrusion speed was below a critical speed. In addition, it was found that applying the ultrasonic vibration had no significant effect on the equivalent plastic strain of the material. A larger reduction in average extrusion force was obtained by either reducing the extrusion speed or increasing the amplitude of vibration. In 2009, Hung and Chiang 9 investigated the influence of ultrasonic vibration on double backward-extrusion of aluminum alloy. The results showed that under the effect of ultrasonic vibration, the forming force would be decreased because the ultrasonic vibration increased the temperature of the workpiece.
Susan et al. 10 in 2010 researched mechanical characteristics of stainless steel tubes in the process of drawing under different drawing speeds while using ultrasonic vibration. They obtained decreased mechanical resistance, much greater extension and increase of drawing surface quality while using ultrasonic vibration.
In 2011, Pazand and Feizi 11 presented application of artificial neural networks in investigations on the effects of ultrasonic vibration on the extrusion process. Siddiq and Sayed 12 in 2012 presented a computational study of ultrasonic-assisted manufacturing processes including sheet metal forming, upsetting and wire drawing. A fully variational porous plasticity model was modified to include ultrasonic softening effects and then utilized to account for instantaneous softening when ultrasonic energy was applied during deformation. Shan et al. 13 in 2012 presented a new mathematical model of the antifriction effect on wire drawing with ultrasonic. The results pointed out that the antifriction effect of ultrasonic had observable effects on the drawing force reduction, while the drawing speed had no visible effect on the drawing force.
In this article, theoretical relations of required axial force in indentation forming process are derived under the condition of longitudinal ultrasonic vibration superimposed on the tool in axial direction. In addition, the ratio of tool speed to vibration speed is investigated. By using these relations, effects of amplitude and frequency of vibration on axial forming force can be extracted. To prove the effects experimentally, longitudinal ultrasonic vibration is applied to the workpiece along the axis, and the results are presented.
Analysis of indentation forming process
The employed model for analysis of indentation forming is shown in Figure 1. Slab method is utilized for analysis of indentation forming without ultrasonic vibration. This method is also known as the free-body equilibrium approach or the force balance method. 14
In this process, the tool geometry consists of two parts, forging and sizing. In the forging zone, the intended internal profile of the tube is formed, and the tube inner diameter, and consequently, its outer diameter, is enlarged. In the sizing zone, both inner and outer diameters of the tube reach their final size and surface finishing is done.
The force balance method is applied on slab elements in forging and sizing zones separately under the following assumptions.
The force required for tube forming without ultrasonic vibration is as follows 14
Longitudinal ultrasonic vibration of the tool in axial direction
In order to study the effects of vibration on the indentation forming process, the tool is axially exposed to ultrasonic longitudinal vibration. The rheological model of material is used for this analysis. This reflects its real elastic, viscous and plastic properties without consideration of hardening. In addition, it aids to describe the forming process while ultrasonic is used. This approach explains the physical mechanisms of ultrasonic effects on the processes of plastic deformation. Moreover, impulsive and continuous regimes of loading have been considered in this analysis. 15 For continuous regime of deformation (Figure 2(a)), the contact between the tool and tube is always preserved and there is no separation, while for impulsive regime, there is a cutoff and separation for a short time (Figure 2(b)). For v¡Ýa¦Ø , continuous regime is present. To make impulsive loading, two conditions of 2a>Ft/Ftk0k0 and v<a¦Ø must be established in a period. Based on theoretical relations, in impulsive loading and at low displacement speeds (v) , the force is applied to the tube at less than half of the period. While in the continuous loading, it is variable and uninterrupted during a period. Dynamic equation (4) explains the relation between the force (F), displacement (u) and tool speed (u?) of the tool. In the equations, parameters, ltube,um,¦¤,k0 and Ft are undeformed length of the tube, longitudinal maximum movement in a period of vibration (total displacement due to continuous forced feed of the tool + vibration amplitude), initial gap distance between the tool and the tube, static stiffness coefficient (a function of ltube ) and conventional tube axial forming force (yield force), respectively. It is noteworthy to mention that ¦¤<0 denotes the initial interference between the tool and the tube.
In equation (5), a,¦Øandv represent the vibration amplitude (m), vibration angular frequency (Hz) and tool constant feed-speed (m/s), respectively. Since the axial force applied by the tool is considered to be impulsive, by simplification and integration of equation (4) and utilizing the impact theory, the required average forming force given to the tool in the presence of axial longitudinal ultrasonic vibration of the tool is obtained as follows.
The required values of parameters in experimental tests have been shown in Table 1. The tube material was DIN CK45 (EN 1.1191), and its properties have been obtained by experiment (see Figure 3 and Table 2). After substituting these values in equations (1) and (6), the required forming force with and without ultrasonic in different vibration amplitudes and speed ratios (v/a¦Ø) can be obtained. The axial force of forming has been shown in Figure 4.
Experimental setup
To verify the theoretical relations with and without employment of ultrasonic vibration, CK45 steel tubes, with the specification given in Table 1, were formed by using tungsten carbide indentation tool shown in Figure 5.
A setup consisting of a forming machine body, a hydraulic jack for tool feed, a tool feed driving rod, fixed and movable constraining holders, ultrasonic system and dynamometer were prepared. Figure 6(a) shows a picture of the whole system assembled and schematic representation of the system.
Machine body was fabricated from a welded structure with some adjustable bolted parts for holding and supporting the dynamometer, ultrasonic system assembly, workpiece tube and tool driving system. The constraining holders were made from thick squared steel blocks to prevent misaligning of the workpiece tube and the tool feed system during forming process. Ultrasonic system consisted of a 2-kW power supply made by MPI of Switzerland, 16 a designed and fabricated 3-kW piezoelectric ultrasonic head for longitudinal vibration (Figure 6(b)) and a titanium matching and titanium connecting part between the ultrasonic head and the workpiece for transferring ultrasonic vibration to the work tube (not to the tool). In addition, the titanium connecting part was fixed on its vibration node to support and bear the axial forming force. The fixing body holding the titanium connecting part was standing on a dynamometer system from Kistler (Type 9255B), 17 which can measure normal compressive forces up to 40 kN. The dynamometer was further supported by an adjustable strong welded square connected to the machine body by bolts.
The dynamometer is connected through interfacing unit to a computer, and the forces exerted on the dynamometer in three normal directions can be monitored and drawn against time by using DynoWare software. The MPI ultrasonic power supply to the transducer is also connected to a computer and is driven and controlled by LabVIEW from National Instrument. The amplitude and power of vibration can be adjusted in the software, and the resonance frequency of the whole system assembly is automatically measured and readjusted every 10 ms by the software/hardware of power supply.
Effect of ultrasonic power given to the workpiece on the forming force
If vibration frequency and tool feed-speed are constant, period of imposed plastic deformation exerted by the tool on the workpiece tube decreases within a period of vibration if vibration amplitude is increased. This is proved in Figure 7 in which three experimental graphs are presented. When no ultrasonic is used, the force curve stands at highest level along the whole length of the tube. At 956 W power of ultrasonic, it stands lower, and at 1705?W, it is the lowest. Decrease of 21% of axial force could be achieved in these tests. Theoretical graphs of 0, 5 (¦Í/a¦Ø=0.018) and 10??m (¦Í/a¦Ø=0.0092) ultrasonic vibration amplitudes, given to the tool, show a good conformity with experimental ones in which vibration is given to the workpiece. This conformity takes place in continuous deformation regimes. Under discontinuous impulsive deformation of theoretical force calculation (amplitude > 27.7??m), there is much further decrease in axial force down to half of the axial force without ultrasonic. Figure 8 presents three graphs of axial forming forces: one without ultrasonic over the whole length of the tube (NU) and two without ultrasonic at about half of the length of the workpiece tube and then switching the ultrasonic on at powers 1264?W (excitation frequency of 25,681?Hz) and 1973?W (excitation frequencies of 24,897?Hz) for the remaining length of the tube. As it is clear, axial forming force starts to decrease at the switching time of ultrasonic. In addition, more decrease is earned at higher power levels.
Effect of ultrasonic vibration of the tube workpiece on the bore dimension
The formed tube has two inner diameters (Figure 9(a)): base and formed circle diameters. The former is approximately equal to the initial inner diameter of the tube, and the latter is close to the tool sizing area diameter. It must be mentioned that before doing the tests, tubes were cleaned by ultrasonic cleaner for 10¨C15 min and then cleaned using alcohol and maintained in silica gel for moisture control. Two methods have been used to measure the tube bore dimensions.
Video measuring machine (VMM) operation is based on identification of dark and light boundaries; therefore, sharper edges of measuring surface improve the measurement accuracy. For using this method, small pieces of sample were cut carefully normal to the axis of the tube, and they were ground finely with burrs removed by emery cloth and then washed and dried before measurement. The pieces were placed on the VMM (KIM-CU Series; ARCS, Taiwan) table, and through adjusting its focal point and lighting, four suitable points on formed circle were selected to determine the diameter.
Percentage of spring back can be calculated from the following relation
Table 3 shows the value of spring back for the formed circle obtained by VMM with and without ultrasonic application. Ultrasonic power for the test was set at 1186 W with frequency of 25,613 Hz at 1260 mm/min tool feed-speed.
in the explained measuring method, human error in selecting the points (dark and light boundaries) is very effective; the calculated percentage value may be affected. To have more secure results, an alternative measuring method was also used as follows.
The coordinate measuring machine (CMM) used in this study (Bridge Type; Leader Metrology Company, China, 2009) 18 has Renishaw PH10T head and Renishaw TP20 probe with 2 mm diameter stylus (Figure 9(c)). The ruby tip of stylus makes contact with the inner surface of the samples prepared from the tube, and by selecting four sample points around the formed circle, the circle diameter was determined. Measurements were performed three times in five sections, and the results given in Table 3 are the average of 15 repetitions with ¡À0.2% of coefficient of variation. The measurement results of this method are also shown in Table 3. It is obvious that there is no significant effect from ultrasonic vibration on the spring back of the tube.
Effect of axial ultrasonic vibration given to the workpiece on the inner surface quality of the formed tube
In order to investigate the effect of ultrasonic vibration on the inner surface quality of the formed zone, two sections of a tube in which ultrasonic vibration was applied for only the second half of its length were cut: one section at the middle of the place without application of ultrasonic and one section at the middle of the place formed under ultrasonic vibration. The pieces separated from the tube were halved along the axis in order to be able to see the deformed bore surface of the tube. Before viewing, the pieces were washed and dried. Figure 10 shows the surface quality of the two sections by using the optical measuring device (VMM). This figure reveals that the presence of ultrasonic vibration improves inner surface quality of the formed tube. In fact, creating a homogeneous surface pattern is one of the advantages of ultrasonic vibration. Although the surface of the tool was fully polished to mirror finish, traces of scratches are evident in Figure 10(a) in the form of vertical lines. Ultrasonic vibration significantly reduces these traces.
Results and discussion
Based on the findings of Figure 4, vibration amplitude and the ratio of tool feed-speed to amplitude of vibration speed have a significant influence on mechanism of process and on the forming force. Theoretical study shows that increase of the latter cause decrease of ultrasonic effect and thereby increasing forming forces. At unity, the effect of ultrasonic vibration is completely eliminated and the forming force will be equal to operation without ultrasonic.
Experimental results with and without axial longitudinal ultrasonic vibration of the tube and theoretical results of applying axial longitudinal ultrasonic vibration to the tool in various amplitudes of vibration (0, 5, 10, 27.8, 40 and 60??m) have been shown in Figure 11. According to the theoretical relations, in the vibro-impact regime, the axial forming force reduction is approximately 50% (5602 N in the presence of ultrasonic vibration of 60??m amplitude). In continuous regime of 10??m amplitude, the theoretical reduction is about 18% (9589?N). In experimental tests, maximum axial forming force reduction is about 21% (9249?N) when ultrasonic vibration of 1705?W power at frequency of 24,748?Hz and tool feed-speed of 840?mm/min is used. Therefore, comparing the experimental and theoretical results, it shows that the loading regime is continuous in experimental tests, that is, during the ultrasonic vibration, tool and tube are in continuous contact in the whole vibration period and no separation occurs.
It should be mentioned that during experimental tests, the longitudinal vibration has been applied to the workpiece, and the tool was forced to go ahead by a separated rod (no pulling was possible by the rod). In practice, due to limitations of tool feed-speed in the present experimental equipment, it was not possible to investigate the effect of the speed ratio (tool feed-speed over vibration speed amplitude) on the forming force. The amount of experimental axial forming force reduction in the presence of ultrasonic vibration was determined about 2¨C3?kN (15%¨C21%). It is expected that there was no separation between the tool and the workpiece and forming was of continuous type, these effects can be attributed to two dominant phenomena.
Nature of indentation forming process when the workpiece tube is longitudinally vibrated
In present experimental studies, feed force was transferred to the tool by a disconnected feeding rod pushing the tool only in one direction (ahead) inside the workpiece tube along the whole length of the tube.
By considering v<a¦Ø in our experimental condition, two forming situations could exist. The first, if the workpiece spring back was higher than twice the amplitude of vibration (2a<Ft/k0) , then the contact and the pressure force in the forging zone between the tool and the workpiece would not be interrupted and thereby the contact between the feed-rod and the tool was not interrupted either, whether the vibration given to the tool or given to the workpiece. In this case, only a little back move from the tool happens in response to elastic spring back of the workpiece.
The second, if the workpiece spring back was lower than twice the amplitude of longitudinal vibration given to the workpiece (2a>Ft/k0) , then although the tool was not separated from the workpiece (because of the lack of solid permanent connection of the tool and feed-rod), the rod could probably be separated from the tool in a half of a period and the rod tip could hammer and strike on the back of the tool in every vibration period.
It must be noted again that in our real case, the longitudinal vibration was given to the workpiece and the tool was driven forward by a disconnected rod, which was touching the back of the tool and pushing the tool ahead. Under this condition, even at low spring backs of the workpiece, the tool does not separate from the workpiece under any circumstances, but the hammering effect of the feeding rod still can occur if the condition of (2a>Ft/k0) is reached.
In our experiments, comparison of theoretical and experimental results (Figures 4 and 7) shows that the condition of 2a<Ft/k0 exists and no separation between the tool and the workpiece tube and the tool and the pushing rod occurs.
Effects of frictional forces, acoustic softening, local temperature rise at discontinuities and material structural changes
Unfortunately, the force monitoring system (Kistler, Type 9255B) has a maximum response speed of 1?kHz, and it is not able to show the force variation within a cycle as the variation frequency is much higher (about 25?kHz). Therefore, it has an averaging effect on the force. Otherwise, the said mechanisms in part 1 could be analyzed experimentally.
Conclusion
In this article, theory of ultrasonic-assisted indentation forming process (axial longitudinal vibration given to the tool) has been studied, and the corresponding analytical relations have been derived. In addition, for comparison purposes, experimental tests have been performed under axial longitudinal ultrasonic vibration given to the workpiece. According to the theoretical and experimental findings, the following conclusions can be made.
1-饶勇超
1-The high-throughput highway to computational materials design

ABSTRACT
High-throughput computational materials design is an emerging area of materials science. By combining advanced thermodynamic and electronic-structure methods with intelligent data mining and database construction, and exploiting the power of current supercomputer architectures, scientists generate, manage and analyse enormous data repositories for the discovery of novel materials. In this Review we provide a current snapshot of this rapidly evolving field, and highlight the challenges and opportunities that lie ahead.

INTRODUCTION
Every technology is intimately related to a particular materials set. The steam engines that powered the industrial revolution in the eighteenth century were made of steel and, information and communication technologies are underpinned by silicon. Once a material is chosen for a given technology, it gets locked with it because of the investments associated with establishing large-scale production lines. This means that changing the materials set in an established technology is a rare event and must be considered as a revolution. Moreover, the initial choice of a material is absolutely crucial for the long-lasting success of a technological sector. Importantly, recent times have seen a surge of new technological niches, each one of them potentially looking for a different materials set. Thus, the pressure on the development of new materials is becoming formidable. These should score on many counts. They should be tailored on the specific property that the technology is based on, they often should be compatible with other technologies, should not contain toxic elements, and, if needed in large quantities, should be made of cheap raw materials. As such, searching for materials is a multi-dimensional problem where many boxes should be ticked at the same time.
Although the demand for materials is endlessly growing, experimental discovery is bound by high costs and time-consuming procedures of synthesis. Is there another way? Indeed, this is the burgeoning area of computational materials science called 'high-throughput' (HT) computational materials design. It is based on the marriage between computational quantum-mechanical–thermodynamic approaches and a multitude of techniques rooted in database construction and intelligent data mining. The concept is simple yet powerful: create a large database containing the calculated thermodynamic and electronic properties of existing and hypothetical materials, and then intelligently interrogate the database in the search of materials with the desired properties. Clearly, the entire construct should be validated by reality, namely the existing materials must be predicted correctly and the hypothetical ones should eventually be made. Such a reality check feeds back to the theory to construct better databases and increase predictive power.
The HT experimental approach was pioneered over a hundred years ago by Edison and Ciamician, but with the advent of efficient and accurate theoretical tools and inexpensive computers, its computational counterpart has become a viable path for tackling materials design. Thus, in the past decade computational HT materials research has emerged following the impetus of experimental HT approaches In the literature, HT materials research is often confused with the combinatorial evaluation of materials properties. Although a few attempts have been made to clearly define the two concepts, the distinction is not yet rigorous. Here we define HT as the throughput of data that is way too high to be produced or analysed by the researcher's direct intervention, and must therefore be performed automatically: HT implies an automatic flow from ideas to results. The confusion of HT with combinatorial approaches is thus resolved. The latter, in fact, specifies how the degrees of freedom are investigated, whereas HT strictly defines the overwhelming and automatic flow of the investigations.
The practical implementation of computational HT is highly non-trivial. The method is employed in three strictly connected steps: (i) virtual materials growth: thermodynamic and electronic structure calculations of materials; (ii) rational materials storage: systematic storage of the information in database repositories; (iii) materials characterization and selection: data analysis aimed at selecting novel materials or gaining new physical insights.
High-throughput is often known for the large databases it generates (for example, the AFLOWLIB.org consortium and the Materials Project. Here we posit that all three HT stages are highly necessary, but that the last one is the most challenging and important. In fact, it is the step that allows one to extract the information and, as such, it requires a deep understanding of the physical problem at hand. The intelligent search of a database is performed by means of 'descriptors'. These are empirical quantities, not necessarily observables, connecting the calculated microscopic parameters (for example, formation and defect energies, atomic environments, band structure, density of states or magnetic moments) to macroscopic properties of the materials (for example, mobility, susceptibility or critical temperatures). In other words, the descriptor is the language with which the researcher speaks to the database, and thus the heart of any effective HT implementation. In Table 1 we illustrate examples of 
recently introduced descriptors.
Once a good descriptor is identified, the search for better materials within the repository can be performed intrinsically or extrinsically, depending on whether the optimum solutions are already included in the set of calculations or not. Intrinsic searches include just step (iii), require only fast descriptors, and may employ various informatics techniques. Examples of previous such searches include the scanning of better cathode materials, and the uncovering of unknown compounds, novel topological insulators16 or thermoelectric materials. Extrinsic searches involve all three steps, because the search for an optimal solution includes iterations leading to an expansion of the repository.
An important component of extrinsic HT computational research is a scheme capable of using the evaluation of descriptors on existing database entries to guide new calculations not yet included in the database. Examples of such schemes published in the literature comprise evolutionary and genetic algorithms7,8, data mining of spectral decompositions3 and Bayesian probabilities10, refinement and optimization by cluster expansion and structure map analysis. Neural networks35,36 and support vector machines have also been utilized in a few cases. These methods may sometimes be used to bypass step (iii) of the HT analysis, that is, the formulation of a physically meaningful descriptor, so that a search can still be implemented even with only a superficial understanding of the physical problem.

RESULT
Areas of current application
Following the general framework outlined above, we describe in this section a few specific examples of computational HT studies reported in the literature, ordered by increasing degree of complexity.
Thermodynamics for the identification of binary and ternary compounds. The identification of stable structures is the first step in the design of materials with various specific functionalities. The proper descriptor of alloy stability, the formation enthalpy, is the simplest example of a parameter used for HT materials development.
Alloys are the workhorse material of many important technological applications. Thus, finding new and improved alloys could be transformative in some areas and would have a substantial economic impact. When improving an existing alloy or designing a new one, scientists rely on databases of alloy thermodynamics and phase diagrams (for example, the Massalski's Binary Alloy Phase Diagrams38 and the Villars's et al. Pauling File). Although the utility of these repositories is tremendous, they could be of even greater use if they were more complete. Experimental completeness is difficult to achieve due to the vast combination space and because experimentation is often difficult: it requires high temperatures or pressures, very long equilibration processes, or may involve hazardous, highly reactive, poisonous or radioactive materials. Computational compilation of the properties of materials is more feasible and will lead to much more complete repositories. Examples that demonstrate this are the almost simultaneos prediction and experimental verification of the previously unknown C11b structure of the Pd2Ti compound9,40, the verification by Niu et al. of an earlier prediction42 that the CrB4 compound, thought for 40 years to have an oI10 structure, is actually more stable in an oP10 structure, and the simultaneous synthesis and solution, by an ab initio evolutionary search, of an unexpectedly complex tI56 crystal structure of CaB6 .
In alloy design, the targets of the formation enthalpy descriptor are stable phases. The HT ab initio method explores the phase stability landscape of alloys by calculating the descriptor for a large number of possible structures. An HT code must perform these calculations automatically, transform the structures into standard forms that are the easiest to calculate, and automatically set the necessary k-point grid densities, basis-set energy cutoffs and relaxation cycles with a convergence tolerance of the order of a few meV per atom. It should also respond automatically to calculation failures, due to insufficient hardware resources or runtime errors of the ab initio calculation itself. These are among the most difficult challenges in HT database generation that have only recently been overcome (ref. 44 gives details about how this automatic data generation is implemented in the AFLOW HT framework). The initial search is performed on a set of known crystal structures, of all lattice types, spanning the entire composition range of the investigated systems3,9. In advanced HT studies this set includes hundreds of structures per system44. In subsequent steps, the search is often aided by data-mining and optimization techniques that refine and accelerate the structure screening. They include a variety of different approaches: for example, cluster expansion with exhaustive evaluation or genetic search algorithms on fixed-lattice systems46,47,48, and evolutionary algorithms for off-lattice structures in mixtures with fixed stoichiometries. These screening and optimization techniques are continuously being improved and adapted for implementation in HT frameworks. The search concludes with the automatic construction of the Gibbs free-energy curve for each system from the minimum-energy structures at various component concentrations.
As of this writing, the largest computational alloy database, the Binary Alloy Project hosted in the AFLOWLIB.org consortium repository24, contains the formation enthalpies for hundreds of thousands of intermetallic structures comprising all the transition metal systems and many other intermetallics. The same framework is also being used to generate similar data for ternary alloys. This information overlaps much of the experimental phase-diagram databases and complements them where the data is partial or missing.
By using HT, massive analyses become possible. Figure 1 illustrates the capacity of HT in dealing with all of the 435 d-electron binary intermetallics (elements ordered by Pettifor's scale54). The top left triangle in Fig. 1 shows the ordering tendency of the mixtures, defined as the maximum temperature at which the entropic term of an ideal solid solution is equal to the formation energy of the mixture, calculated ab initio. This is a measure of the strength of a mixture to oppose disorder. Curtarolo et al.24 demonstrated that HT is capable of reproducing the existence of stable ordered structures, or lack thereof, in 80% of the comparisons: in HT, 65% of the existing binaries are found to be compound forming, whereas in only 58% of the systems compounds have been experimentally reported. The bottom right triangle shows this comparison. In cases of agreement between experiments and calculations on the existence of compounds, HT has been found to reproduce the experimental structure ∼96.7% of the time (equation). The database has been used to extensively study several alloy classes, for example, platinum group metals, Mg- and Hf-alloys and to pinpoint particular missing features of well-known systems (for example, kinetic acceleration in Fe–V alloys59). The availability of low-temperature HT data opens new avenues for high-temperature Monte Carlo simulations, and ultimately for the automatic determination of phase diagrams.
Solar materials. Photovoltaic (PV) cells are specialized semiconductor diodes that convert light into direct electric current. Typically, they consist of a transparent conductive oxide layer (such as In-doped SnO2), an anti-reflection coating (such as Si3N4), a p-type doped crystalline Si semiconductor layer with some n-type dopants diffused at the top, and a metallic electrode layer (typically Al), each layer stacked on top of each other. Although such monocrystalline silicon solar cells can convert a useful amount of terrestrial solar energy into electrical energy60, their construction requires energy-intensive manufacturing at high temperatures (400–1,400 °C) as well as numerous lithographic processes to employ light-trapping techniques. Hence, these conventional cells are too expensive to replace non-renewable energy sources. Indeed, since the introduction of Si solar cells in the 1950s, the search for alternative light-absorbing materials has been an active area of research.
Despite the importance of PV materials, the commonly used materials, such as Si, GaAs and CuInSe2, have been discovered accidentally, and been incrementally improved over the years. A systematic analysis of the over 150,000 entries of the inorganic crystal structure database (ICSD) could provide, given the appropriate search parameters, previously undiscovered materials with the appropriate characteristics: semiconductors with strong optical absorption coefficients, a bandgap of ∼1.3 eV (the Shockley–Queisser criterion), low cost and, ideally, compatibility with existing technologies. Only recently, the in silico screening of PV materials has been attempted with HT techniques: the real difficulty is the characterization of the proper descriptor for the identification of the candidate materials. Yu and Zunger introduced the concept of 'spectroscopic limited maximum efficiency (SLME)', a descriptor combining bandgap, shape of absorption spectra and material-dependent non-radiative recombination losses (all intrinsic materials properties) that were used to tackle the ICSD database63 (Fig. 2). With their procedure, they were able to identify a set of high SLME materials, including the best already known thin-film solar absorbers, such as CuInSe2, CuGaSe2 and CuInS2, and others that have been found experimentally to be feasible solar absorbers but are much less studied. Interestingly, they were able to identify high SLME materials away from a 1:1:2 stoichiometry (for example, Cu7TlS4, Cu3TlS2 and Cu3TlSe2). Although Tl-containing materials might be unfavourable in practice because of the high toxicity of Tl in the +1 oxidation state, these results suggest that replacing Tl with non-toxic elements could be a viable route to novel high SLME materials.
Water photosplitting. Castelli et al. screened a large class of oxide and oxynitride materials (5,400 semiconducting compounds in the cubic perovskite structure covering 52 metals) for optimal solar-light capture in photoelectrochemical cells. Their HT approach, based on the screening of candidate materials by looking at criteria for stability and for the size and position of the bandgap, identified ten oxides and five oxynitrides that are well known in the water-splitting community, and predicted nine new combinations for further experimental investigation. In this study, the descriptor was a combination of materials properties (the gene), which include (i) an appropriate bandgap; (ii) well-positioned band edges relative to the water redox levels; (iii) high mobilities, allowing electrons and holes to reach the surface and reduce or oxidize the targets before recombining; and (iv) chemical and structural stability under irradiation. Two of these descriptors (i and iv) are displayed together in Fig. 3, highlighting the process of materials gene construction.
Carbon capture and gas storage. Lin et al. recently proposed another application of HT to energy and environment: the screening of materials for large-scale carbon dioxide capture and sequestration (CCS) in power plants, where carbon dioxide should be captured at its source for subsequent storage in non-atmospheric reservoirs. Capture materials and processes that reduce the parasitic energy imposed by CCS are of extreme interest not only for their industrial impact but also for long-term management of climate change. A complex process such as CCS requires the introduction of composite descriptors that must go beyond single material properties. In their study, Lin et al. introduced a complex metric, the 'parasitic energy', to identify the optimal process conditions for each material. This descriptor is predicted by the minimization of the electric load imposed on a power plant by temperature–pressure swing capture processes. Previous investigations were limited to only a handful of materials and/or single property descriptors. Lin et al. were able to screen hundreds of thousands of zeolite and zeolitic imidazolate framework structures, and identify many different structures that have the potential to reduce the parasitic energy of CCS by 30 to 40% compared with near-term technologies.
Another interesting example is the work by Wilmer et al. on the screening of metal–organic frameworks (MOFs) for natural gas storage. In this study, the methane-storage capacity of 137,953 hypothetical MOFs was calculated, and over 300 systems were identified to have a potentially better capacity than any known material. One of such predictions, a methyl-functionalized MOF, was also experimentally verified69. In another study, Alapati et al. scanned over 100 dehydrogenation reactions on experimentally known compounds, and uncovered several new reactions for potential hydrogen-storage materials. Experimental confirmation of the predictions for the LiNH2:MgH2 system yielded an 8 wt% capacity and an enthalpy of reaction within about 10% of the predicted value. This particular system had been experimentally studied previously for a different ratio of constituents. The new promising composition was suggested by the large-scale computational scan.
Nuclear detection and scintillators. The design of new scintillator materials for γ-ray nuclear detection has recently attracted much interest due to the potential industrial and security applications. Large-scale experimental studies are already feasible72 and accentuate the necessity of guidance by HT computations.
Computational predictions were first proposed by Ortiz et al., using a highly accurate version of the full potential linear muffin-tin orbital method. They parameterized ∼22,000 compounds from the ICSD database63, and implemented a data-mining reduction, based on electronic structure considerations, to extract 136 potential novel scintillators. Using HT methods, Setyawan et al. later proposed a solution to the 'non-proportionality' puzzle, a long-standing problem in the field of scintillator materials. In ref, they introduced a 'non-proportionality descriptor', the mismatch between effective masses of the carriers, me (mh), near the bottom (top) of the conduction (valence) bands,
which correlates with the non-proportionality response for oxides and semiconductors (non-proportionality defined as NP ≡ Y10keV/Y662keV where Yi denotes the photon light yield resulting from γ radiation with energy E in keV). The explanation of the correlation  NP led to new scientific understanding of the role of the electron–hole mismatch, free or coupled as an exciton, in the spatial distribution around — or in the migration to — a recombination centre (usually a defect or a dopant). The correlation was experimentally verified with the application of hydrostatic pressure, which modifies the effective masses through the induced strain in the crystalline cell, and increases or decreases the non-proportionality depending on the electro–elastic response of the material. In ref. , the descriptor was further optimized by integrating it with theoretical light yield and photo-attenuation length, so that an extensive dataset of compounds could be analysed.
Topological insulators. Quantities directly accessible from the band structure (energy gaps, effective masses and so on) may be used as descriptors or combined in composite descriptors, such as those recently developed to search for novel topological insulator77 materials16. In a first study, Lin et al. searched for ternary thermoelectric Heusler compounds and demonstrated that although most of the well-known ones, such as TiNiSn and LuNiBi, are topologically trivial, the distorted LnPtSb-type compounds (such as LnPtBi or LnPdBi, Ln = fn lanthanides), belonging to the half-Heusler subclass, are topologically non-trivial and could provide a platform for the realization of multifunctional topological devices.
In a more recent study, Yang et al. searched the quantum materials repository AFLOWLIB.org and automatically discovered 28 topological insulators (some of them already known) in five different symmetry families by defining a variational descriptor (Table 1) that represents the topological robustness or feasibility of the candidate system. The newly discovered materials included peculiar ternary halides, Cs(Sn,Pb,Ge)(Cl,Br,I)3, which could hardly have been anticipated without HT means. The robustness descriptor combines information on the energy versus strain variations of band structures with and without spin–orbit coupling. This is an example where the definition of the descriptor combines advanced electronic-structure data with geometrical modifications to obtain the essential gene that is associated with the required property (Table 1).
Piezoelectrics. High-throughput techniques have also been applied to the search of materials with large piezoelectric coefficients, a class of systems with many technological applications. In particular, the discovery of the anomalously large piezoelectric effect in lead zirconate titanate suggests that other materials of the same family could display similar properties. The large piezoelectric response observed in the solid solution between lead titanate and lead zirconate relies on the formation of a morphotropic phase boundary (MPB) between tetragonal and rhombohedral distortions of the perovskite structure. Calculations of the relative stability of those phases is crucial for predicting the formation of the MPB and the potential of materials to exhibit large electromechanical couplings. In a systematic search across all the possible 3,969 ABO3 compositions, Armiento et al.79 developed simple descriptors (and other criteria) to screen viable phase-diagram end-points (Table 1). These descriptors and criteria were used to extract a set of 49 compositions that can be seen as the fundamental building blocks of isovalent alloys for compounds forming MPBs, suitable for high piezoelectric performance. Among those, they identified three primary composition groups, (Sn,Pb)(Zr,Hf,Ti)O3, (Ba,Sr,Ca)(Zr,Hf,Ti)O3 and (Li,Na,K,Rb,Cs)(Ta,Nb)O3, which coincide with three known materials with MPBs.
More recently, Roy et al. screened the ICSD for half-Heusler semiconductor compounds, looking for previously unrecognized piezoelectric systems. In their search, they scanned 987 candidate combinations for insulating character and structural, dielectric and piezoelectric properties, and identified a number of promising systems.
Thermoelectric materials. The efficiency of direct thermoelectric energy conversion devices is directly related to the dimensionless thermoelectric figures of merit (ZT) of their p- and n-type components. ZT is defined as TσS2/κ, where σ is the electric conductivity, S is the Seebeck coefficient, κ is the zero current thermal conductivity and T is the temperature81,82. The ZT depends on temperature and on doping. Thus, for each operation temperature there is a different set of compounds that display the best ZT, once their doping level has been optimized. In this context ZT would be an ideal candidate for an HT descriptor.
An early example of HT was provided by Madsen83, who reported an automated search for new thermoelectric materials among 570 Sb-containing compounds in the ICSD database63. The study suggested the Zintl compound LiZnSb as a potentially interesting n-type thermoelectric. Subsequent experiments84 found good agreement between the ab initio calculated transport properties for the p-type material and the measured ones, and concluded that p-type LiZnSb is not a good thermoelectric. However, it was not possible to assess the n-type material experimentally. Using a comparable approach, Yang et al.85 screened the thermoelectric properties of 36 half-Heusler compounds.
In addition to HT calculations of bulk crystalline compounds, an immense unexplored territory lies in the field of nanostructured materials. A potential area of development for thermoelectric systems is the case of nano-granular materials. In a recent study, Wang et al.15 compared the thermoelectric power factors (σS2) of more than 3,000 compounds in the ICSD database. Figure 4 indicates the compounds that are ranked as worthy of further study. Some potentially good solutions captured by HT would be missed by traditional approaches that typically explore compounds similar to known good ones. In addition to predicting new materials, the HT analysis also pinpoints interesting phenomena. Using principal component analysis on the obtained data, the authors established correlations between the power factor and other structural or electronic properties of the compounds. In particular, it was found that, at a given grain size, higher power factors are more likely to occur in sintered compounds with large charge 
carrier effective masses and bandgaps, and with a large number of inequivalent atoms in the unit cell. The first two rules were already known from previous models86, but the HT analysis was able to obtain this trend through an automated procedure. The third finding, relating power factor and unit-cell size, is new for electronic properties. A similar correlation has been known for thermal conductivity, stating that larger unit cells are associated with lower thermal conductivities. Together, these rules for electronic and thermal transport indicate that higher ZTs are to be expected in materials with complex unit cells.
Materials for catalysis. The search for new solid catalysts is probably the most complex problem tackled ab initio87,88. The complexity stems from the numerous microscopic variables affecting catalysis, such as reactants' energies and transition states on the surface through all the steps of the catalytic process. Although there are several examples of catalytic processes with a complete ab initio characterization89,90,91,92,93,94, an HT screening of a large number of processes in this manner is at present far beyond current capabilities. A significant advance has been made in medium-throughput computational screening for catalysts with increased activity and improved selectivity, representing the foundation on which a future fully HT approach could be developed.
As in the previous examples of HT screening for new materials, the search for improved catalysts relies on simple descriptors. However, because the catalysis problem is very complex, these descriptors are reaction specific and their identification requires preliminary analysis of the reaction steps. This is in contrast to the previous examples where a general descriptor of the sought-for functionality is heuristically deduced from the underlying physics and directly employed to screen for candidate materials.
Computational design studies of catalysts carried out in recent years reflect this complexity and include a few principal steps. First, the elementary steps of the reaction, including all the intermediate chemical species, are identified. The energetics of each step, including dissociation and adsorption energies and reaction barriers are evaluated for a number of transition metal surfaces by density functional theory (DFT) calculations, and the active sites, usually a kink or step on the surface at which the activation energies are lowest, are also determined for the specific reaction. Next, correlations are identified between the adsorption energies of the different adsorbates and intermediate species (scaling relations100), and between these adsorption energies and their associated transition-state activation energies (Brønsted–Evans–Polanyi relations87,88,101). The determination of these relations is of central importance for computational catalyst design, as it allows one to model the catalytic reaction in terms of the minimal number of independent chemical parameters, the descriptors of the process. These descriptors are usually the adsorption energies of the main components of the reactants, which are much easier to evaluate by DFT calculations than activation energies. They can therefore be implemented as practical descriptors of catalytic processes involving dissociation of simple molecules on a metal surface102. It is important to note that each active surface site defines a different set of these relations and therefore it is crucial to identify those that lead to the highest reaction rate and the best catalyst.
Once the appropriate descriptor or descriptors have been identified, they are related to measured or calculated total reaction rates for the various metals87,96, leading to volcano-shaped plots where the optimal catalyst can be identified as the one lying closest to the top of the volcano. These volcano plots emerge due to a fundamental concept in catalysis, the Sabatier principle, stating that for an efficient catalysis the interaction between the surface and reactant should not be too strong nor too weak103. When interactions are too weak, reactants will not bind to the catalyst and reaction will not take place. In contrast, if the binding is too strong the catalyst will get blocked, poisoned by reactants or intermediates, or the products will fail to dissociate. Using the volcano plot, the optimal catalyst may be found from a much larger set of candidates than the transition metals used to construct it, because for these candidates the evaluation requires the calculation of only a small number of 
descripters. Additional considerations, such as the estimated structural stability and selectivity at the appropriate chemical environment, and cost of the candidates, can be included in the search87,88,96.
Several descriptor-based searches for new heterogeneous catalysts have been reported in recent years, each screening a few dozen metal surfaces and surface alloys for various chemical reactions. Figure 5 shows the results of one of these studies (Greeley et al.96): the volcano plot (Fig. 5a) and the optimum active surface alloys for a hydrogen evolution reaction (Fig. 5b). Furthermore, among the predictions made by Studt et al.107, NiZn has been experimentally verified to have better selectivity than the traditional Ag–Pt catalyst at a fraction of the cost, because it contains no precious metals.
Battery materials for energy storage. Efficient energy storage, with high capacity and long cycling lifetime, is one of the main issues in the development of sustainable clean-energy technologies. Lithium-ion batteries are the state of the art in this field. They operate by the transfer of lithium ions from a high chemical potential anode to a low chemical potential cathode through an ion-conducting electron-insulating electrolyte while the electrons flow through an external circuit. Recharging is performed by applying an external potential, forcing the Li ions to migrate back from the cathode to the anode. Almost all anodes currently in use are based on graphite, due to its Li storage capacity, and cycling and safety characteristics109. The electrolytes are solutions of Li salts in organic solvents. Several recent attempts have been made to enhance the anode capacity without degrading cycling and safety, using MoSi2-based compounds. However, the main route taken by current research to improve Li-ion 
batteries is to seek new cathode materials with superior properties. Finding new battery components experimentally might be impractical due to the very large chemical space and the difficulty of the experiments. High-throughput computational research can make a difference, but as pointed out by Ceder et al., a few physical parameters have to be taken into account in addition to structure stability. The search for better materials is a complicated multistep process, with constraints in terms of safety, toxicity, weight, capacity, charge and discharge rates, recyclability and cost28,114.
The search consists of three steps. First, it starts with a set of candidate chemistries, namely, chemical compositions containing high concentrations of Li ions, a redox active metal, and oxygen or oxide ions (carbonate, borate, phosphate, silicate or arsenate) that could stabilize a rechargeable Li-containing compound. The chemistry defines the electron activity that can be achieved during the delithiation process and essentially determines the capacity of the battery. Computational screening of the multi-component system is then carried out to identify stable crystallographic structures. This is performed by ab initio calculations of the total energies of a large number of structures, chosen with ad hoc algorithms10,115 to parameterize the thermodynamic stability of the systems. This step has been fully implemented in an HT framework.
Second, to achieve good cycling behaviour the structures should facilitate intercalation of Li ions, for example, reversible host–guest chemistry where the ions migrate through layers or tunnels in the host structure with minimal changes in the host itself113. The energy density of a battery is determined by the capacity — defined by the chemical process — and the voltage profile, which can be related to the Gibbs free-energy difference between different charged states of the cathode material. Good candidates should exhibit the highest possible voltage, compatible with the limitations of electrolyte stability, usually less than 5 V. Ceder shows an example of the relationship between chemical potential and material resilience to a reducing environment for several classes of compounds. Considerable advances have been made towards a full HT implementation of this step, but it has not been achieved yet. Standard DFT exchange-correlation functionals — for example, the local density approximation (LDA) and the 
various forms of generalized gradient approximations (GGA) — may lead to large errors in voltage predictions in redox processes that involve electron transfer between different orbitals, and more advanced and precise hybrid functionals are currently too computationally expensive for HT116. The alternative, and less expensive, DFT+U method is usually implemented. So far, the most extensive screening of cathode materials through voltage prediction has been carried out by Hautier et al.120 for over 600 phosphate compounds selected out of more than 4,000 structure total-energy calculations. Smaller studies were done on polyanion compounds, with just over 200 different compositions of the sidorenkite structure121, and on 64 tavorite structured materials.
Third, following the screening for voltage and cycling potential, the remaining candidates are evaluated for charge and discharge rates. As illustrated by Ceder et al.27, the rate-limiting processes, which are the ionic and electronic conductivities, are evaluated through ab initio calculations of the diffusion barriers for ion migration and of the availability of charge carriers and their mobility in the polaronic intercalation compounds of the electrodes (the most promising new cathode materials are electrical insulators such as borates, phosphates and silicates). This step of evaluation, the most computationally expensive, is carried out at the end of the screening process and it has not yet been implemented in HT. Indeed, Mueller et al.122 calculated lithium-diffusion activation energies for just three structures out of the 64 tavorite compounds proposed, and no lithium-diffusion or polaron-migration calculations were reported in the larger studies120,121. A future implementation of an HT framework for this step would require a definition of an efficient descriptor for these transport properties, which is currently non-existent.
Outlook

Outlook
The computational HT approach, although at a good level of maturity, is still far from being a magic box for materials discovery. Several important properties and classes of materials have not been addressed in this framework, and further algorithm implementations, repositories and data-mining interfaces, are necessary. Here we give important examples of materials and property challenges to be addressed, and an overview of the possible evolution of the framework.
Stability at finite temperatures. Ab initio calculations of metallic systems often predict ordered compounds that may be difficult to fabricate due to entropy and kinetic effects. The assessment of the thermal stability of structures and the practicality of their realization requires accurate estimation of the various entropic contributions (for instance, configurational, vibrational and magnetic) of the competing phases predicted by HT calculations. For specific sets of structures sharing a common parent lattice, this can be accomplished by high-temperature Monte Carlo simulations. However, a general HT ab initio framework for this problem is still lacking.
Thermoelectrics. Design of better thermoelectrics requires a better estimation of the carrier lifetimes. The standard approximation of a constant relaxation time is not satisfactory. However, recent work has shown that it is possible to predict carrier lifetimes and electronic mobility with good accuracy, fully from first principles. Also, the calculation of the lattice thermal conductivity can now be accurately calculated ab initio. The definition of a simpler descriptor that can speed up massive HT computation remains a considerable challenge. For bulk systems, a possible approach is to relate the lattice thermal conductivity to the Debye temperature and Grüneisen parameter that can be estimated by the 'GIBBS' isothermal–isobaric approach132. For nanostructured materials, an additional challenge is the evaluation of conductivities at interfaces, where the transport problem couples with the difficulty of predicting nanoscale geometries. Here the solution lies in a 
quantum-transport treatment that has only recently been considered for HT applications.
Magnetic materials. Although magnetism can be found in a multitude of materials (several thousands), the choice of magnets available for mainstream applications is much more limited (around two dozen)134. There are two main reasons for such limited diversity. First, any standard application, regardless of the particular technology it concerns, needs to operate in the temperature range between −50 °C and +120 °C, which requires the magnet to have a critical ordering temperature, TC, of at least 550 K. Unfortunately, there are only a few hundred magnets with such a high critical temperature. The second reason is that magnetic materials need to satisfy additional physical constraints for each specific application. Thus, for instance, energy-related technologies (electric turbines, electric motors and so on) require large magnetic energy densities, whereas magnetic sensors often need sensitive magnetic–electric responses.
Only by exploiting HT techniques can one explore the possibility of synthesizing new high-performance magnetic materials, and search into large materials classes that are known to be populated by high-temperature magnets. Particular classes of interest are the intermetallic ternary materials, such as the Heusler compounds. A simple combinatorial calculation gives an upper limit for the number of possible Heusler compounds (including half-Heusler) of about 230,000. Among these, about 1,500 are known and have been synthesized in the past. However, there is still a significant number for which a synthetic strategy has not been designed. Particularly important would be the development of permanent magnets without critical elements (so as to counteract the current 'rare-earth crisis'), or of magnets whose properties are specifically targeted to electronics applications, such as magnetic random access memories. Finally, HT technologies can be used to design entire magnetic heterostructures, and the case of tunnel
magnetoresistance devices seem particularly attractive.
Heusler alloys. This Review includes a few references to research on Heusler alloys. In addition to the reviewed topics (thermoelectricity, topological insulators, piezoelectricity), such systems have drawn general attention in the computational materials community: chemical stability was investigated by the Zunger group, and bandgap and lattice constants were computed, for optoelectronic applications, by Gruhn. A repository of Heusler alloys' calculations would thus be useful for scientific and industrial applications. This is an undergoing task of the AFLOWLIB.org consortium. By combining the wealth of binary intermetallics and the parameterization of all the possible Heusler alloy combinations (full-[AlCu2Mn], half-[AgAsMg] and anti-[CuLi2Sn]), one can rapidly determine the thermodynamic stability and the appropriate electronic-structure features.
Alloy theory at the nanoscale. The extension of the HT framework to predict alloy stability at the nanoscale has great technological implications, especially for catalysis. Many phenomena are chemically dependent on the stable surface of the catalyst, and the proper parameterization of the surface stability (energy) and surface tension (stress) will greatly help the development of new catalysts (for example, the size-dependent phase transitions in Fe–C mixtures have been shown to be responsible for deactivation in nanotube growth).
Catalysis. Considerable improvements are necessary to advance to a more comprehensive HT framework for computational catalyst design. One direction is related to the current limitations of DFT calculations in treating non-metallic surfaces, electronic bandgaps and excited electronic states, and the chemistry of atoms and molecules on such surfaces in various environments143. Improvements in this field would be needed to extend the current framework from transition metals to other useful materials families, such as oxides, sulphides, nitrides and zeolites. Furthermore, for the development of catalysts with long lifetimes, the thermodynamics of the catalyst–product mixture must be elucidated, especially at the nanoscale where the quest for more active surface dictates size reduction and, as a consequence, possible size-induced thermokinetic deactivation.
On the HT conceptual level, developing a systematic methodology to determine appropriate descriptors for an as extensive as possible variety of catalytic properties would be of crucial importance. It is not yet apparent how this challenge could be met, but it is clear that it is the key for the implementation of truly HT computational catalyst discovery and design, concomitant with the necessary experimental validation.
Battery technologies. Additional advancements would be needed to expand energy-storage materials research beyond the current scope of Li-ion batteries. For example, a variety of conversion electrodes, where transition metal binary phases react with lithium, potentially possess much higher energy density than intercalation devices. The wealth of such compounds with different degrees of covalence and transition metal oxidation states could enable the tuning of operation voltages, for positive or negative electrodes, and the possibility of selecting low-cost and environmentally friendly materials145. Likewise, batteries employing a higher valence cation such as magnesium or aluminum, which could have considerably increased capacity with reduced weight and volume109,146, have not yet been considered in computational studies.
Algorithms and repositories. To be effective, the wealth of calculations produced by HT needs to be open-domain, shared in online repositories and equipped with effective search capabilities. Examples are the AFLOWLIB.org consortium, the Materials Project25, the Computational Materials Repository, The Electronic Structure Project74, and the Carnegie Mellon's Alloy Database148. For efficiency, the repositories should be integrated so that they can share information through standardized calculation and communication protocols. This will bring the HT field beyond the monolithic, undistributed approach of each single research group, and allow a better use of the results generated by the entire community. Furthermore, another computational frontier will be the implementation of 'materials daemons', ad hoc artificial-intelligence codes implementing the descriptors and autonomously crawling across the various linked databases, scanning and directing calculations until optimum materials are found.

Summary
In this Review, we described the milestones that have been reached with HT computational materials research. We proposed a comprehensive framework, in which they can be conceptually classified, to address the increasing demands of modern technology. For the many other achievements winking at the horizon, crucial components still need to be put in place: efficient HT codes, open and distributed networks of repositories, fast and effective descriptors, and strategies to transfer knowledge to practical implementations. This is an adventurous journey where players will race on a highway without speed limits.

2-Machine learning in materials informatics: recent applications and prospects

ABSTRACT
Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods—due to the cost, time or effort involved—but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints, also 
referred to as “descriptors”, may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative—extending into new materials spaces—provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven “materials informatics” strategies undertaken in the last decade, with particular emphasis on the fingerprint or descriptor choices. The review also identifies some challenges the community is facing and those that should be overcome in the near future.

INTRODUCTION
When a new situation is encountered, cognitive systems (including humans) have a natural tendency to make decisions based on past similar encounters. When the new situation is distinctly different from those encountered in the past, errors in judgment may occur and lessons may be learned. The sum total of such past scenarios, decisions made and the lessons learned may be viewed collectively as “experience”, “intuition” or even as “common sense”. Ideally, depending on the intrinsic capability of the cognitive system, its ability to make decisions should progressively improve as the richness of scenarios encountered increases.
In recent decades, the artificial intelligence (AI) and statistics communities have made these seemingly vague notions quantitative and mathematically precise. These efforts have resulted in practical machines that learn from past experiences (or “examples”). Classic exemplars of such machine learning approaches include facial, fingerprint or object recognition systems, machines that can play sophisticated games such as chess, Go or poker, and automation systems such as in robotics or self-driving cars. In each of these cases, a large data set of past examples is required, e.g., images and their identities, configuration of pieces in a board game and the best moves, and scenarios encountered while driving and the best actions.
On the surface, it may appear as though the “data-driven” approach for determining the best decision or answer when a new situation or problem is encountered is radically different from approaches based on fundamental science in which predictions are made by solving equations that govern the pertinent phenomena. But viewed differently, is not the scientific process itself—which begins with observations, followed by intuition, then construction of a quantitative theory that explains the observations, and subsequently, refinement of the theory based on new observations—the ultimate culmination of such data-driven inquiries?
For instance, consider how the ancient people from India and Sri Lanka figured out, through persistent tinkering, the alloying elements to add to iron to impede its tendency to rust, using only their experience and creativity3,4 (and little “steel science”, which arose from this empiricism much later)—an early example of the reality and power of “chemical intuition.” Or, more recently, over the last century, consider the enormously practical Hume–Rothery rules to determine the solubility tendency of one metal in another,5 the Hall–Petch studies that have led to empirical relationships between grain sizes and mechanical strength (not just for metals but for ceramics as well),6,7 and the group contribution approach to predict complex properties of organic and polymeric materials based just on the identity of the chemical structure,8 all of which arose from data-driven pursuits (although they were not called as such), and later rationalized using physical principles. It would thus be fair to say that data
either directly or indirectly—drives the creation of both complex fundamental and simple empirical scientific theories. Figure 1 charts the timeline for some classic historical and diverse examples of data-driven efforts.
In more modern times, in the last decade or so, thanks to the implicit or explicit acceptance of the above notions, the “data-driven”, “machine learning”, or “materials informatics” paradigms (with these terms used interchangeably by the community) are rapidly becoming an essential part of the materials research portfolio. The availability of robust and trustworthy in silico simulation methods and systematic synthesis and characterization capabilities, although time-consuming and sometimes expensive, provides a pathway to generate at least a subset of the required critical data in a targeted and organized manner (e.g., via “high-throughput” experiments or computations). Indeed, such efforts are already underway, which have lead to the burgeoning of a number of enormously useful repositories such as NOMAD (http://nomad-coe.eu), Materials Project (http://materialsproject.org), Aflowlib (http://www.aflowlib.org), and OQMD (http://oqmd.org). Mining or learning from these resources or other reliable 
extant data can lead to the recognition of previously unknown correlations between properties, and the discovery of qualitative and quantitative rules—also referred to as surrogate models—that can be used to predict material properties orders of magnitude faster and cheaper, and with reduced human effort than required by the benchmark simulation or experimental methods utilized to create the data in the first place.
With excitement and opportunities come challenges. Questions constantly arise as to what sort of materials science problems are most appropriate for, or can benefit most from, a data-driven approach. A satisfactory understanding of this aspect is essential before one makes a decision on using machine learning methods for their problem of interest. Perhaps the most dangerous aspect of data-driven approaches is the unwitting application of machine learning models to cases that fall outside the domain of prior data. A rich and largely uncharted area of inquiry is to recognize when such a scenario ensues, and to be able to quantify the uncertainties of the machine learning predictions especially when models veer out-of-domain. Solutions for handling these perilous situations may open up pathways for adaptive learning models that can progressively improve in quality through systematic infusion of new data—an aspect critical to the further burgeoning of machine learning within the hard sciences.
This article attempts to provide an overview of some of the recent successful data-driven materials research strategies undertaken in the last decade, and identifies challenges that the community is facing and those that should be overcome in the near future.

RESULTS
Elements of machine learning (within materials science)
Regardless of the specific problem under study, a prerequisite for machine learning is the existence of past data. Thus, either clean, curated and reliable data corresponding to the problem under study should already be available, or an effort has to be put in place upfront for the creation of such data. An example data set may be an enumeration of a variety of materials that fall within a well-defined chemical class of interest and a relevant measured or computed property of those materials (see Fig. 2a). Within the machine learning parlance, the former, i.e., the material, is referred to as “input”, and the latter, i.e., the property of interest, is referred to as the “target” or “output.” A learning problem (Fig. 2b) is then defined as follows: Given a {materials → property} data set, what is the best estimate of the property for a new material not in the original data set? Provided that there are sufficient examples, i.e., that the data set is sufficiently large, and provided that the new material falls 
within the same chemo-structural class as the materials in the original data set, we expect that it should be possible to make such an estimate. Ideally, uncertainties in the prediction should also be reported, which can give a sense of whether the new case is within or outside the domain of the original data set.
All data-driven strategies that attempt to address the problem posed above are composed of two distinct steps, both aimed at satisfying the need for quantitative predictions. The first step is to represent numerically the various input cases (or materials) in the data set. At the end of this step, each input case would have been reduced to a string of numbers (or “fingerprints”; see Fig. 2c). This is such an enormously important step, requiring significant expertise and knowledge of the materials class and the application, i.e., “domain expertise”, that we devote a separate Section to its discussion below.
The second step establishes a mapping between the fingerprinted input and the target property, and is entirely numerical in nature, largely devoid of the need for domain knowledge. Both the fingerprinting and mapping/learning steps are schematically illustrated in Fig. 2. Several algorithms, ranging from elementary (e.g., linear regression) to highly sophisticated (kernel ridge regression, decision trees, deep neural networks), are available to establish this mapping and the creation of surrogate prediction models. While some algorithms provide actual functional forms that relate input to output (e.g., regression based schemes), others do not (e.g., decision trees). Moreover, the amount of available data may also dictate the choice of learning algorithms. For instance, tens to thousands of data points may be adequately handled using regression algorithms such as kernel ridge regression or gaussian process regression, but the availability of much larger data sets (e.g., hundreds of thousands or 
millions) may warrant deep neural networks, simply due to considerations of favorable scalability of the prediction models with data set size. In the above discussion, it was implicitly assumed that the target property is a continuous quantity (e.g., bulk modulus, band gap, melting temperature, etc.). Problems can also involve discrete targets (e.g., crystal structure, specific structural motifs, etc.), which are referred to as classification problems. At this point, it is worth mentioning that the learning problem as described above for the most part involving a mapping between the fingerprints and target properties is referred to as “supervised learning”; “unsupervised learning”, on the other hand, involves using just the fingerprints to recognize patterns in the data (e.g., for classification purposes or for reduction of the dimensionality of the fingerprint vector).
Throughout the learning process, it is typical (and essential) to adhere to rigorous statistical practices. Central to this are the notions of cross-validation and testing on unseen data, which attempt to ensure that a learning model developed based on the original data set can truly handle a new case without falling prey to the perils of “overfitting”. Indeed, it should be noted here that some of the original and most successful applications of machine learning, including statistical treatments and practices such as regularization and cross-validation, were first introduced into materials research in the field of alloy theory, cluster expansions and lattice models.16,17,18,19,20,21,22,23,24 These ideas, along with machine learning techniques such as compressive sensing, are further taking shape within the last decade.25,26
Machine learning should be viewed as the sum total of the organized creation of the initial data set, the fingerprinting and learning steps, and a necessary subsequent step (discussed at the end of this article) of progressive and targeted new data infusion, ultimately leading to an expert recommendation system that can continuously and adaptively improve.
Hierarchy of fingerprints or descriptors
We now elaborate on what is perhaps the most important component of the machine learning paradigm, the one that deals with the numerical representation of the input cases or materials. A numerical representation is essential to make the prediction scheme quantitative (i.e., moving it away from the “vague” notions alluded to in the first paragraph of this article). The choice of the numerical representation can be effectively accomplished only with adequate knowledge of the problem and goals (i.e., domain expertise or experience), and typically proceeds in an iterative manner by duly considering aspects of the material that the target property may be correlated with. Given that the numerical representation serves as the proxy for the real material, it is also referred to as the fingerprint of the material or its descriptors (in machine learning parlance, it is also referred to as the feature vector).
Depending on the problem under study and the accuracy requirements of the predictions, the fingerprint can be defined at varying levels of granularity. For instance, if the goal is to obtain a high-level understanding of the factors underlying a complex phenomenon—such as the mechanical or electrical strength of materials, catalytic activity, etc.—and prediction accuracy is less critical, then the fingerprint may be defined at a gross level, e.g., in terms of the general attributes of the atoms the material is made up of, other potentially relevant properties (e.g., the band gap) or higher-level structural features (e.g., typical grain size). On the other hand, if the goal is to predict specific properties at a reasonable level of accuracy across a wide materials chemical space—such as the dielectric constant of an insulator or the glass transition temperature of a polymer—the fingerprint may have to include information pertaining to key atomic-level structural fragments that may control these properties. If extreme (chemical) accuracy in predictions is demanded—such as total energies and atomic forces, precise identification of structural features, space groups or phases—the fingerprint has to be fine enough so that it is able to encode details of atomic-level structural information with sub-Angstrom-scale resolution. Several examples of learning based on this hierarchy of fingerprints or descriptors are provided in subsequent Sections.
The general rule of thumb is that finer the fingerprint, greater is the expected accuracy, and more laborious, more data-intensive and less conceptual is the learning framework. A corollary to the last point is that rapid coarse-level initial screening of materials should generally be targeted using coarser fingerprints.
Regardless of the specific choice of representation, the fingerprints should also be invariant to certain transformations. Consider the facial recognition scenario. The numerical representation of a face should not depend on the actual placement location of the face in an image, nor should it matter whether the face has been rotated or enlarged with respect to the examples the machine has seen before. Likewise, the representation of a material should be invariant to the rigid translation or rotation of the material. If the representation is fine enough that it includes atomic position information, permutation of like atoms should not alter the fingerprint. These invariance properties are easy to incorporate in coarser fingerprint definitions but non-trivial in fine-level descriptors. Furthermore, ensuring that a fingerprint contains all the relevant components (and only the relevant components) for a given problem requires careful analysis, for example, using unsupervised learning algorithms. For these 
reasons, construction of a fingerprint for a problem at hand is not always straightforward or obvious.
Examples of learning based on gross-level property-based descriptors
Two historic efforts in which gross-level descriptors were utilized to create surrogate models (although they were not couched under those terms) have lead to the Hume–Rothery rules5 and Hall–Petch relationships6,7 (Fig. 1). The former effort may be viewed as a classification exercise in which the target is to determine whether a mixture of two metals will form a solid solution; the gross-level descriptors considered were the atomic sizes, crystal structures, electronegativities, and oxidation states of the two metal elements involved. In the latter example, the strength of a polycrystalline material is the target property, which was successfully related to the average grain size; specifically a linear relationship was found between the strength and the reciprocal of the square root of the average grain size. While a careful manual analysis of data gathered from experimentation was key to developing such rules in the past, modern machine learning and data mining approaches provide powerful pathways for such knowledge discovery, especially when the dependencies are multivariate and highly nonlinear.
To identify potential nonlinear multivariate relationships efficiently, one may start from a moderate number of potentially relevant primary descriptors (e.g., electronegativity, E, ionic radius, R, etc.), and create millions or even billions of compound descriptors by forming algebraic combinations of the primary descriptors (e.g., E/R 2, R log(E), etc.); see Fig. 3a, b. This large space of nonlinear mathematical functions needs to be “searched” for a subset that is highly correlated with the target property. Dedicated methodological approaches to accomplish such a task have emerged from recent work in genetic programing,27 compressed sensing,28,29 and information science.30
One such approach—based on the least absolute shrinkage and selection operator (LASSO)—was recently demonstrated to be highly effective for determining key physical factors that control a complex phenomenon through identification of simple empirical relationships.28,29 An example of such complex behavior is the tendency of insulators to fail when subjected to extreme electric fields. The critical field at which this failure occurs in a defect-free material—referred to as the intrinsic electrical breakdown field—is related to the balance between energy gained by charge carriers from the electric field to the energy lost due to collisions with phonons. The intrinsic breakdown field may be computed from first principles by treatment of electron-phonon interactions, but this computation process is enormously laborious. Recently, the breakdown field was computed from first principles using density functional theory (DFT) for a benchmark set of 82 binary octet insulators.31 This data set included alkali metal halides, transition metal halides, alkaline earth metal chalcogenides, transition metal oxides, and group III, II–VI, I–VII semiconductors. After validating the theoretical results by comparing against available experimental data, this data set was used to build simple predictive phenomenological surrogate models of dielectric breakdown using LASSO as well as other advanced machine learning schemes. The general flow of the LASSO-based procedure, starting from the primary descriptors considered (Fig. 3a), is charted in Fig. 3b. The trained and validated surrogate models were able to reveal key correlations and analytical relationships between the breakdown field and other easily accessible material properties such as the band gap and the phonon cutoff frequency. Figure 3c shows the agreement between such a discovered analytical relationship and the DFT results (spanning three orders of magnitude) for the benchmark data set of 82 insulators, as well as for four new ones that were not included in the original 
training data set.
The phenomenological model was later employed to systematically screen and identify perovskite compounds with high breakdown strength. The purely machine learning based screening revealed that boron-containing compounds are of particular interest, some of which were predicted to exhibit remarkable intrinsic breakdown strength of ~1 GV/m (see Fig. 3d). These predictions were subsequently confirmed using first principles computations.32
The LASSO-based and related schemes have also been shown to be enormously effective at predicting the preferred crystal structures of materials. In a pioneering study that utilized the LASSO-based approach, Ghiringelli and co-workers were able to classify binary octet insulators into tendencies for the formation of rock salt versus zinc blende structures. More recently, Bialon and co-workers34 aimed to classify 64 different prototypical crystal structures formed by AxBy type compounds, where A and B are sp-block and transition metal elements, respectively. After searching over a set of 1.7 × 105 non-linear descriptors formed by physically meaningful functions of primary coarse-level descriptors such as band-filling, atomic volume, and different electronegativity scales of the sp and d elements, the authors were able to find a set of three optimal descriptors. A three-dimensional structure-map—built on the identified descriptor set—was used to classify 2105 experimentally known training examples available from the Pearson’s Crystal Database35 with an 86% probability of predicting the correct crystal structure. Likewise, Oliynyk and co-workers recently used a set of elemental descriptors to train a machine-learning model, built on a random forest algorithm,36 with an aim to accelerate the search for Heusler compounds. After training the model on available crystallographic data from Pearson’s Crystal Database35 and the ASM Alloy Phase Diagram Database the model was used to evaluate the probabilities at which compounds with the formula AB2C will adopt Heusler structures. This approach was exceptionally successful in distinguishing between Heusler and non-Heusler compounds (with a true positive rate of 94%), including the prediction of unknown compounds and flagging erroneously assigned entries in the literature and in crystallographic databases. As a proof of concept, 12 novel predicted candidates (Gallides with formulae MRu2Ga and RuM2Ga, where M = Ti, V, Cr, Mn, Fe, and Co) were synthesized and confirmed to be Heusler compounds. One point to be cautious about when creating an enormous number of compound descriptors (starting from a small initial set of primary descriptors) is model interpretability. Efforts must be taken to ensure that the final set of shortlisted descriptors (e.g., the output of the LASSO process) is stable, i.e., the same or similar set of compound descriptors is obtained during internal cross-validation steps, lest the process becomes a victim of the “curse of dimensionality.”
Yet another application of the gross-level descriptors relate to the prediction of the band gap of insulators. Rajan and co-workers have used experimentally available band gaps of ABC2 chalcopyrite compounds to train regression models with electronegativity, atomic number, melting point, pseudopotential radii, and the valence for each of the A, B, and C elements as features. Just using the gross-level elemental features, the developed machine learning models were able to predict the experimental band gaps with moderate accuracy. In a different study, Pilania and co-workers41 used a database consisting of computed band gaps of ~1300 AA′BB′O6 type double perovskites to train a kernel ridge regression (KRR) machine learning model, a scheme that allows for nonlinear relationships based on measures of (dis)similarity between fingerprints, for efficient predictions of the band gaps. A set of descriptors with increasing complexity was identified by searching across a large portion of the feature 
space using LASSO, with ≥ 1.2 million compound descriptors created from primary elemental features such as electronegativities, ionization potentials, electronic energy levels, and valence orbital radii of the constituent atomic species. One of the most important chemical insights that emerged from this effort was that the band gap in the double perovskites is primarily controlled (and therefore effectively learned) by the lowest occupied energy levels of the A-site elements and electronegativities of the B-site elements.
Other successful attempts of using gross-level descriptors include the creation of surrogate models for the estimation of formation enthalpies,43,44,45 free energies,46 defect energetics,47 melting temperatures,48,49 mechanical properties,50,51,52 thermal conductivity,53 catalytic activity,54,55 and radiation damage resistance.56 Efforts are also underway for the identification of novel shape memory alloys, improved piezoelectrics,58 MAX phases,59 novel perovskite60 and double perovskite halides,43,60 CO2 capture materials,61 and potential candidates for water splitting.62
Emerging materials informatics tools also offer tremendous potential and new avenues for mining for structure-property-processing linkages from aggregated and curated materials data sets.63 While a large fraction of such efforts in the current literature has considered relatively simple definitions of the material that included mainly the overall chemical composition of the material, Kalidindi and co-workers have recently proposed a new materials data science framework known as Materials Knowledge Systems68,69 that explicitly accounts for the complex hierarchical material structure in terms of n-point spatial correlations (also frequently referred to as n-point statistics). Further adopting the n-point statistics as measures to quantify materials microstructure, a flexible computational framework has been developed to customize toolsets to understand structure-property-processing linkages in materials science.
Examples of learning based on molecular fragment-level descriptors
The next in the hierarchy of descriptor types are those that encode finer details than those captured by the gross-level properties. Within this class, materials are described in terms of the basic building blocks they are made of. The origins of “block-level” or “molecular fragment” based descriptors can be traced back to cheminformatics, which is a field of theoretical chemistry that deals with correlating properties such as biological activity, physio-chemical properties and reactivity with molecular structure and fragments,71,72,73 leading up to what is today referred to as quantitative structure activity/property relationships (QSAR/QSPR).
Within materials science, specifically, within polymer science, the notions underlying QSAR/QSPR ultimately led to the successful group contribution methods.8 Van Krevelen and co-workers studied the properties of polymers and discovered that they were strongly correlated to the chemical structure (i.e., nature of the polymer repeat unit, end groups, etc.) and the molecular weight distribution. They observed that polymer properties such as glass transition temperature, solubility parameter and bulk modulus (which were, and still are, difficult to compute using traditional computational methods) were correlated with the presence of chemical groups and combinations of different groups in the repeat unit. Based on a purely data-driven approach, they developed an “atomic group contribution method” to express various properties as a linear weighted sum of the contribution (called atomic group parameter) from every atomic group that constituted the repeat unit. These groups could be units like CH2, C6H4, CH2-CO, etc., that make up the polymer. It was also noticed that factors such as the presence of aromatic rings, long side chains and cis/trans conformations influence the properties, prompting their introduction into the group additivity scheme. For instance, a CH2 group attached to an aromatic ring would have a different atomic group parameter than a CH2 group attached to an aliphatic group. In this fashion, nearly all the important contributing factors were taken into account, and linear empirical relationships were devised for thermal, elastic and other polymer properties. However, widespread usage of these surrogate models is still restricted because (1) the definition of atomic groups is somewhat ad hoc, and (2) the target properties are assumed to be linearly related to the group parameters.
Modern data-driven methods have significantly improved on these earlier ideas with regards to both issues mentioned above. Recently, in order to enable the accelerated discovery of polymer dielectrics, hundreds of polymers built from a chemically allowed combination of seven possible basic units, namely, CH2, CO, CS, O, NH, C6H4, and C4H2S, were considered, inclusive of van der Waals interactions,80 and a set of properties relevant for dielectric applications, namely, the dielectric constant and band gap, were computed using DFT.74,81 These polymers were then fingerprinted by keeping track of the occurrence of a fixed set of molecular fragments in the polymers in terms of their number fractions.81,82 A particular molecular fragment could be a triplet of contiguous blocks such as –NH–CO–CH2– (or, at a finer level, a triplet of contiguous atoms, such as C4–O2–C3 or C3–N3–H1, where X n represents an n-fold coordinated X atom).83,84 All possible triplets were considered (some examples are shown 
in Fig. 4a), and the corresponding number fractions in a specific order formed the fingerprint of a particular polymer (see Fig. 4b). This procedure provides a uniform and seamless pathway to represent all polymers within this class, and the procedure can be indefinitely generalized by considering higher order fragments (i.e., quadruples, quintuples, etc., of atom types). Furthermore, relationships between the fingerprint and properties have been established using the KRR learning algorithm; a schematic of how this algorithm works is shown in Fig. 4c. The capability of this scheme for dielectric constant and band gap predictions is portrayed in Fig. 4d. These predictive tools are available online (Fig. 4e) and are constantly being updated.85 The power of such modern data-driven molecular fragment-based learning approaches (like its group contribution predecessor) lies in the realization that any type of property related to the molecular structure—whether computable using DFT (e.g., band gap, dielectric 
constant) or measurable experimentally (e.g., glass transition temperature, dielectric loss)—can be learned and predicted.
The molecular fragment-based representation is not restricted to polymeric materials. Novel compositions of AxByOz ternary oxides and their most probable crystal structures have been predicted using a probabilistic model built on an experimental crystal structure database.86 The descriptors used in this study are a combination of the type of crystal structure (spinel, olivine, etc.) and the composition information, i.e., the elements that constitute the compound. Likewise, surrogate machine learning models have been developed for predicting the formation energies of AxByOz ternary compounds using only compositional information as descriptors, trained on a data set of 15,000 compounds from the Inorganic Crystal Structure Database.44 Using this approach, 4500 new stable materials have been discovered. Finally, surrogate models have been developed for predicting the formation energies of elpasolite crystals with the general formula A2BCD6, based mainly on compositional information. The descriptors used take into account the periodic table row and column of elements A, B, C, and D that constitute the compound (although this fingerprint could have been classified as a gross-level one, we choose to place this example in the present Section as the prototypical structure of the elpasolite was implicitly assumed in this work and fingerprint). Important correlations and trends were revealed between atom types and the energies; for example, it was found that the preferred element for the D site is F, and that for the A and B sites are late group II elements.43
Examples of learning based on sub-Angstrom-level descriptors
We now turn to representing materials at the finest possible scale, such that the fingerprint captures precise details of atomic configurations with high fidelity. Such a representation is useful in many scenarios. For instance, one may attempt to connect this fine-scale fingerprint directly with the corresponding total potential energy with chemical accuracy, or with structural phases/motifs (e.g., crystal structure or the presence/absence of a stacking fault). The former capability can lead to purely data-driven accelerated atomistic computational methods, and the latter to refined and efficient on-the-fly characterization schemes.
“Chemical accuracy” specifically refers to potential energy and reaction enthalpy predictions with errors of < 1 kcal/mol, and atomic force predictions (the input quantity for molecular dynamics, or MD, simulations) with errors of < 0.05 eV/Å. Chemical accuracy is key to enable reliable MD simulations (or for precise identification of the appropriate structural phases or motifs), and is only possible with fine-level fingerprints that offer sufficiently high configurational resolution, more than those in the examples encountered thus far.
The last decade has seen spectacular activity and successes in the general area of data-driven atomistic computations. All modern atomistic computations use either some form of quantum mechanical scheme (e.g., DFT) or a suitably parameterized semi-empirical method to predict the properties of materials, given just the atomic configuration. Quantum mechanical methods are versatile, i.e., they can be used to study any material, in principle. However, they are computationally demanding, as complex differential equations governing the behavior of electrons are solved for every given atomic configuration. Systems involving at most about 1000 atoms can be simulated routinely in a practical setting today. In contrast, semi-empirical methods use prior knowledge about interatomic interactions under known conditions and utilize parameterized analytical equations to determine properties such as the total potential energies, atomic forces, etc. These semi-empirical force fields are several orders of magnitude faster than quantum mechanical methods, and are the choice today for routinely simulating systems containing millions to billions of atoms, as well as the dynamical evolution of systems at nonzero temperatures (using the MD method) at timescales of nanoseconds to milliseconds. However, a major drawback of traditional semi-empirical force fields is that they lack versatility, i.e., they are not transferable to situations or materials for which the original functional forms and parameterizations do not apply.
Machine learning is rapidly bridging the chasm between the two extremes of quantum mechanical and semi-empirical methods, and has offered surrogate models that combine the best of both worlds. Rather than resort to specific functional forms and parameterizations adopted in semi-empirical methods (the aspects that restrict their versatility), machine learning methods use an {atomic configuration → property} data set, carefully prepared, e.g., using DFT, to make interpolative predictions of the property of a new configuration at speeds several orders of magnitude faster than DFT. Any material for which adequate reference DFT computations may be performed ahead of time can be handled using such a machine learning scheme. Thus, the lack of versatility issue of traditional semi-empirical approach and the time-intensive nature of quantum mechanical calculations are simultaneously addressed, while also preserving quantum mechanical and chemical accuracy.
The primary challenge though has been the creation of suitable fine-level fingerprinting schemes for materials, as these fingerprints are required to be strictly invariant with respect to arbitrary translations, rotations, and exchange of like atoms, in addition to being continuous and differentiable (i.e., “smooth”) with respect to small variations in atomic positions. Several candidates, including those based on symmetry functions, bispectra of neighborhood atomic densities, Coulomb matrices (and its variants), smooth overlap of atomic positions (SOAP),93,94,95,96 and others,97,98 have been proposed. Most fingerprinting approaches use sophisticated versions of distribution functions (the simplest one being the radial distribution function) to represent the distribution of atoms around a reference atom, as qualitatively captured in Fig. 5a. The Coulomb matrix is an exception, which elegantly represents a molecule, with the dimensionality of the matrix being equal to the total number of atoms 
in the molecule. Although questions have arisen with respect to smoothness considerations and whether the representation is under/over-determined (depending on whether the eigenspectrum or the entire matrix is used as the fingerprint),93 this approach has been shown to be able to predict various molecular properties accurately.
Figure 5b also shows a general schema typically used in the construction of machine learning force fields, to be used in MD simulations. Numerous learning algorithms—ranging from neural networks, KRR, Gaussian process regression (GPR), etc.—have been utilized to accurately map the fingerprints to various materials properties of interest. A variety of fingerprinting schemes, as well as learning schemes that lead up to force fields have been recently reviewed. One of the most successful and widespread machine learning force field schemes to date is the one by Behler and co-workers,87 which uses symmetry function fingerprints mapped to the total potential energy using a neural network. Several applications have been studied, including surface diffusion, liquids, phase equilibria in bulk materials, etc. This approach is also quite versatile in that multiple elements can be considered. Bispectra based fingerprints combined with GPR learning schemes have lead to Gaussian approximation potentials,87,90 which have also been demonstrated to provide chemical accuracy, versatility and efficiency.
A new development within the area of machine learning force fields is to learn and predict the atomic forces directly; the total potential energy is determined through appropriate integration of the forces along a reaction coordinate or MD trajectory.105 These approaches are inspired by Feynman’s original idea that it should be possible to predict atomic forces given just the atomic configuration, without going through the agency of the total potential energy.106 An added attraction of this perspective is that the atomic force can be uniquely assigned to an individual atom, while the potential energy is a global property of the entire system (partitioning the potential energy to atomic contributions does not have a formal basis). Mapping atomic fingerprints to purely atomic properties can thus lead to powerful and accurate prescriptions. Figure 5c, for instance, compares the atomic forces at the core of an edge dislocation in Al, predicted using a machine learning force prediction 
recipe called AGNI, with the DFT forces for the same atomic configuration. Also shown are forces predicted using the embedded atom method (EAM), a popular classical force field, for the same configuration. EAM tends to severely under-predict large forces while the machine learning scheme predicts forces with high fidelity (neither EAM nor the machine learning force field were explicitly trained on dislocation data). This general behavior is consistent with recent detailed comparisons of EAM with machine learning force fields.107 It is worth noting that although this outlook of using atomic forces data during force field development is reminiscent of the “force-matching” approach of Ercolessi and Adams,108 this new development is distinct from that approach in that it attempts to predict the atomic force given just the atomic configuration.
Another notable application of fine-level fingerprints has been in the use of the electronic charge density itself as the representation to learn various properties82 or density functionals, thus going to the very heart of DFT. While these efforts are in a state of infancy—as they have dealt with mainly toy problems and learning the kinetic energy functional—such efforts have great promise as they attempt to integrate machine learning methods within DFT (all other DFT-related informatics efforts so far have utilized machine learning external to DFT).
Fine-level fingerprints have also been used to characterize structure in various settings. Within a general crystallographic structure refinement problem, one has to estimate the structural parameters of a system, i.e., the unit cell parameters (a, b, c, α, β, and γ) that best fit measured X-ray diffraction (XRD) data. Using a Bayesian learning approach and a Markov chain Monte Carlo algorithm to sample multiple combinations of possible structural parameters for the case of Si, Fancher and co-workers not only accurately determined the estimates of the structural parameters, but also quantified the associated uncertainty (thus going beyond the conventional Rietveld refinement method).
Unsupervised learning using fine-level fingerprints (and clustering based on these fingerprints) has led to the classification of materials based on their phases or structural characteristics.11,12 Using the XRD spectrum itself as the fingerprint, high-throughput XRD measurements for various compositional spreads have been used to automate the creation of phase diagrams. Essentially, features of the XRD spectra are used to distinguish between phases of a material as a function of composition. Likewise, on the computational side, the SOAP fingerprints have been effectively used to distinguish between different allotropes of materials, as well as different motifs that emerge during the course of a MD simulation (see Fig. 5d for an example).
Critical steps going forward
Quantifying the uncertainties of predictions
Given that machine learning predictions are inherently statistical in nature, uncertainties must be expected in the predictions. Moreover, predictions are typically and ideally interpolative between data points corresponding to previously seen data. To what extent a new case for which a prediction needs to be made falls in or out of the domain of the original data set (i.e., to what extent the predictions are interpolative or extrapolative) may be quantified using the predicted uncertainty. While strategies are available to prescribe prediction uncertainties, these ideas have been explored only to a limited extent within materials science.57,118 Bayesian methods (e.g., Gaussian process regression)15 provide a natural pathway for estimating the uncertainty of the prediction in addition to the prediction itself. This approach assumes that a Gaussian distribution of models fit the available data, and thus a distribution of predictions may be made. The mean and variance of these predictions—the natural outcomes 
of Bayesian approaches—are the most likely predicted value and the uncertainty of the prediction, respectively, within the spectrum of models and the fingerprint considered. Other methods may also be utilized to estimate uncertainties, but at significant added cost. A straightforward and versatile scheme is bootstrapping, in which different (but small) subsets of the data are randomly excluded, and several prediction models are developed based on these closely related but modified data sets. The mean and variance of the predictions from these bootstrapped models provide the property value and expected uncertainty. Essentially, this approach attempts to probe how sensitive the model is with respect to slight “perturbations” to the data set. Another related methodology is to explicitly consider a variety of closely related models, e.g., neural networks or decision trees with slightly different architectures, and to use the distribution of predictions to estimate uncertainty.89
Adaptive learning and design
Uncertainty quantification has a second important benefit. It can be used to continuously and progressively improve a prediction model, i.e., render it a truly learning model. Ideally, the learning model should adaptively and iteratively improve by asking questions such as “what should be the next new material system to consider or include in the training set that would lead to an improvement of the model or the material?” This may be accomplished by balancing the tradeoffs between exploration and exploitation.118,120 That is, at any given stage of an iterative learning process, a number of new candidates may be predicted to have certain properties with uncertainties. The tradeoff is between exploiting the results by choosing to perform the next computation (or experiment) on the material predicted to have the optimal target property or further improving the model through exploration by performing the calculation (or experiment) on a material where the predictions have the largest uncertainties. This can be 
done rigorously by adopting well-established information theoretic selector frameworks such as the knowledge gradient. In the initial stages of the iterative process, it is desired to “explore and learn” the property landscape. As the machine learning predictions improve and the associated uncertainties shrink, the adaptive design scheme allows one to gradually move away from exploration towards exploitation. Such an approach, schematically portrayed in Fig. 6a, enables one to systematically expand the training data towards a target chemical space, where materials with desired functionality are expected to reside.
Some of the first examples of using adaptive design for targeted materials discovery include identification of shape memory alloys with low thermal hysteresis57 and accelerated search for BaTiO3-based piezoelectrics with optimized morphotropic phase boundary.58 In the first example, Xue and co-workers57 employed the aforementioned adaptive design framework to find NiTi-based shape memory alloys that may display low thermal hysteresis. Starting from a limited number of 22 training examples and going through the iterative process 9 times, 36 predicted compositions were synthesized and tested from a potential space of ~800,000 compound possibilities. It was shown that 14 out of these 36 new compounds were better (i.e., had a smaller thermal hysteresis) than any of the 22 compounds in the original data set. The second successful demonstration of the adaptive design approach combined informatics and Landau–Devonshire theory to guide experiments in the design of lead-free piezoelectrics.58 Guided by predictions 
from the machine learning model, an optimized solid solution, (Ba0.5Ca0.5)TiO3–Ba(Ti0.7Zr0.3)O3, with piezoelectric properties was synthesized and characterized to show better temperature reliability than other BaTiO3-based piezoelectrics in the initial training data.
The materials science community is just beginning to explore and utilize the plethora of available information theoretic algorithms to mine and learn from data. The usage of an algorithm is driven largely by need, as it should. One such need is to be able to learn and predict vectorial quantities. Examples include functions, such as the electronic or vibrational density of states (which are functions of energy or frequency). Although, the target property in these cases may be viewed as a set of scalar quantities at each energy or frequency (for a given structure) to be learned and predicted independently, it is desirable to learn and predict the entire function simultaneously. This is because the value of the function at a particular energy or frequency is correlated to the function values at other energy or frequency values. Properly learning the function of interest requires machine learning algorithms that can handle vectorial outputs. Such algorithms are indeed available, and if exploited can lead to prediction schemes of the electronic structure for new configurations of atoms. Another class of examples where vector learning is appropriate includes cases where the target property is truly a vector (e.g., atomic force) or a tensor (e.g., stress). In these cases, the vector or tensor transforms in a particular way as the material itself is transformed, e.g., if it is rotated (in the examples of functions discussed above, the vectors, i.e., the functions, are invariant to any unitary transformation of the material). These truly vectorial or tensorial target property cases will thus have to be handled with care, as has been done recently using vector learning and covariant kernels.102
Another algorithm that is beginning to show value within material science falls under multi-fidelity learning. This learning method can be used when a property of interest can be computed at several levels of fidelities, exhibiting a natural hierarchy in both computational cost and accuracy. A good materials science example is the band gap of insulators computed at an inexpensive lower level of theory, e.g., using a semilocal electronic exchange-correlation functional (the low-fidelity value), and the band gap computed using an more accurate, but expensive, approach, e.g., using a hybrid exchange-correlation functional (the high-fidelity value). A naive approach in such a scenario can be to use a low-fidelity property value as a feature in a machine learning model to predict the corresponding higher fidelity value. However, using low-fidelity estimates as features strictly requires the low-fidelity data for all materials for which predictions are to be made using the trained model. This can be particularly challenging and extremely computationally demanding when faced with a combinatorial problem that targets exploring vast chemical and configurational spaces. A multi-fidelity co-kriging framework, on the other hand, can seamlessly combine inputs from two or more levels of fidelities to make accurate predictions of the target property for the highest fidelity. Such an approach, schematically represented in Fig. 6b, requires high-fidelity training data only on a subset of compounds for which low-fidelity training data is available. More importantly, the trained model can make efficient highest-fidelity predictions even in the absence of the low-fidelity data for the prediction set compounds. While multi-fidelity learning is routinely used in several fields to address computationally challenging engineering design problems,125,126 it is only beginning to find applications in materials informatics.
Finally, machine learning algorithms may also lead to strategies for making the so-called “inverse design” of materials possible. Inverse design refers to the paradigm whereby one seeks to identify materials that satisfy a target set of desired properties (in this parlance, the “forward” process refers to predicting the properties of a given material). Within the machine learning context, although the backward process of going from a desired set of properties to the appropriate fingerprints is straightforward, the process of inverting the fingerprint to actual physically and chemically meaningful materials continues to be a major hurdle. Two strategies that are adopted to achieve inverse design within the context of machine learning involves either inverting the desired properties to only fingerprints that correspond to physically realizable materials (through imposition of constraints that fingerprint components are required to satisfy),83,127 or adopting schemes such as the genetic algorithm or simulated annealing to determine iteratively a population of materials that meet the given target property requirements. Despite these developments, true inverse design continues to remain a challenge (although materials design through adaptive learning discussed above appears to have somewhat mitigated this challenge).

CONCLUSION
Decisions on when to use machine learning
Perhaps the most important question that plagues new researchers eager to use data-driven methods is whether their problem lends itself to such methods. Needless to say, the existence of past reliable data, or efforts devoted to its generation for at least a subset of the critical cases in a uniform and controlled manner, is a prerequisite for the adoption of machine learning. Even so, the question is the appropriateness of machine learning for the problem at hand. Ideally, data-driven methods should be aimed at (1) properties very difficult or expensive to compute or measure using traditional methods, (2) phenomena that are complex enough (or nondeterministic) that there is no hope for a direct solution based on solving fundamental equations, or (3) phenomena whose governing equations are not (yet) known, providing a rationale for the creation of surrogate models. Such scenarios are replete in the social, cognitive and biological sciences, explaining the pervasive applications of data-driven methods in such domains. Materials science examples ideal for studies using machine learning methods include properties such as the glass transition temperature of polymers, dielectric loss of polycrystalline materials over a wide frequency and temperature range, mechanical strength of composites, failure time of engineering materials (e.g., due to electrical, mechanical or thermal stresses), friction coefficient of materials, etc., all of which involve the inherent complexity of materials, i.e., their polycrystalline or amorphous nature, multi-scale geometric architectures, the presence of defects of various scales and types, and so on.
Machine learning may also be used to eliminate redundancies underlying repetitive but expensive operations, especially when interpolations in high-dimensional spaces are required, such as when properties across enormous chemical and/or configurational spaces are desired. An example of the latter scenario, i.e., an immense configurational space, is encountered in first principles molecular dynamics simulations, when atomic forces are evaluated repetitively (using expensive quantum mechanical schemes) for myriads of very similar atomic configurations. The area of machine learning force fields has burgeoned to meet this need. Yet another setting where large chemical and configurational spaces are encountered is the emerging domain of high-throughput materials characterization, where on-the-fly predictions are required to avoid data accumulation bottlenecks. Although materials informatics efforts so far have largely focused on model problems and the validation of the general notion of data-driven discovery, 
active efforts are beginning to emerge that focus on complex real-world materials applications, strategies to handle situations inaccessible to traditional materials computations, and the creation of adaptive prediction frameworks (through adequate uncertainty quantification) that build efficiencies within rational materials design efforts.

3-Machine Learning-Assisted Discovery of Solid Li-Ion Conducting Materials

ABSTRACT
We discover many new crystalline solid materials with fast single crystal Li ion conductivity at room temperature, discovered through density functional theory simulations guided by machine learning-based methods. The discovery of new solid Li superionic conductors is of critical importance to the development of safe all-solid-state Li-ion batteries. With a predictive universal structure–property relationship for fast ion conduction not well understood, the search for new solid Li ion conductors has relied largely on trial-and-error computational and experimental searches over the last several decades. In this work, we perform a guided search of materials space with a machine learning (ML)-based prediction model for material selection and density functional theory molecular dynamics (DFT-MD) simulations for calculating ionic conductivity. These materials are screened from over 12 000 experimentally synthesized and characterized candidates with very diverse structures and compositions. When compared to a 
random search of materials space, we find that the ML-guided search is 2.7 times more likely to identify fast Li ion conductors, with at least a 44 times improvement in the log-average of room temperature Li ion conductivity. The F1 score of the ML-based model is 0.50, 3.5 times better than the F1 score expected from completely random guesswork. In a head-to-head competition against six Ph.D. students working in the field, we find that the ML-based model doubles the F1 score of human experts in its ability to identify fast Li-ion conductors from atomistic structure with a 1000-fold increase in speed, clearly demonstrating the utility of this model for the research community. In addition to having high predicted Li-ion conductivity, all materials reported here lack transition metals to enhance stability against reduction by the Li metal anode and are predicted to exhibit low electronic conduction, high stability against oxidation, and high thermodynamic stability, making them promising candidates for 
solid-state electrolyte applications on these several essential metrics.

INTRODUCTION
All-solid-state Li-ion batteries (SSLIBs) hold promise as safer, longer lasting, and more energy dense alternatives to today’s commercialized LIBs with liquid electrolytes. However, the design of SSLIBs remains a challenge, with the principal technological bottleneck in realizing these devices being the solid electrolyte. A high performance solid electrolyte material must satisfy several criteria simultaneously: it must possess fast Li-ion conduction, negligible electronic conduction, a wide electrochemical window, robust chemical stability against side reactions with the electrodes, and high mechanical rigidity to suppress dendritic growth on the anode. The material should also be cheap and easy to manufacture. Given these many constraints, searching for promising new materials that satisfy all requirements through the trial-and-error searches has yielded slow progress.
The earliest efforts to discover fast Li-ion conducting solids began in the 1970s(1) and have continued to present. More recently, density functional theory (DFT) simulation has enabled high-throughput computational searches, essentially automating the process of guess-and-check.(2,3) Across these four decades, only several solids with liquid-level Li conduction (>10–2 S/cm) at room temperature (RT) have been identified, including notably Li10GeP2S12 (17 mS/cm)(4) and Li7P3S11 (25 mS/cm).(5) This slow progress suggests that continuing in the guess-and-check paradigm of decades past is unlikely to quickly yield the material innovations we need to unlock the high energy density, high cycle life, and unquestionably safe energy storage devices of the future.
Leveraging atomic and electronic structure data from the Materials Project database,(6) we have screened all 12 000+ Li-containing materials for thermodynamic phase stability, low electronic conduction, high electrochemical stability, and no transition metals (to enhance stability against reduction). We also compiled information on the estimated raw materials cost and the earth abundance of the elemental constituents of each material. This identifies 317 materials that may be strong candidate electrolyte materials if they are also fast ion conductors.
Following the guess-and-check paradigm, one would begin to test these materials for fast ion conduction at random, or according to his/her best scientific intuition. To identify the subset of these materials most likely to exhibit fast ionic conductivity, we have instead developed(7) a machine learning (ML)-based model for predicting the likelihood Psuperionic that an arbitrary material exhibits fast Li-ion conduction at RT, based only on features xi derived from the atomistic structure of the unit cell. Throughout this work, we define superionic conductivity to be greater than 0.1 mS/cm, based on the approximate minimum electrolyte ionic conductivity required for battery applications. Experimental reports of ionic conductivity for several dozen materials ranging over 10 orders of magnitude were used to train the model. This data-driven predictor takes the form of a logistic function, , where
Here, LLB is the average Li–Li bond number (number of Li neighbors) per Li; SBI is the average sublattice bond ionicity; AFC is the average coordination of the anions in the anion framework; LASD is the average equilibrium Li–anion separation distance in angstroms; and LLSD is the average equilibrium Li–Li separation distance in angstroms. Since this model does not require any electronic structure information, it is >5 orders of magnitude faster to evaluate than a DFT simulation of conductivity.
Screening this list of 317 candidate materials identifies 21 crystalline compounds that are predicted to be fast ion conductors with robust structural and electrochemical stability, representing a 99.8% reduction in the entire space of known Li-containing materials. One of these 21 materials, LiCl, has been reported to exhibit poor RT Li conduction (∼10–9 S/cm),(8) making it a known false positive prediction. Another material, Li3InCl6, has been reported to have a RT conductivity of approximately 1 mS/cm, making it a correct model prediction.(9) Very little is reported in the literature regarding the remaining 19 materials to our knowledge.
In this work, we perform DFT molecular dynamics (DFT-MD) calculations(10,11) on the promising candidate materials identified by our screening procedure, finding evidence of superionic RT Li conduction in eight and marginal RT Li conduction in two. As a control, we then perform DFT-MD on a similar number of materials drawn at random from the same population of 317. We quantify the increase in research efficiency offered by our ML-based model by comparing the improvements in experimental outcomes against the random case. We consider 41 unique materials in total. We find our ML-guided search offers approximately 3–4× improvement in predictive power for fast Li ion conductors over random guesswork depending on the metric, while on average the predicted RT Li ionic conductivity of any simulated material is over 44× higher.
As a further test of the model’s efficacy, we provided the same list of materials to a group of six graduate students working in the field and asked them to identify the best ion conductors. We found the F1 score of the model outperformed the F1 score of the intuition of the students by approximately two times, while each prediction was made approximately 1000 times faster. This result suggests ML-based approaches to materials selection may provide significant acceleration over the guess-and-check research paradigm of the past. Furthermore, these results provide confidence in our data-driven superionic prediction model, as well as compelling evidence in the promise of machine learning-based approaches to materials discovery.

METHODS
We first perform DFT-MD on the 19 most promising new candidate materials for solid electrolyte applications that are all predicted to be fast ion conductors by our ML-based model eq 1. LiCl and Li3InCl6 were not simulated due to the existence of conductivity data in the literature.(8,9) In order to accelerate Li diffusion to a computationally tractable time scale, we initially seek an upper bound by performing MD at elevated temperature and removing one Li atom per computational cell to introduce a small concentration of Li vacancies to enhance conduction and minimize the number of false negatives identified. We simulate large supercells in order to minimize the effect of the periodic boundary conditions. All materials were initially simulated at T = 900 K; if melting is observed, the simulation is restarted at increasingly lower temperatures until no melting is observed. The vacancy concentration ranges from 3 to 17% depending on the unit cell. The simulation temperatures, computational cell size, and Li 
vacancy concentrations are provided in the Supporting Information, Table S1.
We simulate the candidate materials for a range of times on the tens to hundreds of picoseconds time scale; see Table S1. To calculate ionic diffusivity, which may be isotropic, we compute the average of the diagonal elements of the second rank Li diffusivity tensor (or equivalently, one-third of the trace). We denote this as ⟨Dii⟩, where the average is taken over the three elements ii = {xx, yy, zz}. We first evaluate the mean squared displacement (MSD) of the Li atoms ⟨Δr2⟩ over time (starting at t = 0) and apply the following formula:
To probe for melting, we also calculate the MSD of the sublattice atoms and assume melting if sustained, nonzero diffusivity is observed in both Li and the sublattice (the smallest diffusivity that can be resolved through DFT-MD at 900 K is approximately 0.01 Å2/ps). To assess the degree of convergence in ⟨Dii⟩, we compute the standard deviation in diffusivities when measured from different starting times in the MSD data. We compute the slope of the MSD data for every starting time from t = 0 to up to 75% of the total run time, in 100 fs increments. We compute one standard deviation above and below the mean diffusivity across all starting times to represent the approximate upper and lower limits of the distribution of diffusivities one may observe under these simulation conditions. The diffusivity as measured from t = 0 and the mean diffusivity across all starting times are not necessarily equivalent, and thus the upper and lower uncertainties are not necessarily symmetric around the value predicted from  
Ionic transport in crystalline solids is modeled here as a stochastic phenomenon governed by a Boltzmann (Arrhenius) factor that is exponential in a single energy barrier between equilibrium sites, Ea:
All-solid-state Li-ion batteries (SSLIBs) hold promise as safer, longer lasting, and more energy dense alternatives to today’s commercialized LIBs with liquid electrolytes. However, the design of SSLIBs remains a challenge, with the principal technological bottleneck in realizing these devices being the solid electrolyte. A high performance solid electrolyte material must satisfy several criteria simultaneously: it must possess fast Li-ion conduction, negligible electronic conduction, a wide electrochemical window, robust chemical stability against side reactions with the electrodes, and high mechanical rigidity to suppress dendritic growth on the anode. The material should also be cheap and easy to manufacture. Given these many constraints, searching for promising new materials that satisfy all requirements through the trial-and-error searches has yielded slow progress.
The earliest efforts to discover fast Li-ion conducting solids began in the 1970s(1) and have continued to present. More recently, density functional theory (DFT) simulation has enabled high-throughput computational searches, essentially automating the process of guess-and-check.(2,3) Across these four decades, only several solids with liquid-level Li conduction (>10–2 S/cm) at room temperature (RT) have been identified, including notably Li10GeP2S12 (17 mS/cm)(4) and Li7P3S11 (25 mS/cm).(5) This slow progress suggests that continuing in the guess-and-check paradigm of decades past is unlikely to quickly yield the material innovations we need to unlock the high energy density, high cycle life, and unquestionably safe energy storage devices of the future.
Leveraging atomic and electronic structure data from the Materials Project database,(6) we have screened all 12 000+ Li-containing materials for thermodynamic phase stability, low electronic conduction, high electrochemical stability, and no transition metals (to enhance stability against reduction). We also compiled information on the estimated raw materials cost and the earth abundance of the elemental constituents of each material. This identifies 317 materials that may be strong candidate electrolyte materials if they are also fast ion conductors.
Following the guess-and-check paradigm, one would begin to test these materials for fast ion conduction at random, or according to his/her best scientific intuition. To identify the subset of these materials most likely to exhibit fast ionic conductivity, we have instead developed(7) a machine learning (ML)-based model for predicting the likelihood Psuperionic that an arbitrary material exhibits fast Li-ion conduction at RT, based only on features xi derived from the atomistic structure of the unit cell. Throughout this work, we define superionic conductivity to be greater than 0.1 mS/cm, based on the approximate minimum electrolyte ionic conductivity required for battery applications. Experimental reports of ionic conductivity for several dozen materials ranging over 10 orders of magnitude were used to train the model. This data-driven predictor takes the form of a logistic function, , where
Here, LLB is the average Li–Li bond number (number of Li neighbors) per Li; SBI is the average sublattice bond ionicity; AFC is the average coordination of the anions in the anion framework; LASD is the average equilibrium Li–anion separation distance in angstroms; and LLSD is the average equilibrium Li–Li separation distance in angstroms. Since this model does not require any electronic structure information, it is >5 orders of magnitude faster to evaluate than a DFT simulation of conductivity.
Screening this list of 317 candidate materials identifies 21 crystalline compounds that are predicted to be fast ion conductors with robust structural and electrochemical stability, representing a 99.8% reduction in the entire space of known Li-containing materials. One of these 21 materials, LiCl, has been reported to exhibit poor RT Li conduction (∼10–9 S/cm),(8) making it a known false positive prediction. Another material, Li3InCl6, has been reported to have a RT conductivity of approximately 1 mS/cm, making it a correct model prediction.(9) Very little is reported in the literature regarding the remaining 19 materials to our knowledge.
In this work, we perform DFT molecular dynamics (DFT-MD) calculations(10,11) on the promising candidate materials identified by our screening procedure, finding evidence of superionic RT Li conduction in eight and marginal RT Li conduction in two. As a control, we then perform DFT-MD on a similar number of materials drawn at random from the same population of 317. We quantify the increase in research efficiency offered by our ML-based model by comparing the improvements in experimental outcomes against the random case. We consider 41 unique materials in total. We find our ML-guided search offers approximately 3–4× improvement in predictive power for fast Li ion conductors over random guesswork depending on the metric, while on average the predicted RT Li ionic conductivity of any simulated material is over 44× higher.
As a further test of the model’s efficacy, we provided the same list of materials to a group of six graduate students working in the field and asked them to identify the best ion conductors. We found the F1 score of the model outperformed the F1 score of the intuition of the students by approximately two times, while each prediction was made approximately 1000 times faster. This result suggests ML-based approaches to materials selection may provide significant acceleration over the guess-and-check research paradigm of the past. Furthermore, these results provide confidence in our data-driven superionic prediction model, as well as compelling evidence in the promise of machine learning-based approaches to materials discovery.
We first perform DFT-MD on the 19 most promising new candidate materials for solid electrolyte applications that are all predicted to be fast ion conductors by our ML-based model eq 1. LiCl and Li3InCl6 were not simulated due to the existence of conductivity data in the literature.(8,9) In order to accelerate Li diffusion to a computationally tractable time scale, we initially seek an upper bound by performing MD at elevated temperature and removing one Li atom per computational cell to introduce a small concentration of Li vacancies to enhance conduction and minimize the number of false negatives identified. We simulate large supercells in order to minimize the effect of the periodic boundary conditions. All materials were initially simulated at T = 900 K; if melting is observed, the simulation is restarted at increasingly lower temperatures until no melting is observed. The vacancy concentration ranges from 3 to 17% depending on the unit cell. The simulation temperatures, computational cell size, and Li 
vacancy concentrations are provided in the Supporting Information, Table S1.
We simulate the candidate materials for a range of times on the tens to hundreds of picoseconds time scale; see Table S1. To calculate ionic diffusivity, which may be isotropic, we compute the average of the diagonal elements of the second rank Li diffusivity tensor (or equivalently, one-third of the trace). We denote this as ⟨Dii⟩, where the average is taken over the three elements ii = {xx, yy, zz}. We first evaluate the mean squared displacement (MSD) of the Li atoms ⟨Δr2⟩ over time (starting at t = 0) and apply the following formula:
To probe for melting, we also calculate the MSD of the sublattice atoms and assume melting if sustained, nonzero diffusivity is observed in both Li and the sublattice (the smallest diffusivity that can be resolved through DFT-MD at 900 K is approximately 0.01 Å2/ps). To assess the degree of convergence in ⟨Dii⟩, we compute the standard deviation in diffusivities when measured from different starting times in the MSD data. We compute the slope of the MSD data for every starting time from t = 0 to up to 75% of the total run time, in 100 fs increments. We compute one standard deviation above and below the mean diffusivity across all starting times to represent the approximate upper and lower limits of the distribution of diffusivities one may observe under these simulation conditions. The diffusivity as measured from t = 0 and the mean diffusivity across all starting times are not necessarily equivalent, and thus the upper and lower uncertainties are not necessarily symmetric around the value predicted from 
Ionic transport in crystalline solids is modeled here as a stochastic phenomenon governed by a Boltzmann (Arrhenius) factor that is exponential in a single energy barrier between equilibrium sites, Ea:
Ionic hopping becomes exponentially less likely as the energy barrier increases, and at 900 K (kBT = 78 meV) hopping may not happen on the tens to hundreds of picoseconds time scale if the barrier is much above 0.4 eV (e–0.4/0.078 = 0.006). Thus, high temperature MD simulations are not well-suited to predict a numerical value for ionic conductivity in medium- to high barrier systems because the statistical behavior is not captured on typical DFT simulation time scales. Numerical values can only be predicted through MD in low barrier systems, and even then are still approximate due to the stochastic nature of hopping. Our approach is most suited for high throughput calculations in which the main goal is identifying whether or not materials are superionic conductors, i.e., for making binary predictions of superionic vs nonsuperionic conduction.
From the diffusivity ⟨Dii⟩, we convert to the average of the diagonal elements of the ionic conductivity tensor ⟨σii⟩ via the Einstein relation:(12)
where n is the concentration of Li atoms and q is the charge on the Li atoms. We measure the effective Li charge q using the Bader charge analysis methods of Henkelman et al. Given the linear relationship between conductivity and diffusivity, we compute the spread of possible conductivities by applying eq 4 to the computed bounds in diffusivity as well.
It stands to reason that fast Li conduction will not be observed at RT if it is not observed in these favorable conditions. Our approach therefore should not identify false negatives (materials with poor conduction at high temperature but fast conduction at RT), although there is a risk of identifying false positives (materials with fast conduction at high temperature but poor conduction at RT). To ensure favorable scaling to room temperature and guard against the identification of false positives, we simulate DFT-MD again at alternate temperatures if significant Li diffusion has been observed at the initial simulation temperature. The line connecting the two or three diffusivities on an Arrhenius plot of log10(σ) versus inverse temperature is employed here to extrapolate down to RT without direct calculation of Ea. This assumes Arrhenius scaling applies and there are no structural phase changes or nonstructural superionic transitions (where conductivity changes abruptly as new ionic pathways become 
energetically accessible or inaccessible without any significant sublattice rearrangement) between the simulation temperatures and RT. The spread of possible values in the RT conductivity extrapolations is computed by extrapolating along the upper and lower limits of the deviations in the high temperature conductivity calculations. Using typical values for Li concentration, Bader charge, and site hopping distance, we estimate that the RT conductivity is likely to be 10–9 S/cm or lower if no Li diffusion is observed on the simulation time scale at 900 K (see Supporting Information, Section S1 for calculation).
A flowchart describing this process is shown in Figure 1. This extrapolation scheme requires less computational expense than simulating each material at several different temperatures right away and is the most tractable way for us to quantify the ionic conductivity of the materials studied here on a reasonable time scale. Screening for Li superionic RT diffusion by two-temperature extrapolation has been leveraged in recent work to search sulfide-based compositional spaces, for example.
We perform DFT calculations with the Vienna Ab Initio Simulation Package (VASP)(18) with the generalized gradient approximation (GGA) of Perdew–Burke–Ernzerhof (PBE)(19) and the projector augmented wave (PAW)(20) method. We use the pseudopotentials and plane wave cutoff energy (520 eV for all structures) as recommended by the Materials Project. The VASP input files are generated using the pymatgen.io.vasp.sets module of Pymatgen. Given the large unit cells (and DFT Kohn–Sham bandgaps exceeding 1 eV for all these materials), we use a gamma-point only k-mesh. The pseudopotentials are given in Table S1.
The ultimate metric of the utility of materials selection models is the improvement in likelihood of successfully identifying positive examples over the background probability of these materials. To quantify the superiority of our model to completely random guesswork, we must know the likelihood of discovering a superionic material by chance with no scientific intuition involved. In 1978, Huggins and Rabenau questioned “whether there is anything fundamentally different in such materials, or whether they merely exhibit extreme values of ‘normal’ behavior.”(23) Superionic conductors may have remarkably low diffusion barriers, but how do they compare to the distribution of barriers across the space of all the candidate electrolyte materials?
Experience would tell us that the likelihood of chance discoveries is low, given that only a handful of fast Li conductors have been discovered since the search began nearly 40 years ago. However, we are probing for superionic conduction in a very specific way: we are pulling from a potentially biased group of 317 known materials in the Materials Project database, introducing Li vacancies, and simulating finite computational cells at high temperature. Therefore, this question is best answered by doing a straight-across comparison of high temperature DFT-MD simulations for a similar number of structures chosen at random from the same population.
To accomplish this, we perform a control experiment where we simulate 21 structures chosen uniformly at random from among the 317 materials that satisfy all prerequisite screening criteria (band gap > 1 eV, predicted oxidation potential > 4 V, energy above convex hull = 0 eV, no transition metals) and simulate MD under the same procedure. These 21 structures and their predicted superionic likelihoods according to the ML model are the following: LiLa2SbO6 (0%), Li6UO6 (7.7%), LiInF4 (0.6%), LiBiF4 (0.2%), Li6Ho(BO3)3 (10.1%), RbLiB4O7 (4%), Li4Be3As3ClO12 (0.3%), Li6TeO6 (6.4%), Li3Pr2(BO3)3 (9.2%), NaLiS (36%), LiSbO3 (2.4%), LiCaGaF6 (0.0%), Li2Te2O5 (15.8%), LiNO3 (8.3%), Ba4Li(SbO4)3 (0.0%), Rb2Li2SiO4 (4.5%), NaLi2PO4 (0.0%), Cs4Li2(Si2O5)3 (0.1%), RbLi(H2N)2 (24.5%), Cs2LiTlF6 (0.0%), and LiSO3F (100%). We chose 29 structures in total but removed eight (Na2LiNF6, CsLi2(HO)3, LiU4P3O20, Li3Nd2H6(N3O10)3, Li3La2H6(N3O10)3, Li3P11(H3N)17, Li4H3BrO3, and Li2Pr(NO3)5) due to either melting at 900 K or due to extremely slow or failed electronic convergence, in some cases due to electronic bandgap closure during the simulation. Of the remaining 21 randomly chosen materials we successfully simulate, 20 have a negative ML-based superionic prediction (below <50%) so we do not expect them to conduct. The only material of these 21 random materials that is predicted by our ML-based algorithm to conduct is LiSO3F. The computational parameters used in these randomly chosen simulations are also provided in Table S1. This set of randomly drawn materials also provides a test set to explore the ML model performance versus the predictive power of the intuition of human experts; we explore this in Section III, Subsection v.
The ML-chosen materials and randomly chosen materials exhibit notably different distributions in their compositions. For example, compositions containing oxygen are heavily represented in the randomly chosen materials (71.4% of materials) but poorly represented in the ML-chosen materials (14.3%). The former is closely aligned with the distribution of oxygen-containing compositions in the broader pool of 317 candidates (70.0%) that satisfy the constraints including high oxidation potential, which we expect given the randomly chosen materials are drawn from this pool. Similarly, sulfur-containing compositions make up 9.2% of the 317 candidates and 9.5% of the randomly chosen candidates but 57.1% of the ML-chosen materials, indicating a clear preference by the ML model for sulfides over oxides when ionic conductivity is the primary consideration. We note that this work is focused primarily on finding the best specific target compounds; a follow-up study(24) will focus on the identification of promising new 
target systems.
In this work we report a total of 4.3 ns of molecular dynamics simulation, with a mean simulation time of 74.9 ps and mean computational cell size of 99.1 atoms. The total volume of data reported here corresponds to approximately 330 000 GPU-hours of simulation.

RESULTS
The computed ionic conductivities for the ML-chosen candidate materials and the randomly chosen materials are provided in the Arrhenius diagram on Figure 2a. The computed high temperature diffusivities, ionic conductivities, average Li Bader charge, and extrapolated RT ionic conductivities with predicted deviations (conductivities measured from alternate starting time points) of these candidates are given in Table 1. We do not calculate results for one of the ML-chosen candidates, Li2GePbS4, because of problems with electronic
We simulate 20 materials as identified by the ML model. For each of the simulated materials, we provide the computed values for diffusivity, conductivity, and average Li Bader charge and the extrapolated RT ionic conductivity values. Here, the diffusivity ⟨Dii⟩ represents the average of the diagonal elements of the diffusivity tensor. In parentheses we provide the diffusivities corresponding to one standard deviation above and below the mean when measuring the diffusivity from varying times. We extrapolate these alternate diffusivities to RT to compute expected deivations in the RT conductivity predictions. The second-to-right column gives the ML-based model prediction for superionic likelihood, and the rightmost column communicates whether the ML-based model gave a correct prediction for each material. Eight materials have an extrapolated RT ionic conductivity of 10–4 S/cm or higher, two are below but near this threshold, and ten did not show any Li conduction even at high temperature. Li3InCl6 and LiCl were not simulated (“N/S”) because conductivity values could be found in the literature.
bLi2HIO was only simulated at 400 K due to melting at higher temperatures. This extrapolation to RT assumes a diffusion barrier of 0.35 eV, a typical diffusion barrier in fast conducting systems.
CsLi2BS3 could not be simulated at lower temperatures and thus the extrapolated ionic conductivity at RT was taken to be of the same order of magnitude as that of Li3BS3.(25)
Our simulations show 10 of the simulated candidate materials exhibit significant Li conduction at high temperature. After applying Arrhenius scaling and extrapolating to RT, eight of these materials are predicted to exhibit superionic conductivity (>10–4 S/cm) under the simulated vacancy concentrations: Li5B7S13, CsLi2BS3, LiMgB3(H9N)2, Li2B2S5, Li3ErCl6, Li3InCl6, Li2HIO, and LiSO3F. Two materials, Sr2LiCBr3N2 and LiErSe2, conduct well at high temperature, but their extrapolated RT conductivities are below 10–4 S/cm at RT.
Zhu et al. report(3) that RT superionic conductors will typically exhibit a simulated Li diffusivity of at least 0.01 Å2/ps at 800 K. Of these promising materials that were simulated at 900 K, all have diffusivities significantly above 0.01 Å2/ps. The two marginal conductors are above this limit by a factor of two to three, while the best conductors are above this limit by a factor of 10 or more. This provides additional confidence in their fast RT ionic conductivity.
The rightmost column of Table 1 indicates whether the DFT evidence suggests the superionic prediction of the ML model was correct. In total, 8 materials are observed to show Li conduction of 10–4 S/cm or above at RT, 2 materials are near this threshold, and 10 fall far below.
The results of the simulation on the randomly selected materials are listed in Table 2 and plotted in Figure 2b. Of the 21 randomly chosen and simulated materials, five structures demonstrated measurable Li conduction at 900 K: LiSO3F, LiSbO3, Li6Ho(BO3)3, Li2Te2O5, and RbLi(H2N)2, with Li diffusivities at 900 K of 0.3, 0.1, 0.05, 0.04, and 0.02 Å2/ps, respectively. The remaining 16 structures showed no observable Li conduction on the time scale of tens to hundreds of picoseconds. See Supporting Information for computational parameters and simulation times.
We simulate 21 materials as chosen at random and provide the computed values for diffusivity ⟨Dii⟩ (average of diagonal elements in the diffusivity tensor), conductivity, and average Li Bader charge and the extrapolated RT ionic conductivity values. In parentheses, we provide the diffusivities corresponding to one standard deviation above and below the mean when measuring the diffusivity from varying start times. We extrapolate along these alternate diffusivities to RT to compute expected deviations in the RT conductivity predictions. The second-to-right column gives the ML-based model prediction for superionic likelihood, and the rightmost column communicates whether the ML-based model gave a correct prediction for each material. The Bader charge on the Li atoms in the control group is taken to be +1.0 for computational efficiency. Additional randomly chosen materials that melted at 900 K or whose simulations did not converge were discarded and are not listed here. The success rate of random selection is 
computed to be 3/21 = 14.3%.
The five materials that exhibited diffusion at 900 K were simulated again at alternate temperatures. After constructing the conductivity extrapolation, the conductivities in three of these materials (LiSO3F, Li6Ho(BO3)3, and LiSbO3) were found to extrapolate to above 10–4 S/cm at room temperature. The remaining two, however, showed zero diffusion during simulation at 700 K. Using typical values for the Li concentration and Bader charge, we estimate that zero Li diffusion after 50 ps of simulation at 700 K likely corresponds to an ionic conductivity of 10–9 S/cm or less. Therefore, the RT conductivity of these two materials is predicted to be far below 10–4 S/cm when the assumed 700 K conductivity is factored into the extrapolation.
Between the ML-guided search and the random search, we identify 12 materials total that are predicted by our DFT-MD simulations to exhibit significant Li ion conduction at RT (and the DFT-MD of Zevgolis et al.(9) for Li3InCl6). Aside from Li3InCl6 and Li6Ho(BO3)3, none of these materials have reported DFT or experimental conductivity values in the literature to our knowledge. Due to our additional screening steps, these materials also satisfy several other critical criteria beyond fast ionic conductivity to make them useful as solid-state electrolytes in Li-ion batteries: all materials possess DFT-predicted band gaps greater than 1 eV to ensure limited electronic conduction, are free from transition metal elements which may easily reduce in contact with a Li metal anode, exhibit high predicted electrochemical stability against oxidation by cathodes (>4 V vs Li/Li+), and sit on the convex hull of their phase diagrams to ensure robust structural (phase) stability.
Two of the materials discovered here (Li2B2S5 and Li5B7S13) belong to the Li–B–S system, which appears to be a promising class of materials for realizing fast conducting, electrochemically stable, low mass solid electrolyte materials. The RT ionic conductivity of Li5B7S13 in particular is predicted to be an exceptional 74 mS/cm, six times higher than the material Li10GeP2S12 (12 mS/cm),(4) one of the best known Li-ion conductors. CsLi2BS3 is a Cs-substituted isomorph of Li3BS3, a fast conductor also in the Li–B–S family.(25) These three phases were initially identified and characterized as phases observed within Li–B–S (and Cs–Li–B–S) glasses.(26−28) The glassy Li–B–S–I system was reported to be an exceptional Li ion conductor in 1981,(29) but to our knowledge these phases within the crystalline Li–B–S space have not been studied as solid electrolytes.
Li3ErCl6, an Er-substituted analogue to the fast-conducting Li3InCl6, appears to be a promising Li conductor with a predicted RT ionic conductivity of 3 × 10–4 S/cm, although the high atomic mass and low earth abundance of Er makes it a less attractive candidate for battery applications. Li3ErCl6 was first studied and characterized in 1996 as a potential fast Li ion conductor,(30) but to our knowledge the ionic conductivity was not reported. We find another Er-based compound appears to be a marginal RT Li ion conductor, LiErSe2. LiErSe2 is a Li-intercalated two-dimensional layered material that was initially studied in 1987 for its phase transitions and magnetic properties.
We discover three nitride-based fast ion conductor materials with quite complex compositions: Sr2LiCBr3N2, RbLi(H2N)2, and LiMgB3(H9N)2. Sr2LiCBr3N2 is a solid-state carbodiimide material that was first characterized in 2005;(32) RbLi(H2N)2 was originally synthesized in 1918(33) and subsequently characterized in 2002;(34) and LiMgB3(H9N)2 was initially synthesized and studied in 2014 as a candidate solid-state hydrogen storage material.(35) The exotic stoichiometries of these materials underscores the utility of the ML-based model, as it seems unlikely that such complex materials would be discovered by chance to be fast ion conductors. A potential drawback in using these materials with many elements as solid electrolytes is that the electrochemical stability is likely to be low, given the high number of stable interfacial phases that could form from a subset of these elements.
We discover four oxygen-based materials predicted to be fast RT ion conductors: Li2HIO, LiSO3F, Li6Ho(BO3)3, and LiSbO3. These materials may be particularly promising candidates for further experimental studies, given that oxide materials can often be synthesized in atmospheric conditions. LiSbO3, LiSO3F, and Li2HIO were first studied in 1954, 1977, and 1994, respectively,(36−38) but to our knowledge have not been studied as a candidate solid electrolyte material. Li6Ho(BO3)3 was first characterized(39) in 1977, and experimental work from 1982 reported it to have poor RT ionic conductivity.(40) If the experimental observation is assumed to reflect the “ground truth” of ionic conductivity, this gives Li6Ho(BO3)3 the interesting distinction of having been accurately predicted to be a poor ion conductor by the ML model (which was trained on experimental data) but incorrectly predicted to be a good ion conductor by DFT-MD. It is also possible that the bulk ionic conductivity of the material is fast, which is 
accurately modeled by DFT-MD, but it becomes a poor ion conductor under the experimental conditions of ref (40), e.g., due to defects, stoichiometric variations, or microstructural effects. The calculations performed here suggest that this material warrants a revisit of the experimental results.
iv. Model Performance
Quantifying the accuracy of the original ML-based model should be done based on its performance on a randomly sampled test set, not on a test set consisting only of positive predictions. The 21 randomly chosen materials that were simulated here provide such a set. On this set, the model correctly predicts the DFT-validated material label 19/21 times, yielding an overall predictive accuracy of 90.5%. This value is similar to the 90.0% model accuracy predicted via leave-one-out cross-validation in the original model building process.(7) Only one material (LiSO3F) was predicted to be a superionic conductor by the ML-based model, and our DFT simulation confirmed the prediction was correct; this gives a model precision of 1.0 on this particular test set. Of the three materials that DFT-MD found to be RT superionic conductors (LiSO3F, LiSbO3, Li6Ho(BO3)3), only one was accurately predicted by the ML model; this gives a model recall of 1/3 = 0.333 on the test set. Taken together, these values give the model an F1 
score of 0.50. For comparison, the baseline F1 score of fast ion conductors on the test set (i.e., the F1 score of completely random guesswork) is simply the fraction of positive examples in the test set, which gives a baseline F1 score of 0.143. Therefore, the F1 score of the ML model is 0.50/0.143 = 3.5x higher than the F1 score associated with completely random trial-and-error.
As an alternative means of quantifying the expected improvement in experimental outcomes of Li conductivity measurements in the ML-guided versus random searches, we average the predicted RT ionic conductivities across all randomly chosen materials and all ML-chosen materials. Given that ionic conductivity varies over many orders of magnitude, we average the base-10 logarithm of conductivity. This gives a log-average RT ionic conductivity of the randomly chosen materials of 6.8 × 10–6 mS/cm, while the log-average conductivity in the ML chosen materials is 3.0 × 10–4 mS/cm, 44 times higher. This comparison assumes the average RT ionic conductivity in the materials with no observed Li motion is 10–6 mS/cm; as discussed in Supporting Information Section S1, this value likely serves as an upper bound in most cases on the conductivity in these materials, and thus the 44× improvement in conductivity is a lower bound on the true improvement the model offers, e.g., if the average conductivity of the nonconductors is 
taken to be 10–9 mS/cm, the improvement in conductivity increases to over 350×.
As an additional performance metric, we compare the likelihood of discovering a RT superionic conductor under the ML-guided screening versus random searching. The random search yields 3 fast conductors and 18 nonconductors. This yields a baseline superionic probability of 3/21 = 14.3%. To make a fair comparison to the ML-guided case where materials that melted at 900 K were not discarded, we do not consider in the statistics those ML-chosen materials that melted at 900 K. Two of the eight ML-chosen RT superionic conductors melted at 900 K (LiMgB3(H9N)2 and Li2HIO). Removing these two materials from the count, the ML model identifies six fast conductors, two marginal conductors, and ten nonconductors. Counting the marginal cases as one-half, this gives an ML-guided conductor discovery rate of 7/18 = 38.9%. Therefore, applying the ML-based model to screen candidate materials before experimentation yields an expected 0.389/0.143 = 2.7x improvement in discovery rate. In Figure 3 we provide a histogram showing the ionic conductivity distribution in the ML-chosen and randomly chosen materials; the ML model discovers over five times more materials with ionic conductivity above 1 mS/cm, while the number of nondiffusive materials falls from 86% to 56%. We refer the reader to the Supporting Information Section S2 for further discussion.
We perform a test of the model’s speed and predictive power against human intuition, which is likely to identify fast conducting materials at a higher rate than the background distribution of ion conductors. Several design principles for predicting superionic diffusion have been reported recently,(41−43) but human-guided search efforts may or may not take these principles into account. To understand the precision of human intuition, we polled six Ph.D. students in the Department of Materials Science & Engineering at Stanford University to test their ability to predict superionic conduction in the 21 randomly chosen materials. The students are all actively engaged in research involving ion conductors. The students were allowed to take as much time as necessary to make predictions and could access all structural information about the candidate materials provided on the Materials Project database or any other source. This included structure (CIF) files, space and point groups, and properties like formation 
energy and band gap. This poll sought to test how quickly and accurately the students would naturally predict superionic conduction in the absence of eq 1, and so they were not given access to any of the pretabulated features employed in eq 1, nor were they encouraged to make predictions quickly. After making predictions, we calculated their average precision and recall on the data set; the average precision was 0.25 and the average recall was 0.222, giving an overall average F1 score of 0.235. The baseline F1 score for random guessing is 0.143. The students took approximately one minute to make each individual prediction. For comparison, the ML model made each prediction in approximately one millisecond.
In Figure 4 we compare the performance in speed and F1 score of the Ph.D. students and the ML model. The ML model exhibits more than double the F1 score of the students and is more than 3 orders of magnitude faster. As a reference, we provide the performance of DFT-MD, which we assume has an F1 score of 1.0 and requires approximately 2 weeks to make a reliable prediction.
While DFT-MD is taken here to be ground truth, the logistic regression model utilized in this work is trained on experimental measurements, where grain boundaries, contaminants, and other uncharacterized factors may have altered the result. This may mean the model is best suited to guide experimental searches, as the model has a built-in bias toward expected results under experimental conditions. In contrast, our DFT-MD simulates conductivity in single crystal bulk systems with small Li vacancy concentrations. Although DFT-MD is well-suited to make predictions under these conditions, its predictions have potential to deviate from the ground truth for the ML model; for example, the conflicting DFT-MD and experimental reports for RT Li ion conductivity in Li6Ho(BO3)3 between this work and ref.(40) This work predicts RT ion conductivity of approximately 5.1 mS/cm, while ref (40) reports RT ion conductivity orders of magnitude lower (see Section III, Subsection iii). It is possible that the model may recognize 
that experimental efforts on Li6Ho(BO3)3 are likely to yield a poor conductor even though the single crystal conductivity is fast. We look forward to future experimental reports of the ionic conductivities of some of the materials presented here, to both advance the state of the art in ion conductors and to further quantify the performance of the data driven approach.

CONCLUSION
Guided by machine learning methods, we discover many new solid materials with predicted superionic Li ion conduction (≥10–4 S/cm) at room temperature: Li5B7S13, Li2B2S5, Li3ErCl6, LiSO3F, Li3InCl6, Li2HIO, LiMgB3(H9N)2, and CsLi2BS3. Two additional materials show marginal RT conduction: Sr2LiCBr3N2 and LiErSe2. One of these materials, Li5B7S13, has a DFT-MD predicted RT Li conductivity (74 mS/cm) many times larger than the fastest known Li ion conductors. A search over randomly chosen materials identifies two additional materials with promising predicted RT ionic conductivities: Li6Ho(BO3)3, and LiSbO3. In addition to high ionic conductivities, all these materials have high band gaps, high thermodynamic stability, and no transition metals, making them promising candidates for solid-state electrolytes in SSLIBs. These materials represent many exciting new candidates for solid electrolytes in SSLIBs, and we encourage subsequent experimental investigations into the properties of these materials.
Compared to the machine learning-guided search, the control experiment of searching for fast ion conductors through random guesses yields significantly lower quality results. Screening with our ML-based model improves the likelihood of superionic discovery by nearly three times, with a more than 44× improvement in log-average of RT ionic conductivity. Additionally, the model outperforms the intuition of Ph.D. students actively working on ion conductors by more than a factor of 2. This is a significant acceleration beyond trial-and-error research and provides confidence in our ML-based approach to materials screening. These results are summarized in Table 3. Such data-driven models are expected to improve with every new data point, and with the data we report here we expect to drive significant improvements to ML-based models for Li conduction.
Here we provide the performance metrics (precision, recall, F1 score) and list of discovered fast Li ion conducting materials for the three models explored in this work. The results of the Ph.D. student screening represent the average performance of six students on the same test set of randomly chosen materials as used to compute the “random selection” statistics. The machine learning-based approach to predicting ion conductivity from a small data set of 40 materials significantly outperforms both random chance and the average polled Ph.D. student. This highlights the promise of applying machine learning-based approaches to materials screening before performing computationally expensive simulation techniques like DFT-MD or time consuming experimental tests.
The improvement over random guessing provided by our ML-based model for predicting Li ion conductivity underscores the importance of thorough data reporting, centralized data collection, and careful data analysis for materials. Although significant improvements have been made in the last several years thanks to incentives provided by the Materials Genome Initiative, there are still many materials properties, especially experimentally expensive properties like ionic conductivity, without any comprehensive data repository. This work demonstrates that learning on even small sets of materials data (40 samples) can offer a significant advantage in screening efforts. To that end, we encourage efforts to continue centralizing and learning on diverse types of data on materials.
Energy 200 (2020) 117547
Contents lists available at ScienceDirect
Energy
journal homepage: www.elsevier.com/locate/energy
Design optimization of oil pan thermoelectric generator to recover
waste heat from internal combustion engines
*
Mutabe Aljaghtham , Emrah Celik
Department of Mechanical and Aerospace Engineering, University of Miami, USA
article info abstract
Article history:
Nearly 75% of fuel energy is rejected to the environment and ultimately becomes waste heat in motor
Received 1 June 2019
vehicles. To recover some of this waste heat and enhance fuel efﬁciency, thermoelectric energy gener-
Received in revised form
ators(TEGs)possesshighpotential.WeinvestigatedthefeasibilityofutilizingTEGsintermsofoilpansto
5 March 2020
recover waste heat generated in internal combustion engines. Hot oil at the top surface of TEG and air
Accepted 2 April 2020
cooling at the bottom create a high thermal gradient for the thermoelectric conversion. An extensive
Available online 13 April 2020
multi-physics simulation framework was introduced to accurately simulate conversion of heat into
electricitytakingintoaccountthermoelectricity,jouleheating,heatconductionandturbulentaircooling.
Keywords:
To maximize the thermoelectric power, dimensions and the total number of thermoelectric modules
Thermoelectric generator
wereoptimizedunderdifferentoilpangeometriesanddrivingconditions.Oursimulationsshowthatthe
Simulation
2

maximum power density of 5.77 kW m is achieved with multi-step oil pan geometry under a 76 C
Energy harvesting
IC Engine temperaturedifference between the hot and cold sides. This powerdensity surpassed those reported for
Fuel efﬁciency
the previous, conventional (exhaust and radiator) thermoelectric applications and indicated that har-
Oil pan
vesting thermal energy from combustion engines using oil pans is a feasible energy recovery method-
ology to enhance fuel efﬁciency in automotive vehicles.
© 2020 Elsevier Ltd. All rights reserved.
1. Introduction et al. [5] and Baatar et al. [6] replaced the conventional radiator
withseventy-twoTEmodulesandwithoutaddingextradevicesor
The majority of fuel energy in vehicles is ejected into the envi- changing the engine water cooling system, which generated a
ronment in the form of waste heat through the exhaust and the maximum power of around 75 Watts. Hsiao et al. [7] showed that
engine cooling systems. Therefore, thermoelectric (TE) materials integratingaTEgeneratoronanexhaustsystemcanperformbetter
which convert heat directly into electric power possess a great than radiators. As a result, many studies followed this work,
potential to recover some of this waste heat, improve the fuel ef- implementing TEG into exhaust system to generate electrical po-
ﬁciency of the vehicle engine and reduce carbon dioxide emission wer [8]. Exhaust TE systems were employed in different conﬁgu-
into the atmosphere [1]. Thermoelectric generators (TEGs) are rations including attachment into exhaust pipe [9], in a form of
attractive energy conversion systems because they are durable, shell [10] or a rectangular block [11]. Fernandez et al. produced
silent in operation, reliable, lightweight, vibration free and envi- 106.2 W [12] whenTEG was installed immediately downstream of
ronmentally friendly [2]. catalytic converters, and 270 W [13] when TEG prototype was
AccordingtoFiatResearchCenter,productionof800e1000Wof tested in the exhaust system of a spark-ignition and compression-
TEG power would reduce CO by 12e14 g/km [3]. Automotive ignition engines. Durand et al. [14] placed TEGs at the manifolds
2
companies have recently increased attention to TEG systems to and after the exhaust after-treatment system (ATS) to generate
convert wasted fuel energy in terms of heat, into electricity [4]. In 100 W and 30 W of electricity, respectively. Similarly, the Hi-Z
theseapplications,theexhaustpipeandtheradiatorwereselected Technology, Inc [15]. Designed seventy-two HZ-13 bismuth-
as the implementation sites for TEGs. In the radiator system, Kim telluride (Bi Te ) modules to generate 1 kW power directly from
2 3
the exhaust of a diesel engine at 300 HP and 1700 RPM.
To summarize the recent TE studies in automotive applications
discussedaboveandcomparetheirefﬁciencytogenerateelectrical
* Corresponding author.1251 Memorial Drive Coral Fables, FL, 33146, USA.
power, we prepared Fig. 1 where power density (power per unit
E-mail address: e.celik@miami.edu (E. Celik).
https://doi.org/10.1016/j.energy.2020.117547
0360-5442/© 2020 Elsevier Ltd. All rights reserved.
2 M. Aljaghtham, E. Celik / Energy 200 (2020) 117547
Nomenclature H module height (m)
d gap distance between p and n elements (m)
S Seebeck coefﬁcient for p-type element (V/K)
p
Symbols
S Seebeck coefﬁcient for n-type element (V/K)
n
c speciﬁc heat (J/kg K) N total number of TE modules
2
*
q heatﬂux (watt/m )
Z optimalﬁgure of merit
3
_
q heat generation (watt/m )
R total electric internal resistance (U)
k thermal conductivity (watt/m K)
R load resistance (U)
0

T temperature ( C) ’
m optimum resistance ratio
opt

T top surface average temperature ( C)
h
P thermoelectric output power (watts)

T bottom surface average temperature ( C)
c Q the thermal input to the hot junction (watts)
h

T ambient air average temperature ( C)
air
2
Greek symbols
h air average heat transfer coefﬁcient (watt/m K)
air
3
r density (kg/m )
I electric current (A)
s electrical conductivity (U m)
W width of p-type element (m)
p
a Seebeck coefﬁcient (V/K)
W width of n-type element (m)
n
f electric potential (V)
t p-n type thickness (m)
2
h efﬁciency of thermoelectric generator
TEG
A surface area of p-element (m )
p
2
A surface area of n-element (m )
n
recovery of an IC engine oil pan made out of TE modules using
advanced ﬁnite element methods. More speciﬁcally, we deter-
mined the amount of useable electrical energy converted from
waste engine oil heat in automobiles by incorporating outside
temperature, vehicle speed, size and shape, and TE modules.
Manufacturabilityoftheinvestigatedsystemandthecomparisonof
existing TE applications are also discussed in this study.
2. Methodology
Inthissection,theﬁniteelementsimulationtechniquesusedto
investigate TE device application within IC engines are described.
ThesimulationparameterssuchasTEmodulegeometry,TEmodule
quantity, oil pan shape and temperature difference across the oil
pan are optimized to achieve maximum output power. The details
of this optimization procedure are also described in this section.
2.1. Thermoelectric energy conversion simulation
Fig. 1. Performance of TEGs implemented in automotive applications in the past
decade. If the TEGs are stacked, power density was calculated as the power divided by
A thermoelectric module typically comprises of pairs of the
the number of stacks to ﬁnd power density per single layer [18e38].
alternating p-n type semiconductor elements (generator legs),
whicharestructuredasthermallyparallelandelectricallyinseries
area)dataforasinglelayerofTEmodulesaregivenforeveryyearin
as shown in Fig. 2. The heat ﬂow, which is partly converted into
thepastdecade.Asthisﬁgurepresents,thepowerdensityshowsa
electricalpower,isinducedbyheatingonesideofthearrangement
gradually increasing trend over the years with the advancing TE
(hot surface) while the opposite side is cooled (cold surface). The
technology.TheonlystudystandingoutofthistrendisZhangetal.
Seebeck effect and Joule heating are the two phenomena taking
2
[16]wherehigh powerdensityof5.26 kwatts. m was generated
place in TEG operation. Under steady state conditions, the contri-

with 500 C temperature difference between hot and cold sides.
bution into energy ﬂow can be described by two main equations,
Unlike the radiator and exhaust system applications of thermo-
heat ﬂow equation (Eq. (1)) and continuity equation (Eq. (2)), to
electricity described above, in this study, we present a novel
represent temperature gradient and electric variation ﬁeld,
implementation of TE systems within the oil pans of internal
respectively, which are expressed as follows:
combustion (IC) engines, where a higher amount of heat is gener-
atedandlosttotheenvironment. Asshown inFig.1, applicationof vT
_
rc þVq ¼ q (1)
TE systems in the oil pans of IC engines is feasible, and it can pro-
vt
vide superior performance compared to the exhaust and radiator
 
implementations. Unlike the exhaust systems which require a
vE
V Jþ½ε ¼0 (2)
water supply, heat exchanger and by-pass (backpressure) as
vt
describedbyEderetal.[17],anengineoilpanintegratedTEsystem
does not need a supplementary system and therefore is cost _
where, r, cq,q and ε represent the density, speciﬁc heat, heat ﬂux,
effective to implement on any motor vehicle. To demonstrate the
heatgenerationandthedielectricpermittivitymatrix,respectively.
performance of oil pan integration, we simulated the energy
Above equations can be coupled together by the following
M. Aljaghtham, E. Celik / Energy 200 (2020) 117547 3
analytical model. Details of the validation process for the thermo-
electric simulations is given in the supplementary section of this
study. After validating the numerical model on the single TE
module, we extended the simulation for multiple modules based
on the geometry of an oil pan as described in the next section.
Bismuth telluride (Bi Te ) was selected as the TE material in the
2 3
simulations. The material properties (i.e. Seebeck coefﬁcient, elec-
trical conductivity and, thermal conductivity) were input as
boundary conditions and estimated at the average temperature of
hot and cold surfaces as shown inTable 1.
Poweroutputwascalculatedfrom(Eq.(6))fordifferentexternal
(load) resistance values and an electric current which was deter-
mined at temperature gradient for hot and cold surfaces. The
maximum power was achieved when the external resistance
became identical to the internal resistance of the thermoelectric
system between metal and a p-n type element.
2.2. Optimization of TE module dimensions
Thedimensionsofasinglemoduleheightandplanedimensions
(width and thickness) were optimized for the integrated oil pan
geometry to achieve the highest value of thermoelectric power.
Fig. 2. Single module of thermoelectric generator.
FiniteelementsimulationwasperformedonasingleTEmoduleby
varyingthese geometricaldesignparameterstoachievemaximum
equations: output power. Geometrical design parameters of height, H, plane
dimensions (width W ,W and thickness t), and the gap distance
p n
q¼½k:VT (3) between p-n element, d are shown in Fig. 2.
ReducingthemoduleheightisdesirablesinceTEpowerrapidly
increases as the module height decreases; however, as the TE
J¼½s:ðE½a:VTÞ (4)
module becomes thinner by reducing z-height, the difﬁculty of
modulefabricationarises.supplementaryFig.S2demonstratesthe
E¼Vf (5)
variation of thermoelectric power of a single module with the
module height (H). The height of 2 mm was considered to be the
where, k, s and a denote thermal conductivity, electrical conduc-
optimalheightforeachsingleTEmoduleconsideringthedifﬁculty
tivity, and Seebeck coefﬁcient matrices for p-n type, respectively.
of fabrication below this module height. 2 mm was used for the
Symbolfrepresentselectricpotential.TheperformanceofTEGcan
remaining investigation of the TE design parameters in this study.
be described through the Seebeck coefﬁcient, electrical resistivity,
The height was an independent designparameter to obtain the
andthermalconductivity.Substituting(3e5)into(1e2)andsolving
highest power. However, in optimization of plane dimensions, the
thegoverningequationsyieldsthesolutionfortemperature(T),an
gap distance between p and n elements was selected to be one
electric potential (f) and current (I).
fourthofthetotalmodulewidth(d¼0.25(w þw )).Likewise,the
p n
Wesolvedthegoverningequationsdescribedabovenumerically
ratio of p-element width (W ) to that of n-element (W)was
p n
by using ANSYS Mechanical (APDL) ﬁnite element software. The
dependedupontheratioofSeebeckcoefﬁcients(S ,S )asfollows:
p n
model was constructed starting with a single thermal electric
module containing p- and n-typeof elementsas shown inFig. 2.A
W S
p p
¼ (7)
3-D 20-node hexahedron SOLID226 coupled ﬁeld elements with
W S
n n
structural,thermal,andelectricaldegreesoffreedomatnodeswere
We determined the optimal plane dimensions of an individual
used in the simulation to determine thermoelectric current and
2
TEmoduletobeWp¼1.96cm,Wn¼1.5cm,Ap¼2.9410-2cm ,
voltage as a function of internal load resistance. The power gener-
2 2
An ¼ 2.96  10-2 cm and d ¼ 8.97  10 cm. Since the Seebeck
ated from a TEG was calculated using current and the external
coefﬁcientvalueofp-typebismuthtelluride(Bi Te )ishigherthan
2 3
resistance (R ) according to the following power equation:
0
that of n-type, width and area of the p-sectionwas designed to be
larger than the n-section of the module to achieve maximum
2
P¼I R (6)
0
output power. Similar to the z-height, plane dimensions of the TE
Validation of the simulations were compared to the previous modules attached to the oil pan were also optimized regarding
work by Antonova and Looman [39] where power output for the maximum power output. As the plane dimensions of TE modules
single module was obtained to be 1.44 W, nearly identical to this decreased, the number of modules increased since more TE mod-
study (1.43 W). The simulations were also compared against the ulescanbeﬁtatthesameoilpanareaasmarkedinFig.3.However,
Table 1
Properties of n- and p-type TE materials.
Thermoelectric Seebeck coefﬁcienta (V/K) Thermal conductivity k (W/m.K) Electrical conductivitys (U:m)
Material
6 7
n-type [40] 230 x 10 1.2 1:75 x 10
6 7
p-type [40] 195x 10 1.4 1:35 x 10
4 M. Aljaghtham, E. Celik / Energy 200 (2020) 117547
increasing the number of modules further does not contribute
signiﬁcantly to the power output.
2.3. Thermoelectric simulation of oil pans with different geometries
In addition to the height and the plane dimensions of TE mod-
ules, surface area of the oil panwas also considered to be a design
parameterto optimizethetotal numberof modules. Wesimulated
three oil pan geometries; 1) Flat plate oil pan, 2) Oil pan with a
single step, 3) Oil pan with multi-steps. The bottom surface of the
oilpanisreplacedbyTEmoduleswhichcorrespondtoeachoilpan
geometry as displayed on Fig. 5
2.4. Numerical simulation of air cooling under the oil pan
Thetemperatureatthetop(hot)andthebottom(cold)surfaces
Fig. 3. Different plane dimensions of TE modules at the same surface area of oil pan.
were input as boundary conditions in the thermoelectric simula-
tions to obtain the output power (see Fig. 6). As shown in
Supplementary Fig. S3, signiﬁcantly higher thermoelectric power
asthenumberofTEmodulesincreases,thecostandthedifﬁcultyof
can begenerated if thetemperature gradient is increased between
fabricationtendtoincrease.Therefore,optimalnumberofmodules
hot and cold surfaces The maximum temperature in the cylinder
was determined to be the number obtained for 95% of the

wall of an IC engine is around 2200 C [41]. Although the engine
maximum power output to be achieved. As demonstrated in Fig. 4
temperature can reach to high levels, engine oil temperature is
fortheﬂatplateasanexample,afterthe95%moduleselectionline,
determined based on the type and cycle of the engine [42]aswell
Fig. 4. A) TE modules simulation for ﬂat shape. B) optimal number of TE modules used in ﬂat shape.
Fig. 5. Different TE oil pan geometries, A) ﬂat plate shape, B) single step, and C) multi steps shape.
M. Aljaghtham, E. Celik / Energy 200 (2020) 117547 5
astheheattransferefﬁciency[43].Theheattransferisalsoaffected analytical convective heat transfer calculations for ﬂat oil pan ge-
bytheworkingﬂuidtypeandtheaveragewalltemperature[44].A ometryas detailed in the supplementary section of this study. The
coolantjacketisusedtocoolthecylinderwall,whilethecirculation deviation of temperature calculations between the analytical so-
oilaroundtheengineisusedtocoolandlubricatetheotherengine lution and the CFD simulations was around 2%. Heat convection
parts. In this study, the hot side temperature was assumed to be transfercanbesigniﬁcantlyimprovedbyeitherincreasingtheheat
constant and equal to the average oil engine temperature. Oper- transfercoefﬁcient,h,orincreasingtheheattransferarea,A,which
ating temperatures of engine lubrication are given in literature can be achieved by assuming multi-steps oil pan shapes. Another
 
withintherangefrom100 Cto120 C[45].Hence,inthisstudythe method to remove the heat from the oil pan bottom wall can be
hot side average temperature is assumed at the steady state oil achievedbyconstructingﬁnsontothebottomsurfaceoftheoilpan

operatingtemperatureof110 C .Thetemperatureofthecoldsideis to enhance the convective air-cooling process. The cooling of the
however determined by the cooling performance of the air ﬂow bottom surface with ﬁns reduces the bottom surface wall temper-
around the bottom surface of the oil pan. Air cooling depends on ature which leads to an increase in the output TE power. As illus-
the surface geometry, the outside temperature, and the velocity of trated in Fig. 7 for different oil pan geometries, 28 pin ﬁns with a
thevehicle.Inordertodeterminethebottomsurfacetemperature, 2cmdiameterand6cmlengthareattachedontoaﬂatplateoilpan
ANSYS Fluent computational ﬂuid dynamics (CFD) software was shape. For the single step geometry, the same number of ﬁns are
used to simulate airﬂow under the oil pan at different vehicle usedbutwithalengthof6cmand12cm.Thelengthwas selected
speeds,differentambientairtemperaturesandsurfacegeometries. considering the clearance between the bottom surface of the oil
The COUPLED algorithm was used to solve the steady-state heat pan and the ground.
transfer problem. SSTk u turbulence model was adopted to
simulatetheairboundarylayerﬂowsaroundbottomsurfaceofthe
3. Results
oil pan and to determine the bottom surface temperature. As dis-
played in Figs. 6 and S1, the boundary conditions for airﬂow are
3.1. Temperature ﬁeld at the bottom surface of oil pan
governed by the airﬂow velocity, and air temperature. By varying
these parameters (air velocity and temperature) in CFD simula-
To generate maximum thermoelectric output power, the tem-
tions, bottom surface temperature of the oil pan was determined.
perature gradient between the hot and cold surfaces needs to be
The accuracy of the CFD simulations was validated by performing
large.CFDsimulationresultsshowthestrongrelationshipbetween
the air velocity and bottom surface temperature of the oil pan. In
addition to the effect of air ﬂow velocity, the ambient air temper-
ature also affects the bottom surface temperature. Thus, in a cold
climate, ambient air will create a lower air stream temperature on
the bottomwall of the oil pan than those vehicles in a hotclimate.
CFDSimulationsdemonstratethatatthevehiclespeedof120km/h

and ambient temperature 0 C, the average surface temperature
alongthebottomwallsurfaceareaoftheoilpanforﬂat,singlestep
 
and multi-steps geometries can be estimated around 68 C,64 C,

and 55 C, respectively, as shown in Fig. 8.
Inadditiontoincreasingthecoolingairvelocity,theattachment
of ﬁns onto the bottom surface of the oil pan reduces the bottom
surface temperature and increases the difference temperature.
Therefore,byconstructingﬁns,thebottomsurfacetemperaturesin

the same driving conditions (120 km/h and 0 C ) were further

decreased to 50, 45, and 34 C forﬂat, single step and multi-steps
geometries, respectively, as displayed in Fig. 9.
3.2. Thermoelectric power generation
Aftersimulating thetemperatureﬁeld on thebottomsurface of
theoilpanconsideringthevehiclemovingat120km/h,theoutput
thermoelectric power can be obtained via thermoelectric simula-
Fig. 6. Schematic of TEG integrated single stepped oil pan (conventional oil pan). tions. Fig.10 displays the maximum power generated for different
Fig. 7. Pin ﬁns attached to the bottom surface of ﬂat and single stepped oil pan geometries.
6 M. Aljaghtham, E. Celik / Energy 200 (2020) 117547

Fig. 8. Temperature distribution for bottom surface of the oil pan at 120 km/h and 0 C for (a) ﬂat plate shape, and (b) single step, and (c) multi steps shape.

Fig. 9. Temperature distribution for bottom surface of the pin ﬁns oil pan at 120 km/h and 0 C for (a) ﬂat plate shape, and (b) single step, and (c) multi steps shape.
 
Fig.10. TE power generated from different shaped oil pan geometries at 120 km/h, a) outside temperature of 0 C b) outside temperature of 25 C.
  
oil pan shapes for 0 C and 25 C outside temperature. The power multi-step oil pan shape (at 120 km/h and 0 C ) generated 78.3%
extractedfromthe192TEmodulesattachedtoaﬂatshapedoilpan and 71.3% output power higher than ﬂat plate and single step oil

was around 84 W with the temperature gradient of 43 C . On the pan,respectively.ThevariationofoutputpowergeneratedfromTE
other hand, 216 TE modules were attached onto the single step oil pans as a function of different vehicle speed are given in the
shaped oil pan which generated 108 W and the corresponding supplementary section Figs. S4 and S5. The external (load) resis-

temperature difference of 46 C. In the multi-step shape, the in- tance(R Þisoptimizedtomatchtheinternalresistance(RÞinorder
0
creaseinthesurfaceareainadditiontotheincreaseoftemperature to obtain maximum output power as displayed in Figs.12e15. The
gradient, the output power drastically increased to 393 W with a thermoelectric current corresponds to power generated and ther-

temperature difference of 55 C, and 552 TE modules. At the same moelectric energy conversion are demonstrated in Figs.16 and 17,

vehiclevelocityof120km/hand25 C ambientairtemperature,the respectively. The efﬁciency of thermoelectric generator may be
generated output power decreased to 73, 90 and 287 W, calculated by application of the law of thermodynamics, which is
respectively. deﬁned as the ratio of the electrical power output to the thermal

For ﬁnned structures operating at 120 km/h and 0 C air tem- power input to the hot junction as follows:
perature, the power generated has signiﬁcantly increased for all
three shapes (ﬂat, single step and multi-step) to 163, 215 and
P
out

h ¼ (8)
751W,respectively,atatemperaturedifferenceof60,65and76 C. TEG
Q
h

Similarly, at 120 km/h and 25 C in aﬁnned oil pan, the power has
raisedupto104,130and453W.ImplementingTEmodulesintothe
where, Q is the thermal input to the hot junction and deﬁned as:
h
M. Aljaghtham, E. Celik / Energy 200 (2020) 117547 7
Fig.14. Thermoelectric output power generated at optimum load resistance for (ﬁns)

TE modules in oil pan at 120 km/h and 25 C.
Fig.11. TEG performance for different oil pan shapes geometry.
Fig. 15. Thermoelectric output power generated at optimum load resistance for TE

modules in oil pan at 120 km/h and 25 C.
Fig.12. Thermoelectric output power generated at optimum load resistance for (ﬁns)

TE modules in oil pan at 120 km/h and 0 C.
3.3. Energy harvesting in larger sized oil pans
Oil pans of large duty heavy engines are also considered to
determine the effect of oil pan size onTE power generation in this
study. Increase of the surface area results in generating more
output power due to the larger number of TE modules that can be
ﬁttedintotheoil pan.Thesurfaceareaof theoilpanforlargeduty
heavyengines is larger than the standard passengerautomobile IC
engine oil pan as shownin Fig.18. Fig.18 shows the dimensions of
the Inline (straight)-6 diesel Cummins ISX engine which was cho-
sen for this simulation. Optimizations demonstrate that 776, 972
and 2100 modules were obtained as the optimal number of TE
modulesthatcanbedesignedandattachedintoﬂat,singlestepand
multi-step shapes, respectively. Considering the truck is operating

at 110 km/h and a 0 C outside temperature, the maximum output
thermoelectric power generated was 622, 797 and 2713 W for the

ﬂat, single step, and multi-stepﬁn oil pan. At 25 C and 110 km/h,
Fig. 13. Thermoelectric output power generated at optimum load resistance for TE
385, 486 and 1636 W can be used foraﬂat, single step, and multi-

modules in oil pan at 120 km/h and 0 C.
stepsﬁnoilpan,respectively.ImplementingTEmodulesintolarger
scale oil pan generated nearly 3.6 times higher output power than
in the TE modules implemented into standard passenger automo-
bile IC engine oil pan as shown in Fig.19.
4. Conclusion
 
2
Q ¼ K DTþaT Iþ0:5I R (9)
h h
In this study, we explored the feasibility of TE module-
TE module efﬁciency ranges from 5% to 9% at temperature dif- integrated internal combustion engine oil pan with the goals of
 
ferenceof43 C to76 C.Fig.11demonstratestheTEGperformance reducing fuel consumption, carbon dioxide (CO2) footprint in the
for different oil pan shapes. air, and generating power from the waste oil heat. TE module
8 M. Aljaghtham, E. Celik / Energy 200 (2020) 117547

Fig.16. TE current corresponds to output power for multi-steps oil pan shapes geometry at 120 km/h and 0 C.
Fig. 17. TEG energy conversion efﬁciency for different oil pan shapes geometry at

120 km/h and 0 C.
Fig.19. Thermoelectric power generated within TE oil pan.
dimensions, TE modules quantities, size and shape of the oil pan
wereoptimizednumericallytoachievetheidealTE-oilpandesign.
Temperaturegradientisanessentialparameterdeterminingthe
harvested TE power. If higher oil temperature is used in an IC en-
gine or if the bottom surface of the oil pan is cooled effectively, TE
power is maximized. Using ﬁns to enhance the heat transfer from
thebottomsurface,drivingatcolderclimatesandathighspeedsall
enhance the temperature gradient and therefore harvested TE en-
ergy. Cooling efﬁciency can be enhanced even further using a
nozzle setup where air can be forced through the nozzle channel
toward the bottom surface of the oil pan which increases the air
ﬂow velocity due to the conservation of mass of the air ﬂow.
Increased air velocity results in reduction in the bottomwall tem-
perature of the oil pan, thus, generation of higher thermoelectric
power.

In ideal driving conditions (120 km/h, 0 C) and optimized TE
module geometries, we determined that the multi-step, ﬁnned oil
pan generates thermoelectric power of 751 W for a standard pas-
senger IC engine oil pan and 2713 W fora largeduty heavyengine

multi-stepﬁnnedoilpanatdrivingconditions(110km/h,0 C).The
2
TE power density of 5.77 kW m was achieved which is higher
than the previously reported TE power densities designed for
Fig. 18. Conventional single-step truck engine oil pan and comparison with the
automobile radiators and exhaust systems as shown in Fig.1. The
automobile engine.
M. Aljaghtham, E. Celik / Energy 200 (2020) 117547 9
the art light duty vehicles with thermoelectric elements. Fuel 2018;224:
net TE powerharvestedfromthewasteoil-pan heat would reduce
271e9.
CO by11gCO /kmperpassengervehicleandaround38gCO /km
2 2 2
[15] Bass JC, Elsner NB, Leavitt FA. Performance of the 1 kW thermoelectric
per large duty heavy vehicle.
generator for diesel engines. In: AIP conference proceedings. AIP; 1994.
[16] Zhang Y, et al. High-temperature and high-power-density nanostructured
Inadditiontotheenhancedenergyharvestingperformanceand
thermoelectricgeneratorforautomotivewasteheatrecovery.EnergyConvers
the environmental beneﬁts regarding the reduced CO emission,
2
Manag 2015;105:946e50.
generatingelectricityfromtheoilpandoesnot requireredesignof
[17] Eder A, Linde M. Efﬁcient and dynamicethe BMW group roadmap for the
anyexistingvehiclesystemsuchasradiatororexhaust;instead,the application of thermoelectric generators. In: Second thermoelectric applica-
tions workshop; 2011. San Diego.
conventional oil pan can easily be replaced with the TE oil pan
[18] Hsu C-T, et al. Experiments and simulations on low-temperature waste heat
designedinthisstudyforanycartype.Therefore,weenvisionthat
harvesting system by thermoelectric power generators. Appl Energy
easy implementation, enhanced performance and the low-cost oil 2011;88(4):1291e7.
[19] Love N, Szybist JP, Sluder C. Effect of heat exchanger material and fouling on
TE oil pan systems will help car manufacturers adopt this tech-
thermoelectric exhaust heat recovery. Appl Energy 2012;89(1):322e8.
nologytodevelopmoreenergyefﬁcientvehiclesinthenearfuture.
[20] Liang X, et al. Comparison and parameter optimization of a two-stage ther-
The fabrication method of thermoelectric modules in this
moelectric generator using high temperature exhaust of internal combustion
engine. Appl Energy 2014;130:190e9.
intricate geometry is difﬁcult using conventional manufacturing
[21] Liu X, et al. Experiments and simulations on heat exchangers in thermo-
techniques, therefore, advanced manufacturing technologies such
electric generator for automotive application. Appl Therm Eng 2014;71(1):
as additive manufacturing, should be considered to fabricate these
364e70.
[22] Liu X, et al. Performance analysis of a waste heat recovery thermoelectric
materials in complex shapes. Additively manufactured multi-step
generation system for automotive application. Energy Convers Manag
oil pans could minimize the fabrication difﬁculty and allows con-
2015;90:121e7.
version of a higher amount of waste heat into electricity.
[23] Kim TY, Negash AA, Cho G. Waste heat recovery of a diesel engine using a
thermoelectric generator equipped with customized thermoelectric modules.
Energy Convers Manag 2016;124:280e6.
CRediT authorship contribution statement
[24] Wang Y, et al. The inﬂuence of inner topology of exhaust heat exchanger and
thermoelectric module distribution on the performance of automotive ther-
moelectric generator. Energy Convers Manag 2016;126:266e77.
Mutabe Aljaghtham: Data curation, Formal analysis, Writing -
[25] HeW,WangS,YueL.Highnetpoweroutputanalysiswithchangesinexhaust
original draft. Emrah Celik: Project administration, Supervision,
temperature in a thermoelectric generator system. Appl Energy 2017;196:
Writing - review& editing, Writing - original draft.
259e67.
[26] Orr B, Akbarzadeh A, Lappas P. An exhaust heat recovery system utilising
thermoelectric generators and heat pipes. Appl Therm Eng 2017;126:
Appendix A. Supplementary data
1185e90.
[27] Massaguer A, et al. Transient behavior under a normalized driving cycle of an
Supplementary data to this article can be found online at
automotive thermoelectric generator. Appl Energy 2017;206:1282e96.
[28] Li B, et al. Heat transfer enhancement of a modularised thermoelectric power
https://doi.org/10.1016/j.energy.2020.117547.
generator for passenger vehicles. Appl Energy 2017;205:868e79.
[29] Cao Q, Luan W, Wang T. Performance enhancement of heat pipes assisted
Declaration of interests
thermoelectric generator for automobile exhaust heat recovery. Appl Therm
Eng 2018;130:1472e9.
[30] Wang Y, et al. Performance evaluation of an automotive thermoelectric
The authors declare that they have no known competing
generator with inserted ﬁns or dimpled-surface hot heat exchanger. Appl
ﬁnancial interests or personal relationships that could have
Energy 2018;218:391e401.
[31] Zhao Y, et al. Performance analysis of automobile exhaust thermoelectric
appeared to inﬂuence the work reported in this paper.
generatorsystemwithmediaﬂuid.EnergyConversManag2018;171:427e37.
[32] Kim TY, Kwak J, Kim B-w. Energy harvesting performance of hexagonal sha-
References
ped thermoelectric generator for passenger vehicle applications: an experi-
mental approach. Energy Convers Manag 2018;160:14e21.
[1] Bell LE. Cooling, heating, generating power, and recovering waste heat with
[33] Lan S, et al. A dynamic model for thermoelectric generator applied to vehicle
thermoelectric systems. Science 2008;321(5895):1457e61.
waste heat recovery. Appl Energy 2018;210:327e38.
[2] KimTY,KimJ.Assessmentoftheenergyrecoverypotentialofathermoelectric
[34] Zhao Y, et al. Performance investigation of an intermediate ﬂuid thermo-
generator system for passenger vehicles under various drive cycles. Energy
electric generator for automobile exhaust waste heat recovery. Appl Energy
2018;143:363e71.
2019;239:425e33.
[3] Champier D. Thermoelectric generators: a review of applications. Energy
[35] Mohamed ES. Development and performance analysis of a TEG system using
Convers Manag 2017;140:167e81.
exhaust recovery for a light diesel vehicle with assessment of fuel economy
[4] Huang K, et al. A novel design of thermoelectric generator for automotive
and emissions. Appl Therm Eng 2019;147:661e74.
waste heat recovery. Automotive Innovation 2018;1(1):54e61. [36] Kim TY, Negash AA, Cho G. Experimental study of energy utilization effec-
[5] Kim S, et al. A thermoelectric generator using engine coolant for light-duty tiveness of thermoelectric generator on diesel engine. Energy 2017;128:
internal combustion engine-powered vehicles. J Electron Mater 2011;40(5): 531e9.
812. [37] RamírezR,etal.Evaluationof theenergyrecoverypotentialofthermoelectric
[6] Baatar N, Kim S. A thermoelectric generator replacing radiator for internal generators in diesel engines. J Clean Prod 2019;241:118412.
combustion engine vehicles. Telkomnika 2011;9(3):523. [38] Kim TY, Kwak J, Kim B-w. Application of compact thermoelectric generator to
[7] Hsiao Y, Chang W, Chen S. A mathematic model of thermoelectric module hybrid electric vehicle engine operating under real vehicle operating con-
with applications on waste heat recovery from automobile engine. Energy ditionsvol. 201. Energy Conversion and Management; 2019. p. 112150.
2010;35(3):1447e54. [39] Antonova EE, Looman DC. Finite elements for thermoelectric device analysis
[8] Elankovan R, et al. Evaluation of thermoelectric power generated through in ANSYS. In: Thermoelectrics, editor. Ict 2005. 24th international conference
waste heat recovery from long ducts and different thermal system conﬁgu- on. 2005. IEEE; 2005.
rations. Energy 2019;185:477e91. [40] Angrist SW. Direct energy conversion. 1976.
[9] He W, et al. Performance optimization of common plate-type thermoelectric [41] Noroozian A, et al. Thermodynamic analysis and comparison of performances
generator in vehicle exhaust power generation systems. Energy 2019;175: of air standard Atkinson, Otto, and Diesel Cycles with heat transfer consid-
1153e63. erations. Heat Tran Asian Res 2017;46(7):996e1028.
[10] Weng C-C, Huang M-J. A simulation study of automotive waste heat recovery [42] Ahmadi MH, et al. Optimal design of an Otto cycle based on thermal criteria.
using a thermoelectric power generator. Int J Therm Sci 2013;71:302e9. Mechanics & Industry 2016;17(1):111.
[11] EddineAN,etal.Effectofengineexhaustgaspulsationsontheperformanceof [43] Ahmadi MH, et al. New thermodynamic analysis and optimization of perfor-
a thermoelectric generator for wasted heat recovery: an experimental and manceofanirreversibledieselcycle.EnvironProgSustainEnergy2018;37(4):
analytical investigation. Energy 2018;162:715e27. 1475e90.
 ~
[12] Fernandez-Yanez P, et al. Thermal analysis of a thermoelectric generator for [44] Ahmadi MH, et al. Thermodynamic analysis and optimization of the Atkinson
light-duty diesel engines. Appl Energy 2018;226:690e702. engine by using NSGA-II. Int J Low Carbon Technol 2016;11(3):317e24.
 ~
[13] Fernandez-Yanez P, et al. A thermoelectric generator in exhaust systems of [45] Roberts A, Brooks R, Shipway P. Internal combustion engine cold-start efﬁ-
spark-ignition and compression-ignition engines. A comparison with an ciency: a review of the problem, causes and potential solutions. Energy
electric turbo-generator. Appl Energy 2018;229:80e7. Convers Manag 2014;82:327e50.
[14] Durand T, et al. Potential of energy recuperation in the exhaust gas of state of
Parametric analysis and optimization of a small-scale radial turbine
for Organic Rankine Cycle

Kiyarash Rahbar , Saad Mahmoud, Raya K. Al-Dadah, Nima Moazami
School of Mechanical Engineering, University of Birmingham, Edgbaston, Birmingham B15-2TT, UK
article info abstract
 In this methodology, the mean-line modelling coupled with real gas formulation is employed
to perform parametric studies to identify the key variables that have signiﬁcant effect on the turbine
Keywords:
efﬁciency. Such variables are then used in the GA to optimise the turbine performance. Eight organic
Organic Rankine Cycle
ﬂuids are investigated to optimise the performance of the small-scale radial turbine in terms of efﬁ-
Radial turbine
ciency. Results showed that the achieved radial turbine efﬁciencies vary from 82.9% to 84%; which is
Mean-line modelling
Genetic algorithm optimization higher than the reported efﬁciency values of other types of expanders. R152a showed the highest efﬁ-
Organic working ﬂuids
ciency of 84% with seven degrees (K) of superheating. However, if the superheating is to be avoided,
isobutane exhibited the most favourable characteristics in terms of efﬁciency (83.82%), rotor size
(66.3 mm) and inlet temperature (89.2 C).

1. Introduction exergyefﬁcienciesandminimumsolarcollectorareawiththedual
pressurecycle.Delgado-Torresetal.[3]performedthermodynamic
The ORC (Organic Rankine Cycle) is a promising technology for analysis of the ORC to determine the solar energy required for
conversion of low temperature waste heat sources into useful po- reverse osmosis desalination using four stationary collectors and
wer. The ORC utilizes organic ﬂuids such as hydrocarbons and re- twelve working ﬂuids. Liu et al. [6] conducted thermodynamic
frigerants that boil at low temperature and pressure, with the modelling of a 2kWe micro-scale ORC using biomass heat and
advantagesofsmallsize;lowcapitalandmaintenancecostandlow studiedtheeffectofsuperheatingandsub-coolingof threeorganic
environmental impact. ﬂuids. They concluded that n-pentane has the highest efﬁciency
SeveralstudieshavebeenconductedtoadopttheORCforawide andbothsuperheatingandsub-coolingaredetrimentaltothecycle
range of low-grade heat applications, such as: solar energy [1e5], efﬁciency.Tchancheetal.[2],Drescheretal.[7]andSalehetal.[10]
biomass heat [6e8], geothermal energy [9e12], WHR (Waste Heat carried out detailed studies for the selection of a proper working
Recovery)ofIC(internalcombustion)engines[13e15],WHRofgas ﬂuid by modelling the thermodynamic properties and thermal ef-
turbine exhausts [16e18] and bottoming of the water/steam ﬁcienciesof20,700and31differentﬂuidsrespectively.Cammarata
Rankine cycle [19,20]. Tempesti et al. [1] and Guzovic et al. [11] et al. [12] performed thermodynamic analysis of the ORC for a
carried out thermodynamic modelling of a single and a dual pres- geothermal primary source using ﬂow-chart numerical tool based
sureORCwithsolarandgeothermalheat.TheyshowedthatR245fa onalumpedparametersapproachandhighlightedthepotentialof
and isopentane exhibit the best performance in terms of cycle and numerical tools in predicting the cycle performance. Capata et al.
[15] investigated the feasibility of an on-board innovative ORC
system suitable for all types of thermally propelled vehicles to
recover the heat from exhaust gases and produce extra power.
Hettiarachi et al. [9], Rashidi et al. [19] and Wang et al. [20]
optimized various performance indicators of the ORC such as the angle, the present study utilized a rotor blade geometry with non-
exergyefﬁciency,speciﬁcworkandtheratioof theheatexchanger zeroinletblade angle to achievehigherspeciﬁc workoutputand a
area to the net power output using various optimization methods more compact rotor.
such as artiﬁcial bees colony, GA (genetic algorithm) and steepest
descent method. Even though there have been several studies 2. Working ﬂuid selection
[1e20] regarding the selection of working ﬂuid, thermodynamic
modelling and optimization of the ORC, there has been relatively AnimportantfactorwhendesigninganexpanderfortheORCis
fewer published work on the modelling of the expander. the selection of the working ﬂuid. The properties of the working
Expanders are the key components of the ORCs and their per- ﬂuidhaveamajoreffectontheturbineperformanceandgeometry.
formancehavesigniﬁcanteffectsontheoverallsystem'sefﬁciency. Due to the low temperature of the heat source, dry and isentropic
Modelling and experimental study of the ORC using scroll ex- ﬂuids are more favourable for the ORC systems because of the su-
panders was carried out in Refs. [21e25], with a maximum scroll perheated condition after the expansion in the turbine. This elim-
efﬁciency of 67% reported in Ref. [22]. Compared to scroll ex- inatestheconcernsregardingtheexistenceofliquiddropletsinthe
panders,radialturbinescanoffertheadvantagesofhighefﬁciency, rotor compared to the wet ﬂuids such as water and the need for
light weight, mature manufacturability and high power capacity superheating equipment. Moreover, the working ﬂuids should
[26]. Sauretet al. [27] performed the modellingof candidateradial satisfy the environment related issues and safety criteria. These
turbine rotors for a geothermal power system using ﬁve organic includezeroozonelayerdepletion,minimalGWP(globalwarming
ﬂuids. R143a showed the maximum turbine static efﬁciency of potential),lowatmosphericlifetime,non-ﬂammableandnon-toxic
78.5%withtheturbineinlettotaltemperatureandpressureof413K characteristics.Table1showsthepropertiesoftheselectedorganic
and 60 bars respectively. Kang [28] accomplished the design and workingﬂuids. The temperature - entropy diagram of the selected
experimental study of a 30 kW radial turbine for the ORC using ﬂuids is shown in Fig.1.
R245fa working ﬂuid and obtained the turbine static efﬁciency of
75%withtheturbineinlettotaltemperatureand pressureof 353K 3. Radial turbine mean-line model
and7.3barsrespectively.Peietal.[29]carriedouttheexperimental
study of a small-scale ORC using R123 and radial turbine. They Mean-linemodellingisbasedonaone-dimensionalassumption
examinedtheheattransferand powerconversionprocessthrough that there is a mean streamline through the stage, such that con-
the ORC and achieved the turbine isentropic efﬁciencyof 71% with ditions on the mean streamline are the average of the passage
the turbine inlet total temperature and pressure of 373 K and conditions [34]. Then the thermodynamic properties, geometry
7.8 bars. Rahbar et al. [30,31] developed a procedure for the pre- parameters and ﬂow features are determined at key stations
liminary and detailed design of radial turbines for low power ca- throughout the stage. Euler's turbomachinery equation, conserva-
pacity systems such as ORC using mean-line modelling and CFD tion of mass, momentum and energy, are the basic equations that
(computationalﬂuiddynamics)analysis.Panetal.[32]replacesthe form the mean-line model. Also, losses and blockage in the rotor,
constant radial turbine efﬁciency with an internal efﬁciency to nozzle and volute, have been taken into account. Fig. 2 represents
enhance the reliability of the ORC analysis results. R152a showed thecrosssectionoftheradialturbinestagewiththecorresponding
the maximum turbine efﬁciency of 75.8% at the turbine inlet total enthalpyeentropy diagram that detailed the expansion process.
temperature and pressure of 323 K and 11.8 bars respectively. Mean-line modelling is a highly iterative process as it requires
Fiaschi et al. [33] built a model to design a 50kWe radial turbo- comprehensive studies of many different conﬁgurations by varia-
expander for the ORC in which the ﬂuid dynamics design input tion of a large group of input variables in a speciﬁed range. Fig. 3
data are computer-aided adjusted by the operator. The maximum shows the ﬂow chart of the mean-line model, detailing the over-
total-to-staticturbineefﬁciencyof83%wasachievedbyR134awith all procedure.
the turbine inlet total temperature and pressure of 420 K and Inputstothemean-linemodelconsistofpoweroutput;turbine
38 bars respectively. In most of the studies [27e30,32,33] that inlet total temperature and pressure; non-dimensional design pa-
carried out modelling of the radial turbine, the designer adjusted rameters, as loading and ﬂow coefﬁcients and geometry ratios.
the turbine design parameters iteratively in order to achieve With the provided input parameters shown in Table 2 and the
desirable results and if unacceptable the process was repeated. initial guess of stage total-to-static efﬁciency, the preliminary
However, this procedure has several deﬁciencies as it does not design of the rotor is carried out. Based on the calculated velocity
necessarily assure that the optimum combination of the turbine trianglesandbasicgeometryoftherotor,theoverallcharacteristics
design parameter is achieved for the maximum turbine perfor- for the remaining components such as the nozzle and volute are
mance and it is extremely reliant upon the designer experience. determined. Using these results and the loss correlations, the
Moreover, it does not consider a broad range for the input param- model determines a more accurate estimate of the turbine stage
eters which increases the possibility of overlooking the best solu- efﬁciency. This value is then used as an initial guess for the efﬁ-
tion. In addition, the results should be manually checked by the ciency and the process is repeated until convergence is achieved.
designertoensurethattheapplicationandgeometricalconstraints For high expansion ratios that can lead to a choked nozzle and/or
are satisﬁed. rotorthroat, the mean-line code includes an additional subroutine
Theaimofthisstudyistodeveloparobustmethodologyforthe toaddressthiseffect.Themean-linemodelisimplementedintothe
true optimization of a small-scale radial turbine performance that EES(engineeringequationsolver)software[35].Thisallowstheuse
can be used for ORC applications. This approach integrates the of its extensive and reliable thermodynamic property functions
mean-line modelling, real gas formulation and GA to carry out the throughout the model and accurately predicts the real gas behav-
modelling of the radial turbine for eight organic ﬂuids and allows iour of workingﬂuids during the expansion.
the optimization of the radial turbine efﬁciency based on a wide
range of turbine ﬂuid dynamics design parameters. Imposing the 3.1. Rotor modelling
geometrical and aerodynamics constraints on the optimization
process guarantees the manufacturabilityof the optimized turbine Themodellingofradialturbinerotorisoutlinedintheliterature
geometry with minimum aerodynamic losses. In addition, unlike [36e40], based on the ideal gas relations and with very high tur-

[27e30,32] that used a conventional rotor with zero inlet blade bine inlet temperatures (600e1000 C) suitable for gas turbine
698 K. Rahbar et al. / Energy 83 (2015) 696e711
Table 1
Properties of the selected organic workingﬂuids.
Fluid Molecular weight T (K) P (kPa) T (K) GWP (100 yr) ODP Atmospheric Safety data Reference
cr cr nbp
(g/mol) life time (yr) (ASHRAE 34)
R245fa 134.05 426 3610 288.14 950 0 7.2 B [32]
1
R134a 102.03 374 4060 246.9 1430 0 14 A [32]
1
R123 152.93 456 3660 300.8 77 0.02 1.3 B [32]
1
R236ea 152.04 412 3410 279.34 1200 0 8 e [32]
R152a 66.05 385 4450 249 124 0 1.4 A [32]
2
R236fa 152.04 397.9 3200 271.6 9810 0 240 A [32]
1
n-pentane 72.15 469 3360 309.1 ~20 0 0.01 A [2]
3
Isobutane 58.12 408 3640 261.3 ~20 0 0.019 A [32]
3

temperaturesofabout200 Corless,inwhichthesestressesarenot
severeandageneralshapeoftheturbinerotorcanbeadoptedwith
non-zeroinletbladeangle(b >0),aspresentedinFig.4b.This
4,blade
conﬁgurationobtainshighjvaluesinexcessofunitybylayingout
the inlet velocity triangle with positive b while preserving the
4
optimum incidence. Comparing Fig. 4a and b illustrates that the
latterconﬁgurationyieldsahigherlevel ofinlettangentialvelocity
(C ) with the same wheel speed and consequently higher speciﬁc
q4
work output is obtained as expressed by Eq. (1).
Dh ¼ U C U C (1)
actual 4 q4 5 q5
Dh (J/kg) is actual enthalpy drop, U (m/s) and U (m/s) are
actual 4 5
thewheelspeedsattherotorinletandexit,C (m/s)andC (m/s)
q4 q5
are the tangential absolute velocities at the rotor inlet and exit
respectively.
ThenthevelocitytrianglesshowninFig.4baredeterminedboth
at the rotor inlet and exit using equations (1)e(3) and the input
data given inTable 2.
Fig.1. Temperature - entropy diagram of the selected ﬂuids.
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Dh
actual
U ¼ (2)
4
engines and turbochargers. In these applications the simultaneous
j
existence of high centrifugal and thermal stresses limited the
design andgeometryof the rotor toberadial atthe inletwithzero
C ¼fU (3)
m5 4
inletbladeangle(b ¼0),asdepictedinFig.4a.Thisconstrains
4,blade
the rotor inlet velocity triangle to be aligned with the negative
j () is the loading coefﬁcient,4 () is theﬂowcoefﬁcient and
relative ﬂow angle (b ), for the optimum incidence and with a
4
C (m/s) is the absolute meridional velocity at the rotor exit. The
m5
loading coefﬁcient (j) value of less than unity [36,37]. On the
total and static thermodynamic parameters at the rotor inlet (S ,
t,4
contrary, the ORC turbine operates at much lower inlet
T ,r ,P ,T ,h ,r )arecalculatedusingEq.(4)togetherwiththe
t,4 t,4 t,4 4 4 4
Fig. 2. Enthalpy-entropy diagram of the turbine expansion (left), stage cross section (right).
K. Rahbar et al. / Energy 83 (2015) 696e711 699
Fig. 3. Flow chart of the mean-line model.
real gas EOS (equation of state), rotor inlet absolute and relative P (Pa) and P (Pa) are the total pressures at the turbine inlet
t,1 t,4
3
ﬂow velocities (C ,W ) and the adiabatic assumption (h ¼ h ). and rotor inlet respectively, r (kg/m ) is the turbine inlet total
4 4 t,4 t,1 t,1
densityandh ()istheturbinestagetotaltostaticefﬁciency.
stage,ts
 !
 
r Dh 1h Thetotal,relativetotalandstaticthermodynamicparametersatthe
t;1 stage;ts
actual
P ¼ P  (4)
t;4 t;1
rotoroutlet (S , T ,r ,P ,P , h ,T ,h ,r ) are determined
4h t,5 t,5 t,5 t,5 t,5,rel t,5,rel 5 5 5
stage;ts
using the isentropic expansion state point (h , S ¼ S ), actual
5s 5s 1
enthalpy drop, rotor outlet absolute and relative ﬂow velocities
Table 2
(C ,W ).Therotorexitﬂowareaisfoundbytheradiiratio(r /r )
5 5 5hub 4
Input variables of the mean-line model and their ranges/values.
given inTable 2 and Eq. (5).
Parameter Unit Range/value Reference
Power output kW 15 e
_
m
Loading coefﬁcient (j) e 0.6e1.4 [33,36,37]
A ¼ (5)
5
r fU ð1BK Þ
Flow coefﬁcient (4) e 0.2e0.5 [36,37]
5 4 5
Massﬂow rate (m) kg/s 0.55e0.85 [17,33]
2
_
A (m ) is the rotor exit area, m (kg/s) is theworkingﬂuid mass
5
Rotational speed (u) rpm 35,000e65,000 [17,33]
3
Rotor exit hub to inlet e 0.4e0.65 [36,37] ﬂow rate, r (kg/m ) is the rotor exit density and BK () is the
5 5
radii ratio (r /r )
5hub 4 blockage factor at the rotor exit.
Rotor exit absoluteﬂow deg 1515 [36,37]
The number of rotor blades is required for calculation of the
angle (a )
5
rotor losses and is obtained by the Glassman correlation [40] as
Inlet total temperature (T ) K 333e463 [17,27,33]
t,1
depicted in Eq. (6).
Degree of superheating K0e20 [6,33]
(DT )
superheating
Inlet total pressure (P ) kPa Corresponding saturated e
t,1
p
Z ¼ ð110a Þtanða Þ (6)
vapour pressure at the
rotor 4 4
30
selected temperature
Nozzle inlet to exit radii e 1.2e1.3 [36,37]
Z () is the number of rotor blades and a (degree) is the
rotor 4
ratio (r /r )
2 3
rotor inlet absoluteﬂowangle with respect to the radial direction.
700 K. Rahbar et al. / Energy 83 (2015) 696e711
Fig. 4. Schematic of the blade proﬁles and velocity triangles, zero inlet blade angle (a), non-zero inlet blade angle (b).
Therotorlossesareconsideredastotalenthalpydropandconsistof clearances respectively, C (m/s) is the absolute meridional ve-
m4
friction, secondary, tip clearance, exit kinetic, disk friction and locityattherotorinlet,b (m)andb (m)aretherotorinletandexit
4 5
incidence losses. The friction loss is determined by equations bladeheightsrespectively,l (m)istherotoraxiallength,r (m)
rotor,x 4
(7)e(8) with the Darcy friction coefﬁcient (f) modiﬁed to account and r (m) are the rotor inlet and exit tip radii respectively. The
5tip
for the curvature effects of radial turbines [39]. rotor exit loss is found by Eq. (11) [39].
 
2 32
2
W þW C
5tip 5hub
5
W þ
4 Dh ¼ (11)
2 exit
6 7 l
hyd
2
6 7
Dh ¼ f (7)
curve
friction
4 5
2 d
hyd
Dh (J/kg) is the enthalpy drop due to the wasted energy of
exit
exhaust and C (m/s) is the absoluteﬂow velocityat the rotor exit.
5
Eq. (12) depicts the disk friction loss [37].
sﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
" #
    0:05
2
 d 
d
hyd
0:25 4
3 2
f ¼ f 1þ0:075Re Re (8)
curve
k rU r
f
4 4
2r 2r
c c
Dh ¼ (12-a)
disk;friction
_
4m
Dh (J/kg) is the enthalpy drop due to the blade friction,
friction
 0:1
f () and f () are the radial turbine friction coefﬁcient and
curve
ε
b
r
4
Darcy friction factor respectively, W (m/s), W (m/s) and W
4 5tip 5hub
5
k ¼ 3:7 for Re<10 (12-b)
f
0:5
(m/s) are the relativeﬂow velocities at the rotor inlet and exit, l
hyd
Re
(m) and d (m) are the hydraulic length and diameter respec-
hyd
tively,r (m)isthemeanradiusofcurvature,d (m)istherotorinlet
c 4  0:1
ε
b
diameter and Re () is the Reynolds number. The secondaryloss is
r
4
5
k ¼ 0:102 for Re>10 (12-c)
obtained from Eq. (9) [39]. f
0:2
Re
2
C d
4
4 Dh (J/kg)istheenthalpydropduetotheﬂowleakageat
disk,friction
Dh ¼ (9)
sec ondary
3
Z r
rotor c thebackplateoftherotor,r(kg/m )istheaveragedensitybetween
therotorinletand exit and ε (m)is the rotorbackplate clearance.
b
Dh (J/kg)istheenthalpydropduetothesecondaryﬂows
secondary
The rotor incidence loss is obtained from Eq. (13) [39].
and C (m/s) is the rotor inlet absolute velocity. The tip clearance
4
loss is calculated by Eq. (10) [36].
2
W
q4
Dh ¼ (13)
incidence
3
2
pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
U Z  
rotor
4
Dh ¼ 0:4ε C þ0:75ε C 0:3 ε ε C C
x x r r x r x r
tip;clearance
8p Dh (J/kg)istheenthalpydropduetotheﬂowdisturbance
incidence
(10-a) (incidence angle) at the rotor inlet and W (m/s) is the relative
q4
tangential ﬂow velocity at the rotor inlet. However, the contribu-
  tionoftheincidencelossisverysmallatthedesignpointoperating
r
5tip
1
r
4 condition of radial turbines and can be neglected when evaluating
C ¼ (10-b)
x
the turbine overall losses [36,37].
C b
m4 4
3.2. Nozzle modelling
r 
l b
5tip rotor;x
4
C ¼ (10-c)
r
r C r b
4 m5 5 5
For the modelling of the nozzle, constant blade height equal to
Dh (J/kg) is the enthalpy dropdue tothe rotortipﬂow b was assumed along the length of vanes. The ﬂow absolute ve-
tip,clearance 4
leakage losses, ε (m) and ε (m) are the axial and radial tip locities and thermodynamic properties both at the inlet and exit
x r
K. Rahbar et al. / Energy 83 (2015) 696e711 701
SCr C ¼ r C (17)
1 q1 2 q2
_
m¼r A C ð1BK Þ (18)
2 2 m2 2
k () is the volute total pressure loss coefﬁcient, SC () is the
vol
swirlcoefﬁcient accounting for the effect of volutewall friction, A
2
2
(m ) is the nozzle inlet area, C (m/s) is the absolute meridional
m2
velocity at the nozzle inlet. k , SC and BK are assigned to 0.1, 0.9
vol
and 0.1 respectively, as suggested in Ref. [36]. Assuming a circular
cross section for the volute, the geometry and static thermody-
namic properties are determined iteratively with the known inlet
total thermodynamic properties given in Table 2 and equations
(16)e(18).
With the calculated turbine losses, the newestimate of turbine
total-to-static efﬁciency is determined using Eq. (19). The mean-
line model uses this new value as the initial guess of efﬁciency
and the process is repeated until convergence is achieved.
 
Dh
actual
h ¼ P (19)
stage;ts
new
Dh þ ðDh Þ
actual losses
The stage total-to-total efﬁciency, h (), speciﬁc speed, Ns
stage,tt
(),andvelocityratio,y(),areobtainedwithequations(20)e(22)
respectively.
h h
t;1 t;5
h ¼ (20)
stage;tt
h h
t;1 t;5s
pﬃﬃﬃﬃﬃﬃ
u Q
5
N ¼ (21)
s
0:75
 
Dh
actual
h
Fig. 5. Implementation of the GA optimization loop into the mean-line model. stage;ts
U
4
n¼pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ (22)
2Dh
are calculated iteratively using the conservation of mass and
ideal
angularmomentum,EOSandtheradiiratio(r /r )giveninTable2.
2 3
u (RPM (revolutions per minute)) is the rotor wheel rotational
In addition, Eq. (14) is used to determine the nozzle exit radius
3
velocity, Q (m /s) is the volume ﬂow rate at the rotor exit and
[37]. 5
Dh (J/kg) is the ideal enthalpy drop across the turbine.
ideal
r ¼ r þ2b cosða Þ (14)
3 4 4 3
r (m)isthenozzleexitradiusanda (degree)isthenozzleexit
3 3 4. Methodology for optimization with genetic algorithm
absolute ﬂow angle. The nozzle friction loss is determined by Eq.
(15) [37].
GAisbasedonthebiologicalevolutioninwhichapopulationof
individuals is chosen randomly from the range of independent
l
2
hyd
parameters and they are assessed based on their ﬁtness value. A
Dh ¼ 4fC (15)
friction;nozzle
d
new generation of individuals is generated in a stochastic manner
hyd
by breeding the selected members of the current population ac-
Dh (J/kg) is the enthalpy drop due to the friction in
friction,nozzle
cording to their ﬁtness value. The GA can deal with complex
nozzle passages and C(m/s) is the average of absolute velocities
optimization problems such as multi-dimensional, non-contin-
between the nozzle inlet and exit.
uous, and non-linear problems. The GA locates the global optimal
values reliably from a population of solutions even if many local
3.3. Volute modelling
optima exist and prevents the convergence to sub-optimal
Theratioofthevoluteinletareatoinletradius(A /r )shouldbe
1 1
a linear function of the azimuth angle in order totransfertheﬂuid Table 3
Turbine design parameters for validation.
uniformlyaroundtheperipheryofthenozzlering.However,dueto
the wall friction and boundary layer growth, the actual ﬂow de-
Parameter Unit Glassman case [40] Jones case [42]
viates from ideal and the mean-line model predicts the ﬂow
Workingﬂuid e Argon Air
considering pressure loss, swirl coefﬁcient and blockage using
Rotational speed (u) rpm 38,500 106,588
equations (16)e(18).
Inlet total temperature (T ) K 1083 1056.5
t,1
Inlet total pressure (P ) kPa 91.01 580.4
t,1
Massﬂow rate (m) kg/s 0.2771 0.33
P P
t;1 t;2
k ¼ (16)
Pressure ratio (total-to-static) e 1.61 5.73
vol
P P
t;2 2
Number of nozzle vanes e 16 19
Number of rotor blades e 12 16
702 K. Rahbar et al. / Energy 83 (2015) 696e711
Table 4
Comparison of the developed mean-line model with the published data.
Parameter Glassman case [40] Jones case [42]
Glassman RITAL [43] Present model Jones RADTURB [44] Present model
r [m] 0.09775 0.1033 0.09911 0.074 0.077 0.0748
2
b [m] e 0.01221 0.01219 0.00618 0.00554 0.0066
2
r [m] 0.07983 0.08268 0.08133 0.0635 0.06163 0.0643
3
b [m] e 0.01221 0.01219 0.00618 0.00554 0.0066
3
a [degree] 72 63.39 68.11 77.80 78.1 77.38
3
r [m] 0.0777 0.07874 0.07888 0.0582 0.0587 0.0588
4
b [m] e 0.01221 0.01219 0.00635 0.00554 0.0066
4
r [m] 0.01936 0.02362 0.02312 0.0152 0.01761 0.0154
5hub
r [m] 0.05542 0.05591 0.05546 0.0368 0.0445 0.0409
5tip
a [degree] 71.92 63.13 63.51 76.8 77.3 76.76
4
b [degree] 31.50 26.27 26.64 31.8 32.7 36.87
4
a [degree 0 0.01 0 0.03 0 0
5
b [degree] 54.90 55.58 53.76 57.40 57.3 55.8
5
h [%] 0.82 0.79 0.788 86 86 84.6
stage,ts
Power [W] 38,500 38,500 38,500 121,000 121,000 121,000
4 [-] e 0.4 0.4 e 0.2 0.28
j [-] e 0.8 0.8 e 0.85 0.85
solutions. This distinguishes GA from the traditional optimization Inthisstudy,theGAisemployedtomaximizetheturbinestage
techniques that reliant on the initial guesses, while the GA is far total-to-static efﬁciency (h ). Fig. 5 shows the implementation
stage,ts
less sensitive to the initial conditions enforced on it. The GA will of the GA optimization loop into the mean-line model. The initial
eventually reject any solution that does not show enough promise estimatesoftheinputparametersofTable2andstageefﬁciencyare
which helps to provide moreﬂexibility and robustness during the used to start the optimization, then the GA optimizes these values
optimization [41]. Detailed description of the GA is discussed in for the maximum objective function (turbine stage total-to-static
Ref. [41]. efﬁciency). The new optimized values of the input parameters
_
Fig. 6. Variation of 4 and j for 'R245fa', T ¼ 383 K, P ¼1570 kPa,m ¼ 0.8 kg/s, u ¼ 48000 rpm, r /r ¼ 0.4 and a ¼ 0.
t,1 t,1 5hub 4 5
K. Rahbar et al. / Energy 83 (2015) 696e711 703
_
Fig. 7. Variation ofm and u for 'R245fa', T ¼ 383 K, P ¼1570 kPa, 4 ¼ 0.35, j ¼ 1.2, r /r ¼ 0.4 and a ¼ 0.
t,1 t,1 5hub 4 5

and stage efﬁciency are used again as initial guesses and the pro-  b <65 .Thisvalueissetasanupperlimitformachinability.
4,blade
cess is repeated until convergence is achieved. Eq. (23) is used to calculate b with an optimum incidence
4,blade

angle (i)of 20 as suggested in Refs. [36,37].
4.1. Optimization constraints
The optimization algorithm is constrained so that the optimum
b ¼ b i (23)
turbinemeetsasetofcriticalaerodynamicandgeometricalcriteria 4;blade 4
simultaneously. These ensure high aerodynamic performance and
b (degree) and b (degree) are the rotor inlet blade angle
4,blade 4
manufacturability of the optimized turbine.
and relativeﬂow angle respectively.

 a < 82 . This value is established to allow for high loading co-
4
efﬁcients and a reasonable number of rotor blades. 5. Results and discussion
 Ma <1.Thisvalueissettoavoidsupersoniclossattherotor
5tip,rel
exit. 5.1. Validation of the mean-line model

 b > 73 . This value is established as the compromise be-
5rms
tween the minimum rotor exit absolute and relative Mach The developed mean-line model described in section 3 is vali-
numbers and machining requirements [36]. dated against two published benchmark cases with well-deﬁned
 b > 0.001 (m). This value is considered as the minimum size geometry and real test data for both real and ideal gases namely
4
technically achievable. theGlassmancase[40]andJonescase[42]respectively.Inaddition,
 d /d > 0.4. This value is set to avoid the possibilityofﬂow each case was further compared with two commercial radial tur-
5hub 5tip
blockage due to closely spaced vanes, as suggested in Ref. [38]. bine design software as RITAL [43] and RADTURB [44] from the
 d >0.015(m).Thisvalueisselectedastheminimumthreshold literature.Theturbinedesignparametersforbothcases[40,42]are
4
size for manufacturability. listedinTable3andaresetastheinputstothemean-linecodeand
 b /d < 0.15. This value is established to limit the rotor tip the results are shown inTable 4. The results in terms of the global
4 4
clearance loss. performance parameters, ﬂow features and the turbine principal
 r /r < 0.85. This limit is set to both accommodate the geometry are in a good agreement with the literature and the de-
5tip 4
expandingﬂuidwithsigniﬁcantdecreaseindensityandtoavoid viations are within the acceptable margin for both ideal and real
excessive tip curvature and the associated secondary loss. gases. The turbine total-to static efﬁciencies predicted by the
704 K. Rahbar et al. / Energy 83 (2015) 696e711
Fig. 8. Variation of r /r and a for 'R245fa', T ¼ 383 K, P ¼1570 kPa, 4 ¼ 0.35, j ¼ 1.2,m _ ¼ 0.8 kg/s, u ¼ 38197 rpm.
5hub 4 5 t,1 t,1
developed model are slightly lower than the published literature Figs.6e9showtheoutputofsuchparametricstudiesusingR245fa
due tothe effectof exhaust diffuser which is notconsidered in the as the workingﬂuid.
present model. Thus the mean-line model is considered validated Fig.6showstheeffectofﬂow(4)andloading(j)coefﬁcientson
for the preliminary design and modellingof radial turbines for the the turbine stage total-to-static efﬁciency (h ), the rotor inlet
stage,ts
ORC applications for both real and ideal gases. diameter(d ),therotortipradiusattheexittotherotorinletradius
4
(r /r )andtherotorbladeinletangle(b ).AsshowninFig.6a
5tip 4 4,blade
andb,increasingjleadstoanincreaseintheturbineefﬁciencyand
5.2. Parametric studies and optimization results a reduction in the rotor size. This highlights the advantages of the
proposed conﬁguration with a non-zero inlet blade angle rotor,
The mean-line methodology shown in Fig. 3 and validated in whichallowslargejvaluesinexcessofunitytobeobtainedwhile
section5.1wasusedtocarryoutcomprehensiveparametricstudies preserving the optimum incidence at the rotor inlet. In contrast,
to investigate the effect of input variables listed in Table 2 on the increasing 4 is detrimental to the stage total-to-static efﬁciency,
turbine performance and geometry. These studies are based on h , which is more sensitive to the variation of 4; where
stage,ts
simultaneous variation of the two input variables in the range increasing4from0.2to0.5leadstoareductioninh byabout
stage,ts
shown in Table 2, while keeping the other inputs as constants. 14%.Ther /r andb affecttheshroudcurvatureandpassage
5tip 4 4,blade
K. Rahbar et al. / Energy 83 (2015) 696e711 705
_
Fig. 9. Variation of T and DT for 'R245fa', 4 ¼ 0.35, j ¼ 1.2, m ¼ 0.8 kg/s, u ¼ 38197 rpm, r /r ¼ 0.5 and a ¼15.
t,1 superheating 5hub 4 5
Fig.10. Comparison of the GA with traditional optimization techniques using R245fa working ﬂuid.
turning and the consequent friction and secondary losses. Fig. 6c
and d presents the effects of 4 and j on r /r and b
5tip 4 4,blade
Table 5
respectively. Higher values ofj results in larger r /r andb ;
5tip 4 4,blade
Control operators of the GA.
while 4 has an adverse effect.
Operator Value
Fig. 7 shows the effects of mass ﬂow rate and rotational speed
Maximum number of generations 500
on h , d r /r and b . Fig. 7aec show that increasing
stage,ts 4, 5tip 4 4,blade
Number of individuals 20
the mass ﬂow rate and rotational speed is advantageous in
Maximum mutation rate 0.26
enhancing the efﬁciency and reducing the rotor size; while they
706 K. Rahbar et al. / Energy 83 (2015) 696e711
ORC and with the constant power output the overall cycle efﬁ-
ciency will be reduced.
Fig. 8 shows the effects of the rotor exit absolute ﬂow angle
(a )andtheratioof rotorhub radiusattheexittotherotorradius
5
at the inlet (r /r)on h , d r /r , b , rotor exit area
5hub 4 stage,ts 4, 5tip 4 4,blade
(A ) and the ratio of exit and secondary losses to the total loss
5
(Dh /Dh and Dh /Dh ). Fig. 8aed shows that
exit loss,total secondary loss,total
the effect of r /r is more signiﬁcant on h and r /r
5hub 4 stage,ts 5tip 4
while it has a limited effect on d and b . In addition, a has a
4 4,blade 5
signiﬁcant effect on increasing the efﬁciency and reducing the
losses. As shown in Fig. 8a, better performance is achieved at
smaller (negative) a . This is due to the fact that at smaller
5
(negative)a andwithﬁxedmassﬂowrateandpoweroutput,the
5
rotor exit area (A ) is reduced as presented in Fig. 8e. At smaller
5
A the absolute meridional velocityat the exit (C ) increases and
5 m5
leads to higher exit losses as illustrated in Fig. 8f. However, with
Fig.11. Conversion history of the GA for the investigated ﬂuids.
Table 6
Optimized input variables of the mean-line model for 8 workingﬂuids.
Parameter R245fa R134a R123 R236ea R152a R236fa n-pentane Isobutane
T (K) 381.7 333 391.2 389.8 333 359.9 419 362.2
t,1
P (kPa) 1527 1677 1155 2224 1497 1454 1478 1610
t,1
DT (K) 0800 7 0 0 0
superheating
j () 1.034 1.064 1.072 1.041 1.004 1.181 1.079 1.215
4 () 0.265 0.236 0.236 0.2226 0.237 0.2603 0.225 0.248
m (kg/s) 0.844 0.85 0.763 0.85 0.789 0.7662 0.634 0.646
u (rpm) 38,502 43,439 35,160 43,124 40,574 36,029 43,430 39,849
r /r () 0.64 0.549 0.609 0.5949 0.616 0.6146 0.604 0.65
5hub 4
a (deg) 14.98 8.988 7.112 3.87 8.92 15 1.26 15
5
r /r () 1.3 1.2 1.238 1.227 1.24 1.201 1.203 1.241
2 3
an adverse effect on the r /r ratio. Fig. 7d shows that the blade smaller (negative) a the rotor exit relative ﬂow angle (b ) also
5tip 4 5 5
angle b is not affected by the changes in rotational speed decreases,resultinginlowerﬂowturningandconsequentlylower
4,blade
and ﬂow rate. Although increasing mass ﬂow rate results in secondarylosses, aspresentedin Fig. 8f. It is clear from thisﬁgure
higher efﬁciency, it requires larger amounts of input heat to the that the secondary losses have a larger contribution to the total
Table 7
Results of the mean-line optimization of 8 workingﬂuids.
Parameter R245fa R134a R123 R236ea R152a R236fa n-pentane Isobutane
a (deg) 82.7 83.3 83.9 84.5 82.4 83.98 83.5 83.6
2
a (deg) 80.6 82.2 82.2 82.6 81.6 82.01 82.5 82.5
3
a (deg) 79.9 81.44 81.55 82 80.9 81.35 81.9 82
4
b (deg) 6.3 13.45 17.83 19.3 8.569 36.96 28.1 45
4
b (deg) 13.7 33.45 37.83 39.3 11.43 56.96 48.1 65
4,blade
b (deg) 72.2 72.47 72.91 72.7 73 72.21 72.9 73
5,rms
U (m/s) 131.1 128.8 135.4 130.2 137.6 128.8 148.1 138.3
4
U (m/s) 98.6 91.63 100.1 95.12 102 95.45 108.4 104.8
5,rms
C (m/s) 130.6 135.1 143.8 138.3 136.1 147.1 161.9 162.6
4
C (m/s) 35.9 30.85 32.27 29.1 33.03 34.7 33.3 35.5
5
W (m/s) 23.1 20.7 22.2 20.5 21.8 27.68 26 32.1
4
W (m/s) 113.4 101.1 108.9 97.6 111.9 109.7 112.7 119
5,rms
Ma () 1.004 0.939 1.11 1.181 0.743 1.221 0.89 0.867
4
Ma () 0.177 0.1437 0.1714 0.175 0.119 0.2298 0.144 0.171
4,rel
Ma () 0.262 0.2082 0.2411 0.229 0.178 0.2743 0.174 0.181
5
Ma () 0.915 0.7925 0.921 0.882 0.679 0.9722 0.673 0.669
5tip,rel
d (m) 0.1041 0.0876 0.1109 0.0853 0.1047 0.0988 0.0991 0.103
1
d (m) 0.0913 0.0748 0.0976 0.0755 0.0887 0.0874 0.0852 0.089
2
d (m) 0.0702 0.0623 0.0789 0.0616 0.0716 0.0728 0.0708 0.0717
3
d (m) 0.065 0.0566 0.0736 0.0577 0.0648 0.0683 0.0651 0.0663
4
d (m) 0.0489 0.0403 0.0544 0.0421 0.048 0.0506 0.0477 0.0502
5,rms
b (m) 0.0039 0.0051 0.0048 0.0037 0.0057 0.004 0.0053 0.0051
4
b (m) 0.0068 0.0083 0.0088 0.0072 0.0075 0.008 0.0077 0.0065
5
r /r () 0.8495 0.8432 0.8494 0.8448 0.8477 0.8494 0.841 0.849
5tip 4
Z () 18 202021 192021 21
rotor
N () 0.432 0.4586 0.4222 0.423 0.439 0.407 0.406 0.363
s
y () 0.6632 0.6252 0.6235 0.631 0.6467 0.5925 0.6228 0.5873
h (%) 82.91 83.17 83.36 82.89 84 82.92 83.71 83.82
stage,ts
h (%) 85.45 85.04 85.21 84.54 86.02 85.05 85.37 85.75
stage,tt
K. Rahbar et al. / Energy 83 (2015) 696e711 707
Fig.12. Expansion process and thermodynamic properties of all ﬂuids at the optimized conditions.
losses(62%e68%)comparedtotheexitlosses(13%e16%)andwith efﬁciency;while increasingDT leadstoanincrease inthe
superheating
smaller (negative) a better performance is achieved. efﬁciency. However, increasing the degree of superheat requires
5
Fig. 9 presents the effect of turbine inlet temperature (T ) and larger amounts of heat input and with constant power output, the
t,1
the degree of superheating (DT )on h , d r /r and overall ORC thermal efﬁciency will be decreased. The effects of T
superheating stage,ts 4, 5tip 4 t,1
b . As shown in Fig. 9a, increasing T leads to a reduction in and DT are limited on the rotor geometry, as shown in
4,blade t,1 superheating
708 K. Rahbar et al. / Energy 83 (2015) 696e711
Fig.13. Contribution of the rotor aerodynamic losses for all ﬂuids at the optimized conditions.
Fig. 9b and d. It is only for r /r that the effects of these two pa- The GA optimization technique outlined in section 4 is then
5tip 4
rameters are considerable, as presented in Fig. 9c. employed to maximize h of eight organic ﬂuids with the
stage,ts
ItisclearfromFigs.6e9thattheinvestigatedvariablesshownin constraints shown in section 4.1. Three control operators, namely
Table2havesigniﬁcanteffectson theturbineefﬁciency;therefore, the number of individuals, number of generations and mutation
they have been used in the optimization process of the turbine rate, are used in the GAwith their values presented inTable 5.
stagetotal-to-static efﬁciency(h ) usingthe GA.Superiorityof Fig. 11 shows the GA convergence history of the investigated
stage,ts
the GAoptimization techniqueis highlighted bycomparingit with ﬂuids, indicating the effect of the ﬂuid type on the number of
threetraditionaloptimizationmethodsastheconjugatedirections, generations required to reach the optimum solution.
variable metric and Nelder-Mead simplex which are described in Table 6 showsthe optimised input variables foreach individual
Ref.[45] and the results areshown in Fig.10. With the same initial ﬂuidthatproducedthemaximumturbineefﬁciencyandtherelated
guessesofinputvariablesinallmethods,theGAyieldshighervalue results are shown in Table 7. It is clear from Table 6 that all ﬂuids
of the turbine stage efﬁciencycompared tothe other methods and have j values in excess of unity, leading to a highly loaded and
can reliably ﬁnd the global optimum without converging to the small turbine. The values of4 are in the rangeof 0.22e0.26, which
sub-optimal solutions. is the high efﬁciency region as shown in Fig. 6a. Almost all the
Fig.14. Assessment of the investigated ﬂuids at the optimized conditions.
K. Rahbar et al. / Energy 83 (2015) 696e711 709
Fig.15. Comparison of the optimized h from the present study with the literature for various ﬂuids.
stage,ts
investigated ﬂuids have negative a values, leading to lower sec- The optimized turbine stage total-to-static efﬁciencies (h )
5 stage,ts
ondarylossesandhigherefﬁciencies,ashighlightedinFig.8aandf. shown in Table 7 are compared with the published literature
Table7illustratesthatthemaximumtotal-to-staticandtotal-to- [17,27e29,32,33] which considered the modellingof radial turbine
total efﬁciencies vary between 82.89% to 84% and 84.54%e86.02% forORCandtheresultsareshowninFig.15.Itcanbeseenthatthe
respectively. Moreover, the values of the speciﬁc speed (N ) and proposed constrained optimization methodology produced signif-
s
velocity ratio (y) are close to 0.42 and 0.62 respectively. These icantly higher h compared to the other studies for a wide
stage,ts
values are in the optimal range of efﬁciency as suggested by the range of working ﬂuids. It is possible to achieve a maximum of
generalizedperformancecorrelationchartsforN andyreportedin 14.7% higher turbine efﬁciency when comparing n-pentane from
s
Refs. [36,37,46]. This conﬁrms the validity of the optimization the current study with reference [33] considering that both have
approach used in this work in order to achieve a highly efﬁcient similar turbine inlet total temperature of 420 K with inlet total
turbine. Furthermore, the values of N in Table 7 justify the exclu- pressuresof14.8barsand10barsrespectively.Suchresultsexhibit
s
sion of the diffuser from the analysis. Following the discussion in the effectiveness of the constrained optimization of the radial tur-
Ref.[36],forN valuessigniﬁcantlylowerthan0.7,whichisthecase bine to achieve high performance.
s
of present study, the gain of using the diffuser is much less and
implementation of the diffuser requires an economic analysis 6. Conclusions
which is beyond the scope of the present study.
Fig. 12 presents the thermodynamic properties during the Limited literature is available regarding the design and perfor-
expansion process of all investigated ﬂuids at the optimum oper- mance optimisation of small-scale radial turbines that can be used
ating conditions. The turbine inlet conditions of all ﬂuids are in the ORC for power generation. This work presents an integrated
saturated vapour and it is only for R152a and R134a that a mini- mathematical approach for the development of an efﬁcient and
mumdegreeofsuperheatingisobtainedtoavoidsaturatedmixture compact small-scale radial turbine. This mathematical approach
condition after the expansion. integrates the mean-line modelling with real gas formulation and
Fig.13 shows the contribution of the rotor aerodynamic losses the GA optimisation technique. Such approach allows for con-
included in the model at the optimized conditions. The proportion strained optimisation based on a wide range of geometrical and
of the losses is almost similar forallﬂuids with the secondary loss operating parameters, such as: non-dimensional parameters,
having the highest contribution, varying from 59% to 66%. This is rotational speed and inlet temperature in order to achieve highly
due to the large passage turning obtained from high b and efﬁcient radial turbine.
4,blade
b values listed in Table 7. From the mean-line modelling investigation, it was found that
5,rms
Fig. 14 shows the variation of other critical geometry and the turbine geometry was more sensitive to the variation of the
operatingparameters,suchas:the rotorinletMachnumber(Ma ), loading coefﬁcient (j), the rotational speed (u), the massﬂowrate
4
therotorinletdiameter(d ),overallsize(d ),theturbineinlettotal and the turbine exit geometry ratio (r /r ); while all input vari-
4 1 5h 4
temperature (T ) and the rotational speed (u) with organic ﬂuids ables have a considerable effect on the turbine's performance. The
t,1
at their optimum conditions. As shown in Fig. 14a, R245fa, R123, constrained GA optimization of the turbine stage total-to-static
R236ea and R236fa have supersonic expansion at the rotor inlet, efﬁciency showed that a maximum efﬁciency of 84% can be ach-
which can lead to undesirable shock losses and requires a ieved with R152a. However, R152a required some level of super-
convergent-divergent nozzle arrangement. Fig.14b shows that the heating to prevent a saturated mixture condition after the
minimum rotor and overall diameter are achieved by R134a and expansion.Ifsuperheatingistobeavoided,isobutaneexhibitedthe
R236ea; while R123, R152a and R245fa have the largest size. As highestefﬁciencyof83.82%withahighstageloadingcoefﬁcientof
illustratedinFig.14c,n-pentane,R123andR236eahavethehighest 1.215 and consequently a lower mass ﬂow rate compared to the
inlet temperature and require larger amounts of heat input, other ﬂuids. These predicted efﬁciencies are considerably higher
compared to the other ﬂuids. The minimum rotational speed is thanthoseoftheconventionalradialturbinesandscrollexpanders
obtained by R123, R236fa, R245fa and isobutane, as shown in studied previously in the literature; which highlights the potential
Fig.14d. ofthisapproach.Duetothelackofexperimentaldatafortheradial
710 K. Rahbar et al. / Energy 83 (2015) 696e711

radial-inﬂow turbine through multi-objective optimization algorithm
a,b,* a a
Ayad M. Al Jubori , Raya Al-Dadah , Saad Mahmoud
a
The University of Birmingham, School of Engineering, Edgbaston, Birmingham, B15 2TT, UK
b
University of Technology, Baghdad, Iraq
article info abstract
CFX and Design Exploration based on 3D RANS with a k-omega SST turbulence model. The 3D optimi-
Accepted 4 May 2017
zation technique combines a design of the experiment, a response surface method and multi-objective
Available online 8 May 2017
method. The optimization of the blade geometry was performed using 20 design points for both
nozzle and rotor blades, based on the B-splines’ technique to represent the blade angles and thickness
Keywords:
distribution. The number of blades and rotor tip clearance were included as design parameters. The
Small-scale radial-inﬂow turbine
isentropic efﬁciency and power output were introduced as an optimization objective with two organic
Preliminary mean-line design
working ﬂuids, namely isopentane and R245fa. The results of the optimized geometry with R245fa
ORC
3D CFD optimization showedthattheturbine'sandcycle'sthermalefﬁciencieswerehigherby13.95%and17.38%respectively,
Multi-objective genetic algorithm
compared with a base-line design with a maximum power output of 5.415 kW. Such methodology is
proved to be effective as it allows the enhancing of the turbine's and the ORC's system performance
throughout to ﬁnd the optimum blade shape of the turbine stage.
1. Introduction thermodynamic analysis has been conducted in Refs. [8e10].In
aforementioned studies, the thermodynamic analysis model of
Oneofthemajorchallengesintheworldtodayistheincreasing ORC's systemwas performed based on the assumption of constant
energy demand. Therefore, more attention is dedicated to energy turbineisentropicefﬁciencyfordifferentworkingﬂuidandvarious
saving and reduction of environmental pollution and fossil fuel operating conditions that lead inaccurate ORC performance.
consumption byexploiting renewable energysources. The Organic In an ORC system, the turbine's efﬁciency has a signiﬁcant in-
Rankine Cycle (ORC) provides electricity from low-temperature ﬂuence on the ORC system's performance. For small-scale power
heat sources including renewable energy (i.e. solar and generationusingORC,radial-inﬂowturbineisconsideredasuitable
geothermal) and low-grade waste heat. In this scenario, the ORC choice as reported in literature. A number of studies using radial-
systems offer potential for generating electricity for wide range of inﬂow turbine based on various approaches are summarized in
applications including domestic and remote off-grid communities. Table 1 using the mean-line design approach and CFD analysis for
ManyresearcheshavebeencarriedoutregardingORCsystemby developing radial-inﬂow turbine.
focusing on thermodynamic analyses, optimization and the selec- Although there have been a number of recent attempts to
tion of a suitable working ﬂuid for the cycle for low-temperature develop the turbine's performance using computational ﬂuid dy-
heat source applications, including solar thermal energy as re- namictechniqueslikeANSYSCFX/Fluent,alimitedamountofwork
ported in Refs. [1e3]. While, several studies focused on thermo- has involved using optimization techniques to optimize the blade
economic, and ORC system analysis driven by geothermal energy geometrytoimproveboththeturbine'sandORC'sperformance.Al
[4e7]. In terms of low-grade waste heat sources, ORC Juborietal.[28]presentedanewmethodologythatcoupled1D,3D
CFD analysis and optimization of a small-scale axial turbine with
ORC system modelling, based on a multi-objective genetic algo-
* Corresponding author. The University of Birmingham, School of Engineering,
rithm(MOGA)withsixdifferentworkingﬂuids.Theiroptimization
Edgbaston, Birmingham, B15 2TT, UK.
results exhibited that the maximum turbine and cycle efﬁciencies
Table 1
Number of studies using various investigation approaches for axial and radial-inﬂow turbine.
Authors Investigation approach Expander type Heat source Workingﬂuids Performance (turbine
temperature (K) efﬁciency,
power, cycle efﬁciency)
Sauret& Rowlands [11] 1D mean-line design Radial-inﬂow 413 R134a, R143a, R236fa, 77%, 338 kW
R245fa
Hu et al. [12] 1D mean-line design Radial-inﬂow 363 R245fa 82.3%, 66.9 kW, 5.5%
Rahbar et al. [13] 1D design and optimization Radial-inﬂow 362.2 8 workingﬂuids 84%, 15 kW
Fiaschi et al. [14] 1D design Radial-inﬂow 420 6 workingﬂuids 78.8%, 50 kW
Song et al. [15] 1D design Radial-inﬂow 393.15 7 workingﬂuids 81%, 249.2 kW, 8.7%
Pan and Wang [16] 1D optimization and analysis Radial-inﬂow 363 14 workingﬂuids 75.75%,8.15 kW, 5.2%
White and Sayma [17] 1D optimization and analysis Radial-inﬂow 390 15 workingﬂuids 80%, 7.23 kW, 7.26%
Cho et al. [18] CFD analysis for nozzle only Radial-inﬂow 393 R245fa 53%, 3.8 kW, 6.25%
Al Jubori et al. [19] 1D and 3D CFD analysis Axial and radial-inﬂow 360 R141b, R1234yf, R245fa, 83.48%, 8.507 kW, 10.60%
with ORC modelling turbine n-butane and n-pentane
Nithesh and Chatterjee 1D and 3D CFD analysis Radial-inﬂow 297.5 R134a 70%%, 2 kW
[20]
Russell et al. [21] 1D design and 3D CFD Radial-inﬂow 423 R245fa 76%, 7 kW
simulation for rotor only
Fiaschi et al. [22] 1D design and 3D CFD Radial-inﬂow 395.9 R134a 68.04%, 4.504 kW
simulation
Li& Ren [23] 1D, 3D design and simulation Radial-inﬂow 393 R123 84.33%, 534 kW
Zheng et al. [24] 1D, 3D design and simulation Radial-inﬂow 360 R134a 81.6%, 643 kW
Pei et al. [25] Experimental Radial-inﬂow 373 R123 65%, 1.36 kW, 6.8%
Kang [26] Experimental Radial-inﬂow 358.4 R245fa 78.7%,32.7 kW, 5.22%
Sung et al. [27] Experimental Radial-inﬂow 413 R245fa 177.4 kW and 9.6%
and power output with R123 were about 88%, 10.5% and 6.3 kW in the ORC system modelling and performance. According to their
respectively. Rahbar et al. [29] carried out 3D optimization of the thermo-physical properties, the working ﬂuids have a strong in-
transonicrotorofatwo-stageradial-inﬂowturbineusingagenetic ﬂuence on the ORC system's efﬁciency, the expander's perfor-
algorithm (GA) with R245fa as the working ﬂuid. Their optimiza- mance, the components' size, the system's stability, safety and
tionresultsshowedthatthemaximumturbineisentropicefﬁciency economicfeasibilityandthe environmental concerns[30].Organic
was of 88% with a maximum power output of 26.35 kWand cycle working ﬂuids have large molecular weights and a low boiling
efﬁciency about 14.8%, with a pressure ratio of 10 and total inlet temperatureandpressureandareusuallyheavymixtures.Basedon
temperature of 405.3 K. theirslopes of saturationvapourontheT-s diagram, organicﬂuids
There is limited literature concerning the design and 3D CFD are categorized into dry, isentropic and wet ﬂuids. For low-
analysis and optimization of small-scale radial-inﬂow turbine for temperature heat sources, the dry and isentropic working ﬂuids
ORC's systemwith power output around 5 kW for different power withpositiveandzeroslopes(dT/ds)arefavourablebecauseof the
generationapplications,suchassmallbuildings,ruralareas,off-grid elimination of the need for the superheating condition after the
zones and isolated installations. Therefore, three-dimensional CFD expansionprocess, as presented in Fig.1a. Also, the workingﬂuids
optimization using multi-objective optimization for a small-scale should have lowﬂammability and corrosion features and be envi-
radial-inﬂow turbine stage (nozzle and rotor) is novel and has ronmentally friendly, with zero ozone depletion potential (ODP)
only received limited investigation previously. New methodology and low global warming potential (GWP); as clariﬁed in Table 2
for integrating the mean-line design, 3D CFD analysis, and multi- which illustrates the properties of the selected working ﬂuids.
objective optimizationwith ORC modelling has beenpresented for Due to the selection of working ﬂuids is a main challenge for ORC
the small-scale radial-inﬂow turbine (RIT) stage. Furthermore, it turbines designers and it is based on an acceptable balance be-
seeks to ﬁll the gap by investigating the turbine's performance in tween the aforementioned criteria, environmental concerns, ther-
both design and off-design conditions for baseline and optimum modynamic performance, commercial availability and cost.
designcaseswithtwoorganicworkingﬂuids.Themean-linedesign Therefore, isopentane and R245fa are selected based on these
of the RIT and ORC modelling is developed using the engineering criteria and is recommended in literature as a suitable working
®17
equation solver (EES) software; ANSYS -CFX is employed to pre- ﬂuid for low-temperature heat sources application [30,31].
dict the 3D viscous ﬂow and turbine performance. The real gas The main ﬁve components of a recuperative ORC are the
formulationoftheworkingﬂuidsisappliedtoperformanaccurate
prediction of the real behaviour of the workingﬂuids in a turbine/
ORCmodelusingtheREFPROPdatabase.TheCFDbaselinedesignof
®17
the RIT is optimized using the ANSYS -Design Exploration pack-
age for 3D optimization purposes, based on a multi-objective ge-
netic algorithm (MOGA). The optimized turbine performance
(isentropic efﬁciency and power output) for each organic working
ﬂuid is inserted into the ORC model to determine the best cycle
efﬁciency. The inclusion of constraints in the optimization tech-
nique allows for achieving the highest efﬁciency from optimized
geometry withoutexceedinginputoperating conditions.
2. Working ﬂuids selection and ORC system modelling
Fig. 1. Working ﬂuids T-s diagram (a), layout of recuperative ORC system (b)
The selection of the organic workingﬂuid is an essential aspect organicORC.
A.M. Al Jubori et al. / Energy 131 (2017) 297e311 299
Table 2
Thermodynamic property of sixorganicﬂuids.
Working Fluid Mol. weight (g/mol) T (K) T (K) P (kPa) ODP GWP (100 yr)
nbp cr cr
Isopentane 72.149 300.98 460.35 3378 0 20
R245fa 134.05 288.14 426.16 3610 0 950
evaporator, turbine, recuperator, condenser and a pump, as shown nozzleandrotor,asshowninFig.2a;andthevelocitytrianglesand
in Fig. 1b. In this study, a subcritical ORC cycle is investigated to basic geometry of the rotor as shown in Fig. 2b, which is imple-
avertthesafetyconcernsofthesystemcomplexityofhigh-pressure mented in accordance with the methodology outlined in
systems. The losses of heat and pressure through the ORC piping Refs. [33,34]. The ﬂow and loading coefﬁcients (f,j) are required
system are neglected with assumption of steady state operating and assumed in order to give the best efﬁciency. The rotor inlet

conditions. Heat added from the low-temperature heat source is blade angle (b ) is assumed equal to 70 as recommended by
4
given by: Ref.[34].ThepresentmodeloftheRITrotorincludesthelossesdue
to incidence, passage, trailing edge, tip clearance and windage,
_
_
Q ¼ mðh h Þ (1) which are detailed in Table 4. The number of rotor blades is
e 1 6
calculated based on theﬂowing empirical correlation [35]:
Net power output from the ORC cycle is given by:
p
Z ¼ ð110a Þtanða Þ (5)
_ _ _
rotor 4 4
W ¼ Wh h W (2)
net t p
mech gen 30
Based on the literature, the rotor losses are more signiﬁcant in
where h and h are mechanical efﬁciency and generator
mech gen
comparison with nozzle losses. Thus, the isentropic expansion is
efﬁciency.
assumedthrough the nozzle.The nozzle geometry was carried out
ORC thermal efﬁciency is given by:
for matching dimensions as in the following equation:
_
W
net
Dr¼ 2b cosða Þ (6)
h ¼ (3) 3 4
th
_
Q
e
where b is the nozzle blade width.
3
Thesecondlawefﬁciencycanbedeﬁnedastheproximityofthe
The number of blades is calculated through the following rela-
realthermalefﬁciencyofthecycletotheCarnotcycleefﬁciencyas:
tionship and the nozzle solidity equals to 1.35 as reported in
Glassman [35]:
_
h W
net
th
 
h ¼ ¼ (4)
seceff
h T
_ L
Carnot Q 1 2pr
e 1
T
H
Z ¼ (7)
Nozzle
1:35
The input parameters values of ORC system modelling are pre-
The friction loss through the nozzle is calculated as [34]:
sented in Table 3 in terms of heat source and heat sink tempera-
tures and massﬂow rate with two workingﬂuids.
l
hyd
Dh ¼ 4$f $C (8)
friction nozzle
d
hyd
3. Radial-inﬂow turbine (RIT) design
The performance of the RIT turbine is estimated based on the
losses of rotor, nozzle and volute in terms of enthalpy drop, which
Theﬁrstandcrucialstepof thewholeturbinedesignprocedure
allows the calculation of turbine isentropic efﬁciency within a
is the preliminary mean-line design (PD) to create the correct
highlycomprehensiveiterativeprocess.Thetotal-to-totalefﬁciency
aerodynamicdesignthatdeliversthedesiredoutput.ThePDofthe
and total-to-static efﬁciency are deﬁned as the following [36]:
RITis basedonone-dimensional (1D)mean-lineﬂowanalysis. The
mean-line model refers to the mid-span values of the blades' pas-
Dh
actual
h¼ P (9)
sage and only focuses on the inlet and outlet condition of each
Dh þ ðDh Þ
actual total losses
blade's passage. Its approach allows fast prediction for ﬂuid-
dynamic development and thermodynamic process inside the
blade'spassage.Theaimofmean-linedesignistodelivertheinitial
3.1. Input/output of RIT preliminary mean-line design
dimensions of the turbine's and blade's shape such as hub and tip
diameters,chordlength,bladepitch,numberofblades,thicknessof
In this paper, the PD code is developed using EES software
leading edge and thickness of trailing edge [32].
(engineering equation solver software) [39]. The EES code has the
TheRITstageconsistsofthreemainparts,whicharethevolute,
ability to deliver a wide range of RIT conﬁgurations by accom-
plishing comprehensive studies in terms of different input pa-
rametersandworkingﬂuids,asofferedinTable5.Theoutputofthe
Table 3
PD methodology delivers the turbine layout in terms of turbine
The input parameters of the ORC model.
diameters and blade dimensions and shapes and is presented in
Parameters Unit Value
Table 6. Theﬂow chart of the PD code is presented in Fig. 3 where
Heat source temperature K 365
the PD methodology is a highly iterative procedure.
Heat sink temperature K 293
Pump efﬁciency e 0.75
4. 3D CFD analysis
Generator efﬁciency e 0.96
Mechanical efﬁciency e 0.96
Recuperator effectiveness e 0.8
The actual conﬁguration of the ﬂow inside the turbine stage is
Workingﬂuid massﬂow rate kg/s 0.3
particularlycomplex; so it requires a high-ﬁdelity model based on
300 A.M. Al Jubori et al. / Energy 131 (2017) 297e311
Fig. 2. Stage cross section (left), velocity triangles and rotor blade proﬁle (right) [19].
Table 4
Losses' modelling of RIT rotor.
Type of losses Correlation Reference
2
w
Incidence loss [36]
q4
Dh ¼
incidence
2
3 2
K rU r
Disk friction f [34]
4 4
Dh ¼
disk friction _
4m
2 3
Friction loss   [37]
w þw
5tip 5hub
w þ
4 l
2 hyd
4 5
Dh ¼ f
friction curve
2 d
hyd
2
C $d
Secondary loss 4 [34]
4
Dh ¼
secondary Z $r
rotor c
3 pﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
U $Z
Tip clearance loss rotor [38]
4
Dh ¼ ð0:4$ε $C þ0:75$ε $C 0:3 ε $ε $C $C Þ
x x r r x r x r
tip clerance 8p
3 2
Windage loss r$U $r [33,36]
4 4
Dh ¼ K
windage f 2
2$m _$w
5
2
Exit energy loss [37]
Dh ¼ 0:5C
exit
5
Table 5
Input parameters of PD for radial-inﬂow turbine and their ranges/values.
Parameters Values/Ranges
Loading coefﬁcient (J)() 0.6e1.8
Flow coefﬁcient (f)() 0.1e0.6
Nozzle RIT inlet to exit radius ratio (r /r )() 1.2e1.3
2 3
Rotor RIT exit hub to inlet radius ratio (r /r )() 0.4e0.6
5hub 4
Rotor exit absoluteﬂow angle (a ) (degree) 10
5
Blade speed ratio () 0.7
Rotational speed (rpm) 20000e40000
Inlet total temperature (k) 365
Inlet total pressure (bar) Corresponding saturated vapour pressure at inlet temperature
Massﬂow rate (kg/s) 0.3
Workingﬂuids isopentane, R245fa
an adequately complex ﬂow scheme. Therefore, the integrated forthebladepassagebasedonthebladeangle,andthetypeof the
methodology between the low-ﬁdelity model (i.e. PD model) and leading and the trailing edge. The grid independency study is per-
the high-ﬁdelity model (i.e. 3D CFD model) is essential to directly formed for the base-line design for each working ﬂuid to ensure
predictthemostrelevantﬂowfeatures(3D,turbulentandunsteady thattheCFDresultisindependentofthenumberofmeshnodesfor
ﬂow etc.). Consequently, the main turbine stage's geometric char- each working ﬂuid. Two key decision parameters (i.e. turbine
þ
acteristics from the mean-line design (i.e. exit tip and hub radii, isentropic efﬁciency and dimensionless distance from the wall y )
inlettipdiameter,bladeheightandangles),aslistedinTable6,are areconsideredinthemeshdependencystudy.Themeshisreﬁned
exportedasinputsintothebladedesignmoduletogeneratethe3D and the CFD simulation is re-run and repeated until the mesh in-
®17
geometryofthenozzleandrotorblades,usingANSYS -BladeGen dependence solution is reached, as displayed in Fig. 5. As shown,
software as shown in Fig. 4. The pressure/suction and angle/thick- the CFD solution becomes independent of the mesh elements
nessmodesareemployedincreatingthe3Dbladegeometryofthe numberafter950,000elements.The3DRANSequationswiththek-
®17
nozzle and rotor respectively. The second phase is the computa- uSSTturbulence model equations are solvedusing ANSYS -CFX.
tionalgridgenerationthatisautomaticallyconstructedthroughthe The k-u SST has the advantage of using automatic near-wall
®17
ANSYS -TurboGrid. The structured grid has hexagonal elements treatment by locating the dimensionless distance (yþ) for the
based on the OeH grid. Automatic topology and meshing (ATM ﬁrst node after thewall to capture the turbulence closure. The k-u
optimized)isappliedtoallowthechoiceofanappropriatetopology SST turbulence model is considered for ﬂow separation under an
A.M. Al Jubori et al. / Energy 131 (2017) 297e311 301
Table 6
Output of PD radial-inﬂow turbine mean-line design for each workingﬂuid.
Parameter Workingﬂuids
isopentane R245fa
a (degree) 72.33 66.78
4
b (degree) 48.91 9.75
4
b (degree) 68.91 59.75
blade,4
b 65.47 60.65
tip,5
d (mm) 63.47 50.65
1
d (mm) 55.23 44.09
2
d (mm) 43.81 34.35
3
d (mm) 41.31 31.85
4
d (mm) 30.27 22.48
tip,5
b (mm) 2.84 2.05
4
b (mm) 21.72 14.51
5
Z 15 11
rotor
Nozzle LE beta angle (degree) 20 15
Nozzle TE beta angle (degree) 68 63
Nozzle stagger angle (degree) 33 38
Nozzle throat (mm) 3.481 3.926
Z 29 33
nozzle
adverse pressure gradient, which accounts for the transport of the
turbulent shear stress. Turbulence intensity at the inlet was
maintainedat5%astherecommendedvalue,whennoinformation
was available about the inlet's turbulence as suggested in CFX-
Solver modelling guide [41]. The k-u transport equations carried
outtoﬁndtheturbulentkineticenergyandthespeciﬁcdissipation
rate are:
 !
v v v vk
ðrkÞþ ðrkuÞ¼ G þG Y þS (10)
i K
k k k
vt vx vx vx
i j j
 !
v v v vu
ðruÞþ ðruuÞ¼ G þG Y þS (11)
u u u
i k
vt vx vx vx
i j j
where G and Gu represent the generation of turbulent kinetic
k
energyanditsdissipationrate;Y andYurepresenttheﬂuctuating
k
dilationincompressibleturbulence;S andSuarethesourceterms
k
of the k-u turbulence model.
The 3D CFD model of the small-scale radial-inﬂow turbines is
considered as steady state 3D viscous, turbulent and compressible
and a single phase ﬂow is applied. Also, all the walls are set to be
smooth, non-slip and with adiabatic conditions. The inlet boundary
conditionsatthenozzleinletaresetastheinlet'stotalpressureand
temperature, with static pressure at the rotor outlet. In order to
deliveraconnectionacrossthestationaryandrotatingblades'rows,a
Fig. 3. Flow chart of the preliminary mean-line design.
stage interface (mixing-plane) model is chosen at the nozzle-rotor
interface and generalized grid interface (GGI) is applied for stage
(mixing-plane)analysis andthesteadystateﬂow.Inallthe CFDan-
efﬁciency and power output (i.e. the global performance parame-
alyses,thehighresolutionadvectionschemewasappliedduetothe
ters) are ingoodagreement withtheGlassmancase andVistaRTD
topology and type of mesh, alongside its numerical stability. The
and the deviations were within the acceptable margin for all
convergence was achieved; the maximum RMS was normally no
workingﬂuids as veriﬁed in Fig. 6.
5
higher than 10 (i.e. 1e-5) in all simulations for the mass, mo-
Also,the3DCFDresultsarevalidatedwitharealcase(Jonescase
mentum, energy, and turbulence model. For the most engineering
[42]);consideringthatmostoftherange/valuesinTable7aretaken
applications, the RMS of 1e-5 is considered as a good convergence
from the above-mentioned reference and allow a fair comparison.
and sufﬁcient as recommended in CFX-Solver modelling guide [41].
Fig. 7 presents the comparison results with ref. [42] which only
®17
Therealﬂuidproperties'databaseinANSYS -CFXisintegratedwith
mentions the details of the radial-inﬂow turbine geometry and
REFPROP software, to obtain an accurate thermodynamic model of
operating conditions. The maximum deviation in terms of isen-
theorganicﬂuids'propertiesinthe3DCFDanalysisofORCturbines.
tropic efﬁciency was about 5.01%.
The developed preliminary mean-line design (PD) of the RIT
(detailed in section 3) is validated against a published benchmark
case, namely the Glassman case (i.e. code) as detailed in Ref. [40] 5. Three-dimensional optimization methodology
and ANSYS-Vista RTD (1D radial-inﬂow turbine) design software.
The PD and CFD base-line design results in terms of total-to-total The current optimization technique combines the design of
302 A.M. Al Jubori et al. / Energy 131 (2017) 297e311
Rotor
Nozzle
Fig. 4. Three-dimensional radial-inﬂow turbine stage (left), meshes generation for nozzle and rotor blade passage (right).
Table 7
Details of the radial-inﬂow turbine geometry of a real case (Jones case [42]).
Parameters Value Parameters Value
r (m) 0.074 b (degree) 31.8
2 4
r (m) 0.0635 b (degree) 57.40
3 5
r (m) 0.0582 Z ()19
4 nozzle
r (m) 0.0368 Z ()16
5tip rotor
r (m) 0.0152 Nozzle chord(m) 0.0229
5hup
b (m) 0.00618 Rotor chord (m) 0.0457
4
3
b (m) 0.00635 Radial Clearance (m) 0.23 10
5
3
a (degree) 77.80 Nozzle trailing edge thickness (m) 0.51 10
3
3
a (degree) 76.8 Rotor trailing edge thickness (m) 0.76 10
4
a (degree) 0.03
5
Fig. 5. The grid independency based on total-to-static turbine efﬁciency with R245fa
as the working ﬂuid.
Fig. 6. Comparison: the CFD and PD with Vista RTD and Glassman code in terms of
turbine isentropic efﬁciency (left) and power output (right).
Fig. 7. Comparison: the CFD efﬁciency with experimental efﬁciency of ref. [42].
experiment(DoE)technique,responsesurfacemethod(RSM)anda
multi-objective genetic algorithm (MOGA) through the ANSYS- construct the database of blade conﬁgurations that were initially
Design of Exploration (DE) that is integrated with 3D CFD anal-
tested by CFD. Also, DoE allows the delivery of the required infor-
ysis. In this methodology, the DoE is used to ﬁll the design space mationabout adesignpoint with aminimum numberof sampling
throughout to specify the location of sample design points that points. The DoE can be deﬁned as a provider of database informa-
detects their space distribution for the blade geometry as design tion to a meta-model (surrogate model) that indicates the de-
parameters in efﬁcient way and then feed the response surface pendencyof turbineperformanceonthevariationinbladeshapes.
method(RSM).Thedesignofexperiment(DoE)techniqueisusedto To achieve the equal distribution of the design parameters, the
A.M. Al Jubori et al. / Energy 131 (2017) 297e311 303
optimal-space-ﬁlling design (OSFD) technique in the DoE method Maximize/Minimize f (x)¼(f (x), f (x), f (x)….F (x)) (M
1 2 3 M
wasselectedthroughout the3DCFD optimization.TheOSFDis the functions to be optimized)
Latin hypercube sampling that is used with post-processing Subject to g(x) 0 (inequality constraints)
or j(x)¼ 0 (equality constraints)
throughout the design space within the DoE, aiming to achieve
maximumvisionwith a small numberof the sample designpoints
The blade geometry was parameterized as a unique B-spline
[43]. The surrogate model (i.e. RSM) strongly depends on the
samplepointswhichareobtainedfromtheDoE.ThereforetheDoE means composed of 20 control points, achieving through the DoE
techniqueahighdegreeofgeometricﬂexibilityandrobustness.The
was integrated with the 3D CFD optimization algorithm to deliver
an efﬁcient design space of the design points (i.e. maximum designpoints are the variables used to deﬁne the blade geometry;
while the turbine performance (i.e. turbine isentropic efﬁciency
discernment and minimum number of sample points) [44]. The
details of the 3D CFD optimization procedure are shown in Fig. 8. andpoweroutput)isdeﬁnedastheobjectivefunction.The3DCFD
The response surface method (RSM) was constructed as a sur- optimization is aimed to enhance the turbine performance by
rogate model for each objective function based on the CFD solu- modifying the blade geometry, as shown in Fig. 9, to reduce the
tions achieved at speciﬁed control design points. The RSM is a passage losses (i.e. entropy generation) and minimize the non-
methodologyofﬁttingapolynomialfunctionfordiscreteresponses uniformity of ﬂow (i.e. secondary ﬂow). The 3D optimization can
obtained from DoE. It reveals the association between response be achieved through the following steps: select the design points
(bladegeometry);deﬁnetheobjectivefunctionthatdeterminesthe
functions and design parameters. The second-order polynomial
response can be deﬁned as the following [45]: radial-inﬂow turbine's performance, composed with any con-
straints; and explore the algorithm to determine the optimum
objective function (i.e. turbine performance) corresponding to the
optimum design points of the blade geometry.
N N N
X X XX
Theparameterizationofbladegeometryisacrucialaspectofan
2
fðxÞ¼b þ b x þ b x þ b x x (12)
0 j j jj j ij i j
efﬁcientoptimizationtechnique;whereitis necessarytocreatean
j¼1 j¼1 isj
extensiverangeofaccurategeometrieswithinaminimumgroupof
designpointssimultaneously,withacarefulselectionofthedesign
Where, f(x) is a target function of optimization; x represents a
points of the blade geometry and their ranges. In this 3D CFD
group of design parameters andb is a coefﬁcient of regression.
optimization, the parameterization of the blade geometry was
The multi-objective optimization algorithm is expressed as:
conducted for the nozzle and rotor (i.e. turbine stage) of a radial-
inﬂow turbine.
As shown in Fig.10, B-splines in fourth and third order curves
are used torepresent the blade angle and thickness distribution in
meridional direction (M) where M is non-dimensional distance
from leading edge to trailing edge. In the 3D optimization algo-
rithm,thecontrolpoints'(CP)coordinateswereemployedandtheir
possiblechangesintheaxial(i.e.meridional)andradialdirections,
as presented by the arrows in Fig.10. The blade camber line from
leading edge (LE) to trailing edge (TE) is deﬁned by blade angle
distribution and thickness distribution respectively. Starting from
leading edge to trailing edge (LE to TE), the blade angle (i.e. b)is
parameterizedusingaB-splinecurvetechniquewithsevencontrol
point (CP to CP ) as illustrated in Fig. 10 a,b. Also, the blade
a1 a7
Design points
Fig. 8. Flow chart of 3D CFD optimization of radial-inﬂow turbine [28]. Fig. 9. Deﬁnition and parameterization of the blade geometry.
304 A.M. Al Jubori et al. / Energy 131 (2017) 297e311
CP
a1
CP
CP
a2 CP t3
t2
CP CP
t4
a3
CP
a4
CP
t5
CP CP
a5
t1
CP
t6
CP
a6
CP
a7
CP
t7
Fig.10. Parameterization of the distribution of blade angle (left), and blade thickness (right).
Table 8
thickness distribution is parameterized by seven control points
The optimum design parameters of the blade geometry for both workingﬂuids.
(CP to CP ); with two points were ﬁxed at leading edge and
t1 t7
trailing edge. As can be seen in Fig. 10, the arrows represent the
Parameters Workingﬂuids
movement direction forcontrol points wherethe control points of
isopentane R245fa
blade angle can move only in radial direction while the control
Rotor inlet absoluteﬂow angle,a (degree) 76.25 69.37
4
points of thickness distribution can move in both directions (i.e.
Rotor outlet absoluteﬂow angle,a (degree) 17.36 11.21
5
radial direction and meridional direction from LE toTE).
Rotor inlet relativeﬂow angle,b (degree) 54.75 13.71
4
Also, the number of the blades for both the nozzle and rotor
Rotor blade angle,b (degree) 60.58 65.64
blade,4
Rotor outlet relative angleb 55.17 65.34
was deﬁned as a design parameter in all optimization processes, tip,5
3 3
Rotor radial tip clearance (m) 0.325 10 0.345 10
to allow the optimizer to determine the optimum number of
Rotor blade chord (m) 0.0386 0.0368
blades. The turbine performance (i.e. turbine isentropic efﬁciency
Z 18 14
rotor
and power output) was deﬁned as the objective function to be
Nozzle LE beta angle (degree) 12 10
maximized in the MOGA optimization technique. While the mass Nozzle TE beta angle (degree) 71 67
Nozzle stagger angle (degree) 36 40
ﬂow rate and rotational speed were speciﬁed as two constraints.
® Nozzle throat (mm) 3.25 3.54
The 3D CFD optimizations were conducted using an Intel CPU
Z 33 37
nozzle
core i7 - 4820@ 3.70 GHz with 48 GB RAM in parallel run with 4
Nozzle blade chord (m) 0.0153 0.0147
CPU cores. Typically, each optimization run takes between 72 and
96 h.
the effective distribution of the range of design points to achieve
5.1. 3D optimization results
the required number of samples of the design points throughout
OSFD. As can be seen from Fig. 12, the maximum isentropic efﬁ-
Toobtaintheoptimumdesignforthebladegeometryofasmall-
ciency and power output were 87.40% and 5.415 kW respectively
scale RIT, the 3D optimization is carried out at nominal operating
for R245fa compared with 84.35% and 4.832 kW for isopentane.
conditions for both working ﬂuids (Table 5). The blade geometry
The 3D CFD analyses and optimization exhibited further
parameterization is conducted for both the nozzle and rotor with
improvement in the qualitative performance of the turbine stage
the number of blades. In the optimization process, the mass ﬂow
(i.e.blade loading (pressuredistribution), velocity vectors,velocity
rate and rotational speed were deﬁned as two constraints. There-
stream-lines and entropy generation contours) as shown in
fore, the values of the optimum design points delivered from 3D
CFDoptimizationforthebladegeometryofthenozzleandrotorfor Figs.13e16.Thepressuredistributioni.e.bladeloadingthroughout
the rotor passage in both base-line and optimum design cases is
each working ﬂuid are presented in Table 8. While, the original
shape from the base-line design of the nozzle and rotor blades' presented in Fig. 13 for both working ﬂuids; it shows the highest
the values of the pressure on the pressure side. While the lowest
shapescomparedtotheoptimumdesignforbothworkingﬂuidsis
values of pressure are positioned on the suction side, due to the
illustrated in Fig.11.
highest values ofﬂow velocity at the throat area of the blade pas-
The spline curve in fourth and third order form was used to
sage. The isentropic enthalpy drop (work) is provided by the area
deﬁne the blade angle and thickness distribution as designpoints/
circumscribed by such pressure distribution curves. Where, the
parameters,withtheassumptionoftheuniformdistributionofthe
enclosed area is indicative of the net torque producing aero-
bladethickness.IntheDoEmethodandbasedonthenumberofthe
dynamic force by the rotor turbine shaft.
design points of the blade geometry and their range, they were
As can be seen from velocity vectors in Fig. 14aed for both
divided into 250 sample points throughout the optimal-space-
ﬁlling design (OSFD) technique and varied continuously over working ﬂuids, the base-line design suffers from ﬂow reversal
compared with the optimized blade geometry. For both working
speciﬁed ranges. The number of design points was delivered from
A.M. Al Jubori et al. / Energy 131 (2017) 297e311 305
Nozzle of R245fa
Nozzle of isopentane
                       Base-line design
Rotor of R245fa
Rotor of isopentane
Fig.11. Nozzle and rotor blade shape at the base-line and optimum design for both working ﬂuids.
Fig.12. Variation of total-to-total efﬁciency and power output with design point for R245fa (left) and isopentane (right).
b) R245fa
a) isopentane
Fig.13. Effect of optimization on blade loading at baseline design and optimum design.
ﬂuids, the optimized blade shape has a superior and smooth ﬂow It is evident in Fig.16, at the base-line design for both working
withnoﬂowreversal,asshownbythevelocityvectorsandvelocity ﬂuids; the ﬂow separation and secondary ﬂows resulted in a
stream-lines in the blade passage, depicted in Figs. 14 and 15 considerable entropy generation. Where it is propagated down-
respectively. stream through the turbine stage as shown in Fig.16 in the base-
306 A.M. Al Jubori et al. / Energy 131 (2017) 297e311
a) isopentane base-line design 
b) isopentane at optimum design 
c) R245fa at base-line design d) R245fa at optimum design
Fig.14. Velocity vectors at mid-span for base-line and optimum design with two working ﬂuids.
a) isopentane base-line design 
b) isopentane at optimum design 
c) R245fa at base-line design d) R245fa at optimum design
Fig.15. Velocity streamlines at mid-span for base-line and optimum design with two working ﬂuids.
A.M. Al Jubori et al. / Energy 131 (2017) 297e311 307
a) isopentane at base-line design
b) isopentane at optimum design
d) R245fa at optimum design 
c) R245fa at base-line design 
Fig.16. Entropy contours at mid-span for base-line and optimum design with two working ﬂuids.
line design passage, the entropy generation occupied the majority 6. ORC analysis results
that led to substantial losses and hence reduced the turbine isen-
tropicefﬁciency.Inthesameﬁgure,comparingtheoptimumdesign Using the optimized turbine performance (i.e. isentropic efﬁ-
withthebase-linedesignshowsthattheentropygenerationinthe ciencyand power output) at the designpoint (Table 3) and setting
optimized geometry has substantially reduced throughout the these as inputs to the ORC model (section 2) resulted in the sys-
tem's thermal efﬁciency of 11.27%, compared with 9.56% at base-
optimizationprocess.Theenhancementsintheﬂowaerodynamics
throughthebladepassageclarifytheimprovementintheturbine's line design for R245fa as the working ﬂuid, as shown in Fig. 20a.
performance (i.e. efﬁciency and power). Also, Fig. 20a shows that the optimized ORC system's efﬁciency
The maximum enhancement in the optimized turbine perfor- with isopentane as the working ﬂuid was 9.69% compared with
mance (isentropic total-to-total efﬁciency and power output) is 8.07% at base-line design. Such results demonstrate the effec-
shown in Fig.17a and b with maximum improvement in terms of tiveness of 3D CFD optimization to further improve the ORC's
efﬁciencyof 13.95%andpoweroutputofabout14%withR245faas performance. The assessment of the ORC system's second law ef-
the working ﬂuid; while the maximum enhancement with iso- ﬁciencywithbothworkingﬂuidsatbase-lineandoptimumdesign
pentanewasof10.55%and11.98%intermsofefﬁciencyandpower cases is shown in Fig. 20b. The maximum second law efﬁciency is
output respectively. This improvementin the turbine performance 71.40% for R245fa, compared with about 64.00% for isopentane at
reveals the effectiveness of the 3D CFD optimization technique optimum design. As can be seen in Figs. 21 and 22, clearly the
from modifying the turbine blade's geometry. optimization technique was very effective as it improved the cy-
It is evident from Figs.18 and 19 that the turbine performance cle's thermal efﬁciency in both design and off-design conditions
in terms of efﬁciency and power output improved in off-design for various pressure ratios and total inlet temperatures for both
conditions as well as with in-design conditions for a wide range working ﬂuids.
of pressure ratios and rotational speeds for both working ﬂuids
(R245fa and isopentane). The design point was presented in
7. Conclusions
Table 5 with a pressure ratio of 3.0 and rotational speed of
27500 rpm for isopentane and 35000 rpm for R245fa. When the
In this paper, 3D CFD optimization of the blade's geometry of a
total pressure ratio is increased, the enthalpy drop increases,
small-scale radial-inﬂowturbine stage (nozzle androtor) foralow
leadingtoalargepoweroutput.Whiletheturbineefﬁciencydrops
power ORC system, driven by a low-temperature heat source has
with increasing the rotational speed for both workingﬂuids based
been conducted to enhance the turbine's and the ORC system's
on the deﬁnition of the loading coefﬁcient, it is explained the
performance. R245fa and isopentane were selected as working
variation in enthalpy drop (i.e.speciﬁc work) throughout the radial 
ﬂuids with a temperature heat source of (z90 C). In multi-
turbine stage. These ﬁgures reﬂect the highlighted advantages of
objective optimization algorithm, the turbine performance (isen-
using 3D CFD optimization techniques in off-design conditions as
tropic efﬁciency and power output) was selected as an objective
well.
function subject to maximization. Two constraints (massﬂow rate
308 A.M. Al Jubori et al. / Energy 131 (2017) 297e311
a) 
b) 
Fig.17. Total-to-total isentropic efﬁciency (a) and power output (b) at base-line design and optimum design for both working ﬂuids.
a) b) 
Fig.18. 3D optimization inﬂuence on the turbine performance with pressure ratio; efﬁciency (left) and power output (right) at design and off-design conditions for both working
ﬂuids.
a) b) 
Fig.19. 3Doptimizationinﬂuenceontheturbineperformancewithrotationalspeed;efﬁciency(left)andpoweroutput(right)atdesignandoff-designconditionsforbothworking
ﬂuids.
and rotational speed) were deﬁned to keep the operating condi- Itwasrevealedfromthe3DCFDMOGAoptimizationresultsthat
tions ﬁxed during the optimization process; this allows the theﬂow aerodynamics was improved signiﬁcantly compared with
achievement of the highest performance. the base-line design for both working ﬂuids. Also, the results
A.M. Al Jubori et al. / Energy 131 (2017) 297e311 309
b) b) ) 
a) 
Fig. 20. The cycle's thermal and second law efﬁciencies at base-line design and optimum design for both working ﬂuids.
exhibited that R245fa has a maximum turbine performance with
isentropicefﬁciencyof87.40%andpoweroutputof5.415kWwitha
thermal ORC system efﬁciency of 11.27%. This effective integrated
methodologyinclusion(mean-linedesign,CFDanalysisandMOGA
optimization) proved to give a remarkable improvement in the
performance of the small-scale ORC radial-inﬂow turbine stage
based on the optimization of the blade's geometry. The 3D CFD
optimizationshowedthatamoreefﬁcientturbineperformancecan
be achieved by optimizing the blade's geometry in terms of blade
angles,thicknessdistributionofthebladeandthenumberofblades
for both the nozzle and rotor, throughout assessing their quanti-
tative performance (isentropic efﬁciency and power output) and
qualitative aerodynamic performance (velocity vectors, velocity
stream-lines, pressure distribution and entropy generation
contours).
Furthermore, such optimization results revealed the potential
and effectiveness in design and off-design CFD analysis for a wide
range of rotational speeds and pressure ratios for both working
ﬂuids.Theseresultshighlightthepotentialandadvantagesofusing
Fig. 22. The off-design cycle's thermal prediction with inlet total temperature based
on the optimized turbine performance.
the 3D MOGA optimization technique to achieve high turbine ef-
ﬁciency leading to high ORC system thermal efﬁciency.
Acknowledgement
Themainauthor(AyadM.AlJubori)gratefullyacknowledgesthe
Iraqi ministry of higher education and scientiﬁc research for
funding PhD scholarship at the University of Birmingham, UK
whichfacilitatescontinuationofresearchonthemodellingand3D
optimization of small-scale radial-inﬂow turbine.
A numerical model for transport in ﬂat heat pipes considering wick microstructure effects

Abstract:
A transient, three-dimensional model for thermal transport in heat pipes and vapor chambers is developed. The Navier–Stokes equations along with the energy equation are solved numerically for the liquid and vapor ﬂows. A porous medium formulation is used for the wick region. Evaporation and condensation at the liquid–vapor interface are modeled using kinetic theory. The inﬂuence of the wick microstructure on evaporation and condensation mass ﬂuxes at the liquid–vapor interface is accounted for by integrating a microstructure-level evaporation model (micromodel) with the device-level model (macromodel). Meniscus curvature at every location along the wick is calculated as a result of this coupling. The model accounts for the change in interfacial area in the wick pore, thin-ﬁlm evaporation, and Marangoni convection effects during phase change at the liquid–vapor interface. The coupled model is used to predict the performance of a heat pipe with a screen-mesh wick, and the implications of the coupling employed are discussed.

1.Introduction:
Miniature heat pipes and vapor chambers effectively spread and carry heat from a heat-dissipating microelectronic chip to a remote location where air-cooling techniques may be deployed. These passive cooling devices transfer heat across a low temperature differential so that a safe working temperature for the chip can be maintained. As the size of electronic components continues to de-crease with a simultaneous increase in heat dissipation ﬂuxes, it is becoming increasingly necessary to optimize the shape and design of the heat spreaders for more efﬁcient thermal management. The maximum liquid feeding ﬂow rate that can be supported in a heat pipe wick due to capillary pressure (the capillary limit) restricts the maximum heat input that can be supported without causing dry-out. Improved heat pipe performance requires wick structures with superior wicking ability as well as enhanced thin-ﬁlm evaporation characteristics. In recent work, the authors [1,2] studied the wicking and evaporation characteristics of various microstructures to be used in heat pipes and concluded that sintered powder wicks have the best performance characteristics among those considered. However, models which predict the effects of various microstructures on the heat transfer performance of heat pipes have not, to our knowledge, been employed widely and are the focus of the present work.
Many numerical and analytical models have been developed to study the operation of heat pipes. Garimella and Sobhan [3] re-viewed the state of the art in the understanding and analysis of a large variety of heat pipes, and also identiﬁed the respective limitations. Modeling of heat pipes is complicated by the various mechanisms of heat, mass and momentum transport prevalent in the device, viz., the capillarity of the porous medium, evaporation/condensation in the wick structure, and conduction in the so-lid wall, among others. Models with approximate coupling between the various transport mechanisms have been developed and have led to fair predictions of heat pipe performance. In most of these cases, the wick structure has been assumed to be a continuous porous medium and the implications of micro-scale phenomena such as thin-ﬁlm evaporation and Marangoni convection in the wick pores have not been given much attention. As heat pipes and vapor chambers decrease in size, and surface-to-volume ratios increase, details of transport at the liquid–vapor interface may be-come increasingly important in determining heat pipe performance. Vadakkan et al. [4,5] developed a three-dimensional numerical model to study the performance of ﬂat heat pipes with multiple discrete heat sources. The effects of heat source strength and separation on steady as well as transient performance were studied. The wick–vapor interface was assumed to be ﬂat and local curvature effects were not considered. Ooijen and Hoogendoorn [6] presented a steady-state numerical analysis of the vapor core in a horizontal ﬂat heat pipe. Tournier and El-Genk [7] developed a two-dimensional model for the transient analysis of heat pipes. The analysis determined the radius of curvature of the liquid meniscus formed in the wick pores. Zhu and Vafai [8] studied the startup operating characteristics of asymmetrical ﬂat-plate and disk-shaped heat pipes using analytical models. Carbajal et al. [9] used a quasi-3D numerical analysis to obtain the temperature dis-tribution on the back side of a ﬂat heat pipe. They demonstrated that the ﬂat heat pipe led to a more uniform temperature distribution on the condenser side compared to a solid heat spreader. Koito et al. [10] developed a numerical model to solve the ﬂow and en-ergy equations in vapor chambers and estimated the capillary pressure head necessary to circulate the working ﬂuid inside the vapor chamber.
In a recent study, Do et al. [11] developed a mathematical mod-el for predicting the thermal performance of a ﬂat micro-heat pipe with a rectangular-grooved wick structure. They considered the effects of liquid–vapor interfacial shear stress and contact angle on device performance. Xiao and Faghri [12] developed a three-dimensional heat pipe model which accounted for heat conduction in the wall and ﬂuid ﬂow in the vapor core and porous wicks. Singh et al. [13] studied the effect of wick properties on the heat transfer characteristics of miniature loop heat pipes. Capillary structures with smaller pore size, high porosity, and high permeability showed better heat transfer characteristics in their experiments. Models [14–16] analyzing the performance of heat pipes with given wick structures have also been developed. Marangoni convection under large temperature differences was shown to have a signiﬁcant enhancement effect on heat transfer [17–19]. How-ever, the present authors [2] showed that Marangoni convection in the wick pores of heat pipes does not play a signiﬁcant role in enhancing the rate of evaporation heat transfer.
Most numerical models for predicting heat pipe performance do not consider the microstructure-scale ﬂow and heat transfer effects. Evaporation heat transfer from the thin liquid ﬁlm near the solid–liquid contact line of a liquid meniscus has been shown to account for more than 60% of the total heat transfer occurring from the meniscus [2,20]. In most conventional heat pipes and vapor chambers, the limiting thermal resistance is that of the wick, and the resistance due to the already efﬁcient evaporative heat transfer usually need not be considered. However, as device dimensions fall and wicks become thinner, liquid–vapor interface resistance may begin to play an increasingly important role in determining heat pipe performance. It is therefore important to develop heat pipe models which include the effects of microscale evaporation in wick pores on overall device performance.
Fig. 1(a) shows a schematic diagram of a ﬂat heat pipe. It consists of a solid copper wall, a porous wick structure (sintered powders in this case, and only present in this ﬁgure on the inside of the upper wall), and a vapor core. The evaporator and condenser regions are shown on the upper wall of heat pipe in this depiction. All other external surfaces are adiabatic. The liquid and vapor ﬂows shown in Fig. 1(a) complete the ﬂuid loop of heat pipe. Due to the high vapor pressure, a large capillary pressure is developed across the liquid–vapor interface in the wick pores of the evaporator section. The capillary pressure decreases towards the condenser section and the wet point condition (ﬂat interface) is reached at the end of the condenser section (with complete pressure recovery assumed in the vapor). The curvature of the liquid meniscus in the wick pores is determined by the interfacial capillary pressure. High capillary pressure in the evaporator section leads to higher interfacial curvature compared to the condenser region (as shown in the insets in Fig. 1(a)). The interface shape determines the extent of thin-ﬁlm meniscus formation in the wick pores, which in turn determines the interfacial resistance to heat transfer. In the present work, we have coupled a micro-scale wick-level model [21] with a macro-scale device-level model in order to capture the interface curvature effects in different microstructures. This will lead to a more accurate prediction of the effect of wick microstructure on the heat pipe performance.

2. Model description：
In the present work, a device-level numerical model (macro-model) is utilized to compute the ﬂow and temperature ﬁelds in a ﬂat heat pipe, also known as a vapor chamber. The macro-model solves for the continuity, momentum and energy equations in the solid wall, wick and vapor regions of the heat pipe. The wick-vapor interface is numerically modeled as a ﬂat interface while the wick structure is treated as a porous medium. The macro-model does not account for the curvature of the liquid meniscus formed inside the wick pores and thus ignores micro-scale details of thin-ﬁlm evaporation and Marangoni convection occurring during the evaporation process. To capture these micro-scale evaporation effects, a micro-model [21] which computes the evaporation of liquid in wick microstructures is coupled with the macro-model. Details of the two models and the algorithm used for coupling the two are now presented.

2.1. Device-level macro-model：
The device-level macro-model employed here is adapted from the work of Vadakkan et al. [4,5]. An equilibrium model for heat transfer and a Brinkman–Forchheimer extended Darcy model are employed for ﬂuid ﬂow in the wick. The transient change in vapor density due to pressurization is calculated using the ideal gas state equation. The vapor ﬂow is assumed to be laminar and incompressible. The phase-change mass ﬂow rate due to evaporation/condensation, and the temperature and pressure at the liquid–va-por interface are determined using an energy balance at the inter-face in conjunction with kinetic theory and the Clausius–Clapeyron equation. The energy balance at the interface includes convection and conduction on the liquid and vapor sides. The vapor ﬂow, and the temperature and hydrodynamic pressure ﬁelds are computed from coupled continuity/momentum and energy equations in the vapor and wick regions, and a conduction analysis in the wall. The model assumes that the wick is saturated with liquid throughout, which is required to prevent dryout.
To accommodate transient changes in the vapor and liquid mass under the assumption of a liquid-saturated wick, the volume-aver-aged density of the liquid is modiﬁed based on the mass balance. The model assumes all thermophysical properties to be constant except for the vapor density, which is found from the operating pressure Pop and the local temperature using the perfect gas law.

2.1.1. Governing equations and boundary conditions: macro-model：
Under the assumptions discussed above, the generalized governing equations for the wick and vapor regions may be written as shown below. The continuity equation for the wick and the va-por core is The above expression has been obtained with the assumption that the mean evaporation coefﬁcient is equal to the mean condensation coefﬁcient, where their variation with temperature and pressure may be assumed to be small [23]. The value of accommo-
dation coefﬁcient ^r in the above expression for evaporation mass transfer rate has been observed to vary over four orders of magni-tude, as described in [24]. A value of unity for the accommodation coefﬁcient has been widely used [25–27]. However, experimentally obtained accommodation coefﬁcients have been smaller due to contamination of the surface. The accommodation coefﬁcient has also been shown in experiments to depend on temperature [28,29] and vapor pressure [30]. Badam et al. [30] reported the accommodation coefﬁcient of water to be in the range of 0.028–0.15, which was deduced from evaporation experiments per-formed on water with the vapor pressure kept below 1200 Pa. Rose [31] reviewed the value of the accommodation coefﬁcient of water and suggested that its value lies in the range of 0.5–1. The value chosen for accommodation coefﬁcient determines the thermal resistance offered by the liquid–vapor interface in a vapor chamber, and it is critical that this value be determined carefully via experiments. Owing to the fact that the thermal performance of a vapor chamber may be greatly affected by the chosen value of
^r, we present our analysis for two values of ^r, viz., 0.03 and 1, to bracket the performance. The evaporated and condensed mass is assumed to ﬂow normal to the interface when accounting for transport due to evaporation/condensation.

2.1.2. Computation of operating pressure in vapor core：
Under the incompressible ﬂow assumption, hydrodynamic pressure differences are small as compared to the absolute operating pressure in the domain. To prevent round-off in the hydrodynamic pressure gradient computation, and to allow for system pressurization under the incompressible assumption, the pressure is split into two components: Additional details of the numerical method are available in [4,5]. The governing equations are solved in their transient form using the commercial CFD solver FLUENT [33]. A transient computation of the governing equations is necessary to set the level of pressure in the vapor domain. A steady, incompressible ﬂow calculation with all-velocity boundary conditions does not allow the pressure level to be determined uniquely since only the gradient of pressure appears in the governing equations. The pressure level is rendered unique by initial conditions; the speciﬁcation of the initial mass allows the pressure to be determined uniquely at each time step. It should be emphasized that this is not an artifact of the numerical scheme, but is a property of the incompressible Navier–Stokes equations. A detailed discussion of these issues can be found in previous work by the authors [4,5]. Suitable user-deﬁned functions (UDFs) have been developed to compute the evaporation/condensation mass ﬂow rates, temperature and pressure at the wick–vapor interface, as well as the liquid and vapor densities at every time step. The attainment of steady state is identiﬁed in this work as the time at which the heat transfer rate on the condenser side reaches within 2% of the value at the evaporator.

2.2. Wick-level micro-model：
The wick-level micro-model computes the evaporation heat transfer rate from the liquid meniscus formed in the pores of common wick structures used in heat pipes. The model considers only the top layer of wick pores, where the liquid meniscus is formed. Details of this model are available in Ranjan et al. [2,21]. An idealized two-dimensional representation of screen mesh wick struc-ture, viz., parallel horizontal wires for screen mesh wick, is used. The model assumes a static shape for the liquid meniscus formed inside wick pores. Static liquid meniscus shapes in the given wick geometries are obtained using the program Surface Evolver [34]. Surface Evolver computes the total surface energy of a given system and the equilibrium minimum energy conﬁguration is obtained by a gradient descent method. Details of this method can be found in [1]. With known curvature of the liquid meniscus in the wick pores, the capillary pressure generated by the wick structure (for a given wick porosity, length scale and solid–liquid con-tact angle) is also determined using the Young–Laplace equation. It should be noted that the meniscus level in the wick pore has been assumed to be equivalent to the radius of the wires in the present work. As presented in [1,2], this meniscus level leads to the highest capillary pressure generation in the wick pore; the evaporative heat transfer rate, on the other hand, was found to be unaffected by a change in the meniscus level. For future reference, this model is referred to as the static-meniscus micro-model.
Fig. 1(b) shows the equilibrium shape of a liquid meniscus formed between parallel horizontal wires (idealized representation for screen mesh wick; only top layer is shown here). The solid–li-quid contact angle is assumed to be 30, the porosity is 0.56, and the non-dimensional radius (radius taken as the length scale) of the wires is 1.The static liquid meniscus shapes thus computed are exported from Surface Evolver to GAMBIT [33] and the domain is discretized for ﬁnite volume computations in FLUENT [33]. It is noted that the assumption of a static meniscus shape under evaporative conditions would be valid for the very small Weber and Capillary numbers encountered in this work as is veriﬁed using simulations. The FLUENT model is used to compute ﬂow and heat transfer in the microstructure. This model is referred to as the wick-level micro-model. The mathematical formulation for modeling evaporation in wick structures is now presented.
2.2.1. Governing equations and boundary conditions: wick-level micro-model：
Fig. 1(c) shows the computational domain used for modeling evaporation from the liquid meniscus in wick microstructures. A unit cell representing the wick topology is modeled. Incompressible, laminar and steady ﬂow of water with constant properties is assumed in the liquid domain. The vapor is assumed to be saturated at a given temperature, as in an operating heat pipe. The wick-level micro-model is utilized only to correct the phase change mass ﬂow rates (Eq. (9)) at the wick–vapor interface in the macro-model. The vapor domain is not modeled since the absolute mass ﬂow rates in the heat pipe are not computed from the micro-model. The following continuity, momentum and energy equations are solved:
The boundary conditions for the case of a 2D meniscus between horizontal wires are shown in Fig. 1(c). The incoming liquid is assumed to be at a constant temperature, Tinlet, with a given pressure inlet boundary condition. A constant temperature boundary condition is applied at the bottom solid wall. A convective heat transfer boundary condition is imposed on the non-wetted portion of the so-lid particles exposed to vapor, labeled top wall in Fig. 1(c). The value of the natural convection coefﬁcient hnat from the surface of spheres and long wires is given by the Nusselt number correlation presented in [35]. The liquid–vapor interface is modeled as a ﬁxed interface with a convective boundary condition. To promote numerical stability, the heat transfer due to evaporation is implemented through an equivalent convective heat transfer coefﬁcient, hevap, which is obtained using the evaporative mass ﬂux expression (Eq. (9)), so that:
Evaporation from the liquid–vapor interface is modeled under the assumption of saturated vapor conditions, simulating heat pipe operation. The evaporative mass ﬂux at the interface is obtained according to Schrage [22] and is given by Eq. (9). Two values for
^r, 0.03 and 1.0 are used in the micro-model computations as mentioned earlier. Mass transport due to evaporation at the liquid–va-por interface is implemented by imposing mass sink terms in the liquid cells adjacent to the interface.
Fig. 2(a) and (b) show the temperature and ﬂow ﬁelds inside the wick pore for a meniscus between parallel horizontal wires, for
^r ¼ 0:03 and 1, respectively. The vapor is assumed to be saturated at 298 K. Standard thermophysical properties of water at 298 K are used in these simulations. The liquid–vapor interface is seen to be at the minimum temperature in the whole domain due to evapora-tion heat transfer. Non-uniformity in the meniscus thickness leads to differential evaporation rates, and consequently, a temperature gradient along the interface. This leads to the formation of two counter-rotating Marangoni vortices in the liquid domain. Evapo-rative mass ﬂux on the interface (Eq. (9)), from the solid–liquid contact line to the center of meniscus, is plotted in Fig. 2(c) for the two values of accommodation coefﬁcient. The evaporative mass ﬂux for ^r ¼ 0:03 is smaller than for ^r ¼ 1 by more than an order of magnitude. Also, for ^r ¼ 1, the evaporative mass ﬂux de-creases sharply to zero from the thin-ﬁlm region towards the central meniscus. From the detailed results in [2,21] obtained using ^r ¼ 1, thin-ﬁlm evaporation occurring near the solid–liquid contact line was found to account for more than 80% of the total evaporation from the interface. However, for ^r ¼ 0:03, it is seen
in Fig. 2(c) that the evaporative mass ﬂux decreases gradually from the contact line thin-ﬁlm region towards the central region. The contribution of the thin-ﬁlm region itself to the total evaporation from meniscus is insigniﬁcant for ^r ¼ 0:03. The authors previously
[2,21] showed that Marangoni convection is insigniﬁcant in enhancing evaporation in wick pores, and therefore, heat transfer at this scale is determined to ﬁrst order by thermal conduction.
The effects of changing the value of accommodation coefﬁcient on the rate of evaporation and the evaporation heat transfer coefﬁcient (as deﬁned in Eq. (26)) are considered further by simulating the evaporation of water from a liquid pool, using the wick-level micro-model. The domain for this problem is shown in Fig. 3(a), in which the vapor domain is assumed to be saturated at 298 K. Fig. 3(b) shows the variation of evaporation rate and evaporative
heat transfer coefﬁcient with ^r. With an increase in ^r from 0.18 to 1.6, the evaporative heat transfer coefﬁcient increases from 8.3 􀀁 104 W/m2 K to 3.32 􀀁 106 W/m2 K. However, the evaporation rate (ml) increases with an increase in ^r from 0.18 to 0.5, but be-comes independent of the value of ^r with further increases in ^r. This shows that for higher values of ^rð> 0:5Þ, the rate of evaporation is determined by the conduction heat transfer across the liquid pool (from inlet to the vapor domain) and the liquid–vapor inter-face resistance for heat transfer becomes negligible.

2.3. Coupling of macro- and micro-models：
The static-meniscus and wick-level micro-models described in the previous section are coupled with the device-level macro-mod-el in order to incorporate the effect of different microstructures on heat pipe performance. The macro-model does not account for the liquid–vapor interface shape in the wick pores, thin-ﬁlm evaporation or Marangoni convection. However, it has been noted [2,21] that thin-ﬁlm evaporation effects are critical for predicting the evaporative mass ﬂow rate correctly. Coupling the micro-models with the macro-model captures the micro-level aspects of evaporation from the liquid–vapor interface and examines the importance of the liquid–vapor interface in determining the device performance for given conditions. The algorithm for coupling the two types of models is now presented.
For a given heat input to the heat pipe, the local liquid–vapor interface shape is determined by the balance of surface tension The ratios of evaporative mass ﬂuxes in the wick pore for ^r ¼ 0:03 and 1.0 are obtained as a function of the solid–liquid con-
tact angle and wick porosity. The vapor temperature and solid wall superheat (see Tbot in Fig. 1(c)) are also varied to determine their effect on the mass ﬂux ratio. The mass ﬂux ratio changes by less than 5% for a 7 K variation in vapor temperature and a 15 K varia-tion in solid superheat, showing that the mass ﬂux ratio may be as-sumed to be unaffected by the vapor temperature and superheat. Variations of the mass ﬂux ratio with contact angle for various wick porosities are shown in Fig. 6(b) and (c) for ^r ¼ 0:03 and 1.0,
respectively. The mass ﬂux ratio is always greater than 1; this is due to the formation of a thin-ﬁlm meniscus at the solid–liquid contact line, and to a lesser extent, enhanced convection effects. The mass ﬂux ratio decreases with increasing contact angle and porosity. This is because the thin-ﬁlm area of the meniscus in a wick pore decreases with increase in contact angle and porosity. It can also be observed that the mass ﬂux correction ratios are
much greater for ^r ¼ 1 as compared to those for ^r ¼ 0:03. As it was discussed earlier (refer to the discussion regarding Fig. 2(c)), thin-ﬁlm evaporation effects near the solid–liquid contact line are more pronounced for ^r ¼ 1. This leads to higher mass ﬂux cor-rections for ^r ¼ 1 as compared to r^ ¼ 0:03. The following correla-tions are obtained for the mass ﬂux ratios as functions of contact angle (in degrees) for a porosity = 0.56 using the least-squares curve-ﬁt feature of MATLAB [36]:
Step 5: Correct the local evaporative mass ﬂow rate at the liquid–vapor interface in the macro-model using area and mass ﬂux ratios. Using the capillary pressure at the point of interest on the inter-face, the local contact angle h is ﬁrst computed using Eq. (30). Once h is known, the area and mass ﬂux corrections are computed from Eqs. (32)–(34), and the local evaporation mass ﬂow rates in the heat pipe macro-model are corrected using Eq. (31). The correc-tions are also employed in the calculation of interface temperature (Eq. (7)), and the computation of system pressure (Eq. (18)). This process is repeated during every iteration of the macro-model sim-ulation until convergence is obtained.

3. Results and discussion：
Simulation results for a 2D vapor chamber (as shown in Fig. 7(a)) using the device-level macro-model and coupled micro–macro models are presented in this section. The tempera-ture and ﬂow ﬁelds in the vapor chamber, as predicted by the macro-model, for given input heat ﬂux and condenser side condi-tions are discussed for two values of the accommodation coefﬁ-
cient, ^r ¼ 0:03 and 1. This is followed by the implications of using the micro–macro coupled model for the prediction of device performance under a given set of conditions. A smaller vapor chamber with a thinner wick structure and higher wick thermal conductivity, is discussed next in order to examine the role of the liquid–vapor interface on the performance of vapor chambers as they are miniaturized.

3.1. Description of vapor chamber under study：
A schematic diagram of a 2D ﬂat vapor chamber modeled in the present work is shown in Fig. 7(a). It incorporates a 5 mm wide evaporator region at the center of its top surface while the entire lower surface is the condenser region. A screen mesh is assumed for the wick. The vapor chamber has a height of 3 mm and a length of 30 mm. A solid wall thickness of 0.25 mm, a wick thickness of 0.2 mm and a vapor core thickness of 2.1 mm are chosen. All sur-faces other than the evaporator and condenser regions are mod-eled as adiabatic solid walls. The wall and wick are made of copper and the working ﬂuid is water. The porosity, permeability and thermal conductivity of the sintered screen mesh wick are 0.56, 2.97 􀀁 10􀀂11 m2, and 40 W/m K, respectively. Ergun’s coefﬁ-cient, CE, is taken to be 0.55. The radius of the wires in the screen mesh is assumed to be 100 lm. The wick is present on all sides of the vapor chamber. The heating and cooling boundary condi-tions are applied on opposite sides of the chamber. An input heat ﬂux of 10 W/cm2 is applied at the evaporator region of the heat pipe, leading to a heat input of 500 W/m. Standard thermophysical properties for water and copper at 298 K are utilized. The thermo-physical properties of the vapor chamber material and working ﬂuid are tabulated in Table 1. The coolant water temperature and the heat transfer coefﬁcient on the condenser are 298 K and 400 W/m2 K, respectively. The initial temperature of the heat pipe is 298 K and the vapor is assumed to be saturated.

3.2. Flow and temperature ﬁelds inside the vapor chamber：
Fig. 7(b)–(g) shows the temperature and ﬂow ﬁelds in the vapor chamber, predicted by the macro-model for ^r ¼ 1 and 0.03. The temperature drops across the vapor chamber for ^r ¼ 1 and 0.03 are 0.73 and 1.62 K, respectively. For the smaller value of the accommodation coefﬁcient, the thermal resistance of the liquid–vapor interface inside the vapor chamber is signiﬁcant, thus lead-ing to a higher thermal resistance of the vapor chamber. Fig. 7(b) and (c) shows the temperature contours in the vapor chamber. The working liquid evaporates at the liquid–vapor interface below the evaporator region of the vapor chamber and the vapor con-denses on the lower liquid–vapor interface. Fig. 7(d) and (e) shows Darcian approach appropriate in the wick medium. The liquid pres-
sure drops are found to be 323 and 320 Pa for ^r ¼ 1 and 0.03, respectively. The steady-state operating pressure of the heat pipe
is 31.9 and 32.1 kPa for ^r ¼ 1 and 0.03, respectively.
Fig. 8(a) and (b) shows the temperature values on the outer walls of the vapor chamber for the two values of accommodation coefﬁcient. The temperature is highest at the center of the evapo-rator region while that on the condenser side is uniform, as desired for a vapor chamber heat spreader. The predicted temperature drops show that the thermal resistance offered by the vapor cham-
ber is 1.46 􀀁 10􀀂3 K/W and 3.24 􀀁 10􀀂3 K/W for ^r ¼ 1 and 0.03, respectively. The evaporation/condensation mass ﬂuxes are plot-ted along the liquid–vapor interfaces on the evaporator and con-
denser sides of the vapor chamber in Fig. 8(c) and (d) for r^ ¼ 1 and 0.03, respectively. It is seen that the working ﬂuid evaporates from the upper liquid–vapor interface over a region approximately 1 cm long, which is twice the size of the evaporator heat input area. The vapor condenses uniformly over the liquid–vapor interface on the condenser side.

3.3. Effects of micro- and macro-model coupling：
Steady-state results from the micro–macro coupled heat pipe model are presented for the 2D vapor chamber in Fig. 7(a). The va-por chamber material, working ﬂuid properties and the thermal boundary conditions used are the same as those considered in the macromodel in the previous section. The thermal performance of the vapor chamber for both values of accommodation coefﬁcient is predicted with the coupled model and the results compared to the uncoupled macromodel.
The coupling of the micro–macro models is implemented by the means of correcting the interface mass ﬂow rates and the interfa-cial area based on the interfacial capillary pressure drop inside the vapor chamber. Fig. 9(a) and (b) presents the interfacial pressure drop and the local contact angle along the liquid–vapor interface in the vapor chamber for ^r ¼ 1 and 0.03, respectively. Points 1,
2, 3 and 4 correspond to different locations on the liquid–vapor interface in the vapor chamber, as marked in Fig. 7(a). The wet point is assumed to be at the center of the condenser region (at location 3) in the vapor chamber. The liquid pressure in the wick is highest at this location, as it would be at the end of the con-denser section in a cylindrical heat pipe. The interfacial capillary
pressure drop (318 Pa for ^r ¼ 1 and 317 Pa ^r ¼ 0:03) is highest at the center of the evaporator section, leading to the highest cur-
vature and smallest contact angle (69.9 for ^r ¼ 1 and 70 ^r ¼ 0:03) at this location. The contact angle increases from 69.9 in the evaporator section to 90 in the center of the condenser re-gion (i.e., at the wet point).
Fig. 9(c) and (d) shows the mass ﬂux correction, area correction and mass ﬂow rate correction ratios along the liquid–vapor inter-
face in the vapor chamber for ^r ¼ 1 and 0.03, respectively. The area correction ratio is predicted using Eq. (32) while the mass correc-tion ratios can be computed using Eqs. (33) and (34) for the two values of accommodation coefﬁcient. As can be observed from the ﬁgures, the corrections in interface area and mass ﬂow rate are highest at the point of highest curvature and lowest contact an-gle, i.e., the center of the evaporator region (point 1). The correc-tions become smaller as the condenser section is approached since the meniscus curvature in the wick pore decreases from the evaporator center towards the center of the condenser region. Mass ﬂux correction ratios are higher for ^r ¼ 1 as compared to those for ^r ¼ 0:03. The total correction ratio is the product of area and mass ﬂux correction ratios and it shows a similar variation as the mass ﬂux correction ratio along the liquid–vapor interface in
the vapor chamber. For ^r ¼ 1, the total correction ratio is greater than 1 leading to an enhancement in the evaporation/condensation
rates in the vapor chamber. For ^r ¼ 0:03, the total correction ratio is smaller than 1, thus reducing the rate of evaporation/condensa-tion inside the chamber. With a decrease in the phase change rates, it is expected that the thermal resistance of the vapor chamber will increase when a working ﬂuid with very small accommodation coefﬁcient is used in the vapor chamber.
A comparison between the non-coupled and coupled models is now presented. For ^r ¼ 1, as presented in Fig. 9(c), the mass ﬂow
rate correction ratios are greater than 1 (in the range of 1.4–1.6). This leads to an increase in the evaporation/condensation heat transfer coefﬁcient at the liquid–vapor interface, thus reducing the thermal resistance offered by the interface in the device. How-ever, as discussed at the end of Section 2.2.1, the rate of evapora-tion from a liquid meniscus will be only slightly affected if the
evaporation heat transfer coefﬁcient is enhanced when r^ > 0:5. A similar observation is made by comparing the temperature drops across the vapor chamber from the non-coupled and coupled mod-els. The non-coupled model predicts the temperature drop across the vapor chamber to be 0.72 K while the coupled model predicts it to be 0.7 K (􀀄3% decrease in the temperature drop). However,
for ^r ¼ 0:03, the temperature drop predicted by the coupled model is 1.92 K, while the non-coupled model predicts a temperature drop of 1.62 K. The temperature drop predicted by the coupled model is 􀀄16% higher than that from the non-coupled model. The increase in thermal resistance can be explained on the basis of the mass ﬂow rate correction ratios being smaller than 1 for ^r ¼ 0:03, as presented in Fig. 9(d). Hence, the coupled model incor-
porates the increase in the thermal resistance of the vapor cham-ber by accounting for the liquid–vapor interface thermal resistance in the vapor chamber. With the coupled model, the stea-dy-state operating pressure of the vapor chamber decreases from
31.9 kPa (non-coupled model) to 31.8 kPa for ^r ¼ 1. This is due to the decrease in the working temperature of the vapor chamber.
However, for ^r ¼ 0:03, the working temperature of the vapor
chamber increases as a result of coupling and so, the coupled mod-el predicts the operating pressure to be 32.2 kPa (32.1 kPa pre-dicted by the non-coupled model).
Fig. 10(a) and (b) shows the temperature contours in the vapor chamber and the temperature values on the outer surfaces of the vapor chamber, as predicted by the coupled model for ^r ¼ 0:03.
3.3.1. Vapor chamber with a thinner, higher-conductivity wick：
As discussed above, the coupling of the wick-level micro-model with the device-level macro-model leads to an accurate assessment of the role of the liquid–vapor interface inside the vapor chamber for determining its spreading behavior, i.e., the thermal perfor-mance. It is shown that the coupling is important for accurate predictions in the case of working ﬂuids with small accommodation coefﬁcients. From a ﬁrst-order approximation, it can be shown that the liquid–vapor interface thermal resistance (􀀄1/hevapAi) is equiv-alent to that offered by the wick structure itself (􀀄wick thickness/(kwickAi)) when the wick thickness is 40 lm (calculated with hevap =106 W/m2 K for r^ ¼ 1, kwick = 40 W/m K). hevap is much smal-ler (􀀄104 W/m2 K) for ^r ¼ 0:03 and so the interface thermal resis-tance becomes signiﬁcant in comparison to the wick resistance
for even thicker wicks (􀀄4 mm) when r^ ¼ 0:03. Also, the wick thermal resistance decreases when the wick thermal conductivity is increased and it is no longer the controlling thermal resistance in vapor chamber operation. An accurate determination of the interface thermal resistance becomes increasingly important for thinner wicks of high thermal conductivity. Novel wick structures containing metallic nanowires and carbon nanotubes [37], have been proposed recently. In this section, the numerical simulations are repeated for the same vapor chamber geometry described in the previous section except with a thinner wick (50 lm) and higher wick thermal conductivity (176.3 W/m K). The thermal conductiv-ity of the solid in the porous wick medium is set to be 400 W/m K. Some novel wick microstructures, such as carbon nanotubes and metallic nanowires, have been shown [38,39] to possess such high values of thermal conductivity. Also, the wick permeability is taken to be 2.97 􀀁 10􀀂10 m2. All other properties and boundary con-ditions for the vapor chamber are kept the same as discussed in the previous section.
Fig. 11(a) and (b) shows the temperature contours in the vapor chamber and velocity magnitude contours in the vapor core of the device. The temperature on the outer wall of the vapor chamber is plotted in Fig. 11(c). Results in Fig. 11 correspond to the coupled model and an accommodation coefﬁcient of 0.03. For this case, the liquid pressure drop inside the wick structure is 524 Pa while the steady-state operating pressure of the device is 29.4 kPa. dicted by the coupled and non-coupled models are 0.11 and 0.13 K, respectively. Hence, the non-coupled model over-predicts the tem-
perature drop by 􀀄15%. For ^r ¼ 0:03, the temperature drops across the vapor chamber as predicted by the coupled and non-coupled models are 1.49 and 1.18 K, respectively. This shows that the non-coupled model under-predicts the temperature drop across the vapor chamber by 􀀄26%. Fig. 12(a) presents the interfacial pressure drop and the local contact angle along the liquid–vapor
interface in the vapor chamber for r^ ¼ 0:03. The interfacial pres-sure drop (524 Pa) is highest at the center of the evaporator sec-tion, leading to the highest curvature and the smallest contact angle (55.7) at this location. Again, the contact angle increases from 55.7 in the evaporator section to 90 at the center of the con-denser region (i.e., at the wet point). Fig. 12(b) shows the mass ﬂux correction, area correction and mass ﬂow rate correction ratios along the liquid–vapor interface in the vapor chamber for
^r ¼ 0:03. The corrections in interface area and mass ﬂow rate are highest at the point of highest curvature and lowest contact angle, i.e., the center of the evaporator region (point 1). Again, the correc-tions become smaller as the condenser section is approached since the meniscus curvature in the wick pore decreases from the evap-oration center to the center of condenser region. 

4. Conclusions：
A three-dimensional numerical model for ﬂat heat pipes which accounts for the effect of wick structure in determining the evapo-ration rate at the liquid–vapor interface is developed. A device-le-vel macro-model is coupled with a wick-level micro-model. This coupling leads to corrections to the evaporative mass ﬂow rates at the liquid–vapor interface in the wick pore as well as to the interfacial area. These corrections are based on the local contact angle of liquid in the wick structure, and change along the location in the vapor chamber. Due to this coupling, thin-ﬁlm evaporation, meniscus curvature effects, and Marangoni convection effects on the heat transfer rate from the liquid–vapor interface are ac-counted for in the device-level macro-model. 
Flow and temperature ﬁelds in a two-dimensional ﬂat heat pipe/vapor chamber are computed using the original macro-model and the coupled macro-model. The simulations are performed for two different values of the accommodation coefﬁcient of the work-ing ﬂuid, viz., 0.03 and 1. From the results obtained using the cou-pled and non-coupled model, it is inferred that the thermal resistance offered by the liquid–vapor interface inside the vapor chamber may become signiﬁcant and affects the performance of the vapor chamber as the device is miniaturized, and as novel high-conductivity thin wicks become available. In these situations, the transport effects at the liquid–vapor interface must be cor-rectly incorporated in order to accurately predict the performance of the vapor chamber. The need for a coupled model becomes imperative when (a) the thickness of the wick structure is very small (in the order of 100 lm), (b) the thermal conductivity of wick structure is very high (>100 W/m K), or (c) when the accommoda-tion coefﬁcient of the working ﬂuid is very small (<0.1). The basic methodology developed in this work is applicable to other types of microstructures, operating conditions and device designs as well.Molecular dynamics simulation of thermal conductivity of silicon nanowires
ABSTRACT
We investigate the thermal conductivity of silicon nanowires based on molecular dynamics (MD) simulations. 
The simulated thermal conductivities of nanowires with square cross sections are found to be about two orders of magnitude smaller than those of bulk Si crystals in a wide range of temperatures (200–500K) for both rigid and free boundary conditions. 
A solution of the Boltzmann transport equation is used to explore the possibility of explaining the MD results based on boundary scattering.

1 Instroduction
Although a number of studies have been carried out on the thermal conductivity of thin films/superlattices, 
it is only recently that the investigation of heat conduction in one-dimensional structures such as nanowires has been initiated.
In two-dimensional structures, the effective thermal conductivity is a function of both boundary and internal phonon scattering. 
As a consequence of the boundary scattering, the thermal conductivity values may be reduced by one order of magnitude. 
The small cross section in nanowires implies a more significant reduction of the thermal conductivity than in thin films because of a higher surface to volume ratio. 
Furthermore, quantized thermal conductance has been predicted theoretically for the lowtemperature regime. 
Experimental work has thus focused exclusively on this temperature regime.
The thermal behavior of nanowires at moderate temperatures is little understood at this stage, both from an experimental and a theoretical point of view.
A study of the thermal conductivity in nanowires is reported here using the molecular dynamics ~MD! method. 
In a MD simulation, all the particle trajectories for a given system with a reasonably small number of atoms are calculated, which allows the derivation of the thermodynamic and transport properties of the system.
Direct MD simulation of infinite length nanowires is feasible because the boundaries of the wire provide realistic limitations on the phonon mean free path. 
Accordingly, we showed that the MD results are independent of the longitudinal wire size. 
Because of the surface reorganization, however, it is difficult to obtain convergent results for free boundary conditions. 
To overcome this difficulty, the computation is carried out such that the surfaces reorganize and stabilize before the commencement of the thermal conductivity calculation. 
The results show that the simulated thermal conductivity values of nanowires are about two orders of magnitude smaller than those of bulk crystals. 
Although there may be a number of reasons for this reduction, we explore the possibility of explaining the MD results with the Boltzmann transport equation ~BTE!, assuming a partly diffuse-scattering interface.

2 Method
Using the MD technique and the Stillinger–Weber interatomic potential model for Si, the trajectories, i.e., 
the time-dependent positions ri(t) and velocities vi(t) of the nanowire atoms, are computed. 
The temperature T can be readily calculated from the velocity vi of the atoms in the simulation domain since the Boltzmann distribution function allows the straightforward derivation of the mean kinetic energy ^Ec&
where kB is the Boltzmann constant, N the number of particles in the system, and M the atomic mass. However, Eq.
~1! holds only if the specific heat is not temperature dependent, which is true for temperatures above the Debye temperature. Between 200 and 500 K, if the simulated MD temperatures are compared to the Debye temperature u D
5645 K of Si, a quantum correction to both the MD temperature and the thermal conductivity must be carried out as described in previous publications.13 The heat flux expression is derived from the integration of the energy balance
equation combined with the definition of the total instantaneous heat flux in a statistical ensemble of constant energy. For the computed two- and three-body potentials, the heat flux can be calculated from
where Fi j and Fijk are the two- and three-body forces and V is the system volume. We perform the thermal conductivity simulations based on the equilibrium fluctuations of the heat flux q0(t)  since the Green–Kubo formula can be derived
from q0(t) according to the fluctuation-dissipation theorem is the heat flux autocorrelation function and T0 is the equilibrium temperature.
To perform MD computations of the nanowire thermal conductivity, we construct the simulation domain from unit cells forming a face-centered cubic lattice with two Si atoms as basis. Systems with cross sections ranging from 434 to
10310 unit cells and 16 unit cells in the longitudinal direction are computed. These simulations correspond to atom
numbers ranging from 2048 to 12 800 since a diamond-like unit cell includes eight atoms. The silicon density is imposed by setting the lattice constant to 0.543 nm and periodic boundary conditions are applied in the longitudinal direction.
A 0.766 fs time step is taken in the integration of the equations of motion, which is a hundredth of the reference time scale tu50.0766 ps in Si. It is considered reasonable to extend the time of the simulation to one phonon mean relaxation time '20 ps.
The rigid transverse boundaries are first imposed by freezing surface atoms. The initial potential energy was applied to the system by placing atoms at the lattice nodes. A Gauss thermostat9 is used to fix the temperature and the final
results are checked to ensure that they are not modified by a change in the thermostat parameters. Once the temperature reaches the required value, the thermostat is removed and the heat flux calculation executed. We also check that the total
energy is well conserved throughout the thermal conductivity simulation. 
To verify that the longitudinal length Lx of the wire does not affect MD results, the sensitivity of the thermal conductivity to the simulated wire length is probed and representative results are illustrated in Fig. 1. The figure shows that the
thermal conductivity is not Lx dependent when Lx is larger than 16 lattice constants for wires with cross-sectional areas smaller than 28.62 nm2 . On the contrary, using the same approach in bulk Si crystals12 has led to thermal conductivity
values that increase with increasing simulation domain size, which confirms the hypothesis that the mean free path in nanowires is short and the Green–Kubo expression can be applied.
When systems with free transverse surfaces, i.e., free standing nanowires, are computed, surface atoms follow a rearrangement process so that a long relaxation takes place during which the kinetic energy increases while potential
energy fades to its minimal value. Due to the atomic diffusion, neighbor lists of each atom are frequently updated and the time step is reduced to t50.005tu in the computation instead of 0.01tu for rigid boundaries. For a
2.14 nm32.14 nm36.42 nm wire with 1536 atoms and rigid boundary conditions, the average computation time is 5 h using a HP9000-C180 workstation. For the same system with free boundaries, the computational time triples.

3 Results and discussion
In Fig. 2, the temperature dependence of the thermal conductivity is plotted for different nanowire cross sections with free or rigid boundary conditions in the transverse directions. The difference between data from both types of
boundary conditions remains small, i.e., less than 10%. We observe that the thermal conductivity is slightly temperature dependent and increases with increasing cross-section area of the wires. Another striking feature is that thermal conductivity values are in the interval of 1–5 W/mK, which is one to
two orders of magnitude smaller than the corresponding data for bulk Si (lBulk5241 W/mK at 200 K and lBulk 581 W/mK at 500 K!.
There are several possible explanations for the reduction of the nanowire thermal conductivity found in the MD numerical experiments ~Fig. 2!: ~i! the change of phonon spectrum in 1D structures, which modifies the phonon group velocity and the scattering mechanisms,20,21 and ~ii! the
boundary inelastic scattering which increases diffuse reflections on the surfaces. Although a few studies have reported the effects of phonon group velocity reduction on the thermal conductivity of thin films,20 many experimental data can be
explained by classical size effects based on the Boltzmann transport equation.1,2,6 Here, we will explore the second point based on a general solution of the Boltzmann transport equation for phonon distribution function inside a thin wire
where g is the deviation of the phonon occupation number n from equilibrium g5n02n, LBulk is the average phonon mean free path and m is the cosine of the angle between the phonon group velocity and the longitudinal direction. In Eq.
~4!, the term inside the brackets includes ~i! the unity corresponding to the bulk material contribution and ~ii! a ratio representing the correction implied by the size effect. In this ratio, the phonon-phonon scattering of heat carriers starting
from boundary point rB and ending at the position r, along the direction defined by m, is included in the exponential terms. If Eq. ~4! is introduced into the expression of the heat flux perpendicular to the cross section S
where n is the frequency and D the phonon density of states, we find out the ratio of the thermal conductivity lnw of the nanowire to the bulk thermal conductivity lBulk where G(r) represents the ratio in Eq. ~4!.
Numerical results of Eq. ~3! are plotted as lines in Figs. 2 and 3. The latter shows the thermal conductivity as a function of the cross-sectional area. We note that the BTE and MD data are in good agreement when the interface specularity parameter p value is about 0.45. Figure 3 reveals that the
cross-sectional section dependency of the thermal conductivity follows a power law. Although the boundary scattering can explain the MD results, we should caution that other phenomena such as group velocity reduction may occur and
be responsible for the reduced thermal conductivity.

4 Conclusion
In conclusion, we have calculated the thermal conductivity of silicon nanowires using a MD technique based on the Green–Kubo formula. For nanowire cross sections ranging
from 2.58 to 28.62 nm2 , we computed the thermal conductivity for different longitudinal sizes and found that the thermal conductivity values become length independent when
the wire is longer than 8.56 nm, suggesting that the results approach those of infinite wires. The MD results reveal a one to two orders of magnitude reduction in the thermal conductivity of nanowires compared to those of corresponding bulk
Si crystals. We solved the Boltzmann transport equation to explore the possibility of diffuse boundary scattering causing the thermal conductivity drop as observed in the MD simulation. With an interface specularity parameter value of 0.45,
the MD results match the BTE solutions reasonably well. However, other mechanisms, such as the phonon group velocity and the scattering mechanism changes, may also be responsible for the thermal conductivity reduction.



Blocking Phonon Transport by Structural Resonances in Alloy-Based Nanophononic Metamaterials Leads to Ultralow Thermal Conductivity
ABSTRACT
Understanding the design rules to obtain materials that enable a tight control of phonon transport over a broad range of frequencies would aid major developments in thermoelectric energy harvesting, heat management in microelectronics, and information and communication technology. Using atomistic simulations we show that the metamaterials approach relying on localized resonances is very promising to engineer heat transport at the nanoscale. Combining designed resonant structures to alloying can lead to extremely low thermal conductivity in silicon nanowires. The hybridization between resonant phonons and propagating modes greatly reduces the group velocities and the phonon mean free paths in the low frequency acoustic range below 4 THz. Concurrently, alloy scattering hinders the propagation of high frequency thermal phonons. Our calculations establish a rationale between the size, shape, and period of the resonant structures, and the thermal conductivity of the nanowire, and demonstrate that this approach is even effective to block phonon transport in wavelengths much longer than the size and period of the surface resonant structures. A further consequence of using resonant structures is that they are not expected to scatter electrons, which is beneficial for thermoelectric applications.

1 Instruction
Controlling heat flow using nanophononic materials and devices is growing more and more appealing due to its broad applications in energy conversion and modern computations, including thermoelectrics, thermal cloaks, thermal logic gates, and thermal diodes [1–8]. For thermoelectric applications, one needs materials with a high electronic power factor, as in crystals, but a low thermal conductivity (TC) as in a glass [9]. Designing the TC of a crystalline system to approach or even be lower than the corresponding amorphous limit remains an open challenge both conceptually and technically [10]. The most common route to reduce TC is to exploit phonon scattering at interfaces in nanostructured composites or superlattices [11–14]. In general, only a limited range of phonon frequencies gets scattered efficiently, typically at a relatively high frequency, above 1–2 THz. Nevertheless, low frequency modes also have a significant contribution to the total TC due to their long mean free path (MFP) [15,16]. The best results along this line were obtained using hierarchical structures in heavy element materials, which introduce scatterers from the mesoscale to the atomic scale [17]. In silicon based materials this approach was followed by synthesizing SiGe nanocomposites [18], superlattices [13], and silicon nanomeshes [19,20]. However, in most cases grain boundaries, inclusions, and pores have a negative impact on electronic transport, thus hampering the thermoelectric performance.
As an alternative, the phonon resonant effect in metamaterials [21,22] has been recently proposed for TC design [23–25]. With a careful design of resonant structures, a set of resonant frequencies can be obtained, evidenced by a set of flat bands in the phonon dispersion. Because of the band anticrossing, the resonances interact with the propagating modes and reduce their phonon group velocities. This method allows for manipulating waves with wavelengths much larger than the structural features of the system [26]: low frequency resonances can be easily introduced with small resonant structures.
Branched nanowires (NWs) have been synthesized by the dislocation-driven mechanism [27] and the vapor-liquid-solid process [28–30]. Standing waves (resonances) can be generated inside the branches due to the total reflection of waves at the end of branches. As a result, branched NWs provide an ideal platform for phonon resonance studies and for low TC design. In this work, we report a systematic study on heat transport in Si based resonant structures. Our molecular dynamics simulations show that branched NWs can indeed produce numerous resonances in a broad frequency range. Those resonances interact with phonons in the main NW and reduce their group velocities. The heat flow in the main NW can be manipulated through the design of adequate resonators. We propose a promising methodology to block phonon transport at both low and high frequencies: an extremely low TC of 
0.9W/m⋅K in SiGe alloy crystalline NWs has been achieved with a combination of the phonon resonant and scattering effects.
We first created NWs in the ⟨100⟩ orientation with a periodic set of branches outside the NWs. The cross section of the main NW is a square with an edge length Le=4.34nm. The branches also have a squared cross section with an edge length 
Lbs=1.09nm and the height of the branches is set to Lh=1.63nm. To investigate the effect of the number of branches on thermal transport, we designed a set of NWs with branches on one side, two sides, and four sides of the main NW. Sketches of a two-side branched NW are shown in Figs. 1(a) and 1(b). Simulations are carried out with the lammps code [31]. The atomistic 3D models of the NWs and simulation details can be found in Ref

2 Method
We calculated the TCs from equilibrium molecular dynamics simulations. The TC of the one-side, two-side, and four-side branched NWs as a function of the period length is depicted in Fig. 1(d). For reference, the TC of the pristine NW computed with the same approach is 
19.8W/mK. This value is comparable to those obtained in former simulations of a square-section Si NW with a similar small diameter [40]. Experimental estimates for so thin NWs are currently not available, and it is worth stressing that our pristine reference model exhibits perfect flat surfaces, which may not be attainable in experiments [41,42] (see the discussions in Ref. [32]). The TC of branched NWs is much smaller than that of the pristine NW, especially at short period lengths. With the increase of the period length, the TC increases gradually for all cases. Besides, the TC is also reduced when more sides of the NW are branched due to the increased hybridization, which further reduces the group velocity as will be discussed later.
Since more branches produce stronger hybridization, we extend the branched structure to square wall surrounded structures [hereafter referred as “surrounded NWs,” see Fig. 1(c) and Fig. S1 [32] ], to further promote the TC reduction. The thickness of the walls is 1.09 nm while their height is 1.63 nm, i.e., the same dimensions as the studied branches. This structure can be fabricated by the vapor-liquid-solid method with the catalysis diffusion technique [43]. As expected, the TC is further reduced at all periods when compared to all other cases [Fig. 1(d)]. The TC reduction can reach up to 67% at the period of 2.17 nm compared to the one of the pristine NW. The increased anharmonicity at high temperature will reduce the relative TC reduction in resonant NWs. However, due to the strong hindering of low frequency phonons, the relative TC reduction in the surrounded Si NWs with respect to the value of the pristine Si NW can still be 50% at 800 K (Fig. S5 [32]). It is worth noting that the shape of the resonant nanostructures does not affect the TC significantly: the same TC is found in the square and circle wall surrounded Si NWs (Fig. S4 [32]). The TC reduction achieved solely by introducing the resonant structures is comparable to the one achieved in surface engineered NWs
To more deeply understand the origin of the phonon resonant effect, we calculated the phonon dispersion relations and the group velocities 
vg of different structures based on harmonic lattice dynamics [44,45]. The comparison of phonon dispersion relations between the pristine structure and the two-side branched structures with 
Lp=2.17nm is illustrated in Figs. 2(a)–2(c), where Lh in Fig. 2(c) is twice Lh in Fig. 2(b). With the addition of periodic branches on the main NW surfaces, a series of resonant flat bands crossing the entire Brillouin zone is generated. The resonant modes hybridize with the modes of the main wire due to the band anticrossing effect, which occurs when two modes with the same polarization located on different objects are coupled at the same frequency. Specifically, in our case, if a resonant mode from the pillars has the same polarization as a propagating mode in the main NW, they will hybridize and form two new modes (up and down branches) when their frequencies converge. This mechanism leads to the formation of mini band gaps, the widths of which depend on the coupling or hybridization strength as observed both in theoretical models [46–49], and experimentally [50,51] in acoustic metamaterials.
To provide a picture of how propagating modes hybridize with the resonant modes, we have visualized the modes indicated by the symbols in Fig. 2(b). The hybridization effect between the longitudinal acoustic (LA) mode and the lowest resonance is examined by the eigenvectors. Starting from the mode far away from the resonant frequency (circle), a pure LA mode can be identified from Fig. 2(e): all atoms vibrate along the phonon transport direction 
z with the same amplitude, following the feature of the LA mode. For the mode on the resonant flat band (triangle, a bending mode), it only distributes on the resonant branches; no atoms in the main NW participate in the vibration [Fig. 2(g)]. A hybridized mode resulting from the coupling between the LA mode and the resonant phonon can be identified near to the resonant frequency [square, Fig. 2(f)], where the mismatch in the vibrational amplitudes in the main NW and in the resonator results in a reduced 
vg. The effect on the upper side of the resonance is similar to that on the lower side but the atom vibrations in the main NW and on the branches are in opposite phase [Fig. 2(h)]. The enhanced hybridization effect produced by increasing the number of branches is evidenced by the reduced 
vg when more surfaces are branched [Fig. 2(d)]. A stronger hybridization enlarges the resonant minigaps as can be seen from the group velocities by polarization in Fig. S3
The proposed resonant structures offer several advantages in manipulating phonon transport. First, the resonators can produce numerous resonant frequencies in almost the entire frequency range. As a result, the phonon group velocity can be dramatically suppressed at both superwavelength and subwavelength frequencies [Fig. 2(d)]. Second, the resonant frequency can be tuned by altering the branch size and material. The comparison between Figs. 2(b) and 2(c) provides a clear picture of the size effect: when the branch height doubled from 1.63 nm to 3.26 nm, the lowest resonant frequency shifts from 0.2 to 0.06 THz. The increased size of the resonators will also increase the number of resonances. One can therefore adjust the number and the frequency of the resonances by modifying the size of the resonant structures. Consequently, the TC can be easily designed and the heat flow in the main wire can be controlled in a more accurate way. Another important feature of this structure is that the low frequencies can be easily manipulated with small branches, which yields the third advantage of the structure. As discussed above, the lowest resonant frequency can reach down to 0.2 THz with a branch height 
Lh=1.63nm and even to 0.06 THz with Lh=3.26nm. The ability to manipulate such low frequencies with small branches is the key advantage of the resonant mechanism over the scattering mechanism, where the size of scattering sources has to be larger than or comparable to the phonon wavelength. Generally, the phonon scattering mechanism provides an efficient way to block the high frequency phonons while it remains challenging in dealing with phonons with, for example, 
f≤2THz in Si NWs [53] and carbon nanotubes [54]. In contrast with scattering strategies, where the scattering source is located inside the material, the resonant mechanism applies the wave nature of phonons and is produced from a position outside from the main structure. As a result, the resonators are not supposed to scatter electrons. Although there is a substantial modification of the phonon band structure by the resonances, first principles calculations on core-shell NWs [55,56] and elastic theory studies on superlattices [57] have shown that phonon band modifications also have a minor effect on the electrical conductivity.
To achieve the most effective reduction of the TC, one should design a material in which contributions from both the high and low frequency modes are reduced. Since the scattering mechanism is efficient to block high frequency modes and resonances can stop low frequency phonons promisingly, the combination of both effects provides the possibility to block phonons in the full frequency range. Alloying is one of the most efficient ways to scatter high frequency phonons. As a consequence, we have designed a set of surrounded 
Si1−xGex NWs with Ge concentrations ranging from 0 to 1.

3 Results and discussion
Figure 1(e) reveals the Ge concentration dependent TCs for the pristine and surrounded NWs. The TC of the amorphous Si NW of the same size ( 1.7W/m⋅K) calculated at 300 K with the same method is shown as the dashed line for comparison. The TC of pure Si/Ge can be extensively reduced down to 2.5 W/m⋅K with a Ge concentration in the range of 0.3–0.7. Although extremely low TCs have already been obtained via alloy scattering, those values can still be reduced by a factor of 2 with the introduction of surrounded walls 1.63 nm in height. The simulated surrounded alloy NWs have TCs below the amorphous Si NW limit except in the case with 
x=0.1. This indicates that adding resonant structures could be crucial for improving the thermoelectric performance of SiGe alloys. Since the height of the walls can modify the resonant frequency, we also simulated the surrounded SiGe NWs with a doubled wall height. Because of the modification of the low frequency resonances, the TC is further reduced and a value of 
0.9W/m⋅K is obtained. This extremely low value is about half the corresponding amorphous Si NW TC and 36% of the SiGe alloy NW minimum. In other words, the combination of phonon resonant and scattering mechanisms produces a 22-fold decrease of the pristine Si NW TC.
To quantitatively clarify the respective role of phonon scattering and resonant effects on the TC reduction, we calculated the length dependent phonon transmission functions (Fig. 3) from nonequilibrium molecular dynamics [58,59] (details in Ref. [32]). The ballistic transmission, which is essentially the number of modes in the leads, is shown in all cases to facilitate the comparisons. Because of the anharmonic effect, the transmission of the high frequency phonons in the pristine Si NW decreases when the sample length increases [Fig. 3(a)]. However, ballistic transport can be clearly observed below 1 THz, where the transmission is only slightly decreased even for the NW length of 
1μm. In the surrounded Si NW [Fig. 3(b)], the most noticeable difference with the pristine Si NW case is that the transmissions at frequencies smaller than 4.5 THz are significantly suppressed. This phenomenon demonstrates the importance of the resonant effect at low frequencies, where the number of phonon branches is small but the group velocities are large. Contrary to the features in surrounded Si NWs, the transmission in the 
Si0.5Ge0.5 NW is dramatically reduced at high frequencies even at a very short length [Fig. 3(c)]. However, similar to the pristine Si NW case, ballistic transport is preserved below 1 THz. The strong transmission reductions at low frequencies caused by the resonant effect and at high frequencies due to the scattering effect finally result in reduced transmissions in the whole frequency range in the surrounded 
Si0.5Ge0.5 NWs [Fig. 3(d)]. Note that in the alloyed cases (both pristine and surrounded), the modes at high frequencies (typically above 10 THz) are not propagating modes due to the introduction of heavy Ge atoms. This is evidenced by the extremely low transmission even with a sample length of only 3.8 nm.
From the length dependent transmission functions, we evaluated the frequency resolved phonon MFPs [54,59] using Eq. S4 [32]. As revealed in Fig. 4, the MFP of the pristine Si NW increases rapidly as the frequency shifts below 4 THz. When adding the resonant structure on the surface of the Si NWs, the MFP above 4 THz is reduced roughly by one half. The MFPs of the low frequency phonons are even more reduced by orders of magnitude. Instead, if the scattering sources (Ge atoms) are embedded in the Si NW, the MFP is greatly reduced above 2 THz, while it is not significantly affected below 2 THz. The comparisons among the three samples provide additional evidence that the scattering and resonant mechanisms appear as an ideal combination to design extremely low TCs. This statement is supported by the MFP of surrounded 
Si0.5Ge0.5 NWs, where the MFP in the entire frequency range is reduced by orders of magnitude. The frequency response of the MFPs on resonances and alloys is also revealed by the TC cumulative functions [60] (Fig. S6 [32]).

4 Conclusion
In conclusion, we have performed molecular dynamics simulations on the TC of Si based resonant NW structures, i.e., branched and wall surrounded NWs. The results reveal that phonon resonance is a powerful TC reduction mechanism. Differing from the commonly adopted scattering method, low frequency phonons can be easily manipulated with small resonators. Both the resonant frequency and strength can be tuned by changing the resonator size, offering the freedom in controlling the heat flow more accurately. The combination of phonon resonant and scattering mechanisms yields a TC of 
0.9 W/m⋅K, which is only 53% of the corresponding Si NW amorphous TC limit and 4.5% of the value of the pristine Si NW. This achievement relies on the hindering of phonon transport in the whole frequency range. The extremely low TC here obtained also supports the potential impact of resonator based structures on thermoelectric conversion.

Interface diffusion-induced phonon localization in two-dimensional lateral heterostructures
ABSTRUCT
We report that the interface composition diffusion, which often  occurs  in the synthesis of  two- dimensional lateral heterostructures, can significantly suppress the thermal transport property. Our molecular dynamics simulations show that the thermal conductivity of graphene/h-BN lateral heterostructures can be largely tuned by varying the interface composition diffusion length. The under- ling mechanism is explained by Anderson localization of phonons, which is corroborated by the exponen- tial decay of the phonon transmission with composition gradient length. Phase breaking interactions are shown to delocalize the modes at elevated temperatures. The findings in this work suggest that compo- sition graded interfaces can be used to tune the thermal transport of heterostructures via phonon local- ization, which is important for their applications in electronics, thermoelectrics and thermal insulators

1 Instruction
Two-dimensional (2D) materials such as graphene, hexagonal boron nitrides (h-BN), and transition metal dichalcogenides (TMDs, e.g., MoS2) have intrigued experimental and theoretical workers for their exceptional electrical, thermal, and optical properties. These different 2D materials can be integrated into lateral 2D heterostruc- ture. The resulting artificial 2D structures provide access to new properties and applications beyond their component 2D atomic crystals and therefore, they are emerging as a new exciting field of research [1].
The synthesis of 2D lateral heterostructures has achieved signif- icant progress in recent years [2–15]. The composition diffusion at the interface, which can cause nonabrupt, graded boundaries, is a key issue of interface formation in conventional heterostructures as well as their monolayer analogs [4]. Notably, compositional transition is often reported at the interface of these 2D heterostruc- tures, which is typically in the range of 1 35 nm. Liu et al. [5] demonstrated the creation of graphene and h-BN (Gr/h-BN) in- plane heterostructures by using lithography patterning and sequential chemical vapour deposition (CVD) growth steps. The
sharpness of lateral interface between the h-BN and graphene layers was observed to be within 1 nm. Levendorf et al. reported the synthesis of Gr/h-BN lateral heterostructures with a patterned regrowth technique [6]. The obtained heterojunction has a compo- sitional transition width of no more than 10 nm. Sutter et al. [4] report that high-temperature growth leads to intermixing near the interface of Gr/h-BN lateral heterostructures, similar to interfa- cial alloying in conventional heterostructures. Duan et al. [7] designed a thermal CVD process to synthesis WS2-WSe2 lateral heterostructures. The junction shows a transition from WS2 to WSe2 phases over a length scale of 35 nm. Slight inter-diffusion of transition-metal elements is also often observed along the arm- chair interfaces of WS2 and MoS2 lateral heterostructures, typically over a width of 1–3 unit cells
There are several factors that lead to the composition-graded interface, including the temperature of the growth [4], the switch- ing rate of the vapour-phase reactants and (or) ion diffusion across the interface [7], and the edge stability during the epitaxial growth [8], etc. The Gr/h-BN interface can even be tuned from compara- tively sharp to graded by controlling the duration of a preheating period of the BN source [10]. While clean and sharp interfaces of  2D heterostructures are essential for preserving their optoelec- tronic properties [8], composition graded interface may have significant impact on the heat transport of 2D heterostructures. Unfortunately, little is known about the thermal property of 2D heterostructures with composition-graded interfaces, which is urgently needed for evaluating their potentials in the applications of electronics and thermoelectrics.
In this work, we report that the interface composition diffusion, even with a width of several nanometers, can significantly hinder the thermal transport property of Gr/h-BN lateral heterostructures. Our molecular dynamics (MD) simulations show that the thermal conductivity j of Gr/h-BN heterostructures can be largely tuned
by the interface composition diffusion length. Phonon localization is identified to be the cause of heat transfer suppressing in such structures at room temperature. The findings in this work suggest that introducing nonabrupt, graded interface to materials can be beneficial for their applications in thermoelectrics and thermal insulators, where low thermal conductivity is desired.

2 Method
We started from a 4.2  111.2 nm Gr/h-BN lateral heterostructure with an abrupt interface. The interface was built with C-B zigzag linking edges in our models, since the zigzag edges were reported to be more preferably formed than the armchair counterparts
[9]. The CAC, CAB, and BAN bond lengths were all set as 1.42 Å in the simulation with a lattice mismatch of  2:1% for h-BN, which has negligible effects on its thermal properties. The structures with composition-graded interfaces were
then generated by randomly switching the C (B/N) atoms by their B/N (C) counterparts across the interface. Within a distance L, the number of atoms ratio for C:B/N gradually decreases from 90%:10% to 10%:90%, as shown in Fig. 1(a). We studied the systems
where L equals 8 nm, 16 nm and 24 nm, respectively. Here we define L as the compositional transition width. At the same composition proportion, the heterogeneous atoms (for example, carbon atoms in h-BN matrix) can appear individually, alternatively, several C atoms can aggregate and form a ‘‘particle”. Thus in our study, models with four different sizes of the heterogeneous particles in
the composition grading were built, each respectively consists of one, two, six and ten atoms (Fig. 1(b)).
Models shown in Fig. 1 were used in the non-equilibrium MD (NEMD) simulations, for the phonon transmission spectra calculations.
Because of the non-linear temperature gradient in the composition graded structures, it is difficult to obtain the thermal conductivity (j) from the direct method in NEMD. We thus resort to equilibrium MD (EMD) simulations and the Green–Kubo method.
The initial structures shown in Fig. 2 were relaxed using a conjugate gradient energy minimization algorithm. 
Periodic boundary conditions were used in all directions. To mimic the free-stand structures, a vacuum of 20 nm is used in the direction perpendicular to the sheets. The MD time step was set to 0.5 fs. An MD run of
200 ps in the canonical ensemble (NVT) was performed to impose a temperature of 300 K. Then, the microcanonical ensemble (NVE) was applied for another 200 ps to achieve thermal equilibrium.
Finally, the instantaneous heat fluxes along the armchair axis jAðtÞ were collected from MD runs with lengths equal to 2 ns. The Green–Kubo formula [24] was used to evaluate the thermal conductivity (jA) along the armchair direction:
where kB is the Boltzmann constant, T the temperature, and V the system volume. The angular brackets denote the ensemble average, performed here over twenty independent MD simulations
To quantitatively clarify the respective role of phonon scattering mechanisms, we calculated the composition transition width (L) dependent phonon transmission functions (T(x)) from NEMD.
T(x) was calculated based on the method developed by K.Saaskilahti et al. [25,26]. The frequency resolved heat flux between any atom iand jcan be expressed as:
where tsimu and x are the simulation time and the frequency, respectively. ^vai ðxÞ and ^vbj ðxÞ are the Fourier transformed atomic velocities of atom i in direction a and atom j in direction b, respectively. For calculating the flux spectra, the force constant
matrix is calculated with the finite displacement method by LAMMPS, i.e., after relaxing the structures to the potential minimum, atom i is moved in the directions x;y; and  z with a small
value (D = 0.01 Å). After each displacement, the forces F of each atoms are calculated. As a result, the element of the force constant matrix can be evaluated as:
Here Fb j and Fbþ j denote the force on atom j in b direction when atom i is displace to -a and +a directions, respectively.
The heat current across any interface separating adjacent atom sets L and R can be obtained by summing over atoms in each set:
The phonon transmission function can thus be defined according to the spectral heat current as
where DT is the temperature difference of the two thermal baths in NEMD simulations. The atoms at the two ends of the structures shown in Fig. 1 are
fixed. The atoms located within the distance Lbath = 30 nm from left and right adjacent to fixed areas are coupled to hot and cold Langevin heat baths at temperatures L þ DT=2 and L  DT=2, respectively.
The MD time step was set to 0.5 fs, and the coupling time constant of the Langevin thermostat was chosen as stime = 1 ps.
The mean thermal bath temperature was fixed as T = 300 K, and DT was set as 60 K.

3 Results and discussion
Fig. 3 shows the calculated thermal conductivity of Gr/h-BN
with different composition diffusion lengths and different sizes
of heterogeneous particles. The j of Gr/h-BN with abrupt interfaces
is calculated as 324 W/mK, and the structures with composition
graded interfaces clearly possess much lower j. In general, j
decreases against L for all the models, indicating that the composition
transition width is an influential factor in tuning the thermal
transport property. This result can be understood as that when
the spatial extent of the composition diffusion is larger, phonon
scattering will occur in a broader spatial range along the structure
and results in a shorter phonon mean free path, which is corroborated
in the next section. Similar phenomenon was observed in
SiGe superlattice nanowires with composition graded interfaces
[27]. Note that j of the BNC hexagonal structures with random
composition distributions is calculated as 58 W/mK, indicating the
lower limit of j in Gr/h-BN lateral heterostructures.
At L = 8 nm, the size of the heterogeneous particle has obvious
impact on j: smaller particles lead to lower j. It can be seen from
Fig. 1(b) that at the same position along the structure, smaller
heterogeneous particles are more scattered, which leads to a
greater chance for phonon scattering. Indeed, the phonon mean
free path of graphene with h-BN domains was reported to decrease
with decreasing domain size [28]. As L increases, the particle size
impacts j less because L predominates the j reduction.
Fig. 4 depicts the composition transition width-dependent phonon
transmission spectra at 300 K. Here we focus on the systems
with heterogeneous particles consist of one atom and six atoms
(Fig. 4(a) and (b), respectively). For a comparison, the system with
uniform interface intermixing (no composition grading) is also
studied. In general, the investigated systems exhibit similar behavior:
the phonon transmission decreases with L for most of the frequencies.
A lower phonon transmission indicates more hindrance
in thermal transport, which is in accordance with the
L-dependent thermal conductivity shown in Fig. 3. The ballistic
transport can be observed below 1 THz for the composition
graded structures, where the transmission is insensitive to L.
For higher frequencies, the crossover from the ballistic to the
diffusive regime is expected to occur, where a power law decay
with composition grading length (T(x)/ L1) should be observed
[29]. As L increases, the heterogeneous particles-induced backscattering
effects may become increasingly important, and the localization
regime can be established, featured by the exponential
decay of phonon transmission [30] as T(x)/ eL=n. Here n denotes
the phonon localization length, which indicates the typical length
needed for the occurrence of phonon localization in disordered systems.
Phonon localization has been previously observed in
graphene-based systems such as amorphous graphene [31], disordered
graphene phononic crystals [32] and graphene nanomesh
[33], etc. It was also found in silicon nanowires [34,35]. To identify
the phonon suppressing mechanism in our systems, we fit T(x)
with both power law decay (Eq. (6)) and exponential decay ((Eq.
(7)) with different L for each frequency:
where A is a fitting parameter, and T0ðxÞ is the phonon transmission
coefficient of the structure with abrupt interface. We used
the coefficient of determination, R2, to evaluate the goodness of
the fit [36]. An R2 of 1 indicates that the model perfectly fits the
data.
Interestingly, fitting TðxÞ to Eq. (6) gives low values of R2
(below 0.7) over the whole frequency range of the three structures,
suggesting that the diffusive regime in our structures is not distinct
and phonon mean free paths (lmfp) and n are not well-separated. In
contrast, fitting TðxÞ to Eq. (7) can give very high values of R2,
depending on the frequency, which provides the evidence of
Anderson localization for the modes of these frequencies. From
the well-fitted curves (R2 P0.96), we can obtain the frequency
dependent phonon localization length, as plotted in Fig. 5(a). It
can be clearly seen that more phonon modes (especially with frequencies
below 10 THz) are localized in the models with
composition-graded interfaces, with respect to the one with uniform
intermixing section. It also shows that larger scatter size (larger
heterogeneous particle) traps more phonons due to Anderson
localization.
Anderson localization for a certain phonon frequency will happen
only if incoherent effects are weak. At higher temperatures
where the anharmonic relaxation length (l/) is comparable to or
smaller than the localization length (i.e. l/ 6 n), the localization
effects will not take place. In other words, the anharmonicity will
destroy phonon localization at elevated temperatures [37,38]. To
gain more insights, we calculated n at higher temperatures
(500 K and 800 K), and the results are shown in Fig. 5(b), (c) and
(d). Indeed, for all the three models, the population of localized
modes obviously decreases with temperature, over the whole frequency
range.
From the L-dependent phonon transmission spectra, we evaluated
the frequency resolved lmfp [39] from the relationship
TðxÞ ¼ NðxÞ 1 þ L lmfp 1, where NðxÞ is the number of phonon
modes at a given frequency in pure graphene. The calculated lmfp
for the model with 6-atom heterogeneous particles and the one
with abrupt interface are plotted in Fig. 6(a). Also plotted is the
anharmonic relaxation length (l/) in graphite materials, which is
roughly estimated [40] from l/ ¼ BT1x2; B ¼ 3:35  1023 mKs2.
As revealed in Fig. 6(a), lmfp of Gr/h-BN lateral heterostructure
decreases with composition diffusion width, which complies with
our thermal conductivity calculations. l/ is estimated to be orders
of magnitude larger than lmfp in Gr/h-BN lateral heterostructures,
which confirms that the incoherent effects are weak in our systems
at 300 K.
In addition, we calculated the Thouless length [41,42]
n0 ¼ NðxÞlmfp, which is a crude estimation of the localization length.
According to Fig. 6(b), n0 is larger than n, suggesting that the Thouless
relation does not hold in our case. There are two possible reasons
for this discrepancy: firstly, the Thouless relation is under the
assumptions of weak disorder and isotropic scattering. Our systems
with interface diffusion has an anisotropic feature in composition
through the graded interface, which may cause anisotropic
scattering. Secondly, the Thouless relation requires the diffusive
and localized regimes are clearly separated [43]. However, the failure
of fitting TðxÞ to Eq. (6) (R2 60.7) indicates that the diffusive
regime in our particular system is indistinct, thus the separation
of transport regimes is not so clear-cut. The relatively small number
of transport channels (NðxÞ  20) also verifies this argument
for the occurrence of phonon localization in the systems under
investigation remains in the length scale of tens of nanometers,
which is not quite affected by the phonon frequency.
The interface roughness is reported to slightly increase the
interface thermal conductance by changing the contact area [44–
46]. It is interesting to note that the interface diffusion introduces
disorders over large spatial extent (to tens of nanometers) around
the interface rather than local topography change, which severelly
scatters phonons over the whole disordered structure.

4 Conclusion
In summary, we report that the thermal conductivity of
graphene/h-BN lateral heterostructures is largely dependent on
the structure of the interfaces. Our molecular dynamics simulations
show that the length of the interface composition diffusion
can be used to tune the thermal conductivity of the heterostructures.
The analyses of phonon transmission function prove the
existence of the localized phonons, which is identified to be the
primary cause of the phonon suppressing. Phase breaking interactions
are shown to delocalize the modes at elevated temperatures
due to the increased anharmonicities. This work suggests that the
interface formation of 2D lateral heterostructures has significant
impact on the thermal transport, and nonabrupt, graded interface
can be beneficial for their applications in thermoelectrics and thermal
insulators, where low thermal conductivity is desired.Co-designing electronics with microfluidics for more sustainable cooling

Abstract:
Thermal management is one of the main challenges for the future of electronics1–5. With the ever-increasing rate of data generation and communication, as well as the constant push to reduce the size and costs of industrial converter systems, the power density of electronics has risen6. Consequently, cooling, with its enormous energy and water consumption, has an increasingly large environmental impact7,8, and  new technologies are needed to extract the heat in a more sustainable way—that is, requiring less water and energy9. Embedding liquid cooling directly inside the chip is a promising approach for more efficient thermal management5,10,11. However, even in state-of-the-art approaches, the electronics and cooling are treated separately, leaving the full energy-saving potential of embedded cooling untapped. Here we show that by co-designing microfluidics and electronics within the same semiconductor substrate we can produce a monolithically integrated manifold microchannel cooling structure with efficiency 
beyond what is currently available. Our results show that heat fluxes exceeding 1.7 kilowatts per square centimetre can be extracted using only 0.57 watts per square centimetre of pumping power. We observed an unprecedented coefficient of performance (exceeding 10,000) for single-phase water-cooling of heat fluxes exceeding 1 kilowatt per square centimetre, corresponding to a 50-fold increase compared to straight microchannels, as well as a very high average Nusselt number  of 16. The proposed cooling technology should enable further miniaturization of electronics, potentially extending Moore’s law and greatly reducing the energy consumption in cooling of electronics. Furthermore, by removing the need for large external heat sinks, this approach should enable the realization of very compact power converters integrated on a single chip.

1.Introduction:
In the USA alone, data centres consume 24 TWh of electricity and 100 billion litres of water to satisfy their cooling demands8, correspond-ing to the residential needs of a city of the size of Philadelphia12–14. The environmental impact of this information technology infrastructure is expected to increase dramatically9, for example, accounting for up to 31% of Ireland’s electricity demand by 2027 (ref. 15), in large part due to the power consumption of cooling systems. This development is accompanied by the constant push to shrink the size of semiconductor devices, which results in higher heat fluxes that become increasingly challenging to extract and require new cooling solutions. A similar need is observed in power electronics, as the electrification of our society demands more powerful, more efficient and smaller energy conver-sion systems. Wide-bandgap semiconductors, such as gallium nitride (GaN), are promising candidates for this purpose16. These materials enable much smaller dies than those of traditional semiconductors as well as the monolithic integration of power devices, supporting the miniaturization of complete power converters into a single chip17. However, to unlock the full potential of GaN, strategies for sustainable cooling of high-heat-flux applications are required.
Substantial research efforts have focused on improving the thermal path between the hotspot and the coolant. However, heat extraction capability is fundamentally limited by the thermal resistance between the semiconductor die and packaging. Furthermore, relying on large heat sinks reduces the power density and hinders integration, since devices cannot be densely packed. Bringing the coolant in direct contact with the device may be a way to overcome this limiting factor, for exam-ple, by impinging coolant on a bare die18 or by etching micrometre-sized channels directly inside the device to turn the substrate into a heat sink. The latter technique demonstrated state-of-the-art cooling per-formance due to the highly efficient heat transfer at the microscale19–21. The high pressure drop and large temperature gradients associated with these straight, parallel microchannels (SPMCs) were overcome by splitting the flow into multiple parallel sections, and distributing the coolant over these channels using manifolds22Early investigations23–26 and systematic numerical studies27–30 of manifold microchannel (MMC) heat sinks showed a large reduction in pumping power requirements and thermal resistance compared to SPMCs. Excellent heat extraction has been demonstrated with copper MMCs31, compact micro-fabricated multilayer silicon structures32–35 and by using additive manufactur-ing36,37. However, in all these approaches, the heat sink and electronic structure and fabrication process are considered separately, either by integrating a simple resistive heater functioning as the heat source, or by bonding the MMC structure to a commercial device38. This leaves the large potential of MMCs untapped. Improving the thermal coupling between the heat source and cooling has been investigated for hotspot mitigation39–41, but has remained unexplored in a complete device structure. Furthermore, despite much MMC heat sink research, the increasing complexity and associated reliability concerns caused by  the multiple bonded layers required for coolant delivery have pre-vented the adoption of MMCs in commercial devices.
In this work, we address these concerns by combining cooling and device design, using an approach in which a MMC heat sink is designed and fabricated in conjunction with the electronics. We present a mono-lithically integrated manifold microchannel (mMMC) heat sink in a single-crystalline silicon substrate with an epilayer, produced without the need for cumbersome bonding steps. Here the device design and heat-sink fabrication are combined within the same process, with bur-ied cooling channels embedded directly below the active area of the chip. Coolant thus impinging directly on the heat sources provides local and efficient heat extraction (Fig. 1a). On the back of this same substrate, manifold channels spread the liquid over the die (Fig. 1c) to obtain high temperature uniformity and low pressure drop, lead-ing to a very low pumping-power consumption and vastly improved cooling performance. Since the electronics and microfluidics are fully coupled and aligned (Fig. 1b), we call this approach microflu-idic–electronic co-design. We demonstrated microfluidic–electronic co-design on GaN-on-Si, a low-cost platform that is promising for real-izing high-power converters on a chip, comprising a GaN epilayer a few micrometres thick on a low-cost silicon substrate. The passive silicon substrate typically lacks functionality, but by turning it into an active cooling layer, it has the potential to extract extreme heat fluxes,  without requiring the added cost of high-thermal-conductivity  substrates. Our results show that considering cooling as an integral part of device design can result in orders-of-magnitude improvements in cooling performance. We use this embedded-cooling approach  to demonstrate a super-compact GaN-on-silicon integrated  alternating–direct current (a.c.–d.c.) converter, containing four  power devices on the same microfluidic-cooled chip, and yielding a power density of 25 kW dm−3. A simple multi-layered printed cir-cuit board (PCB) was designed to direct the coolant flow into the  semiconductor device

2.Co-design concept and fabrication:
Our co-design approach, in which each heat source is coupled to an individual buried cooling-channel serving as a local heat sink, is particularly of interest in GaN power electronic applications with a lateral high-electron-mobility transistor (HEMT) structure. Typi-cal source–drain spacing for HEMTs in >1-kV applications matches  the optimum dimensions for microchannel cooling of about 20 µm (refs. 19,42–44). Therefore, we investigated a GaN-on-Si device structure in which liquid impinges directly onto the epilayer below each contact, ensuring minimum thermal resistance between the hotspot and cool-ant. In this structure (Fig. 1a), the GaN epilayer provides the power electronics (Fig. 1b), and the silicon functions as a microchannel cooling and fluid-distribution network in a three-dimensional arrangement(Fig. 1c). Figure 1d illustrates the corresponding fabrication method.  A staggered pattern of slits was formed in the Si by anisotropic deep etching through narrow incisions in the AlGaN/GaN epilayer to achieve the desired microchannel depth. This pattern provided better struc-tural integrity of the epilayer during fabrication compared to con-tinuous slits. During the subsequent isotropic gas-etch, the channels widened and coalesced in the silicon substrate, while being monitored through the transparent GaN epilayer using an in situ optical etch-rate tracking. This two-step etching process provides independent control over channel width and depth, making it suitable to a wide range of contact pitches. The incisions were finally hermetically sealed during the device metallization step. The Methods and Extended Data Fig. 1 explain the fabrication procedure in detail. Figure 1e shows a scan-ning electron microscope (SEM) image of the device after the metal-lization step with sealed channels. Because of the narrow incisions in the epilayer, the contacts do not require substantial oversizing. The microchannels are in direct contact with the active area of the chip, thus providing excellent thermal coupling between the hotspot and the cooling channel (Fig. 1f). Through micrometre-sized openings in the epilayer, 125-µm-deep and 20-µm-wide channels were created in the silicon substrate (Fig. 1g, h).
A series of devices was fabricated with SPMCs with equal width and spacing of 100 µm, 50 µm and 25 µm, and a channel depth of 250 µm in GaN-on-Si power devices, functioning as reference heat sinks (Fig. 2a) for evaluating the performance of the co-designed electronic-microfluidic mMMC devices. Three mMMC chips with 2, 4 and 10 inlet and outlet manifold channels and identical 20 × 125 µm microchannels were fab-ricated, referred to as the 2×-, 4×- and 10×-manifold chips (Fig. 2b). Figure 2c shows a picture of the mMMC device with the 10×-manifold, including a schematic (Fig. 2d) to illustrate the flow path with coolant impinging directly onto the bottom of the GaN epilayer.
Thermo-hydraulic evaluation:
A thermo-hydraulic analysis, using de-ionized water as a coolant, was performed on the cooling structures (Fig. 2a, b) to assess the cool-ing performance by measuring the thermal resistance, pressure drop and the resulting cooling coefficient of performance (COP), which indicates the energy efficiency of the heat sink. Figure 3a shows the total thermal resistance (Rtotal) between the surface temperature rise and the inlet temperature for the evaluated structures. By reducing the SPMC channel dimensions from 100 µm to 25 µm at identical flow rates, Rtotal reduces, which can be attributed to the increased surface area for heat transfer. However, the 4×- and 10×-manifold heat sinks show an additional substantial reduction in Rtotal compared to the 25-µm SPMC, approaching the limit of single-phase water-cooling (defined by its heat capacity). Rtotal was separated into three components: the contribution due to the heating of the water based on its heat capacity (Rheat), the contribution due to convective heat transfer in the micro-channels (Rconv), and the contribution due to conduction (Rcond). The full data reduction procedure to obtain these values is explained in the Methods and in Extended Data Fig. 3. A breakdown of Rtotal is shown in Fig. 3b, revealing a strong relation between Rconv and microchannel size, where smaller channels reduce Rconv. A large decrease in Rconv was achieved with the 10×-manifold, resulting in an 85% and 76% reduc-tion compared to the 50-µm and 100-µm-wide SPMCs, respectively. In combination with a very low Rcond for the co-designed manifolds, at a flow rate of 1.0 ml s−1, a thermal resistance of 0.43 K W−1 was achieved. The 10×-manifold design thus allows heat fluxes up to 1,723 W cm−2 for a maximum temperature rise of 60 K, which is more than twice that of a 25-µm-wide SPMC.
Narrow channels, however, require a higher pressure to achieve equal flow rate (Fig. 3c). For a flow rate of 0.5 ml s−1, SPMC widths of 100 µm, 50 µm and 25 µm require pressures of 160 mbar, 260 mbar and 810 mbar, respectively. The manifold structure substantially low-ers the pressure drop by reducing the length of the flow path through the microchannel. When splitting the flow into smaller sections with the 10×-manifold, the pressure drop reduced to just 210 mbar. This highlights the benefit of the MMC structure: a lower thermal resistance than SPMCs can be obtained at a reduced pumping power consumption. However, although the manifold structure can reduce the pressure drop, the additional contractions and turns of the fluid can hinder this reduction. For example, 20-µm-wide microchannels in a 4×-manifold require a higher pressure of 1,300 mbar compared to the 25-µm-wide SPMC, which in part can also be attributed to the higher fluid veloc-ity given that the mMMC channels (125 µm) are not as deep as in the SPMC (250 µm). These findings demonstrate the need for a carefully optimized geometry of the microchannel and manifold.
Figure 3d shows a clear trend for SPMCs of increased effective base-area-averaged heat-transfer coefficient (heff) for smaller micro-channels. This is due to the combined effect of the increased sur-face area and local heat-transfer coefficient in the fully developed laminar-flow regime. The co-designed 4×-mMMC structures matches this trend with heff = 3.1 × 105 W m−2 K−1, but a large deviation from this pattern is observed when the effective length through which the cool-ant flows in the microchannel is reduced. For the 10×-manifold, heff more than doubles to 7.3 × 105 W m−2 K−1, a rise that can be attributed to the high Nusselt number owing to the developing flow in the MMC structure27,45. This effect becomes more pronounced when consider-ing the wall-area-averaged heat-transfer coefficient (hwall) (Fig. 3e), which eliminates the contribution of the increased surface area from the heat-transfer coefficient, as well as accounts for the limited fin efficiency of the channels. Over a threefold increase in hwall is observed between 25-µm-wide straight microchannels and the 10×-manifold heat sinks, up to 2.4 × 105 W m−2 K−1. This value corresponds to a very high Nusselt number of 16, generally only achieved in larger-scale systems, or in more complex two-phase cooling systems, highlighting the superior thermal performance of this structure.
The combination of improved heat transfer and reduced pressure drop leads to much lower pumping power requirements. The cooling COP is defined as the ratio of extracted power to the pumping power required to provide such a level of cooling, while maintaining a max-imum surface temperature rise of 60 K. Higher heat fluxes require higher flow rates, reducing the COP owing to the larger pumping power required. Figure 3f benchmarks the evaluated devices, along with other technologies found in the literature. For SPMC, channel widths of 100 µm, 50 µm and 25 µm show a consecutively higher COP for higher heat fluxes, with a COP in the range between 102 and 104 and heat fluxes between 350 W cm−2 and 800 W cm−2. The 10×-manifold device vastly outperforms these SPMCs. At an identical COP of 5.0 × 103, the 10×-manifold can sustain heat fluxes up to 1.7 kW cm−2 at 1.0 ml s−1, compared to 400 W cm−2, 450 W cm−2 and 550 W cm−2 for the 100-µm, 50-µm and 25-µm SPMCs, respectively. Furthermore, at a heat flux of 780 W cm−2, the 10×-manifold provides a 50-fold increase in COP with respect to 25-µm SPMCs. Compared to MMC heat sinks presented in the literature, the proposed mMMC device outperforms the current state of the art, and demonstrates a large potential for energy-efficient cooling by having a thermal-centred approach in the device design 

3.Power integrated circuit with embedded cooling:
The lateral nature of AlGaN/GaN electronics enables the monolithic integration of multiple power devices onto a single substrate. This opens up opportunities for power electronics, whereby an entire con-verter can be integrated on a small chip, with large potential for energy, cost and space savings. However, the resulting high heat fluxes limit the maximum output power of the chip. To demonstrate the potential of embedded cooling in a semiconductor device, we monolithically integrated a full-bridge rectifier onto a single GaN-on-Si die. Rectifi-cation was provided using four high-performance tri-anode Schottky barrier diodes with a breakdown voltage of 1.2 kV and high-frequency capability up to 5 MHz (ref. 46). 50-µm-wide cooling channels were integrated on the silicon substrate (Fig. 4a). To fully benefit from the compactness of high-performance microchannel cooling, a three-layer PCB with embedded coolant delivery channels was developed and used to guide the coolant to the device (Fig. 4b). The full fabrication of this monolithically integrated power device and the PCB is described in the Methods and shown in Extended Data Fig. 8. The device was finally fluidically connected to the PCB using laser-cut liquid- and solvent-resistant double-sided adhesive, providing a leak-proof con-nection. This method is low-cost and easy-to-prototype, and translates well to conventional solder bonding. Figure 4c, d shows the converter implemented, with a very compact form factor, rectifying an a.c. signal with peak voltage and current of 150 V and 1.2 A, respectively (Fig. 4e). Integrated liquid cooling led to a small temperature rise of 0.34 K per watt of output power. For a maximum temperature rise of 60 K, this single die can thus produce an output power of 176 W at a flow rate of only 0.8 ml s−1. Furthermore, the reduced operating temperature led to an increased conversion efficiency (Fig. 4f) by eliminating self-heating degradation from the electrical performance. The a.c.–d.c. converter was experimentally evaluated up to 120 W of output power, while the temperature rise stayed below 50 K (Fig. 4g). Considering the small converter volume (4.8 cm3), this corresponds to a high power density of 25 kW dm−3. Moreover, since all cooling occurs within its footprint, multiple devices can be densely packed onto the same PCB to increase the output power. This is a clear benefit over conventional heat sinks relying on heat spreading to large areas. These results show that the proposed high-performance cooling approach can enable the realiza-tion of high-power (kilowatt range) converters of the size of USB sticks in the foreseeable future.

4.Discussion and outlook:
We present an approach for co-designing microfluidics and electronics for energy-efficient cooling, and demonstrate it on GaN-on-Si power devices by turning the passive silicon substrate from a low-cost carrier into a high-performance heat sink. COP values above 10,000 for heat fluxes surpassing 1 kW cm−2 could be obtained by focusing on cooling in an early stage of the device design. As a practical implication, the average added-energy expenditure of more than 30% for cooling in data centres could potentially drop below 0.01% by adopting this design approach. The entire mMMC cooling structure can be monolithically integrated within the substrate, requiring only conventional fabrica-tion procedures, thus making this economically viable. To realize this concept, solutions for the packaging and interconnects are required. The PCB-based fluid delivery presented provides an example of a way to use these co-designed chips, based on components familiar to the electronics designer. This means that, in order to provide maximum energy savings, cooling should be an integral step in the entire elec-tronic design chain, from the device to the PCB design, and not merely an afterthought. If these practicalities can be addressed, we anticipate that the co-design of microfluidic and electronics will be appropri-ate for energy-efficient thermally-aware electronics design. This may aid in solving critical challenges in electronics applications, as well as enabling future integrated power converters on a chip to support the electrification of our society in a sustainable manner.ABSTRACT
An interconnected energy hubs (EHs) framework aims to enhance the efficiency of the multicarrier
energy system through realizing optimal coordination among existing players. In this paper, EH concept
is studied in networked microgrids (MGs) structure to exploit the potential capabilities of microgrids in
satisfying various types of energy demands. In the proposed model, MGs and distribution network are
independent entities which have their local scheduling problem. To coordinate the operation of entities,
a distributed algorithm based on the alternating direction method of multipliers (ADMM) is exploited to
resolve the conflict of exchanged power between multi-MGs and distribution network. Considering the
uncertainties, a distributed robust model is employed to precisely analyze the performance of multi-
carrier energy networked MGs in different robustness levels. The efficiency of the ADMM model on
integrated energy systems is tested on a networked MGs. The achieved results can ensure the light
computational burden and convergence of proposed distributed algorithm. The proposed EH optimiza-
tion problem is solved via Gurobi optimizer packages. According to the results, the ADMM converges to
the final solution after 5 iterations, and with increasing the robustness level, the operation costs of EHs
increases. The obtained results by Gurobi is more optimal than heuristic algorithms.
1. Introduction
1.1. Motivation and significance
The development of dispatchable and non-dispatchable
renewable energy sources has affected the energy networks and
introduced brand new energy concepts. The energy hub (EH)
concept is one of the important consequences of renewable energy
utilization. The energy hub is a concept by which the energy con-
version, storage and generation can be realized in a higher effi-
ciency. The EH integrates a wide range of energy sources aiming at
increasing a network efficiency and decreasing the energy wasting.
Besides, deployment of distributed energy resources (DERs) can
lead a system towards an economical, environmentally friendlyand
flexible network [1]. According to the energy hub concept, various
energy carriers are exploited to satisfy a range of energy demands
at any given time slot [2]. Today’s energy hubs take advantage of
modern technologies such as storage systems, and optimal control
centers for energy management and energy trading purposes.
Considering this capability, the size of EHs can include a wide range
of the energy system depending on demands, infrastructures and
financial budget [3].
The necessity of energy hubs is realized when the optimal
operation of integrated energy systems leads the network toward
efficient utilization of existing energy sources to satisfy different
demands with minimum economic and emission costs [4]. To
achieve optimal operation of multiple EHs, different methods have
been already proposed. In Ref. [5], a mixed integer nonlinear pro-
gramming (MINLP) model is applied for the sake of direct inter-
plant heat integration to make the controllability of energy hubs
flexible when accidentally shutdowns happen; however, the coor-
dination between EHs is not discussed and guaranteed for the large
networks. In Ref. [6], three different schemes are discussed. In the
first one, energy hubs operate independently without any energy
sharing with other entities. In the second scheme, EHs can share
energy with each other through pool-market concept, while the
impact of aggregator can be observed in last scheme in which the
game-theoretic approach based on Stackelberg game is proposed to
solve the energy hub problem in a bilevel scheme. The conditional
value-at-risk (CVaR) algorithm is employed in Ref. [7] to ensure the
optimal operation of smart EHs when it comes to a potential risk in
energy hub dailyscheduling. To simplifya complicated EH, Liu et al.
[8] have divided the complicated model to several simple EHs, and
then an approach based on virtual nodes insertion is employed to
solve the EH problem. To ensure the optimal costs of energy hubs, a
probabilistic approach based on Monte Carlo method is used totake
variations of renewable sources into account [9]. Benders decom-
position (BD) algorithm is employed to address the environmental
and social objectives of EHs and decrease the computational
burdenof EH problem [10]. In Ref. [11], the uncertainty dealing with
plug-in hybrid electric vehicles (PHEVs) is modeled using
information gap decision theory (IGDT) to exactly determine the
risk-averse and risk-seeking strategies of the EH. To model the
optimal bidding strategy of energy hub, a stochastic optimization
model is adopted in Ref. [12] to handle the uncertainties associated
with market price and wind energy.
Smart grid technology specifically microgrids (MGs) concept has
changed the reconfiguration of distribution networks and made the
power system sustainable, reliable, and economical [13]. The U.S.
Department of Energy (DOE) clearly defines the microgrid as a key
component of smart grid aiming at enhancing reliability, resiliency,
energy efficiency, and quality [14]. Although microgrids have many
technical benefits, they may have some environmental issues,
which should be considered. In Refs. [15], the authors have dis-
cussed the fuel-based and renewables microgrid in Canada.
Nowadays, in Canada, the experts and organizations try to consider
social and environmental benefits of microgrids along with
economical aspects. Besides, the authors in Ref. [16] have consid-
ered the environmental aspects of islanded microgrids by propos-
ing an optimal planning model aiming at decreasing the
environmental issues of microgrids. The concept of the micro en-
ergy grid is employed in Ref. [17] to realize the main goals of EHs
considering the optimal coordination between energy carriers. It
should be mentioned that MGs were previously utilized as a
framework for satisfying the electrical power scheduling in distri-
bution networks. The most important objectives of electrical MGs
are to minimize the operation cost of small-scale energy zones [18],
enhance the resiliency of the grid in case of natural disasters [19],
and increase the reliability [20]. Moreover, in Ref. [21], the stability
issue of microgrids in both interconnected and islanding mode is
improved using large-signal-based control topology for distributed
generator (DG) power converter. The power quality is another
important issue in power systems, which is improved in inter-
connected DC microgrids [22]. In Ref. [23], the authors have used a
two-stage stochastic optimization problem to solve optimal oper-
ation of energy hubs. The proposed solution ensures the optimal
scheduling of energy resources and reserve capacity. Moreover, the
authors have assumed that the limits for energy flows, voltage
magnitudes, and nodal natural gas pressures are in acceptable
range so that the security of the network is guaranteed. In Ref. [24],
EH takes advantage of the generation-storage framework of a
microgrid to ensure the optimal scheduling of multi-energy car-
riers; however, the proposed structure of the MG and EH formu-
lation is simple and the proposed model can face with significant
challenges in larger and multiple interconnected energy entities. In
Ref. [25], AC/DC microgrid is employed to increase the energy
trading between energy providers and consumers through
ensuring efficient energy conversion using a two-stage chance-
constrained bidding scheme. Interconnected MGs are used in
Ref. [26] aiming to optimally coordinate the energy performance of
entities, and a distributed stochastic method is used to solve the
daily scheduling problem which requires a minimum range of in-
formation; however, the performance of proposed method is not
precisely discussed the optimal coordination between MGs in en-
ergy interaction.
1.2. Related works
Several studies have been carried out so far to address the co-
ordinated operation of multiple MGs in which microgrids have
energy interactions with each other through the distribution
network [27]. In Ref. [28], a decent study has been reported on
various energy management systems (EMS) applicable to multi-
MGs, and a new structure based on nested energy management is
presented to ensure the security of each entity and fix the existing
drawbacks of conventional EMSs. In centralized energy manage-
ment system of MGs, EMS aims to optimize the existing objective
function for the entire network [29]. In this type of management
system, all available entities should send their local and private
data and information to the central EMS operator or distribution
network operator (DNO), and then the operation cost of each MG is
optimized by external decision-maker. Therefore, the centralized
energy management system not only cannot guarantee the security
of the entities but also cannot ensure the optimal coordination
between MGs. On the other hand, decentralized energy manage-
ment system of MGs is offered to fix the drawbacks of centralized
EMS when it comes to interconnected operation of networked MGs
[30]. To solve the optimization problem in decentralized mode of
networked MGs, distributed algorithms are needed. In Ref. [31], a
model predictive control (MPC) is employed to optimally manage
the interaction of power among microgrids. Nevertheless, this
technique strongly needs to identify the system model. Besides, the
alternating direction method of multipliers algorithm is employed
in Ref. [32] to satisfy optimal power trading between MGs and
distribution network. In Ref. [33], Lagrangian relaxation is used for
decentralizing the isolated MGs so that power exchanging is real-
ized. A non-cooperative game-theoretic approach is employed in
Ref. [34] to guarantee the optimal coordinating control among
multi-MGs. Nevertheless, a few studies have been carried out in
multiple energy carriers networked MGs. The existing studies have
tremendously focused on electrical energy in networked MGs and
paid less attention to other types of energy carriers.
In [35], the authors have utilized an iterative algorithm to
optimally manage the existing DERs. In the proposed algorithm, the
main input data consist of prices, efficiencies, forecasted power
generation of distributed energy resources and load. In other
words, the local controllers should keep the main controller of the
network informed of all data of existing local control centers to
minimize the operation cost of the system. Therefore, the proposed
structure is centralized. In Refs. [23], the authors have taken
advantage of a two-stage stochastic optimization to address the
operation planning of multi-carrier energy systems. In the pro-
posed architecture, a simple microgrid structure without consid-
ering the complicated function of control centers is used in
coordinating the microgrid and main grid. In Ref. [36], an isolated
MG providing the electricity and heating demands is considered.
The proposed structure cannot guarantee the efficiency of the
isolated microgrid in case of lack of electrical and heating energies.
Moreover, meta-heuristics algorithms are employed to optimize
the operation cost of the energy hub. Therefore, the optimal solu-
tion obtained by heuristic algorithms can be doubtful due to local
answers. Similarly, in Ref. [37], a single MG architecture is deployed
to provide the electrical and heating loads of the network. The
proposed energy management system is centralized, which means
that the microgrid operator has access to all data of network loads
and generation level of generators. Therefore, this type of frame-
work can be challenging even after a simple cyberattack, in which
some communication signals can be manipulated to mislead the
optimal scheduling of the microgrid. It should be noted that the
proposed heuristic algorithm may not be able to guarantee the
global minimum for the problem. In Ref. [3], there exists an energy
sharing between energy hubs. The proposed EMS is based on
centralized operation mode. Besides, the authors have utilized a
numerical method, called interior-point to overcome the conver-
gence of the proposed heuristic algorithm to a local minimum.
1.3. Contributions
In this paper, the energy hub concept is realized in a networked
MGs environment. As it was mentioned in the related works, all
studies have been carried out in a centralized operation mode of
energy management system. In other words, the EMS should have
access to data and information of all entities to fulfill an optimal
scheduling of the network. Therefore, the privacy of the entities are
not important in centralized EMS. Besides, the most works in en-
ergy hubs are studied either in regular distribution network
without presence of microgrids or a single microgrid structure in
islanded mode, while in this paper, a networked microgrids
structure is presented, inwhich all MGs are interconnected and can
share energy in a secure and safe environment with least data and
information sharing with distribution network operator.
According to the U.S. Department of Energy reports, microgrids
are the future of smart distribution grids. Additionally, according to
IEEE 1547 Standard, the interconnection and coordination of
distributed resources is critical to create the networked microgrids
structure [38]. Hence, the necessity of power systems studies in
networked microgrids is inevitable. In this paper, multiple energy
carriers networked microgrids is studied to satisfy various energy
demands of the customers. The interdependence of different en-
ergy infrastructures is an inevitable part of modern power systems.
In this regard, networked microgrids can provide a secure, efficient,
reliable, and resilient opportunity to various energy infrastructures
to satisfy the energy demands. Besides, the secured optimal coor-
dination between existing entities can guarantee the energy
preparation at any given time. Due to conventional structure of
distribution grids, providing energy is much more expensive than
networked microgrids framework. In networked microgrids, each
entity can connect to other entities to supply its energy re-
quirements in a distributed operation mode to guarantee the pri-
vacy and optimal energy trading, while in centralized mode or
conventional structures, the main network supervisor has to collect
all data from all existing entities and then optimize the energy
scheduling without considering the privacy of them. To sum up, the
networked microgrids as the future generation of distribution grids
can lead the power systems toward efficient and resilient energy
management, and in this regard, energy hubs concept can be
realized in this architecture considering the optimal operation of
natural gas and electricity infrastructures.
As the power and gas networks are interdependent, the optimal
energy trading between microgrids are inevitable. Compared to the
previous works in interconnected energy hubs, this paper proposes
a distributed method to manage the energy demands in a decen-
tralized operation mode. Therefore, the energy trading will flow
safely between entities, while in centralized mode the control
center of microgrids share all information and data with distribu-
tion network operator. To clarify the main contributions of this
paper, Table 1 is provided to list the most recent published papers
in the energy hub field, and compare the contributions with the
proposed paper.
In the proposed architecture, each MG has its objective function
including electricity, heating, and cooling demands, and each MG
coordinates its operation with the distribution network and other
MGs. Alternating direction method of multipliers (ADMM) is
employed to decentralize the daily operation of networked MGs
using a robust optimization method. Therefore, the main contri-
butions of this paper can be categorized as the following:
In this paper, networked microgrids are considered as multi-
energy hubs to satisfy various energy demands. The micro-
grids operate in an interconnected mode to realize the optimal
energy interaction among existing entities. Therefore, this study
exploits the multi-energy interconnection through a decen-
tralized operation mode of MGs, whereas previous studies have
focused more on electrical power interaction among MGs.
A new model is proposed to consider the energy interactions
among microgrids and distribution network. In this regard, a
decentralized energy management system is exploited to opti-
mally respect the ownership of all entities, while the private
information and security of them are realized.
The ADMM is introduced to ensure the privacy and indepen-
dence of the entities. The ADMM-based decentralized solution is
applied to interconnected operation mode of MGs and distri-
bution network to take the various energy interaction among
entities into account. According to the previous studies, the
ADMM model has been employed in those networks in which
only electricity is assumed to be provided.
A distributed robust model is applied to networked MGs
embedded energy hubs. The proposed robust ADMM model
tremendously decreases the computational burden and enjoys
higher convergence.
1.4. Organization
In section 2, the networked MGs structure for interconnected
energy hubs is presented. In section 3, the problem description is
described. In section 4, the distributed robust model of ADMM is
discussed. Finally, in section 5, the simulation results are depicted.
2. Networked microgrids description embedded energy hubs
In the proposed model, there exist two different decision-
makers. The first decision-maker has to do with distribution
network (DN). In this stage, distribution network operator mini-
mizes the cost of the system including generation cost, power
trading cost with the wholesale market, exchanging power cost
with microgrids, energy storage systems, and load interruption
costs considering wholesale market price and distributed energy
resources. The second decision makers are microgrids, in which
each microgrid is informed of the exchanging power of distribution
network with the MG through the existing communication links.
Then, each MG gathers data from the local controllers to optimize
its costs, and finally, the MG informs the distribution network of the
exchanging power between MG and distribution grid. Here, it will
be conflicts between MGs and distribution network in the amount
of exchanging power between the two identities, which this issue is
resolved by ADMM method. The structure of MGs and DN is
depicted as Fig. 1:
In this paper, a multi-carriers energy microgrids are considered
to satisfy the electricity and thermal loads. The proposed energy
hub model enables the sharing of different types of energy between
MGs and distribution network. In the presented model, there are
two entities: multi-microgrids and distribution network. The en-
tities in addition to generating energy through various energy
sources, can share different types of energy with each other to
address the energy demands of costumers. In Fig. 2, a typical
scheme of an EH can be shown.
3. Problem formulation
In this section, the deterministic-based objective function of
each entity is discussed separately.
3.1. Distribution network
As it is shown in Fig. 2, each entity includes two energy inputs to
address the different energy demands of the costumers. The
objective function of distribution network includes two different
parts: electricity, and heating costs. Therefore, the following
objective function can be utilized for distribution grid operation
cost:
The electricity part of distribution network objective function is
affected by electricity input of the energy hub. To supply the elec-
tricity demand of the EH, the objective function consists of elec-
tricity interaction of DN with MGs and wholesale (WS) market, and
charging/discharging state of batteries. Therefore, the electricity
interaction and charging/discharging, load interruption costs along
with the cost of provided electricity for electric chiller are included
in the objective function as following:
In aforementioned equations, the cost dealing with electricity
transaction is mentioned in (3), in which distribution network can
exchange electricity with wholesale market and microgrids. The
purchasing power increases the economic costs of the entity, while
distribution network can get profit through selling surplus elec-
tricity to the MGs and wholesale market. Similarly, battery bank
cost comprises the charging and discharging costs in (4). The
interruption cost is described in (5).
The electrical constraints of the problem for DNare as following:
In (6) and (7), the range of transacted electricity between dis-
tribution network and wholesale market is determined. Similarly,
the electricity interaction limit between distribution network and
each MG is specified in (8) and (9). The charging and discharging
constraints are described in (10) and (11).The constraints dealing
with generation units can be seen in (12)-(15). Finally, constraint
(16) describes the supply-demand balance of distribution network,
which should be guaranteed at any time slot.
It is worthwhile to be mentioned that the energy level of battery
storage system at any given time can be described as follows:
The second term of the proposed objective function for distri-
bution network has to do with gas-related cost. In addition to
electricity, existing EHs have gas input, in which the thermal and
cooling demands are supplied by gas network. Gas input provides
electricity, heating and cooling energies to the EHs. The output of
micro turbines, fuel cells, and diesel generator is electricity while
theyare supplied bygas network input. Besides, combined heat and
power (CHP) units canproduce heating energyas well as electricity,
and the input energy for CHP units is gas as well. Moreover, there
are several gas boilers inside each entity to supply the heating for
the thermal demands. According to the presented energy hub
structure, cooling demands of the entities can be supplied by both
electric chillers which has electricity input, and absorption chiller
which the input energy is gas. Therefore, the objective function
associated with gas energy input can be formulated as below:
In (18), all terms of objective function are shown. As micro
turbines (MTs), fuel cells (FCs), and CHPs are supplied by gas
network, the generation cost of these units is considered in gas
objective function. In this paper, it is assumed that all outputs of
MTs and FCs are electricity. In other words, according to (20) and
(21), the input gas energy is entirely converted to electricity. There
is a diesel generator in distribution network which its generation
cost is described by a quadratic function in (23). The cost dealing
with heating storage systems is shown in (24). In each EH, gas
boilers play an important role in supplying the thermal loads, and
the generated heat cost by gas boilers is formulated as (25). Each
energy hub can interact heating energy with gas network to
guarantee the supply-demand balance. The cost of the heating
transaction is formulated in (26). The heating interruption is
considered in case of lack of enough heating energy, which has the
cost as (27). On the other hand, CHP units have two energy outputs,
electrical energy and heating energy. Therefore, each output of the
CHP can be obtained as follows:
In (28), the electrical contribution of CHP is the value of total
energy output of the unit, while the heating portion of CHP is of the
total output energy in (29).
The heating-related inequality constraints of the problem for
distribution network are similar to electrical part. The inequality
constraints of heating section includes the generation limits of gas
boilers, charging and discharging state of heating storage, heating
energy trading of the entity with the external gas network, and
absorption chiller limits.
The supply-demand balance of heating hub is shown by (30).
3.2. Multi-microgrids
In this paper, each microgrid is considered as an EH which tries
to supply different type of energy demands in any given time.
Similar to distribution network, MGs can address the electricityand
thermal demands using local energy sources, energy storage sys-
tems, and energy interaction with upper level supplier. Microgrids
and distribution network are operating in decentralized mode
meaning that each entity optimizes its objective function without
interference of other entity. So, this operation mode greatly helps
the entities to protect the important data and information and
guarantee the security of them. Likewise, the distribution network,
MGs have to address the electricity and thermal loads at any time
slot.
The electricity part of each MG objective function is influenced
by electricity input of the energy hub. To supply the electricity
demand of each MG, dispatchable and dispatchable generation
units, electrical energy storage systems, and power transaction
between entities are considered. On the other hand, other output of
electricity hub is provided energy for electric chillers. The electrical
objective function of MGs is exactly like distribution network as
mentioned in (2). The costs dealing with electricity transaction,
electrical energy storage, and interruption are formulated like
distribution network (3-5). However, in case of transaction elec-
trical power of each MG, it should be noted that each MG has only
energy interaction with distribution network. Therefore, the elec-
trical power transaction of each MG can be defined as below:
Similar to distribution network inequality constraints, each MG
has the same constraints as (6)-(15). The equality balance of each
MG can be written as below:
The heating-related objective function of each MG is similar to
distribution network mentioned in (18)e(27).The equality heating
balance of each MG can be written as follows:
4. Distributed robust model based on ADMM
4.1. Robust approach
In this paper, the uncertainty dealing with wholesale market
price is modeled using robust optimization. The proposed objective
function can be described as follows:
The proposed objective function considering the robust model
can be formulated as below:
4.2. ADMM model
The prevalent form of objective function and constraint set
which is optimized by ADMM algorithm is as follows:
The augmented Lagrangian function associated with proposed
ADMM algorithm is as below:
As ADMM method is iterative approach, the corresponding it-
erations can be denoted as following:
As it is seen both x and z variables are separately minimized in
an iterative procedure. Therefore, according to (49) and (50), it can
be concluded that ADMM method is appropriate for decentralized
optimization problems. Due to iterative instinct of ADMM, the
approach should be stopped when the following convergence
criteria is fixed:
4.3. ADMM model for networked MGs embedded energy hubs
In this paper, microgrids and distribution network are consid-
ered as separate entities which are willing to interact with each
other from energy standpoint. It should be mentioned that variable
x consists of distribution network variables at any given time spot
as follows:
Similarly, the variable z includes the following variables for each
MG at any given time spot:
In this regard, the augmented Lagrangian function can be
formulated as (54):
As it was mentioned, microgrids and distribution network can
operate in decentralized mode, which means that the operation
cost can be minimized by each entity considering the energy in-
teractions. Considering the decentralized operation of entities,
ADMM is utilized to fix the conflicts of transacted electricity be-
tween each MG and distribution network. The steps of ADMM are
denoted as follows:
In Fig. 3, the flowchart of proposed decentralized model using
ADMM is depicted.
5. Numerical results
The ADMM method is tested in a distribution grid including
networkedmicrogrids with three MGs shownin Fig.1. Each MG and
distribution network operate as an EH which is shown in Fig. 2.
In Table 2 and Table 3, the performance parameters of devices
are listed. The hourly electrical, heating, and cooling demand of all
entities are depicted in Fig. 4.
In the distribution network, diesel generator, MT, FC, CHP,
photovoltaic panel (PV), and wind turbine (WT) are responsible to
provide electrical energy to the entity, while PV, WT, and CHP are
suppliers of electrical power for MG1, and MT, CHP, WT are in
charge of producing electricity for MG2. Besides, PV and CHP are
utilized in MG3 to supply the electricity. In Fig. 5 and Fig. 6, the
contribution of each power generation unit is shown. In Fig. 5, the
generation level of distribution network and MG1 is depicted. As it
can be observed the diesel generator is utilized when either the
generation level of wind turbine and PV panel is lower or the
electrical demand is higher especially in peak hours. Moreover, in
both distribution network and microgrids entities, the non-
dispatchable generation units including wind turbine and PV
panel have priority in power production. Then, the dispatchable
units are contributed to supply the required power. In other words,
the priorities for power generation are non-dispatchable, dis-
patchable, and diesel generators due to generation costs.
In addition to electrical sector, there exist different units which
are responsible to supply heating for consumers. Gas boilers and
CHP units are considered in heating generation process. The
contribution of gas boilers, CHP, and solar thermal units can be
observed in Fig. 7 and Fig. 8 for the entities. It should be mentioned
that MG1 and MG3 can take advantage of solar thermal, as well.
Similar to electrical part, for the heating production, the solar
thermal units are the first priority of entities.
In Fig. 9, the contributions of cooling generation units can be
seen. Absorption chillers (AC) and electric chillers (EC) which are
supplied by gas and electricity, respectively, are utilized to supply
the cooling demand of MGs and distribution network.
Fig. 10 shows the transacted power between MGs and distri-
bution network. As it was mentioned in previous sections, the
ADMM approach is used to coordinate the electrical power inter-
action between entities in a decentralized operation mode. The
optimal results are achieved after 5 iteration which shows the fast
convergence of ADMM.
According to Fig. 10, it can be seen that MGs are willing to
purchase power from distribution network instead selling power to
that entity. It is due to this fact that the internal power generation
units are not able to supply electrical power to their customers.
Also, the Fig.10 describes that the MG2 provides more powers from
energy trading with distribution network compared to other
microgrids due to higher electrical demand of this microgrid. On
the other hand, it is seen that MG1 sells the surplus power to the
distribution network at hour 6 because of surplus generated power.
Totally, the cost of energy trading of MG2 should be more than
other entities.
In Fig.11, the optimal results of distribution network is depicted
for both electrical and heating sectors. Similarly, the optimal out-
puts of microgrids are shownin Figs.12e14. It should be mentioned
that the negative value for exchanged power describes that the
entity is willing to sell power to other entity.
In Fig. 11, In case of electrical operation, the generation level in
distribution network is depicted along with electrical demand. The
amount of energy trading of distribution network with wholesale
market, as well as other microgrids are depicted. In this regard, the
negative amount of energy trading with MGs describes the amount
energy selling to those entities. It is observed that the distribution
network is willing to sell more power to MG2 compared to other
microgrids. Besides, the charging and discharging level of batteries
are shown. From heating point of view, the heating energy gener-
ation using gas network is shown along with heating demand. In
Fig. 11, considering the heating demand, the distribution network
has to connect to external gas company to provide the required
heating energy to the clients. The negative amount of exchanged
heating means that the entity sells heating energy to the gas
company, while the amount of purchased heating energy is
described by positive numbers. It should be mentioned that in peak
hours, the entity requires to purchase energy rather than selling to
gas company. Also, the charging and discharging level of heating
storage systems are shown. Similarly, in Figs.12e14, the generation
level, energy trading, and state of charging are described for elec-
trical and heating purposes of all microgrids.
In Fig. 15, the optimal results for cooling goal of entities are
depicted.
As it is seen in Fig. 15, cooling load interruption can occur in
some hours, which is due to lack of input energy for both electric
chiller and absorption chiller. It is observed that the cooling load
shedding is prevalent in peak hours. It should be noted that cooling
interruption is directly depends on heating and electricity sections.
As an example, there is no cooling load interruption in MG3, which
means that either the load profile is lower or is there enough
electricity and heat in electric and heating hubs.
To analyze the operation cost of entities, Fig. 16 shows the total
operation cost of entities, which is the sum of electricity and gas
inputs. In this figure, the total operation cost is shown for robust-
ness level G 0 ¼ 12. It is seen that the operation cost of distribution
network is more than each microgrid because of higher electrical
and heating demands compared to other entities.
In Table 4, the total operation cost of distribution network is
described considering different robustness level.
As it was estimated, the operation cost increases with increasing
the robustness level. In other words, the system is more conser-
vative once the robustness level goes up. Therefore, when the
robustness level is getting twice, the operation cost of distribution
network increases by 19% which is a significant cost. Besides, to
compare the obtained results by the commercial package, a heu-
ristic algorithm, namely particle swarm optimization (PSO) is
employed to validate the optimal solution of proposed method. In
this paper, the obtained results are achieved and optimized by
commercial package (Gurobi optimizer package) and then
compared with PSO in centralized and distributed operation
modes. The achieved operation cost by Gurobi optimizer package is
reliable and optimal compared to PSO. The reason is that the
heuristic algorithms obtain local optimal solutions rather than
global answers.
To show the performance of ADMM, the operation cost of dis-
tribution network is depicted in Fig. 17 for each iteration at
robustness level 12.
According to Fig. 17, the fast convergence of ADMM can be seen
for the operation cost of distribution network. ADMM can reach to
the optimal cost after 5 iterations. Besides, In Fig. 18, the residual
value of problem which is described as step 4 of ADMM in (57) is
depicted. The rapid convergence of the method can be clearly seen.
6. Conclusion
In this paper, optimal operation of networked MGs embedded
interconnected energy hubs was discussed. A distributed robust
model was proposed to optimally coordinate the power trading
between microgrids and distribution network. A robust optimiza-
tion method is employed to analyze the impact of uncertainty
associated with wholesale market price. In this study, networked
MGs and distribution network were assumed to be independent
entities aiming at minimizing their own operation cost while the
privacy and security of entities are realized. To coordinate the
exchanged energy between MGs and distribution network, ADMM
is exploited to settle the conflicts of entities. The main goal of this
study was applying ADMM model to interconnected energy hubs to
show the strength of this distributed algorithm in coordinating the
existing conflict between entities. The achieved results proved this
fact that the convergence of ADMM model is significant and can
rapidly reach the optimal results in less iterations. Comparing the
obtained result by the proposed distributed algorithm with
centralized model shows the accuracy of the presented method.
Also, the optimality of proposed optimization method solved by
Gurobi solver is compared with a heuristic algorithm tovalidate the
accuracy. In the proposed problem, the obtained results by Gurobi
solver is better than heuristic algorithm. Distributionally robust
optimization model based on ADMM considering demand response
programs can be considered for optimal operation analysis of net-
worked MGs embedded EHs in future works.Heat pipe dryout and temperature hysteresis in response to transient heat pulses exceeding the capillary limit

Abstract:
The balance between the capillary pressure provided by the wick in a heat pipe or vapor chamber and the ﬂow resistance to liquid resupply at the evaporator determines the maximum heat load that can be sustained at steady state. This maximum heat load is termed as the capillary limit; operation at steady heat loads above the capillary limit will result in dryout at the evaporator wick. However, different user
needs and device workloads can lead to highly transient heat loads in applications ranging from con- sumer electronic devices to server processors. In these instances, the operation of heat pipes must be assessed in response to brief transient heat loads which could be higher than the notional capillary limit that governs dryout at steady state. In the current study, experiments are performed to characterize the
transient thermal response of a heat pipe subjected to heat input pulses of varying duration that ex- ceed the capillary limit. Transient dryout events due to a wick pressure drop exceeding the maximum available capillary pressure can be detected from an analysis of the measured temperature signatures. It is demonstrated that under such transient heating conditions, a heat pipe can sustain heat loads higher than the steady-state capillary limit for brief periods of time without experiencing dryout. If the heating
pulse is suﬃciently long as to induce transient dryout, the heat pipe may experience an elevated steady- state temperature even after the heat load is reduced back to a level lower than the capillary limit. The steady-state heat load must then be reduced to a level much below the capillary limit to fully recover the original thermal resistance of the heat pipe. This characteristic temperature hysteresis following tran- sient dryout has signiﬁcant implications for the use of heat pipes for pulsed power dissipation. Further experiments are performed to bound the range of heat loads over which the temperature hysteresis is present following a transient dryout event.

1.Introduction:
Heat pipes and vapor chambers utilize the latent heat of phase change of an internal working ﬂuid to passively transport heat over long distances at a small temperature gradient. Heat pipes ﬁnd use in thermal management of electronic devices in both low- and high-power applications owing to their eﬃcient heat transport [1–3].  
The hollow metal case of a heat pipe is lined on the inside by a porous wick that is saturated with liquid; the interior core of the heat pipe supports vapor ﬂow [4].  A local heat source is applied to the evaporator region of the heat pipe, and the surface dedicated for heat rejection to the ambient is called the condenser. Application of a heat input causes the liquid to evaporate at the wick and ﬂow through the vapor core. The vapor condenses on the wick that lines the condenser surface and is passively drawn back to the evaporator via capillary forces generated in the wick. An increase in the heat input increases the rate of evaporation from the wick, and induces a higher ﬂuid ﬂow rate and pressure drop in the heat pipe. The maximum heat load that a heat pipe can sustain at steady state while maintaining liquid resupply to the evaporator is governed by the capillary pressure of the wick. Any heat load inducing a ﬂuid pressure drop higher than the capillary pressure head will result in dryout at the evaporator wick; this threshold heat load is commonly known as the capillary limit. 
The capillary limit, which governs the maximum steady-state heat load, is primarily dependent on the heat pipe geometry, working ﬂuid properties, and wick properties. Working ﬂuids are typically chosen based on thermophysical liquid properties that would yield the maximum capillary limit [5,6].  Recent studies have explored the use of ﬂuid additives [7,8] to increase the maximum 
heat transport rate in heat pipes. A number of studies have inves- tigated the capillary limit-induced dryout phenomenon in porous wick structures in heat pipes. Hanlon and Ma [9] characterized the dependence of dryout heat ﬂux in sintered particle structures on wick thickness, particle size, and porosity. Chun [10] studied the dryout of screen wicks pumping vertically against gravity, and developed a model to predict the dryout heat load as a function of the wicking length. For screen mesh wicks, Brautsch and Kew [11] showed that the heat ﬂux required for dryout increases with an increasing number of mesh layers. A model for steady-state temperature and pressure drop in a ﬂat heat pipe developed by Lefevere and Lallemand [12] was used to show that the maximum heat ﬂux of a grooved heat pipe increases with an increase in the groove width [13]. 
Rather than considering the individual effects of the working ﬂuid and wick on performance, several studies have characterized the coupling between the capillary behavior and different gov- 
erning parameters of vapor chambers and heat pipes. Rice and Faghri [14] numerically simulated the temperature and pressure ﬁelds in sintered wick heat pipes, accounting for changes in menis- cus shape due to evaporation as a function of applied heat load and location of the heat source. The parametric dependence of heat 
pipe performance on wick properties (thickness, radius of curva- ture, porosity) and heater diameter was investigated by Pruzan et  al. [15];  the physical insights from the study were used to de- 
sign wicks capable of dissipating high heat loads. Other physical mechanisms, such as the effect of viscous drag along the wick substrate on liquid ﬂow in a wick [16] and heat spreading in the 
wall/wick from localized heat sources [17] can also have a promi- nent effect on the capillary limit of a heat pipe. Fluid loading in the wick also affects the capillary limit. Kempers et al. [18] showed that capillary limit was unaffected by over-ﬁlling the wick, but greatly diminished due to under-ﬁlling. In another study, Lips et al. [19] showed that a thinner vapor core improves the thermal resistance but reduces the capillary limit for an over-ﬁlled wick, and hence the vapor core thickness and ﬁlling ratio need to be carefully optimized. In our recent work [20],  we concluded that 
the choice of wick and working ﬂuid should Lips et al. [19] showed that a thinner vapor core improves the thermal resistance but reduces the capillary limit for an over-ﬁlled wick, and hence the vapor core thickness and ﬁlling ratio need to be carefully optimized. In our recent work [20],  we concluded that 
the choice of wick and working ﬂuid should be made simulta- neously to obtain the optimal thermal performance while utiliz- ing the maximum capillary pressure available, as the total thermal 
resistance is coupled to both the wick and working ﬂuid proper- ties. 
understand dryout mechanisms at steady-state operating condi- tions. However, electronic devices are rarely operated in a steady manner in practice, but rather, the operation is highly transient. For instance, in smartphones, the typical user behavior consisting of short periods of device interaction is well recognized in the literature [21] and is often referred to as ‘bursty’ or micro-usage [22,23].  These micro-usage device interaction patterns are also common in other applications, such as user activity on the internet [24] or while streaming media content [25].  For both low- and high-power applications, thermal design and thermally-limited performance throttling is increasingly dictated by such transient use-cases, often including short power spikes [26,27].  Hence, it is important to study the dryout behavior of heat pipes in response to a brief power pulse that exceeds the conventional capillary limit value at steady state.
A number of studies have investigated the thermal and hydro- dynamic response of vapor chambers and heat pipes under tran- sient operating conditions. A seminal example by Tournier and El-Genk [28] is a two-dimensional transient model of a heat pipe that predicted the effective radii of curvature of the liquid menisci 
along the liquid-vapor interface, in response to step-function heat- ing and cooling. Wang and Vafai [29] experimentally investigated the thermal performance of a ﬂat-plate heat pipe and proposed a heat pipe time constant. A model was developed [30] which indicated that the thermal penetration time is dependent on the effective thermal diffusivity along the heat pipe; the time to steady state is strongly dependent on the heat transfer coeﬃcient to the ambient. Several studies have also benchmarked the transient performance of ﬂat heat pipes against solid metal heat spreaders. Harmand et al. [31] compared vapor chambers against solid heat spreaders in terms of total thermal resistance using a transient numerical model, and found solid heat spreaders to have lower temperature gradients than vapor chambers for short transient thermal cycles. Recently, Patankar et al. [32] investigated the physical mechanisms governing vapor chamber performance under transient operating conditions, using an experimentally validated time-stepping analytical model [33].  The maximum temperature in a solid heat spreader was found to be lower than the vapor chamber for short periods of time just after start-up owing to the low relative thermal capacity of the vapor chamber. At later times, the vapor chamber performed better than the solid spreader as the operating temperature and effective vapor core conductance increased. Other factors such as wick ﬁlling ratio, gravity, heat ﬂux, and convective cooling can also affect the transient performance 
of a heat pipe [34].  However, these prior studies have only charac- terized the transient operating characteristics; to the knowledge of the authors, none have focused on the transient thermal behavior up to the point of dryout or above the steady-state capillary limit. 
In this paper, we experimentally demonstrate that a heat pipe can brieﬂy operate, without the occurrence of dryout, at heat loads higher than the value corresponding to the steady-state capillary limit. Furthermore, the response of the heat pipe to a pulsed heat load above this notional capillary limit is characterized to assess the thermal signatures of the dryout phenomenon under such transient conditions. A threshold time-to-dryout corresponding to the applied heating pulse power is identiﬁed; pulse durations exceeding time-to-dryout are shown to cause dryout. Hysteresis in the heat pipe temperature is observed following dryout. That is, if dryout occurs, the heat load must be reduced to a level signiﬁcantly lower than the capillary limit in order to fully recover the original performance. 

2. Experimental setup：
A schematic diagram of the experimental facility used to characterize the heat pipe transient thermal behavior is shown in 
Fig. 1.  The setup was designed to isolate the heat pipe from exter- nal components or materials (such as insulation) that would add thermal capacity and affect the transient thermal response in an experiment. The heat pipe sample used in this study was 150 mm long, 9 mm wide, and 0.62 mm thick (Novark Technologies). The two ends of the heat pipe have a solid copper cross section, such that the effective length of the heat pipe having a vapor 
core is 120 mm. The heat pipe was suspended in air by hang- 
ing from long, thin steel wires to minimize heat losses through this pathway. The left end of the heat pipe was covered with a layer of polyimide tape over a length of 8 mm, and a sheathed Nichrome wire wrapped on top to form a heater. Thermal grease (OMEGATHERM 201) was applied between the heater wire and polyimide surface. Power input to the heater was controlled using a programmable power supply (Korad KA3005P) to induce Joule heating. The supply current was calculated by measuring the voltage drop across a shunt resistor (0.01 􀀁); the voltage drop across the Nichrome wires was measured directly. 
Temperatures were measured along the length of the heat pipe using T-type thermocouples at the locations shown in Fig. 1.  The thermocouple beads labeled B were attached to the outer surface of the heat pipe using small pieces of copper tape. To measure 
the temperature at the evaporator, thermocouple A1 was sand- 
wiched between the polyimide tape and heat pipe surface. The thermocouples were all calibrated in a dry block calibrator (Jupiter Isotech 4852) using the same procedure that was previously demonstrated to yield an uncertainty of ±0.3 K [35].  The reference temperature for the thermocouples during the calibration and the experiment was maintained near 0° C using an ice-point reference (OMEGA CL122). The temperatures of the ice-point junction and the ambient were measured using RTDs. The temperature measurements were acquired using a data ac- quisition chassis (National Instruments (NI) cDAQ–9178) equipped 
with thermocouple (NI–9214) and RTD (NI–9217) modules. Volt- ages were measured using a separate data acquisition unit (NI USB–6255). Data were recorded using a LabVIEW interface at 4 Hz. 
Heat dissipation to the ambient was facilitated by placement of two fans that blow ambient air over the heat pipe, with the air ﬂow direction as indicated in Fig. 1.  The heat transfer coeﬃcient via forced convection cooling was estimated by performing an analogous experiment with a solid copper ﬁn of length 150 mm, width 8.6 mm, and thickness 2.4 mm, and ﬁtting the measured temperature readings to an analytical solution. The heat transfer coeﬃcient was found to be in the range of 85–110 W/m 2 -K for input powers from 1 W to 7 W. This analog experiment also enabled estimation of the net heat input to the ﬁn, which was found to be in agreement with the measured electrical heat input, showing that the heat loss from the heater was small. Speciﬁcally, the difference between electrical input and measured heat load dissipated from the ﬁn was ~9% at lower powers (1 W to 2 W), and ~5% at higher powers. Heat losses are therefore neglected during testing of the heat pipe. 

3. Results and discussion：
The following subsections discuss a series of experiments that were performed to identify the response of a heat pipe subjected 
to transient heat loads exceeding the capillary limit. Each subsec- 
tion brieﬂy presents the experimental test procedure, followed by a detailed discussion of the results. 
Prior to investigating the transient thermal response of the heat pipe sample, the steady-state thermal performance is benchmarked and compared to that of a solid copper ﬁn 
(150 mm × 8.6 mm × 1.6 mm) under identical boundary con- ditions. A given heat load is applied to the sample (heat pipe or copper ﬁn), and the system is allowed to reach steady state. Steady state is deﬁned as when the standard deviation in the time-averaged temperature of all thermocouple measurements over 37.5 s is less than ~0.05° C. Fig. 2 shows a comparison of steady-state temperature proﬁles along the length of the heat pipe and copper ﬁn for two different power levels (2 W and 5 W).The heat source extends from 0 to 8 mm, as indicated by the shaded region in the ﬁgure. The temperatures are obtained from point A1 within the heated region and from B1 to B6 along the exposed length. At both power levels, the maximum temperature in the heated region (A1) is lower for the heat pipe as compared to the copper ﬁn, and the temperature proﬁle (B1-B6) is more uniform. This temperature uniformity over the heat pipe surface is a result of effective heat transport through the vapor core at a small temperature gradient [36].  
The steady-state capillary limit is found by measuring the temperatures along the heat pipe at increasing heat loads from 1 W to 5.2 W. The heat load is increased in small increments, and the heat pipe is allowed to reach a steady state at each heat input. When the heat load is increased from 5.1 W to 5.2 W, a sharp increase in the evaporator temperature and a momentary decrease in condenser temperature is observed, indicating that a dryout has occurred. Hence, 5.1 W is identiﬁed as the maximum heat load that can be supplied at steady state to the heat pipe sample under the test boundary conditions without the occurrence of dryout (i.e., it is the steady-state capillary limit of the heat pipe). 

3.1. Transient dryout in response to a power pulse exceeding the steady-state capillary limit ：
The thermal response of the heat pipe to a power pulse input is shown in Fig. 3.  The heat pipe is initially operated at a steady- state power of 3 W, as shown in the temporal heat load proﬁle of Fig. 3 (a). At t = 0 s, the power input to the heat pipe is suddenly increased to 10 W, which is higher than the measured steady-state capillary limit (5.1 W). The power input is maintained at 10 W for approximately 10 s, after which it is reduced to 3 W. 
Fig. 3 (b) shows the corresponding temporal variation in the evaporator temperature at location A1 in response to this transient power pulse. The temperature is initially steady at 48.5° C when the power input is 3 W. At t = 0 s, the evaporator experiences a sharp temperature rise due to the increase in power input to 10 W. The evaporator temperature increases monotonically throughout the 10 s long pulse, during which an inﬂection is observed at t = 5.25 s. Appearance of this inﬂection in the temperature at the evaporator indicates that a dryout event has occurred due to insuﬃcient liquid supply by capillary action. A more distinct temperature signature is observed in Fig. 3 (c), which shows the variation of the condenser temperature with time. At the start of the power pulse (t  = 0 s), the condenser temperature increases 
monotonically until t = 5.25 s. At this point in time, the temper- 
ature proﬁle begins to ﬂatten out and a plateau is observed. The temperature dips momentarily, and then starts to increase again. This occurrence of a plateau in the temperature response at the condenser also indicates the occurrence of dryout. Dryout at the evaporator is immediately reﬂected in the condenser temperature response because of the very short time scales associated with diffusion in the vapor core. 
The occurrence of an inﬂection point in the evaporator tem- perature at the point of dryout, along with the plateau in the condenser proﬁle, is attributed to a transition in the primary heat transfer pathway before and after dryout, as discussed further in Ref. [37].  Heat transfer before dryout is primarily due to phase change at the evaporator, while heat conduction in the wall is the primary heat transfer pathway after dryout. The time at which dryout occurs, termed here as the time-to-dryout, is 5.25 s from the time the power input is increased. 
At the end of the power pulse, the power input is dropped back to 3 W and the heat pipe is allowed to reach steady state. The new steady-state evaporator temperature attained by the heat pipe after the power pulse has ended is a further indication that dryout has occurred during the pulse. It is observed from Fig. 3 (b) that the steady-state evaporator temperature after the power pulse (t  > ~80 s) is ~12° C higher than the initial temperature at the same steady power input of 3 W before the pulse input (t  < 0 s). We attribute the presence of this temperature hysteresis to the characteristic hysteresis in the rewetting process of the wick after a dry patch has formed [38–40].  Thus, we infer that the temperature hysteresis observed before and after the power pulse ( Fig. 3 (b)) is due to the occurrence of transient dryout of the heat pipe at t = 5.25 s. 

3.2. Temperature hysteresis after the occurrence of transient dryout ：
Hysteresis in the evaporator temperature after a power pulse indicates that a transient dryout event has occurred in the heat pipe. The dryout occurs due to wick pressure drop exceeding the 
available capillary pressure during the power pulse at the time-to- dryout. In this section, the duration of the power pulse is varied to determine its effect on the hysteresis behavior. Fig. 4 shows the thermal response of a heat pipe operated under heating pulse durations of 4 s, 10 s, 14 s, and 16 s. The heat pipe is initialized at a power of 3 W (t  < 0), pulsed above the capillary limit to 7 W for the different durations, and reduced back to 3 W, as shown in the load proﬁles in Fig. 4 (a). The evaporator temperature proﬁles corresponding to each of these four different pulse durations is plotted in Fig. 4 (b). 
For the shortest 7 W pulse duration of 4 s, no inﬂection is observed in the evaporator temperature during the power pulse (blue curve; Fig. 4 (b)). Moreover, the evaporator temperature does not exhibit any observable temperature hysteresis after the pulsed loading ends; the steady-state evaporator temperature measured after the pulse ends and the heat input reverts to 3 W (t  > ~40 s) is the same as it was before the pulse (t  < 0). The absence of an inﬂection in the evaporator temperature during the pulse or of any temperature hysteresis indicates that the heat pipe did not undergo dryout. 
It is noted that the heat pipe operated without the occurrence of dryout or any hysteresis for a brief period of time at a heat load (7 W) that is higher than the notional steady-state capillary limit (5.1 W). The ability of the heat pipe to operate above the capillary limit without experiencing dryout results from the lag 
between the thermal/hydrodynamic response and the heat in- crease. Based on the mechanisms governing transient performance of a heat pipe [32],  the effective in-plane diffusivity governs the time required for the temperature proﬁle to develop at the start of the transient load. A higher in-plane diffusivity would lead to a quicker thermal and hydrodynamic proﬁle development, and thus, a shorter time-to-dryout in response to a pulse load input. time-to-dryout, the heat pipe undergoes dryout and exhibits tem- perature hysteresis at the evaporator upon a return to the lower steady-state heat input. From Fig. 4 (b), it is observed that the hysteresis in the evaporator temperature increases in magnitude with an increase in pulse duration longer than the time-to-dryout. The difference in temperature at the 3 W input before and after the power pulse is 4° C for a pulse duration of 10 s, and increases  to 10° C for a pulse duration of 14 s. However, as the pulse duration further increases, the temperature hysteresis asymptotes towards a maximum value. With an increase in pulse duration from 14 s and 16 s, the magnitude of temperature hysteresis remains at ~10° C, and does not increase any further with longer pulses. 
For a heating pulse from a 3 W base to 7 W, evaporator temperature hysteresis was found to occur after transient dryout. However, the occurrence of a transient dryout is not necessarily accompanied by hysteresis after the power pulse ends; instead the occurrence of hysteresis depends on the characteristics of the power pulse. For example, the cases in Fig. 5 (a) explore a different power pulse proﬁle initialized at a lower steady-state power input of 2 W (t  < 0), and pulsed to the same increased load of 7 W for durations of 14 s, 16 s, 18 s, and 20 s. All of these pulse durations 
are longer than the time-to-dryout, as Fig. 5 (b) reveals an inﬂec- tion in the evaporator temperature response at t = 9.75 s, the time at which dryout occurs for all cases. Interestingly, with the lower baseline (steady-state) power of 2 W, the evaporator recovers to 
the same temperature after the pulse, without any observable hys- teresis in temperature, even though the pulse duration exceeded the time-to-dryout and transient dryout occurred. The presence of temperature hysteresis at the baseline load of 3 W, and its absence at 2 W, suggests that the steady-state operating power governs the 
recovery of the heat pipe after the dryout. Therefore, the next sec- tion investigates the temperature hysteresis under steady state op- eration, to probe whether the steady state performance character- istics can be used to explain the recovery after a transient dryout. 

3.3. Post-dryout temperature hysteresis envelope under steady operation ：
To investigate the steady-state temperature hysteresis behavior of a heat pipe after dryout at the capillary limit, the heat pipe sample is ﬁrst subjected to increasing steady power inputs until dryout occurs. During this powering-up procedure, the heat load is incremented by small, ﬁnite amounts; the increment is 0.5 W at low heat loads (1 W to 4 W) and is reduced to 0.1 W as the heat load approaches the capillary limit of 5.1 W. At each input heat load, the heat pipe is allowed to reach a steady state. After reaching the capillary limit, the temperature is allowed to reach steady state in the dried-out condition, and the power is further 
increased to 5.4 W by a small amount to complete the powering- up procedure. From this dried-out condition above the capillary limit, the heat load to the heat pipe is then decreased from 5.4 W following a powering-down procedure during which the power is reduced in small decrements back to the initial condition, again allowing steady state to be reached at each power. Fig. 6 traces the variation of the steady-state evaporator temperature (location A1) with increasing and deceasing input heat load along these powering-up (black) and powering-down (gray) pathways. 
During the powering-up procedure, the evaporator temperature increases with an increase in heat load (1 W to 5.1 W) as expected. When the heat load is incremented from 5.1 W to 5.2 W, a sharp increase in evaporator temperature is observed. The steady-state evaporator temperature rises from 58° C at 5.1 W to 83° C at 5.2 W. This sharp rise is due to an increase in total thermal resistance of the heat pipe after the dryout, and indicates that the heat pipe has reached its capillary limit. The hysteresis in the steady-state evaporator temperature after dryout is clearly evident from Fig. 6.  During the powering-down trace from 5.4 W, the evaporator has a markedly higher temperature than measured at the same heat loads during the powering-up trace. This temperature discrepancy (i.e., severity of the temperature hysteresis) gradually reduces as the heat load is decreased, until there is ultimately no observable temperature difference at a power of 2 W. 

3.4. Discussion ：
In Section 3.2,  we demonstrated that there is signiﬁcant tem- perature hysteresis before and after a transient dryout event when the heat pipe is pulsed from 3 W to a heat load of 7 W (exceeding the capillary limit of 5.1 W), and then returned to 3 W. However, when pulsing to the same 7 W pulse but initialized from a lower 
baseline power of 2 W, there is no observable temperature hystere- sis after transient dryout. In Section 3.3,  the temperature hysteresis 
after dryout is characterized under steady-state operating condi- 
tions by decreasing the power after the heat pipe has reached its capillary limit. These experiments revealed that the temperature hysteresis is observed only for heat loads above 2 W. The absence of temperature hysteresis at 2 W –both in cases of dryout caused by a transient power pulse ( Fig. 5)  and when reached by steadily powering down from above the capillary limit – indicates that 
the envelope of temperature hysteresis under steady-state condi- tions ( Fig. 6)  may serve to bound the input power range in which 
hysteresis would be observed after the occurrence of dryout. In ad- 
dition, the severity of the temperature hysteresis at a given power appears to be bounded by the steady-state operation of the heat pipe after the capillary limit has been reached, as illustrated by the 
magnitude of temperature hysteresis at 3 W. In Fig. 6,  the mag- nitude of temperature difference between the powering-up and powering-down traces at 3 W is ~14° C. This temperature difference is comparable to the upper limit of temperature hysteresis (~10° C) observed after transient dryout when pulsing from 3 W to a heat load above the capillary limit and returning back to 3 W ( Fig. 4 ).  Based on these observations, we speculate that the temperature hysteresis range and severity (magnitude) in Fig. 6,  measured at steady-state by incrementally powering down the head load, is a  bounding envelope. Moreover, the maximum heat load for which no hysteresis is observed after steady-state dryout (~2 W for the present case) represents a critical threshold below which the heat pipe must be operated in order to recover its original performance, if an unintended transient dryout event were to occur. 

4. Conclusions：
In this study, we explored the transient response of a heat pipe to pulsed heat load inputs above the notional capillary limit. From the series of experiments, we conclude that: 
1 A heat pipe can operate for brief periods of time at heat loads higher than its capillary limit at steady state, without the occurrence of dryout. 
2 In response to a power pulse exceeding the steady-state cap- illary limit, we measured a time-to-dryout at which the heat pipe experiences dryout. 
3 Operation longer than the time-to-dryout for a power pulse higher than the capillary limit may lead to hysteresis in temperature at the evaporator (i.e., the temperature will not recover back to its original magnitude even after the power is reduced back to the pre-pulse level). 
4 The magnitude of temperature hysteresis observed by allowing a heat pipe to reach steady state after the capillary limit has been exceeded serves as the limiting (maximum) severity for the temperature hysteresis observed after transient dryout in response to a power pulse. 
5 A threshold heat load exists below which the heat pipe does not exhibit any temperature hysteresis after dryout; this may represent a critical power that the heat pipe must drop below in order to fully recover performance after a dryout event. 

Mathematical modelling of a diesel common-rail system
In diesel common-rail systems, the exact knowledge of the injection pressure is important to accurately control the injected diesel mass and thus the combustion process. This paper focuses on the mathematical modelling of the hydraulic and mechanical components of a common-rail system in order to describe the dynamics of the diesel rail pressure. Based on this model, an average model is derived to reduce the model complexity and to allow for a fast calculation of the mass flow into the rail for different crank shaft revolution speeds and openings of the fuel metering unit. The main purpose of this average model is to serve as a basis for a model-based (non-linear) controller design. The stationary accuracy of the models is validated by means of measurement data.
Keywords: diesel engine; common-rail injection; control-oriented modelling; physics-based modelling; 37N10; 37N35; 93C10; 93C95
1. Introduction
Common-rail injection technology is typically used in modern diesel engines. The major advantage compared to prior injection systems, e.g. in-line injection pumps, is that the process of injection and high-pressure generation is decoupled. This enables injections with arbitrary timing and quantity, even so-called multiple injections, which can significantly increase the efficiency and reduce exhaust emissions of the engine [[ 1]].
Figure 1 depicts a schematic diagram of the considered common-rail injection system. The central component of the system is a rail volume filled with highly pressurized diesel, which delivers diesel to the injectors. By opening and closing the injectors, the desired amount of diesel can be injected into the combustion chambers of the engine. The high pressure in the rail is generated by a radial piston pump, which is actuated by one or more eccentrics at the camshaft of the engine. The amount of diesel flow into the radial piston pump can be controlled by an electromagnetically actuated variable displacement valve (fuel metering unit, FMU). The FMU is supplied by a constant pressure, which is generated by a gear pump and an overflow valve. Due to leakages in the FMU, it is necessary to install a zero delivery throttle, which enables zero volume flows of the radial piston pump. In many configurations, a pressure control valve (PCV) is installed in the rail to transport diesel from the rail to the tank and thus to actively reduce the rail pressure.
Accurately controlling the rail pressure is an important prerequisite to achieve the desired quantity of fuel injected. Mathematical models provide the basis for the analysis of the dynamical behaviour and the design of control strategies. Stationary characteristic maps are frequently used to describe the system behaviour. These models must be calibrated by measurements and do not account for the dynamical behaviour. These models are thus not directly applicable for the controller design. First-principle models are more suitable for this purpose.
Graph: Figure 1. Schematic diagram of a common-rail system.
The authors of [[ 2]] developed a model of a diesel injection system, where the focus was set on the injection process itself. The models developed in [[ 3]] concentrate on a detailed description of the rail and the injection process as well. In [[ 4]], models of common-rail systems are presented which were primarily intended for the control design. These models are characterized by a low dimension and are based on a number of simplifications, which neglect some important effects, e.g. of the dynamics of the radial piston pump. Moreover, [[ 2], [ 4]] do not consider an FMU such that the models are not suitable for the considered common-rail system. The authors of [[ 6]] include the FMU in their model but do not consider a physics-based model of the radial piston pump. A more detailed model is given in [[ 8]] where a hybrid model for the common-rail system is developed. Although an approach for the description of cavitation in the radial piston pump is presented, a measured curve is utilized to describe the flow rate of the radial piston pump as a function of the current of the FMU.
Therefore, the aim of this paper is to develop a mathematical model of the common-rail system which ( 1) allows for a fast and yet accurate simulation of the dynamical behaviour and ( 2) which can be easily parameterized by means of construction parameters of the system. Section 2 summarizes the mathematical equations of this model and gives some simulation results. A comparison with measurements shows that the model is capable of accurately reproducing the real system behaviour. Due to its complexity, this model is, however, not directly applicable to design a controller. Thus, a reduced model of the system is developed in Section 3, which is based on averaging the system variables over one cycle of the piston motion of the radial piston pump. Simulation results show that the reduced model still exhibits a high accuracy and captures the essential dynamical behaviour of the system.
2. Detailed model
In this section, a detailed mathematical model of the common-rail system is derived. This model is based on the following assumptions and simplifications:

The pressure at the high-pressure side of the FMU is kept almost constant by means of the overflow valve. Thus, models of the gear pump and the overflow valve are not required. Instead, a constant pressure is presumed.

The effect of zero diesel flow for closed FMU, which is achieved by the zero delivery throttle in the real system, is approximated by an idealized FMU without leakage flow.

In this paper, a common-rail system without PCV is considered. This is a common configuration in cost-sensitive applications. The control of the rail pressure is thus only achieved by means of the FMU.
The exact time evolution of the injected diesel flow into the cylinders of the internal combustion engine strongly depends on the actual load of the engine. The model presented in this paper is primary developed for the design of a controller for the rail pressure. As will be seen later, it is not possible to influence the fast pressure fluctuations in the rail caused by the injection process. Therefore, a simplified model will be given, which, however, captures the main effects of the injection process.
2.1. Isentropic fluid model
Diesel pressure in a common-rail system varies from lower than atmospheric pressure (e.g. in the cylinders of the radial piston pump) to very high pressures in the rail. At pressures lower than vapour pressure, the fluid starts to vaporize. Moreover, the compressibility of diesel considerably decreases for high pressures. As these effects cause significant changes in the fluid properties, a model that describes the fluid for both cavitation and high-pressure cases is strived for, see also [[ 9]].
The subsequent model is based on an isentropic fluid, where the mass density and the bulk modulus (both depending on the pressure p) meet the relation
( 1)
Graph
To characterize diesel depending on its pressure p, three cases are distinguished:

Diesel at pressures higher than the upper vapour pressure , , is liquid.

In the pressure range diesel starts to vaporize, i.e. there is some liquid and some vapour part.

At pressures lower than the lower vapour pressure , , all diesel mass is vaporized.

The next sections deal with these three cases in detail and provide equations for and .
2.1.1. Case 1: p ≥ p vap,u
When describing fluids at moderate pressures, a constant bulk modulus can be used, see, e.g. [[11]]. However, for diesel pressures in a common-rail system, which can increase beyond 2000 bar, an increasing bulk modulus is observed in measurements, see Figure 2.[ 1] The bulk modulus is described as
( 2)
Graph
with the bulk modulus at reference pressure and the gradient . With Equation ( 1), the corresponding mass density is given by
( 3)
Graph
where is the mass density at reference pressure .
Graph: Figure 2. Mass density and bulk modulus .
2.1.2. Case 2: p vap,1  < p < p vap,u
In this case, the fluid is considered to be partly liquid and partly vaporized. Given the overall mass m, let us consider that a mass fraction is vaporized, while the mass fraction is still liquid. It is assumed that all fluid is liquid at , i.e. , and all fluid is vaporized at , i.e. . In general, the vaporized fraction is a non-linear function of the pressure p. In this work, it is supposed that increases linearly with decreasing pressure, see, e.g. [[ 9], [11]],
( 4)
Graph
The bulk modulus and mass density of the liquid fraction are given by Equations ( 2) and ( 3)
(5a)
Graph
(5b)
Graph
such that the volume of the liquid fraction is given by
( 6)The behaviour of the vaporized diesel is described by an isentropic process of ideal gas with the isentropic exponent
( 7)
Graph
where is the vapour density and is the volume at . Hence, the volume filled with the vaporized diesel is given in the form
( 8)
Graph
The density of the liquid–vapour mixture in this case reads as
( 9)
Graph
The bulk modulus is derived using Equation ( 1)
(10)
Graph
2.1.3. Case 3: p ≤ p vap,1
For very low pressures below , the fluid is considered to be completely vaporized. This case is obtained by means of Equations ( 7) and ( 3) resulting in the following simplified equations for the mass density and the bulk modulus
(11a)
Graph
(11b)
Graph
Figure 3 shows the mass density and the bulk modulus for low pressures. Note that the logarithmic scaling of the -axis has been used to depict the rapid rise at . Moreover, the discontinuity of at is due to the linear approximation of in Equation ( 4). Using a continuously differentiable approximation would eliminate this discontinuity. However, since continuity of is not necessary in the following derivations, the linear approximation in Equation ( 4) is feasible.
Graph: Figure 3. Mass density and bulk modulus at low pressures p.
2.2. Fuel metering unit
The FMU is used to control the diesel flow into the radial piston pump. Figure 4 shows the hydraulic part of the FMU, given by an inlet orifice, the variable cross section and an outlet orifice connected in series. The turbulent flow through the orifices can be described by
(12a)
Graph
(12b)
Graph
(12c)
Graph
where , , are the discharge coefficients, are the corresponding opening areas, are the mass densities of the diesel and are the pressure drops over the orifices.
Graph: Figure 4. Hydraulic set-up of the FMU.
Balance of mass gives . It is further assumed that . This is reasonable due to the small changes in pressure. Then, the mass flow through the FMU reads as
(13a)
Graph
Graph: Figure 5. Shape of the variable cross section of the FMU.
(13b)
Graph
(13c)
Graph
and , with the gear pump pressure and the gallery pressure . The mass density is approximated by the density at the mean pressure according to Section 2.1 in the form
(14)
Graph
The variable area is given as a function of the position of the valve spool. Since the opening area exhibits a triangular shape, increases quadratically with , see Figure 5. The movement of the valve spool is controlled by a solenoid, generating the (positive) magnetic force . The restoring force is due to a preloaded spring with stiffness . The mechanical system of the FMU can be described by
(15a)
Graph
with the position , the velocity , the mass of the spool (including the mass of all moving parts of the solenoid), and the damping coefficient . The preload of the spring is chosen as the position which corresponds to the maximum mass flow. The position is limited within the mechanical stops . The contact with the end stops is modelled by a perfectly inelastic collision, dissipating the whole kinetic energy. At the end stops, the equations of motion read as
(16)
Graph
and outside these stops, the motion is governed by Equation (15).
The magnetic force is generated by a solenoid, where the force is a function of both the position and the current applied to the electromagnet of the FMU. The current is controlled by a subordinate current controller such that the setpoint of the current serves as the control input to the system. An analytic model of the overall dynamics of the solenoid, including the current controller, is not meaningful due to the resulting complexity. Instead, the dynamics of the current-controlled solenoid is approximated by a first-order lag element with the time constant in the form
(17)
Graph
Here, is the stationary characteristic of the magnetic force as a function of the position and the current setpoint of the current controller. Figure 6 depicts the measured characteristic .
Graph: Figure 6. Magnetic force as a function of the current setpoint and the position .
2.3. Gallery volume
The mass flow of the FMU, Equation (13), is delivered into the small gallery volume between FMU and radial piston pump. The diesel mass flow out of is the sum of all flows into the cylinders of the radial piston pump. Therefore, the pressure is given by the balance of mass
(18)
Graph
with the bulk modulus and the density of diesel in this volume according to Section 2.1, , . Moreover, is the mass flow through the inlet valve of the i-th cylinder.
2.4. Radial piston pump
The radial piston pump delivers diesel into the rail by a periodic motion of the pistons. Starting at the top dead centre (TDC), the cylinder volume is increased due to the downward motion of the piston and the diesel in the cylinder is expanded until the inlet (suction) valve, SV in Figure 1, opens. Then, diesel flows from the gallery volume into the cylinder until the inlet valve closes again, which is the case when the cylinder pressure is higher than the gallery pressure. With the upward motion, the diesel is compressed until the outlet (high pressure) valve, HV in Figure 1, into the rail opens and diesel flows into the rail.
The radial piston pump is driven by the combustion engine such that its shaft rotates with the angular velocity of the crank shaft. During one revolution of the shaft, the cylinders are moving times up and down due to the eccentricities of the shaft. Moreover, the cylinders are mounted with a relative angle to each other, see Figure 1, and the motion of the cylinders is sinusoidal. Thus, the volume of the i-th cylinder reads as
(19)
Graph
with the number of cylinders . The volumes and are the dead volume of a cylinder and its displaced volume, respectively. In this paper, a radial piston pump with cylinders, cylinder strokes per revolution and is used. The resulting volumes and of the cylinders are shown in Figure 7.
Graph: Figure 7. Cylinder volumes and during one revolution of the crank shaft.
Using the balance of mass, the pressure in the cylinders reads as
(20)
Graph
with the diesel mass flow through the inlet (suction) valve into the cylinder, through the outlet (high pressure) valve into the rail and the leakage mass flow .
The areas of the spring loaded inlet valves depend on the difference in pressure . If the pressure difference is lower than , then the valve is closed, i.e. . For pressures higher than , the valve is completely opened, i.e. , with the maximum area of the suction valve. For pressures between these two levels, the area is assumed to increase linearly with the pressure drop. This results in the mathematical model of the form
(21)
Graph
The mass flow through the inlet valve is then given by
2.5. Rail
The rail is used as storage for pressurized diesel. It is connected to the radial piston pump, which supplies the mass flow , and the injectors, which inject the mass flow into the combustion chambers of the engine. The diesel pressure in the (constant) rail volume is formulated as
(25)
Graph
with the bulk modulus and the density of diesel in the rail according to Section 2.1. The overall injector leakage mass flow is modelled in the form of a laminar flow into the tank (pressure ), i.e.
(26)
Graph
with the leakage coefficient .
As already mentioned at the beginning of this section, the injection process is rather complex and therefore an accurate description of the injector mass flows , , with the number of injectors (equal to the number of cylinders of the combustion engine), is beyond the scope of this paper. A simplified description is meaningful, since the exact time evolution of the injector mass flow is irrelevant for the design of a control strategy for the rail pressure. Instead, a model is proposed, which covers the following essential characteristics:
The periodic injection of diesel yields fluctuations in the rail pressure with the frequency . These variations in the rail pressure should be included approximately in the mathematical model.

Of course, the average mass flow of the injectors of one injection cycle must be accurately described by the model.
Based on these considerations, a sawtooth characteristic of the form
2.6. Simulation results
In this section, simulation results are shown to analyse the behaviour of the modelled system, and the stationary mass flow is compared with measurements to validate the model.
Figure 8 depicts simulation results of the cylinder pressures and the gallery pressure , as well as the mass flows and for a constant FMU position 0.85, a crank revolution speed and a constant rail pressure Taking a closer look at cylinder 1 first, the pressure decreases during the downward stroke of the piston until the cylinder pressure is lower than the gallery pressure and the inlet valve opens. Since the FMU is rather widely opened, the increasing cylinder volume can be completely filled by the diesel flowing through the inlet valve, i.e. . After some time, however, the cylinder volume increases faster than it can be filled with diesel such that the diesel in the cylinder starts to cavitate. Then, the cylinder pressure is almost constant and thus also the mass flow into the cylinder remains almost constant. The cylinder pressure begins to rise again when the cylinder is completely filled with fluid. If this happens before the bottom dead centre (BDC) (as it is the case in Figure 8), the cylinder is completely filled at the BDC and the mass flow of the radial piston pump is solely determined by the geometrical displacement and the angular velocity of the pump. Moreover, in this case, the two cylinders do not influence each other since the inlet valve of cylinder 1 closes before the inlet valve of cylinder 2 opens.
In contrast to this, the FMU opening is smaller () in the simulation scenario depicted in Figure 9. This brings along that the cavitation in the cylinder immediately starts after the inlet valve opens and the cylinder is not completely filled when passing the BDC. Thus, in this case, the mass flow of the radial piston pump is also influenced by the opening of the FMU. Time intervals do occur where the inlet valves of both cylinders are opened and thus both cylinders draw diesel from the gallery.
Measurements of the cylinder pressures and mass flows are not available for a validation of the model. Thus, the average mass flow of the radial piston pump for various FMU positions and crank shaft revolution speeds are used to validate the stationary accuracy of the model. The results in Figure 10 show a comparison of the measurement data (cross symbols) with simulation results (solid lines). It can be seen that for small openings of the FMU, the mass flow is basically determined by the FMU opening and is almost independent of the angular velocity of the crank shaft. For higher openings, the mass flow is, however, determined by the geometrical displacement of the pump. A comparison of the measurement data with simulation results of the model shows that the maximum errors are well below 10%, which is a very good result since the model has been parameterized by the nominal geometrical parameters of the system only.
inally, in Figure 11, the dynamics of the modelled system is discussed. Here, the current is switched from 0 to 1 at and back to at . Due to the preload of the spring, this implies that the FMU is entirely opened for at the beginning and the end of the simulation and closed in between. The average injected diesel mass flow is set to . Since the injected mass flow is lower than the mass flow delivered by the radial piston pump from the beginning until and after , the rail pressure is increasing. During , the FMU is closed and no diesel is delivered to the rail. Thus, the rail pressure decreases in this time interval. When the current in the solenoid is switched at , the magnetic force leads the valve spool to move and close the FMU, see the time evolution of . Note that the rail pressure is still increasing a short time after the FMU is closed. This is due to the fact that one cylinder is still filled with diesel which is delivered into the rail. The same effect occurs when the FMU is opening. The cylinders have to be filled and the diesel compressed, before diesel is delivered to the rail. This results in some kind of dead time in the system, which is a function of the angular velocity of the crank shaft, see also [[ 8]].
3. Average model
As the ripples in the rail pressure due to the piston strokes of the pump and the injection process cannot be affected by the FMU, only the average value of the rail pressure during one stroke is relevant for the control strategy.[ 2] Typically, control strategies for the rail pressure are based on a stationary (measured) characteristic map of the mass flow of the pump as a function of the FMU position and the rotational speed of the pump . This approach, however, has two basic drawbacks:

In the controller design, handling of characteristic maps might be difficult, since higher derivatives of these maps are frequently required, e.g. in a flatness-based controller. The numerical calculation of these derivatives might become cumbersome.

It is difficult to analyse the influence of parameter variations or different installation sizes by means of maps obtained from measurements.
Thus, an average model based on the detailed model of the previous section is strived for. In order to derive an average model, only a single cylinder of the pump is considered in the first step. The influence of the second piston is taken into account in a second step.
3.1. Single cylinder
Figures 12 and 13 show the pressure and the mass flows and of one cylinder for one piston stroke, a constant FMU position (approximately 50% opening in Figure 12 and approximately 30% in Figure 13), and a constant rail pressure. The piston stroke can be divided into the subsequent phases:
The high-pressure valve is closed at the TDC and the pressure in the cylinder is equal to the rail pressure at . During phase I, both valves are closed such that the diesel mass in the cylinder is constant and the pressure decreases as the volume increases.

The inlet valve is opened at when the cylinder pressure is equal to the gallery pressure . In this phase, the mass flow through the inlet valve is high enough to completely fill the increasing volume of the cylinder.

If the pressure in the cylinder further drops beyond the vapour pressure , the diesel starts to vaporize. However, below the lower vapour pressure , all diesel is vaporized and the cylinder pressure is basically constant. In order to simplify the treatment of cavitation, is set in the average model. This is reasonable, since is small and thus errors connected to this simplification are negligible.

In this phase, the volume of the cylinder can again be filled completely before the piston reaches its BDC. Note that this phase is not present in Figure 13, where the FMU is opened only approximately 30% of its maximum value. Figure 12 showing the results for a wider opened FMU of 50% of its maximum value contains such a phase.

When the pressure rises above at , the inlet valve is closed and the diesel is compressed until the pressure reaches at .

In this phase, the outlet valve is open and diesel is delivered into the rail.
It can be seen that the mass of diesel delivered into the rail during one cycle of the piston stroke is given by the difference of the maximum diesel mass in the cylinder before the inlet valve closes and the mass at , i.e. the time when the inlet valve opens, . This directly gives the average mass flow in the form
4. Conclusions
In this paper, a detailed mathematical model of a common-rail diesel system was derived. A special focus was laid on the accurate modelling of the radial piston pump in combination with the FMU, where it was shown that cavitation occurs in the cylinders of the pump. This effect is typically not taken into account analytically in mathematical models of the system but has a significant influence on the system's behaviour. Based on this detailed model, an average model of considerably reduced complexity was derived. The stationary accuracy of both models was proven by means of measurement results of a real common-rail system.
One major advantage of the proposed model is that it can be easily parameterized by means of the geometric parameters of the system, not requiring extensive measurements. Thus, it is also possible to simulate different designs of the system (e.g. different size and number of pistons) and to estimate their performance without relying on extensive measurements. The average model also features these benefits and is, due to the reduced complexity, very well suited for a (non-linear) controller design.
Current research is dealing with new model-based control strategies based on the average model, where already first promising results have been obtained. These control strategies are currently further investigated by our project partner Robert Bosch GmbH.
Acknowledgements
The authors highly appreciate the support provided by Robert Bosch GmbH, in particular Matthias Bitzer, Johannes Nitzsche, Carolina Passenberg, Daniel Seiler-Thull and Adrian Trachte.
Notes 
1 Quantities labelled with tilde refer to normalized quantities. Here, the mass density and the bulk modulus are normalized to their values and at ambient pressure . All other quantities are normalized to their respective maximum values .
2 Instead of controlling the average pressure, it is often desired to control the peak pressure over one cycle of the cylinder stroke. Since the average and the peak pressure are directly coupled, controlling the peak pressure is equivalent to controlling the average pressure. Thus, subsequently only this case is considered.
3 Note that the closing time of the inlet valve of the second cylinder can be directly obtained from the closing time of the inlet valve of the first cylinder by using the fact that the time behaviour of the second cylinder is equal to the time behaviour of the first shifted by .
Numerical and Experimental Assessment of a Solenoid Common-Rail  Injector Operation with Advanced Injection Strategies Andrea Piano and Federico Millo Politecnico di Torino Lucio Postrioti, Giulia Biscontini, and Andrea Cavicchi Università degli Studi di Perugia Francesco Concetto Pesce General Motors Powertrain Europe
ABSTRACT The selection and tuning of the Fuel Injection System (FIS) are among the most critical tasks for the automotive diesel engine design engineers. In fact, the injection strongly affects the combustion phenomena through which controlling a wide range of related issues such as pollutant emissions, combustion noise and fuel efficiency becomes feasible. In the scope of the engine design optimization, the simulation is an efficient tool in order to both predict the key performance parameters of the FIS, and to reduce the amount of experiments needed to reach the final product configuration.
In this work a complete characterization of a solenoid ballistic injector for a Light-Duty Common Rail system was therefore implemented in a commercially available one-dimensional computational software called GT-SUITE. The main phenomena governing the injector operation were simulated by means of three sub-models (electro-magnetic, hydraulic and mechanical).
The model was validated using experimental data obtained by a Zeuch’s method injection analyzer. To this end, the experimental injection rate profiles and injected volumes along with rail pressure profiles were acquired in several multi-event injector operation strategies pertaining to different engine operating conditions, typical of the NEDC operation. The use of different Energizing Time (ET) and Dwell Time (DT) values allowed the evaluation of the injector potential in applying advanced actuation strategies and the assessment of the model capability to simulate the injection system operation in challenging operating conditions such as close injection events.
INTRODUCTION The fuel injection apparatus is a key sub-system for direct-injected internal combustion engines as its operation controls to a large extent both the mixture formation and the combustion processes. In particular, in compression ignition engines the fuel metering accuracy, the required injection rate time-profile and the uniform fuel spray spreading in the combustion chamber are strictly governed by the injection system behavior. The achievement of these complex tasks is mandatory in order to fulfill the current automotive market design targets in terms of fuel conversion efficiency, combustion noise control and pollutant emissions level. [1], [2], [3], [4].
In the last years the injection systems design has been continuously developed in order to allow the application of complex, multi-event injection strategies so as to gain a better control of the fuel injection rate time-profile, which is the most effective approach to control the 
combustion process development in Diesel engines. The application of injection strategies based on reduced dwell-time, multi events and/ or on the modulation of the fuel discharge rate in the single event enabled significant improvements in the combustion control capabilities with respect to first-generation, common rail injection systems. The injection system ability to actuate complex strategies is largely based on an improved injector dynamic response, which in turn allows lower dwell-times; obviously, the injector dynamic characteristics are realistically exploitable only if the system shot-to-shot repeatability in terms of fuel metering and injection rate is preserved, along with the prescribed jet-tojet evolution uniformity. [5].
In this scenario, the introduction of ballistic common-rail injectors, in which the needle lift is not limited by a mechanical stop but it is determined by the system hydraulics and the needle inertia, is a significant step in the development of fast-response systems. Given the improved capabilities of this last generation common-rail systems, the need of both experimental and numerical tools assisting the injection system tuning process is even higher than in the recent past. [6]. The huge amount of potentially interesting injection and combustion strategies requires validated and robust numerical tools simulating both the injection and combustion systems operation, possibly in a coupled numerical environment, in order to reduce the number of experimental tests at the hydraulic and engine test benches.
In the frame of a medium term research program aiming at the development of a comprehensive numerical tool to simulate both the injection system and the engine operation, a numerical 1-D model of a solenoid, ballistic injector was built and validated in the GT-SUITE environment. The injector model was validated by means of experimental data acquired by a Zeuch’s Method-based Injection Analyzer. [7], [8], [9].
As the present research final goal is developing a design tool for the Fuel Injection System (FIS) operating in part load conditions typical of the NEDC cycle, the injection system 1-D model was developed and tuned focusing on complex injection strategies. As a matter of fact, although several papers are present in literature concerning the development of 1-D models for Diesel injectors, unfortunately only partial validations of such models are generally presented: the reported validations are often carried out for relatively long injection events, which are only moderately interesting for the implementation of advanced, multi-event injection strategies currently used for automotive applications.
The injection system was operated in several multi-injection strategies, at different rail pressure values The results obtained at the hydraulic bench (injection rate profiles and rail pressure profiles) were used to tune the 1-D injector model, obtaining a satisfactory accuracy of the predicted injection rate profiles also with complex, reduced dwell-time actuation strategies.
EXPERIMENTAL FACILITIES As it is described in the introduction section, the goal of this work is the model building and calibration of a commercial second generation solenoid common-rail injector and, then, the validation against the experimentally measured mass flow rate and total quantity injected data.
Since in order to validate the 1-D injector model, in addition to all the main geometric characteristics of the injector, experimental hydraulic data are required mainly in terms of rail pressure time history and solenoid current profile (inputs to the model) and injection rate curve (to be compared with the model main output). The experimental facilities which were exploited to this end will be described in detail in next paragraphs.
Internal Geometry The detection of the internal geometry of the injector is needed to obtain a good geometric characterization. Nozzle, Z-hole and A-hole are the critical parts of the injector where cavitation can appear and, consequently, a precise detection has to be done. In literature, the most most used technique for the evaluation of geometric parameters, consists of the use of special silicone to obtain a mold of the physical part. Then, using a Scanning Electron Microscope, pictures are obtained and processed by means of CAD software. [10], [11], [12].
In this work a different methodology was adopted: more specifically, a 3D Computed Tomography (CT) of the injector was carried out. This technique is nowadays among the most advanced available in the industry; CT is a completely nondestructive and noncontact method for obtaining three-dimensional representation of the scanned object both externally and internally. A CT scanner consists of an x-ray emitting source, a positioning system, a detector and electronic and computational devices for data acquisition and elaboration. The principles are described below: radiographic images of the part are acquired from different angles; then, combining together all the acquired images, a virtual slice through the part is reconstructed. When different consecutive slices are computed, a 3D visualization is obtained. The reason why it was decided to use this technique is that it has many advantages with respect to traditional dimensional measuring systems, especially in terms of reduced measuring time, including scanning, reconstruction and analysis time. [13], [14]. An example of the images of the injector obtained through CT is reported in Figure 1.
Injection Analyzer The experimental hydraulic data used to tune and validate the GT-SUITE 1-D model of the solenoid injector were obtained at Perugia University SprayLab by a proprietary STS Injection Analyzer. This instrument is designed to simultaneously measure 
Piano et al / SAE Int. J. Engines / Volume 9, Issue 1 (April 2016)566
both the global injected volume and the injection rate time history (along with other dynamic signals such as rail pressure, injector current, needle lift) in a given operating condition. The operating condition is defined by the rail pressure level and by the injector actuation strategy, based on 2n-1 parameters (with n ET values and n-1 DT values); the instrument can vary any of the 2n-1 parameters to analyze the system behavior. The mean injected volume and the dynamic signals are acquired in the same batch of consecutive injection events, allowing both the mean behavior and the shot-toshot dispersion evaluation. In case of multi-injections strategy, the injected volume from the single injection events can also be derived.
The injection rate measurement by the STS Injection Analyzer is based on the Zeuch’s Method, i.e. the injection in a closed, fixedvolume chamber filled with the same injected fluid. At the injection occurrence, the injected volume ΔV is forced to enter the measuring chamber, in which before the injection event the Pbase level (50 bar in this campaign) is maintained, causing its pressure to rise according to Equation 1:
(1)
where k is the fluid Bulk Modulus and V is the measuring chamber volume. Differentiating Equation 1 allows the injection rate evaluation:
(2)
The pressure history in the measuring chamber, along with the fluid temperature, is detected by a piezo-resistive sensor (Kistler 4075 A100). At the end of each injector actuation cycle, the globally injected volume is evacuated from the measurement chamber by a fast acting solenoid valve in order to re-set the Pbase pressure level for the next injection cycle. The fluid escaping the Injection Analyzer flows through a Coriolis mass flow meter (Siemens Sitrans CF 2100, accuracy ± 0.5% of the actual reading in the range 0.1-4 kg/h) to measure the mean injected mass and the fuel density over an assigned set of injector actuation cycles. Further, the mass flow meter allows a continuous instrument calibration overcoming the difficulties in determining the fluid bulk modulus in the actual operating conditions. More details about the STS Injection Analyzer are reported in [15], [16], [17].
The rail supply was provided by a CP1H Bosch pump driven by an electric motor, with a transmission ratio set to simulate a 2100 engine rpm operation. A pickup on the pump shaft triggered the pulse train generation for the injection system, with an injection phasing to the pump TDC analogous to the engine operation. The rail pressure is set by a two-way, PWM-modulated inlet metering valve controlled by a self-developed PID strategy. In stand-by (noinjection) conditions, a rail pressure ripple within the minimum between 2% of the set point and 10 bar was obtained. The rail was submerged in an oil-heated vessel so as to regulate the fuel temperature in the rail-injector pipe @40±1 °C with all the injection pressure levels and injection strategies. The used fluid for the experimental test is a commercial, ISO4113 compliant Diesel pump test fluid [18]. The same standard test fluid has been selected from the GT-Suite database for the numerical simulations.
COMPUTATIONAL INJECTOR MODEL DESCRIPTION GT-SUITE is a simulation tool for design and analysis of engine and vehicle system and components [19]. It is capable of modeling the coupled behavior of hydraulic and mechanical components, and it is also able to study steady-state and transient response. GT-SUITE is based on one-dimensional, compressible, unsteady fluid dynamics and multi-body dynamics. Moreover, flow and heat transfer in the piping and volumes can be modeled, as well as mass dynamics. As it can be seen in the next sections, the flow solver includes models for cavitation, frequency dependent friction, species transport and non-equilibrium aeration everywhere in the system. [20].
Electromagnetic Valve The solenoid model is built from electric and magnetic primitives representing the electromagnetic system: a current source, a coil, radial and axial magnetic components, and the air gap between the solenoid and armature. The main geometrical characteristics of the solenoid valve components are reported in Table 1.
The model solves for the magnetic flux induced by current in the coil. This flux passes through the magnetic circuit, generating a force on the armature according to the flux and air gap, as shown in Figure 2, where the flux is represented by a dashed red line.
Control Valve The performance of a servo driven injector is mainly ruled by the control valve, typically a solenoid actuated valve. The fluid-structure interaction of the valve is modeled using the 1D mechanic library and some fluid-mechanical sub-models or templates (FMT). Starting from the Bosch CRI 2.5 model on, a new generation of valve called ‘pressure-balanced’ has been introduced. With the ‘pressure-balanced’ valve it is possible to increase rail pressure without increasing at the same time the spring’s preload, the magnetic force and the static leakage [21]. Because of its layout, the pressure-balanced valves have a sealing seat-diameter three times higher than the standard ones. To get the same flow area of the standard valve these valves have a reduced stroke-end thereby improving the injection performances. The valve-seat region was modeled by two fluid mechanical templates with opposed pressure forces. Two different templates were used to model correctly the valve-seat region of the valve. More specifically, a “FluidPiston” template was used to model the interaction between flow and mechanical system, while a “FlapperValve” template was used to calculate the change in flow area as a function of the lift of the poppet, the pressure force on the poppet and the redistribution of the attached volumes. For the latter, a linear variation of the pressure acting in the flapper seat area was assumed. The net hydraulic force acting on the armature results from the difference between the flapper valve’s diameter and the fluid piston’s diameter. This difference of diameters generates a total force which is consistent with the values found in literature [21]. The flapper valve template also defines the flow area as a function of the armature’s lift.
In order to model the armature’s movement accurately it has been necessary to simulate both all the pressure surfaces of the armature and the fuel path across the pilot valve.
Control Chamber As known, the needle motion results from an imbalance of the pressure forces acting on the needle. This imbalance results from a reduction of the pressure in the control chamber which is initiated by opening the control valve. When the control valve is opened, fuel drains from the high pressure at the inlet of the injector, through the inlet orifice into the control chamber, and through the outlet orifice to the low pressure downstream of the control valve. Once the control valve is fully open, the design of these orifices has a significant impact on the pressure in the control chamber and therefore the imbalance of forces on the needle. The large pressure drop across these orifices can cause cavitation so care is required to investigate the flow through them.
The geometry of the inlet and outlet holes strongly affects the dynamic behavior of the needle. As a result, the control volume orifice of the injector need to be hydraulically characterized. An example of a possible experimental setup for the characterization of the flow through these controlling orifices was proposed by Salvador et al. [9].
Figure 4. Control chamber model - Inlet and outlet holes
Z-Hole The Computed Tomography of the injector shows a conical section of the orifice. In this case, the sub-model adopted to evaluate the discharge coefficient of the inlet orifice, does not consider the cavitation phenomenon. This hypothesis is consistent with hydraulic characterization made by Salvador et al., where the mass flow rate has a linear behavior with the square root of the pressure [10].
The value of the maximum discharge coefficient is related to the orifice geometry and it can be defined by the mass conservation equation:
(3)
Where ṁf is the fuel mass flow rate, A0 is the geometric area of the orifice, ρf is the fuel density and Δp is the difference between upstream and downstream pressure.
A-hole As opposed to the Z-hole, the A-hole has not a conical section but a cylindrical one. Moreover, from the hydraulic characterization a different behavior can be seen [10]: there is a value of pressure drop above which the mass flow rate is constant and the orifice cavitates. The discharge coefficient was therefore estimated by means of the GT-SUITE sub-model for liquid flow through an orifice, including cavitation effects, based on Sarre et al. work [22]. The transition into cavitation is defined by a critical cavitation number and the discharge coefficient will be a constant value for no cavitation and calculated based on the cavitation number in cavitating regime. The cavitation number K is defined as [20], [22]:
(4)
where p1 is the pressure upstream of the orifice, p2 is the pressure downstream of the orifice, and pv is the vapour pressure of the fluid.
The onset of cavitation is specified by the critical cavitation number. If the cavitation number is lower than the critical cavitation number, the orifice is cavitating and the discharge coefficient will be calculated as [20], [22]:
(5)
where Cc is the contraction coefficient, defined as [20], [22]:
(6)
in which  is the ratio of the inlet radius to the orifice diameter. The contraction coefficient is limited to be ≤ 1. If > Kcrit , there is no cavitation and the discharge coefficient is calculated as [20], [22]:
(7)
The geometrical and hydraulic parameters are shown in Table 3. Given the impossibility to measure R (inlet radius of the orifice) with the 3D CT resolution, the Contraction Coefficient was considered similar to the value found in literature. [10].
Table 3. Geometrical and hydraulic parameters for outlet orifice
Figure 5. Sketch of the control piston model
Control Piston and Needle Mechanics The pressure force acting on the control piston is calculated with a dedicated sub-model that determines the pressure force using the upstream and downstream pressures. The pressure acting on the needle varies due to the design of the nozzle and the tight fit of the needle in the nozzle. In order to accurately model the pressure force on the needle, the flow path through the nozzle is modelled using fluid volumes and pipes. In particular the needle poppet valve is modelled by calculating the flow area through the valve based on the position of the mass attached to it. The valve's discharge coefficient is determined based on the Reynolds number of the flow through the valve. The volume changes due to the poppet motion are also taken into account.
Figure 6. Sketch of the needle model
The high-pressure fuel acting on the control piston and needle leads to significant axial deformation of their bodies given the small needle lifts in injectors. This is taken into account by separating each body into two masses connected by a spring and a damper. The spring represents the axial stiffness of the body and the damper accounts for material damping.
Hydraulic Network Explicit solver is used for time integration of the fluid dynamics, adopting short time steps in order to capture high frequency events, since the wave dynamics can significantly affect the hydraulic behavior of the injector and because a short time scale is desired in terms of injection rate profile respect to crank angle.
As far as the friction and the heat transfer settings for the flow circuit are concerned, it is worth to mention that, while typical friction models are based on Moody chart fits, in an unsteady flow, as in this case, a model that adapts to the amplitude and frequency of the flow pulsations is needed. Usually this model is called frequencydependent and considers that the friction and heat transfer are enhanced due to boundary layer regrowth [20].
Nozzle The nozzle orifices or holes are modelled in a detailed way, where the injector internal flow volumes, mechanics and hydro-mechanical interactions are considered. As it has been done for the Z-hole, the conical section of the nozzle holes allows neglecting the cavitation phenomenon. One of the most critical aspects for modelling the nozzle is the characterization of the orifices, dimensionally as well as hydraulically. In order to obtain a proper calibration of the nozzle orifices parameters, a stationary flushing of the nozzle, corresponding to the test for the determination of the nozzle flow number, was simulated. The so called nozzle flow number is the total volumetric amount of fuel that passes through the nozzle holes in 30 seconds considering a pressure drop equal to 100 bar between the boundaries.
VALIDATION The model was validated using experimental data obtained by a Zeuch’s method injection analyzer. To this end, the experimental injection rate profiles and injected volumes along with rail pressure profiles were acquired in several single and multi-event injector operation strategies.
Test Matrix The experimental measurements relative to the nozzle #1 correspond to rail pressures and energizing time highlighted in Table 5. The test matrix selection was based on the evaluation of the injection system operating conditions typical of the NEDC cycle.
Table 5. Test matrix for nozzle #1
Results of the Injector Model Nozzle #1 The predictive capability of the hydraulic model was firstly assessed by comparing numerical results with actual injected volume quantities and flow rates for different rail pressures and energizing times in case of a single injection events for nozzle #1, as reported in Figures 9, 10, 11 and 12. The comparison of the experimental total injected volume with the simulation results over a wide range ET values is shown in Figure 9: as one can see, the model predictive capability can be considered as more than satisfactory for all the analyzed injector command profiles, with errors which are, on average, lower than 10%. The blue dots in Figure 9 indicate the injections events that will be further analyzed in Figures 10, 11 and 12. As far as the instantaneous volume flow rates are concerned, the experimental and simulated values are compared in Figures 10, 11 and 12 for 3 different Energizing Time values for each of the 3 rail pressure investigated. As the figures show, a high level of accuracy was achieved in replicating the behavior of the injector with single-event injection strategies. More in details, it can be observed that both the injection start and end timings are accurately predicted, along with the injection rate level rising and lowering trends during the transient phases of the process. Some moderate inaccuracy can be observed for the high flow rate phase at high rail pressure levels (see Figures 11 and 12, bottom graphs), where the predicted maximum flow rate is about 5% lower than the experimental value
Finally, the model predictive capability was also assessed for multiple injection events, and, as an example, the results obtained for a Pilot+Pilot+Main event for a rail pressure equal to 1000 bar, compared with the simulation result are shown in Figure 13. The high accuracy of the model in predicting the experimental injection rate also with multiple injections is confirmed, even for relatively complex injector command strategies, characterized by reduced DT values between consecutive injection events.
Nozzle #2 In order to further assess the predictive capabilities of the injector model, a different nozzle (identified as Nozzle #2 in Table 4) was tested and simulated. To this end, the experimental injection rate profiles and injected volumes along with rail pressure profiles were again acquired in several multi-event injector operation strategies pertaining to different engine operating conditions, typical of the NEDC operation. As an example, the comparison of the experimental volume flow rate with the simulated one is shown in Figure 14, for 3 different injection patterns which could be adopted for a 2000 rpm, 8 bar bmep engine point (the main parameters of these injection patterns are reported in Table 6). The more than satisfactory accuracy of the model in predicting the actual behavior of the injector even for multiple injection events can therefore be confirmed
CONCLUSIONS A methodology for obtaining a complete characterization of a solenoid ballistic injector for a Light-Duty Common Rail injection system and building a simulation model including three main 
sub-models (electro-magnetic, hydraulic and mechanical) in a commercially available one-dimensional computational software called GT-SUITE was presented in this work.
In order to obtain a good accuracy of the model, different crucial issues need to be addressed: 
1. an accurate detection of the internal geometry of the injector is needed to obtain a proper geometric characterization. In this work a 3D Computed Tomography (CT) of the injector was adopted. 2. An extensive dataset of experimental measurements is needed for model validation. In this work instantaneous injection flow rates measured by means of an STS Injection Analyzer, that is based on the Zeuch’s Method, were used.
The comparison between the experimental and simulated volumetric injection rates showed a more than satisfactory accuracy of the model in predicting the actual behavior of the ballistic injector both for single and multiple injection event patterns, even for relatively complex injector command strategies, characterized by reduced DT values between consecutive injection events.
The satisfactory assessment of the predictive capability of the injector model can therefore pave the way for the coupling with predictive engine combustion models, in order to obtain an integrated numerical model which could then be used as a “virtual test rig”, for evaluating the effects of different engine calibrations in terms of combustion development and emissions production.
OPTICAL DEVICE FOR MEASURING THE INJECTORS OPENING INCOMMON RAIL SYSTEMS
Riccardo Amirante, Carlo Coratella, Elia Distaso, Gianluca Rossini and Paolo Tamburrano*
Department of Mechanics, Mathematics and Management, Polytechnic University of Bari, Bari 70125, Italy

ABSTRACT−Since the needle displacement exerts a fundamental influence on the operation of Common Rail injection
systems, accurate measurements of the control piston position can be crucial for a more thorough analysis of the behaviour
of injectors, in particular when multiple injections are employed. Eddy current sensors have traditionally been used in lab
activities to measure the control piston position inside injectors; apart from the high cost, the scientific literature clearly shows
their inadequacy, which is mainly due to the presence of electromagnetic disturbance: the current pulse, which controls the
opening of the injector, generates electromagnetic fields which strongly affect the acquisition of data. Many attempts have
recently been made either to solve the interference occurring during such measurements or to propose alternative displacement
transducers whose operation is not influenced by electromagnetic interference. In this paper, a new device for measuring the
injector opening is proposed: it is an optical transducer characterized both by simple and very cheap construction and by a
reliable physical principle for measuring the control piston lift. The reliability of the proposed sensor is assessed by a thorough
experimental campaign and by comparing the experimental results with the numerical predictions achieved by a Common Rail
injector model. Since the assembly of the optical sensor does not affect the injector operation, it can efficiently be used both
for experimental tests and for on-board diagnosis and monitoring of the injector operation.
KEY WORDS : Common rail, Injector, Diesel engine, Optical sensor, Displacement measurement

1. INTRODUCTION
The advent of the electronic Diesel engine control plays an important role in the development of diesel engines, in
terms of performance improvement and optimization of consumptions. As is well known, the Common Rail (CR)
MultiJet system has replaced the previous UniJet system, allowing Diesel engines to satisfy all the environmental
targets (euro 3 and euro 4 standards) concerning the reduction in exhaust emissions and combustion noise
(Tanaka et al., 2002; Celikten, 2003; Vanegas and Peters, 2009). However, in order to satisfy the stricter limits
imposed by new European emission standards (Euro 5 and, after 2014, Euro 6), reliable predictions of the engine
behaviour, including all the parameters affecting it, are required. Current research studies have been focused on the
injection process. The effects of different parameters upon the quantity of fuel injected by a common rail injector was
thoroughly analyzed by Bai et al. (2016).
An effective one-dimensional spray model was developed by Xu et al. (2016), in order to understand the effect of gas
flow on the penetration of diesel sprays. The impact of different injection types and after-treatment systems on
particulate emissions of light-duty vehicles was deeply investigated by Oh and Cha (2015). Nanoparticles and
other harmful exhaust emissions in accordance with injection strategies and air-fuel ratio changes for small
diesel engines were analyzed by Eom et al. (2017). All the abovementioned papers along with other relevant
research papers (e.g., the studies developed by Lim and Lee (2016) and Wang et al. (2015)) have provided
advancements in modelling, predicting and understanding some important aspects regarding the injection process.
In this scenario, this paper is focused on a very important aspect of the injection process, which is the measurement
of the needle lift inside an injector. Infact, since the displacement of the needle exerts a strong influence on the
injection performance, accurate measurements of the needle position can be instrumental in achieving a more
thorough analysis of the behaviour of injectors. Precise knowledge of the needle lift during injections
can also help researchers better investigate misfire in Diesel engines (which is due, as is well known, to injection
problems) in order to improve current methods for detecting the phenomenon (Chang et al., 2002; Hu et al., 2011; Liu et
al., 2013; Fan et al., 2014).
Significant improvements must still be done as far as the measurement of the needle lift is concerned, in spite of the
several advancements achieved in measuring systems for internal combustion engines, such as highly-effective
measurement techniques for the characterization of particle emissions (see, e.g., the studies developed by Amirante et

al. (2015a), Myung et al. (2014) and Choi et al. (2014)) or
novel solutions for the in-cylinder pressure measurement
(see, e.g., the cost-effective solution proposed by Amirante
et al. (2015b), and novel NOx prediction methods based on
the in-cylinder pressure measurement developed by Chung
et al. (2016).
To the authors’ knowledge, all the commercially available
displacement transducers cannot be used in serial application
for on-board measurements of the needle lift because of the
complexity of their assembly which in turn affects the
normal operation of injection systems. In laboratory
applications, eddy current sensors have traditionally been
used to measure the displacement of the control piston,
which is equal to that of the needle because they are always
in contact inside the injector; however, in spite of their
widespread use, the scientific literature clearly shows their
inadequacy for such applications, because of the presence
of electromagnetic disturbances (generated by current
pulses controlling the opening and closing phases of
injectors) which strongly affect signal acquisition. As far as
this issue is concerned, very complex and expensive
solutions have been proposed to date in an attempt to avoid
the electromagnetic interference that usually occurs during
measurements (Sienkiewicz and Shukla, 1997; Prelle et al.,
2006; Virdee, 2004). Among these solutions, the possibility
of mounting the eddy current sensor directly in the nozzle
to obtain a much higher signal quality could be very
promising; however, apart from its high cost, the extension
of this technique to on-board measurements seems
unfeasible because of its complexity.
In contrast, this paper proposes the design of a new and
cheap sensor for reliable measurements of the control
piston lift, namely an optical transducer that guarantees
lower manufacturing costs and times in comparison with
all the solutions proposed recently. The choice of using an
optical measurement system is justified by its capability of
ensuring high standards of signal cleanliness even during
measurements affected by high interference.
Apart from its simplicity and cost-effectiveness, the
optical sensor has the potential to outperform the previous
solutions because its implementation in a commercial
injector does not affect the injector operation; therefore, for
the first time, the proposed sensor can efficiently be used
also for on-board diagnosis and monitoring of the injection
process.
2. MATERIALS AND METHODS
2.1. Operating Principles and Assembly of the Optical
Sensor
Figure 1 shows two cross sectional views of a typical CR
injector. The position of the needle (1) and the control
piston (2), which are kept in contact by the nozzle spring
(3) and an intermediate element (4), is determined by both
the fuel pressure in the delivery chamber (5) and the fuel
pressure in the control chamber (6). When the solenoid (7)

is energized, the check ball (8) lifts from its seat and the
fuel pressure in the control chamber drops because the fuel
is discharged through hole A (9). The pressure in the
control chamber cannot rise quickly because of hole Z (10),
therefore the higher pressure acting on the needle forces the
needle to lift from its seat, thus commencing the injection.
The measurement of the needle displacement is not a
trivial task because the needle is placed in the engine head,
which operates at very high temperatures. Another issue is
the mechanical resistance of the needle, which is surrounded
by fuel at high pressure and an eventual drilling operation
aimed at placing a sensor on the needle could affect its
mechanical resistance.
To overcome these issues, technical solutions provided
to date are based on the use of displacement transducers
positioned in the injector body in order to measure the
position of the control piston, rather than the needle position.
Specifically, eddy current sensors are traditionally used to
accomplish this task. However, the development of this
solution needs to face up several technical difficulties,
which are still unresolved. First of all, the installation of an
eddy current sensor requires material removal from the control piston by means of machining operations in order
to realize the conical surface on which its components are
usually placed (see Figure 2). Despite removing material
from the control piston is less critical than removing the
same amount of material from the needle, such a conical
part can introduce a point of mechanical weakness even in
the control piston and alter its total deformation when
subjected to the compression forces.
This technique is very expensive and its application is
only used in laboratory tests. Moreover, measurements
provided by an eddy current sensor are disturbed by the
electromagnetic interference generated by the current that
circulates in the solenoid of the injector, resulting in
degradation and partial loss of data that make the signal
less accurate.
To overcome the interferences and high costs arising
from the employment of eddy current sensors, the device
presented here is an optical sensor. It is mainly composed
of a cheap light emitting diode and a compatible photo
receiver, which are positioned opposite each other. As
shown in Figure 3, the two photo elements are placed
outside the injector body and in correspondence of the
control piston.
A beam of infrared light (λ ≈ 900 nm) is generated by the
photo diode and then reaches the photo receiver by virtue
of both the two holes (d = 4 mm) machined in the injector
body and the hole (d = 1.5 mm) machined in the control
piston.
The hole in the control piston, because of its great
hardness, was achieved by means of a drilling operation
with cemented carbide (Widia) tools.
The motion of the control piston during the injector
opening and closing causes the variation in the overlap
between the holes of the injector body, which are motionless,
and the hole of the control piston, which can move
perpendicularly to the light beam (see Figure 4). As a
result, the receiver is reached by a light beam whose
intensity is proportional to the area of the intersection
between the fixed circles and the moving one, and such
intensity variations are indicative of the displacement of
the control piston.
Figure 5 shows the geometrical characteristics of the
emitter and the receiver used in this work. Both the emitter
and the receiver require a power supply of 5 V. The emitter
consists of a photodiode, made of gallium arsenide which
works in the infrared, emitting a light beam whose
wavelength is around 900 nm. In the circuit emitter, a
limiting resistor of the value of 40 ohms ± 5 % is placed in
series with the photodiode in order to limit the current
flowing through it, since the maximum allowable value is
100 mA (with +5 V supply voltage). The receiver is a NPN
silicon Photodarlington and is loaded with a resistance of
100 Ω placed between the emitter and the ground potential.
During a light pulse, the receiver is turned on and provides
an output voltage signal indicative of the piston
displacement. The time delay (rise and fall time) of the
receiver is 75 ns, which is much shorter than the duration
of the signal. Cheap receivers with higher dynamic
response are also available and could be employed to
further increase the system response.
As shown in Figure 6, two plates of polycarbonate
(capable of transmitting infrared light, stemming the
phenomena of the beam dispersion), whose pins are forced

into the aforementioned holes, were utilized to support the
diodes (Figure 6).
The dimensions of the plates did not constitute an issue
for the experimental test rig; therefore, for simplicity
reasons, large plates (compared to the dimensions of the
diodes) were used in the prototype to support the diodes.
However, it should be noted that the supports of the diodes
must be manufactured as small as possible in the case of
on-board measurements; considering that the diodes have
very short dimensions (see Figure 5), also their supports
can be designed very small. Even the layout of the supports
could be changed in order to better adapt the device to the
engine on which it is mounted.
Figure 7 reports the voltage-overlap relationship (blue
line), which was retrieved experimentally in a preliminary
test carried out to setup the device. The graph shows the
convenience of working in the middle part (approx. from
40 % to 70 %), in order to have both a linear trend (see
orange line) and the maximum voltage gradient (approx. 1
Volt) which increases the sensitivity of the sensor (the
voltage gradient could further be increased by acting on the
shape of the hole and employing an effective optimization
process based on genetic algorithms (Amirante et al., 2014;
Amirante and Tamburrano, 2014).
The proposed sensor does not affect the normal behaviour
of the injector on which it is mounted, because the emitter
and the receiver can be placed externally to the injector
body, and because the hole drilled in the control piston does
not alter its total deformation under compression. Figure 8
shows a drawing with dimensions of the control piston
used in the experimental campaign; in the case of the most
critical compression load of 2000 bar (the maximum
achievable injection pressure in typical CR systems)
applied both to the reference control piston (without the
hole) and to the same control piston with the hole, one can
easily verify that the percentage difference between their
deformations is less than 0.01 ‰ and hence absolutely
negligible. As a result, the drilled control piston does not
undergo any significant variations in its behaviour with
respect to the normal configuration (without the hole).
Similarly, the fatigue resistance is not significantly
changed. In fact, one can verify that, in the case of the most
critical fatigue loading cycle (alternate loads ranging from
the minimum pressure of 1 bar to the maximum achievable
pressure of 2000 bar), the coordinate given by the
intersection between the mean and the amplitude of the
stress cycling calculated in the critical section (in
correspondence of the hole) lies under the limit curve of the
Haigh diagram.
In conclusion, since the total deformation under
compression and the fatigue resistance of the control piston
are not changed significantly, the optical sensor can
successfully be used for on board application in addition to
typical lab tests.
2.2. Minor Modifications to the Injector Body
During the common operation of a CR injector, a little
amount of fuel, flowing through the internal channels of
the injector, goes back from the high pressure zone
surrounding the needle to the fuel tank (see Figure 9
leakage zone). Since the leakage reaches the optical device
Figure 7. Signal vs Overlap relationship.
Figure 8. Drawing with dimensions (mm) of the control
piston.
Figure 9. Leakage flow through the internal channels of the
injector.
zone and the presence of fuel in the measuring chamber
can prevent the transit of the light signal, the injector
system needs a few modifications to isolate the volume
inside which the light passes, keeping the fuel away from
the measuring ambient.
In the prototype, this problem was easily solved by using
two elastomer rings, forced on the control piston and used
as mechanical seals, and by drilling two additional holes
above and below the measuring zone to allow drainage of
the oil, which was discharged from this zone to the fuel
tank by means of external tubes (see Figures 10 and 11). To
guarantee a greater tightness of the measuring ambient, the
zone comprised between the two elastomer rings was fed
with compressed air entering through another hole drilled
in the body of the injector (see Figure 11 n.1). In such a
way, the generated air cushion protected the measuring
zone from the reflux of the oil.
It is important to highlight that the pressure of the
supplied air can be kept slightly greater than 1 bar, and it
can be supplied by the turbocharger mounted in the vehicle
even without energy loss because there is no air consumption
in the measuring zone.
It should be noted that the absence of fuel in the
metering chamber does not increase friction between the
external surface of the control piston and the internal
surface of the injector body because, as shown in Figures 1,
9 and 11, these surfaces are not in contact in this zone and
here lubricating action is not required. In addition, all the
modifications shown in Figure 1 do not alter the regular
reflux of fuel towards the zones in which the surfaces are in
contact and require to be lubricated, so the overall
lubricating action is certainly not influenced.
2.3. Test Bench
The validity of the proposed solution was assessed by
several tests carried out on the Common Rail test bench of
the Polytechnic University of Bari.
The test bench, shown in Figure 12, is engine-free and is
composed of some commercial automotive components
Figure 10. Two gaskets placed above and below the hole of
the control piston (on the left); Holes drilled in the external
body for oil drainage (on the right).
Figure 11. Devices to avoid reflux of fuel in the measuring
zone: (1) Pipe delivering pressurized air; (2) Hole to
insufflate air; (3) Holes for housing the optical sensor; (4)
Hole on the control piston; (5) Two gaskets; (6) Sealed
chamber for the optical measurement; (7) Holes to drain
fuel from the high pressure zones; (8) Injector body
(device patented in October 2012, with PCT extension
numbered PCT/IB2013/000815)
along with additional instrumentation which were properly
assembled in order to reproduce the main working
conditions of a typical Common Rail Injection system
(Catalano et al., 2002; Amirante et al., 2006; Carlucci et
al., 2008; Papagiannakis et al., 2007). Figure 12 also shows
a scheme of the test rig. There are two fuel circuits,
working at high (dark line) and low pressure (outlined
line), respectively. The mineral oil ISO 4113, which has the
same properties of Diesel fuel, was used during the
experimental tests. The high pressure pump (2) is equipped
with an electrical discharge valve (3) to regulate the rail
pressure. The fuel is injected into a cylindrical capacity,
charged with air at 20 bar, which replaces an engine
cylinder and also avoids cavitation inside the nozzle (6).
The injector is opened by a power supply equipment (12),
which provides the current to the coil. A low pressure
pump is installed inside the fuel tank (5) and supplies fuel
to the high pressure pump (2), driven by an electrical
engine. High pressure pipes connect the pump to the rail
(9) and the rail with the injector (14). A high frequency
transducer measures the pressure inside the nozzle (8). The
opening of the injector is driven by the current waveform,
which is constituted by a high current pulse (peak current)
to overcome the inertia of the injector, and a lower current
(hold current) to hold the injector opened.
Figure 13 shows the injector that was used to test the
optical sensor (identifiable by the code 0 445 110 183).
A piezo-resistive high pressure sensor was also adopted
to measure the time history of the injection pressure, which
represents a valid indication of the dynamic behaviour of
the injector. In fact, since the first experimental
investigations carried out by Fiat Research Centre and
successively by Bosch (Stumpp and Ricco, 1996),
measurements of the nozzle pressure have been used to
study the injector opening. The reliability of this approach
has been confirmed by other investigations that have
demonstrated the clear correlation among injector opening,
fuel spray across the nozzle and nozzle pressure
(Heimgärtner and Leipertz, 2000; Catania et al., 2008).
In the first experiences, the nozzle pressure was measured
indirectly by means of strain-gages fixed on the nozzle
(Stumpp and Ricco, 1996), while nowadays it is measured
directly by a pressure transducer placed on the nozzle
(Amirante et al., 2006).
The sensor employed in this work is the Kistler 4067
A2000 A2, with 2000 bar full scale and a precision of 0.5
% in the temperature range 20 ~ 200 °C. To allow the
placement of the pressure transducer, a hole of 1.5 mm was
drilled in the sprayer; the assembly is shown in Figure 14.
Such a modification does not affect the injector behaviour
as demonstrated in Amirante et al. (2006).
3. RESULTS AND DISCUSSION
3.1. Experimental Results
The experimental tests on the Common Rail bench were
performed to assess the effectiveness of the proposed
sensor in measuring the control piston displacement during
UniJet and MultiJet injections. To that end, the parameters
of the injection process were properly set so as to
reproduce each possible scenario on the test bench. Figure
15 shows a first example reporting the time histories both
of the voltage signal from the optical sensor (red line) and
of the current supplied to the coil (blue line), both obtained
during a main injection. It is noteworthy that the trend of
the voltage signal traces the expected needle lift; in fact, the
voltage curve suddenly decreases when the peak of current
occurs (approx. at 3300 µs) and, as expected, the injector
starts opening (approx. at 3500 µs). Afterwards, as a
consequence of the increasing lift of the needle, the voltage
curve decreases until the maximum lift is reached (approx.
at 4800 µs) and the voltage assumes a constant value.
Similarly, the increase in the voltage from 5200 µs to 5800
µs indicates the transient during the injector closure. The
time delay between the start of the peak phase and the
injector opening is equal to about 200 microseconds, which
is a typical value for a CR injector (Stumpp and Ricco,
1996).
The reliability of the optical sensor was further assessed
by several experimental tests during which the coil of the
injector was provided with different current profiles that
are typical both of pilot injections and of main ones. The
pressure acting on the needle was measured by means of
the pressure transducer shown in Figure 14 and its time
history was recorded and used to validate the proposed
optical sensor.
Figures 16 ~ 18 report the time histories of the lift
voltage signal, of the nozzle pressure and of the current in
the coil obtained during three injections with the rail
pressure being set to 400 bar, 900 bar and 1050 bar,
respectively. Figure 16 refers to a main injection with a
peak current of 19 A, while Figures 17 and 18 refer to two
pilot injections with a peak current of 16 A and 17 A,
respectively.
The comparison among the curves of the current,
pressure and lift reported in Figures 16 ~ 18 confirms the
effectiveness of the proposed optical sensor. In fact, it is
possible to observe the correspondence between the large
decrease in the voltage, which indicates the needle lift, and
the large pressure drop in the nozzle, caused by the sudden
opening of the injector. Furthermore, as expected, the
voltage drop that indicates the opening phase increases
with the rail pressure (see Figures 17 and 18). In fact, the
greater the rail pressure, the higher the needle lift and, as
shown by the voltage curves, the longer the closing phase.
It is also possible to observe the typical pressure
oscillations occurring in the nozzle after the closing of the
injector.
Similarly, Figures 19 ~ 21 refer to three Main injections
that differ from one another because different values of
energizing time (peak + hold), peak & hold currents and
rail pressure, whose values strongly affect the opening of
the injector, were employed.
The different duration of the opening phase (from 3600
µs to 6700 µs in Figure 20 and from 3500 µs to 7200 µs in
Figure 21) is due to its strong dependence on the overall
energizing time, which was changed from 1100 µs (Figure
20) to 1500 µs (Figure 21).
Figures 22 and 23 report other measurements obtained
with two main injections, performed with quite similar
current profiles and energising time but with a large
difference in the rail pressure (400 bar and 900 bar,
respectively). As shown by the comparison of the two
graphs, the higher rail pressure makes the voltage fall (i.e.
starting lift) faster, because of the bigger force acting on the
needle. In both cases, the voltage signal is again consistent
with the expected behaviour of the needle, evidencing the
reliability and accuracy of the proposed optical sensor once
more. As additional information, it is also possible to
observe the typical pressure oscillation in the nozzle
(Catalano et al., 2002; Amirante et al., 2006) which is due
to the propagation of the compression waves that are
created by the opening and closing of the injector, e.g. see
Figure 16 (the pressure increases from 4500 µs to 5000 µs
and decreases from 5000 µs to 6000 µs and so on).
As visible in Figures 16 to 23, the signal of the control
piston displacement does not maintain a constant value but
presents small fluctuations around a mean value; this can
be attributed to the corresponding pressure oscillations
shown in the same figures. These pressure oscillations can
also be fed by downstream cavitation processes from
which reverse pressure waves may be propagated. The
pressure trends shown in Figures 16 to 23 are consistent
with the expected behaviour of the pressure inside the
nozzle; in fact, the pressure trends are very similar to those
registered in a previous experimental campaign (Amirante
et al., 2006), in which the goal was to demonstrate the reliability of the nozzle pressure measurement in a
commercial injector (without implementing the proposed
optical system). More explanations for the pressure trends
and associated phenomena are provided by Amirante et al.
(2006).
Similar oscillations both in the lift of the control piston
and in the injected mass flow rate were also encountered by
other authors during tests of commercial injectors (Seykens
et al., 2004; Suh and Lee, 2008); their explanation for such
oscillations is the occurrence of cavitation associated with
a rapid opening and closing of the injector which induce
pressure waves (Seykens et al., 2004, 2005).
In order to evaluate whether the optical sensor is capable
of providing reliable measurements even for different
injector geometries, the control piston was replaced with
another one, having a smaller diameter of the upper part
(the difference from the original one is two hundredths of
millimetres). Figures 24 and 25 report the signals recorded
during two injections, which were characterized by a
similar current profile but with different rail pressures (650
and 850 bar, respectively), both performed with the
modified control piston. The comparison shows that, as
mentioned earlier, the higher rail pressure causes both the
faster lift of the needle and the longer duration of the
closing phase: it is noteworthy that both are accurately
predicted by the optical sensor. Furthermore, the smaller
diameter of the upper part of the new control piston causes
leakage losses in the control chamber and hence the
slowdown in the filling process through the “Z” orifice (see
Figure 1); as a result, a slowdown in the needle closure is
also expected. Even this behaviour is well reproduced by
the optical sensor, in fact it is seen that the slope of the
voltage signal during the closing phase of Figures 24 and
25 is lower than that of the previous figures.
Finally, additional tests were successfully performed by
simulating multiple injections, in order to demonstrate the
reliability of the proposed sensor in each possible scenario
of the injection process. Even in this case, the optical
sensor provided the same excellent results; as an example,
Figure 26 reports the signals recorded during multiple
injections (3 injection strategy) performed with the second
control piston.
With regard to the multiple injections technology, the
importance of the optical sensor must further be remarked
because such reliable measurements can allow the values
of dwell times to be accurately chosen in order to optimize
the performance of Multi-jet injection systems
3.2. Numerical Validation of the Experimental Tests
Although it has widely been demonstrated that the nozzle
pressure is strongly connected to the needle position
(Stumpp and Ricco, 1996), and hence the previous analysis
(based on the comparison between time histories of the
nozzle pressure and lift measurements) can be considered a
reliable tool for the validation of the optical sensor, the
reliability of the sensor was further assessed by the
comparison between the experimental data and the
numerical predictions provided by a numerical model of
the injector.
In the last years, several accurate models for the
simulation of high pressure Diesel injection systems were
proposed in the scientific literature (Marcer et al., 2010;
Seykens et al., 2005; Liu and Wang, 2012) in order to
predict the dynamic behaviour of injectors. Also in this
case, the numerical model of the Common Rail injector
was developed by means of the AMESim Software
(Advanced Modelling Environment for Simulation
(AMESim, 2004) with the aim of accurately reproducing
the hydraulic and mechanical layout, since it is well known
that the layout of the system and its geometrical features
strongly affect the injection process. The choice of
AMESim is justified by its capability of reproducing
physical phenomena with great accuracy, allowing the
realization of a model as similar as possible to the real
system (Marcer et al., 2010; Seykens et al., 2005; Liu and
Wang, 2012).
The injector behaviour is assumed one dimensional, and
a lumped parameter model is considered. The model also
accounts for the cavitation phenomenon, since cavitation
influences the control piston lift (Ficarella et al., 1999;
Giannadakis et al., 2008).
With regard to the cavition model, AMESim uses the
same approach as that described by Seykens et al. (2005),
which takes into account the occurrence of cavitation by
only reducing the discharge coefficient (defined as the ratio
of the actual flow rate to the maximum theoretical flow
rate). In particular, the discharge coefficient within a
metering section is changed according to the cavitation
number, which is a non-dimensional parameter indicating
the expected cavitation intensity. The AMESim cavitation
model has proved to be very accurate, as shown by
different studies present in the literature (Seykens et al.,
2004, 2005).
Both the combustion chamber pressure, downstream of
the nozzle, and the rail pressure are held constant. The
latter was set constant since it is subjected to variations
whose duration is much longer than the injection time
(Amirante et al., 2006).
With regard to the forces acting during the injection
process, the model takes into account the electromagnetic
forces (relevant to the activation and control of the
injector), the internal forces between the needle and the
control piston, the pressure forces exerted by the oil in the
control and delivery chambers and, finally, the external
elastic force generated by the centring spring of the
solenoid valve. The good accuracy of the model, according
to the common scientific approach (Marcer et al., 2010;
Seykens et al., 2005; Liu and Wang, 2012), allows the
above listed actions to be reproduced with good precision.
Figure 27 shows a sketch of the AMESim model of the
Common Rail injector employed in the experimental
campaign to test the optical sensor; the correspondence
between the mechanical and electrical parts and their icons
is also depicted in detail.
The “piecewise linear” submodel was employed to
achieve the same current profiles used in the experimental
campaign for ease of comparison. The electro-hydraulic
valve was modelled by means of a solenoid, a “Mass block
with stiction and coulomb friction and end stops”, a “Ball
valve with conical seat” and a “Piston with spring”.
With regard to the control piston - needle system, the
needle (13) and the control piston (11) are considered as
two different elements separated by a spring-damper (12)
in order to account for the possible relative motion between
them, although they are expected to have the same
displacement.
Moreover, the model takes into account the fuel
compressibility during the injection.
The ducts inside the injector and the pipe connecting the
rail with the injector are modelled as undeformable by
means of the “Simple wave equation hydraulic pipe” sub
model. The rail is modelled by a “constant pressure source”
sub model.
In order to increase the accuracy of the model, the
geometrical data of the injector and the properties of the oil
(type ISO 4113) used in the experimental campaign were
considered in the simulations.
Since the numerical investigation aims at further proving
the reliability of the sensor, the comparison against the
experimental results is mainly focused on the control piston
displacement.
Figures 28 ~ 31 show the comparison between the
numerical predictions and the experimental measurements.
The red line denotes the measurement of the control piston
lift, while the green one denotes the simulated lift of the
control piston; the trend of the current (experimental
measurement) is plotted as a blue line. Figures 28 ~ 30
refer to three Main injections while Figure 31 refers to a
Pilot injection.
All these Figures show good agreement between the
experimental measurement and the simulated lift in terms
of both the rise time and the value of the lift at full opening,
and this confirms the accuracy and reliability of the optical
sensor. The numerical predictions of the lift are very
regular, following an ideal behaviour; instead, the
measured lift shows some small oscillations both during
the transient phase and at full opening. As discussed earlier,
such oscillations of the lift must be attributed to highfrequency fluctuations in the upstream pressure and
cavitation occurrence. These phenomena are not captured
by the AMESim simulations, which therefore predicted a
regular and constant lift at full opening.
The agreement between numerical predictions and
experimental values also suggests that the numerical model
can effectively be used to correctly predict both the lift at
full opening and the rise time

4. CONCLUSION
Accurate measurements of the instantaneous position of the
control piston can be instrumental in predicting the real
behavior of Common Rail injectors, in particular when
multiple injections are employed. In this paper, a cheap and
reliable optical sensor suited to measure control piston
position has been proposed and experimentally tested with
the aim of overcoming the unresolved problems
(interferences and high costs) arising from the employment
of eddy current sensors which deeply affect measurement
accuracy. The proposed optical sensor is mainly composed
of a cheap light emitting diode and a compatible photo
receiver, and it can easily be implemented in a typical
injector assembly by using simple drilling operations that
do not affect the operation of the injector and its
mechanical resistance. The injector system also needs a
few modifications to avoid the presence of fuel in the
measuring zone which could alter the accuracy of the
measurements. These modifications consist of two
additional holes and two elastomer rings, which are
necessary to avoid the presence of fuel in the optical
chamber by generating an air cushion inside the measuring
zone. The small amount of air serving as air cushion is
taken from the turbocharger; there is no reduction in the
flow rate delivered by the compressor, because the air is
“blocked” inside the metering zone. As a result, there is no
compressed air consumption and the operating conditions
of the turbocharger are not changed. These modifications,
which can be easily implemented in the manufacturing
process of a commercial injector, are not as intrusive as
those required by typical measurement systems present in
the literature. For example, the installation of an eddy
current sensor (the most used measurement system in
laboratory tests) is very complex and requires removal of
much material from the control piston, thus increasing the
mechanical weakness of the control piston (subjected to
high compression forces), in addition to being affected by
electromagnetic interference. The optical sensor was
mounted on a commercial injector and several experimental
tests were performed on a test bench to validate the
solution proposed. Different current profiles, typical of
pilot and main injections, were supplied to the coil of the
injector; the time history of the pressure acting on the
needle was recorded and was used to evaluate the validity
of the displacement signal. Similar experimental tests were
carried out with a different control piston to assess the
reliability of the measurements even for different
characteristics of the injector geometry. In all the tests, the
voltage signal from the optical sensor was consistent both
with the expected lift of the needle and with the transients
of the pressure acting on the needle, evidencing the
reliability and accuracy of the solution proposed. The
reliability of the sensor was further demonstrated by
comparing the experimental data with the numerical
predictions provided by the AMESim numerical model of
the injector. The experimental results and numerical
predictions show excellent agreement and also suggest the
possible use of the simulation model as a reliable tool for
predicting the needle lift.
The proposed optical sensor is very innovative because
its characteristics can also make its use profitable for realtime on board controls and because it is cost-effective, in
fact the overall cost of its components amounts to about
twenty Euros, while common eddy current sensors can cost
as much as a few thousand Euros.
ACKNOWLEDGEMENT−The authors thank Michele Mizzi
and Vito Mele for their support in the experimental campaign.
The study described in this article is the subject of a patent
application (Patent filing October 2012, under submission for
PCT extension)ABSTRACT
This paper provides a systematic review of recent publications on simulation and analysis of integrated multi-
vector energy networks (rather than energy hubs) and carries this out through the lens of energy trilemma.
This review is essential for energy research community to move forward in effective manner toward 2050 net
zero carbon targets. This paper presents a holistic view of state-of-the-art of research in this field and identifies
gaps in knowledge which should be addressed by future research urgently. Furthermore, this paper introduces a
taxonomy of energy networks analysis, offering a unified description of findings of relatively large number of
publications devoted to the subject. Moreover, this work analyses and classifies current research trends in the
field of analysis of energy networks integration, and also identifies future trends in this field. This review serves
as a guide to researchers regarding the main findings of energy networks integration evaluated through the lens
of energy trilemma. The reviewed papers have been classified into three groups: (i) Operational analysis; (ii)
Optimal dispatch; and (iii) Optimal planning. The focus of the paper is energy networks, since they play
fundamental role in integrated energy systems and there is a lack of understanding of interactions and in-
terdependencies between multi-vector networks. Also, focus of this paper has been on key findings of published
research rather than details of individual energy models. The paper provides useful insights for energy research
community by presenting several novel ideas for future research and facilitating the path to a decarbonised
economy, due to the fulfilled comprehensive systematic review.
1. Introduction
The international aspiration to reach net zero carbon in energy sys-
tems by 2050 is growing. In the UK, the government has set a target of
‘Net Zero’ Greenhouse Gas (GHG) emissions by 2050 in order to reduce
contribution to global warming [1]. This necessitates performing energy
evaluation through a system-of-systems approach, in order to under-
stand the intrinsic properties of the main layer/sections of the Integrated
Energy Systems (IESs), from natural resources and distribution to the
final energy user as well as the interactions and interdependencies
within each layer/section [2]. These interdependencies and interactions
occur between different energy vectors including natural gas, electricity,
cooling and heating. Energy networks are key components of the whole
energy system that transport the energy extracted from the natural re-
sources to the final energy users. Therefore, there is a growing consensus
that there is a need for a step change in research and development in
multi-energy networks and their interdependencies in order to facilitate
the path to achieving the carbon reduction targets.
Energy networks (including gas, electricity and district heating/
cooling networks) are still predominantly planned and operated sepa-
rately. However, there are several drivers for integrated planning and
operation of these networks including reduction of the use of primary
energy, increasing integration of Renewable Energy Sources (RESs) and
facilitating a low carbon economy [3]. The synergies between energy
networks [4,5] and the interdependencies and interactions between
these networks have been shown to have the potential to bring several
benefits to the integrated planning, optimal dispatch and operation of
energy networks including increasing energy conversion efficiency,
maximised utilisation of primary energy sources, improving the energy
system flexibility, resilience and security and carbon emission reduction
[3,6,7]. Modelling of IES is crucial for understanding the energy net-
works and the potential benefits of their integrated operation. Several
energy system models have been developed, which have been presented
and reviewed in the literature [3,6,8,9]. Moreover, Information and
Communication Technologies (ICTs) have a substantial role in IESs since
they support the coordinated operation of the system by integrating
different parts of the energy system through information sharing
frameworks [7]. Consequently, there are several complexities associated
with planning, management and operation of Integrated Energy Net-
works (IENs), which needs close collaboration of several fields of
expertise including computing science and several engineering disci-
plines to address the challenges of this multi-disciplinary field of
knowledge.
An overview of the benefits of integrated operation of energy net-
works has been carried out in Ref. [3]. However, this does not include a
significant amount of detail and does not relate the findings to the
internationally accepted concept of the energy trilemma, i.e. flexibility,
security and affordability. This greater detail and link to the trilemma is
required in order to identify gaps and directions for an appropriate
future research for meeting carbon reduction targets. To the best of the
knowledge of the authors, no other comprehensive or systematic re-
views were found, that have linked together and organised the findings
of the most important literature on the analysis of energy networks in
relation to the energy trilemma.
Slightly different definitions of the three elements of the energy tri-
lemma exist. Based on the frequency of the definitions used by the au-
thors of the reviewed papers, the following most common definitions of
the energy trilemma have been used in this paper:
(i) Flexibility of operation:
This element of the trilemma denotes how much IENs can respond to
any change in the coupled network(s), including the increase in demand,
without violation of the operational conditions or technical limits.
(ii) Security of supply:
This element of the trilemma indicates the extent that one network
vector can support (in terms of meeting the demand) the operation of
other different but coupled network vectors in the case of a fault
occurrence in part of those coupled network(s), or a shortage of supply
in those coupled network(s).
(iii) Affordability:
This element of the trilemma denotes the amount of costs associated
with the operation, dispatch or planning of the IENs. These costs include
operational and maintenance costs as well as the cost of purchase and
deployment of new assets.
It should be noted that although environmental sustainability (as an
indication of the environmental performance of IENs) has not been
explicitly considered in this definition of the energy trilemma, any
changes to the planning or operation of IENs will have impact on the
environment. In this paper, the environmental impact of any variation in
the above energy trilemma elements are also investigated and evaluated.
It should be noted that the aforementioned definition of the energy
trilemma is in line with the goal number 7 (‘affordable and clean en-
ergy’) of Sustainable Development Goals. This goal aims to ensure access
to affordable, reliable, sustainable and modern energy for all.
Several vectors are incorporated in multi-vector IENs including
natural gas, electricity, heating, cooling and hydrogen. Table 1 shows
the possible coupling components between the vectors that were
frequently investigated in the literature, i.e. gas, electricity and heat. As
can be seen from the table, almost all these vectors, i.e. gas, electricity
and heat, can be converted into another vector and vice versa. This also
shows that these networks can impact the operation of the coupled
network(s). For example, the electricity network can impact the opera-
tion of the gas network: (i) either through electricity to gas conversion
units (Power-to-Gas (P2G), electrolyser and electric driven gas com-
pressors); or (ii) through gas to electricity conversion units (Combined
Heat and Power (CHP) and gas turbine). Therefore, these bring both
benefits and challenges to the operation of the IENs in terms of the el-
ements of the energy trilemma as follows:
(i) Flexibility of operation:
Benefits include, optimal scheduling and planning of the networks for
minimum costs and carbon emissions, and optimised utilisation of the
available assets. Challenges include unexpected potentially negative
impact of networks on one another in case of malfunctioning of the
coupling components and hence losing some energy and increasing the
costs and carbon emissions;
(ii) Security of supply:
Benefits include, availability of any form of energy in any network to
meet the unexpected increased demand in the coupled network(s), and
mutual support between network(s) in case of faults in some part of
them or shortage of supply from upstream or from limited natural re-
sources. Challenges include: (a) the optimal placement of the coupling
components to ensure the security of supply; and (b) networks may not
be able to support the coupled network(s) in case of malfunctioning of
the available coupling components;
(iii) Affordability:
Benefits include, reduction of costs either through capturing and
converting renewable energy via different coupling components into
any of the coupled networks, or through optimised utilisation of avail-
able assets. Challenges include increase in costs due to load shedding or
due to malfunctioning of the coupling components.
There are economic and environmental benefits associated with in-
tegrated operation of the networks through these coupling components.
As an example, it was shown that introduction of P2G in the national gas
and electricity transmission networks of Great Britain had reduced the
costs by £35 M and reduced the carbon emissions by 250 kton during a
month [10]. However, it should be noted that these figures cannot be
generalised since they depend on several factors, including the scale of
the networks, the load profile condition (summer or winter), the ca-
pacity and number of available coupling components and storage
devices.
In order to integrate the networks, through utilising the coupling
components listed in Table 1, the following technical interventions are
required: sizing and operating the coupling components optimally.
Operating the coupling components also requires data acquisition sys-
tems, in which the associated data need to be shared with the control
room(s) of different network vectors. This challenging intervention will
be discussed in Section 4. Also, among the technologies that are pre-
sented in Table 1, the ones that are underrepresented in this paper, can
be considered as potential areas for further work.
In this paper, the most important findings drawn from the literature,
on analysis of energy networks, rather than energy hubs, fall in to the
following three categories:
(i) Operational analysis:
This area denotes assessing the operational conditions of the net-
works and ensuring the networks are meeting the demand without
violating the operational limits, i.e. ensuring the voltages in the elec-
tricity network, the pressures in the gas network and pressures and
temperatures in the District Heating Network (DHN) are all within
acceptable and operable ranges. Once, this assessment has been carried
out, a picture of the operation of the network in terms of the power flows
in the electricity network, gas flows in the gas network, and hot water
flows in the DHN is obtained. Then, the final step is to evaluate the
Techno-Economic-Environmental (TEE) performance of the IEN.
(ii) Optimal dispatch:
Optimal dispatch denotes the optimal operational scheduling of
IENs, which is normally performed for the next few hours. This means in
order to meet the demand with the least operational costs (objective
function), how much (amount and scheduling) energy from different
sources needs to be supplied, or how much the amount and scheduling of
different energy conversion units (coupling components) needs to be.
Optimal dispatch also considers the operability of the IEN. Additionally,
the environmental performance is either considered as an element in the
objective function along with the operational costs, or as a constraint.
Hence, optimal dispatch performs an optimal TEE operational sched-
uling of IENs.
(iii) Optimal planning:
Optimal planning denotes investment planning of all the assets of
IENs, including pipes in the gas and district heating networks, cables and
generation units in the electricity network, and coupling components,
for a horizon of time, e.g. next 20 years. In this respect, all the capital
and operational costs (mapped to a base year) will be minimised in order
for the IEN to meet all the current and future demand subject to being
operable and optimally dispatched at all the time snap shots. Similar to
optimal dispatch, the environmental performance is either considered in
the objective function along with all the costs or as a constraint.
Therefore, optimal planning carries out the optimal TEE operational
planning of the IEN for meeting the current and future demand.
In order to provide guidance for engineers and researchers regarding
the operational analysis, optimal dispatch and optimal planning of IENs,
the selected articles in the related field published predominantly in the
last five years are reviewed. The key findings in these articles are ana-
lysed and categorised covering a range of different aspects, which are
linked to the aforementioned energy trilemma elements, i.e. flexibility
of operation, security of supply, affordability. Additionally, a compre-
hensive review of the problem formulation and solving algorithms for
optimal planning and optimal dispatch of IENs has been carried out.
Consequently, the comprehensive systematic review presented in this
paper provides a holistic picture of the state-of-the-art of research in this
extensive field and identifies useful insights, directions, and guidelines
for valuable future applied research in pursuit of the low carbon
transition.
The papers on energy hubs were not considered since hub level
analysis does not consider the operation of the network to ensure the
network is physically operable and is able to meet the load without
violation of operational constraints (see Refs. [11,12] for review of en-
ergy hubs). Also, the focus of this paper is on findings rather than the
energy models since the models developed for simulation and analysis of
IENs have been reviewed in several other literature [3,6,8,9].
The structure of the paper is as follows: section 2 explains the
methodology adopted and describes the way the publications summar-
ised in this paper were identified. Also, key characteristics of published
research were categorised and key findings are described in this section.
Section 3 describes the most important findings of the papers around the
elements of energy trilemma, in three subsections corresponding to the
three key categories of operational analysis, optimal dispatch and
optimal planning. Section 4 presents several directions for valuable
future applied research in optimal planning, optimal dispatch and
operational analysis of IENs. Finally, section 5 concludes the insights of
the paper.
2. Methodology
This section explains the methodology adopted in this paper to find
the published research on analysis of IENs. Also, key characteristics of
published research were categorised and key findings are described. For
this purpose, databases including IEEEXplore, Sciencedirect and Scopus
were searched with the following key words in their title or keywords:
- A combination of two of: ‘gas’, ‘heat’, ‘power’/‘electric’
- ‘multi-energy’, ‘multi-vector’, ‘integrated energy systems’, or ‘inte-
grated energy networks’
- ‘energy system’ or ‘energy network’
Among the resulting papers, the ones based on a model of energy
networks, rather than based on a model of energy hubs, were considered
since energy hubs are already reviewed in Ref. [11,12]. Then, an
assessment was carried out and hence the papers were filtered further
and the ones: (i) with reasonable citations; (ii) from influential re-
searchers in this field; (iii) published in prestigious journals or presented
in credible conferences; and (iv) that were rigorous, most interesting and
complete, were selected to be reviewed in this paper. Given the fact that
English is a universal language, whilst considering only the papers
written in English, a homogenous cover of the research on analysis of
IENs, that is carried out in the world, is captured in this paper. Also,
several papers had performed analysis of a single vector (i.e. gas, elec-
tricity or heat) as well as the point(s) of connection to the network of
other vector(s) and the impact of those networks without considering
the operation of those coupled network(s). Those papers were excluded
since the scope of this paper is multi-vector energy networks. In this
way, the authors are confident that the most important and influential
papers on analysis of energy networks that have been published in the
well-known databases have been captured and scanned in this review.
Once the papers were filtered according to the aforementioned
eligibility criteria, 186 papers were considered to be reviewed in this
paper. Fig. 1 shows that 115 journal papers and 71 conference papers,
with majority of 59 IEEE conference papers, were investigated. Fig. 2
illustrates the share of different journals on publishing the reviewed
papers on analysis of IENs. These two figures show the majority of the
reviewed papers are published in prestigious journals and conferences.
Fig. 3 shows the number of the papers from different countries according
to the affiliation of the first author of these papers, which indicates the
importance of this topic for different countries around the world. As can
be seen, the majority of these papers have been published by the re-
searchers from China (72), UK (31), United States (17) and Iran (12).
The rest of the countries have less than 10 publications.
As shown above, the geographical coverage of the literature review
was diverse and extensive but not exhaustive. However, it should be
noted that the three categories listed above deal with issues that are
applicable to all the energy networks regardless of their vector or
geographical location and whether they are dealing with legacy net-
works or new build infrastructure. It is possible that some issues specific
to developing countries may not have been identified in this review but
these are likely to be driven by commercial or regulatory aspects which
are not the focus of this paper. Therefore, it is reasonable to claim that
the findings from this review have global relevance and value.
These 186 papers were classified into each of the groups below ac-
cording to their models, research questions and scenarios:
- Papers on operational analysis [13–54].
- Papers on optimal dispatch [10,55–157]
- Papers on optimal planning [158–200].
Fig. 4 shows the number of papers found in the above three groups
published in journals and conferences (denoted by ‘conf.’ in the figure)
by year for the last five years. As can be seen from Fig. 4, the total
number of papers published in these three groups has increased in recent
years especially for optimal dispatch and optimal planning groups. In
these three groups, 82.9%, 82.5%, and 80.9% of the reviewed papers,
respectively, are published in the last five years.
3. Classification of IEN analysis methods and key findings
In this section, the methods for IEN analysis adopted in the reviewed
papers for operational analysis, optimal dispatch and optimal planning
of IENs, the algorithms developed and implemented for formulation and
solution of the problem and the key findings of these publications
around the elements of energy trilemma are presented.
The aspects that were more frequently investigated in the papers are
shown in diagrams in Fig. 5 for operational analysis, in Fig. 6 for optimal
dispatch and in Fig. 7 for optimal planning of IENs. These aspects were
identified based on the keywords of the findings of the papers. All these
aspects have impact on analysis of the performance of IENs in terms of
the elements of energy trilemma, i.e. flexibility, security and afford-
ability. The proximity of any of the aspects to any of the above elements
of energy trilemma in Figs. 5–7 does not mean that aspect has more
impact on that element of energy trilemma. Also, the darker the arrow
connecting the aspect to the whole energy trilemma, the more frequent
the appearance and discussion of the aspect in the reviewed papers, as
shown by the legend in these figures.
Among these aspects, the ones that were discussed more frequently
than the others in the papers are described below Figs. 5–7 in separate
paragraphs. It should be noted that ‘Affordability’, is included in almost
all the paragraphs and hence has not been summarised exclusively in
one separate paragraph similar to the other elements of energy
trilemma.
The references that have validated their developed model by
comparing the results of their model with reference data, measurements
or a commercial tool without any scenarios or key findings are not
explained in the sub-sections of this section, since they have not
demonstrated the evolving scenarios to address the energy trilemma.
Also, the results and findings of the papers that are not cited in the ‘Key
findings’ sections (i.e. sections 3.1.2, 3.2.2 and 3.3.2) were not related to
the subjects discussed more frequently in majority of the papers.
Therefore, although some of the papers are scanned and mentioned in
the ‘Methodology’ section, they are not cited in terms of their results and
findings, in order to maintain the organisation of the paper. This is due
to the fact that the purpose of this review is to summarise the most
important findings of the available publications in relation to the
elements of energy trilemma. Additionally, the findings presented in this
section have been taken from the clear statements in the papers and are
not the personal perception of the authors.
3.1. Operational analysis
3.1.1. Problem formulation and solving
The problem of operational analysis is to calculate the values of state
parameters in an IEN, i.e. voltages in electricity, pressures in gas and
mass flow rates and temperatures in heat/cooling networks. Once the
values of the state parameters are calculated, the values of energy flows
within the IEN are obtained. In this way, a suitable understanding of the
performance of the IEN in terms of energy trilemma and operability of
the IEN without violation of operational constraints is obtained.
In order to perform operational analysis, the set of equations of
balance of power/gas flows at the buses/nodes of the electricity/gas
network are formed. Then, the equations are re-written in terms of the
state parameters in each network.
For the heating or cooling networks, the set of equations of balance
of mass flow rate of hot or cold water at the nodes, the correlation be-
tween mass flow rate, load, and difference of temperature at the loads
and the equations for losses of heat in the pipes are formed. These
equations already contain the state parameters of heating or cooling
network.
Regarding the coupling components, since the value of the energy
flow on one side is known and input to the problem, the value on the
other side can be calculated and considered as the load or generation in
the corresponding network.
In order to solve the set of nonlinear equations most of the papers
have explicitly mentioned they have used Newton-Raphson method.
This is due to the fact that this method finds the problem solution with
local quadratic convergence irrespective of the size of the network
provided all the state variables are initialised with suitable values and
the Jacobian matrix is not singular at the solution point [201,202]. Once
the set of equations is solved, the values of state parameters in all the
networks and consequently the power/gas/hot or cold water flows in all
the branches of the IEN are determined.
In this way, it is possible to examine if the IEN is physically operable.
Also, it is possible to evaluate the performance of the IEN in terms of the
energy trilemma.
A summary of the popular methods used by the research community
for problem formulation and problem solving for operational analysis of
IENs is presented in Table 2. It can be seen that the problem can be
formulated either as: (i) sequential, in which the model of each network
is solved separately and the impact of coupling components is consid-
ered as load or source for the other network; or (ii) integrated, in which
the models of integrated networks with the coupling components are
considered as one model and solved as a whole.
3.1.2. Key findings
3.1.2.1. Flexibility of operation. Flexibility, as one of the elements of
energy trilemma, has been discussed frequently in the papers. There are
several options that provide flexibility to the operation of IENs, as
follows:
(i) Line pack storage, which is the amount of gas stored in a pipeline,
is one of the options that provide flexibility to the gas network as
well as to the other coupled networks. The difference between the
upper and lower line pack limits of the pipes (ΔLP) and the
amplitude of the line pack swing, impact the line pack flexibility.
In a ‘Gas based heating’ scenario studied in Ref. [13] it was found
that high heat demand levels lead to a reduction in ΔLP, and
hence to a reduction in the flexibility of the gas network. In this
scenario, these high heat demands also led to an increase in the
magnitude of the line pack swing. Therefore, the heat demand
levels need to be taken into account in order to evaluate the
flexibility of IEN operation. The method developed in Ref. [13] to
assess the flexibility may be used in scheduling of operation of
transmission network. Hence, it is possible to quantify the flexi-
bility in different regions using weather forecasts of day-ahead.
(ii) P2G facilities, which provide flexibility to the IEN especially at
higher levels of wind generation, reduce the power losses and
fluctuations of gas pressure [14]. These facilities, which are
alternative to gas network reinforcement, effectively benefit the
operation of both electricity and gas networks. This is due to the
fact that they contribute to the reduction of wind curtailment and
to the relief of congestion in the electricity network as well as in
the coupled networks to decrease the insecurity risks [15,16].
(iii) Any form of storage devices offers flexibility to the operation of
IENs. Different types of energy storage systems that are utilised in
gas, electricity and district heating networks are summarised in
Table 3. All these different forms provide support, in terms of
flexibility of operation, security of supply and affordability, to the
network that is utilising the storage device directly, as well as to
all the network(s) coupled to this network through coupling
components. It is shown in Ref. [17] that the heat networks can
be considered as thermal storage devices to some extent and
provide flexibility to other coupled network(s) through the qua-
si-dynamic 2  interactions, which can be used in security
enhancement. It is found that in smart energy systems, proper
control strategies, which have large influences on the
quasi-dynamic interactions, should be developed to use these
interactions for increasing IEN flexibility [17].
(iv) Compressors with dual flow capability can provide flexibility to
the IEN in order to accommodate the changes in supply in any of
the networks [18].
3.1.2.2. Security of supply.Security of supply is another element of
energy trilemma and is investigated frequently in the papers. It was
shown that IENs support one another in the case of occurrence of faults,
however, at a higher cost and emission condition [19]. It was demon-
strated that these faults can propagate from one network to the other
coupled networks and cause risks to the IEN operation [20]. Several
options were proposed in the literature in order to reduce the risk of
occurrence of faults and improve the security of operation of IENs, as
follows:
(i) Electrification: Electrification of heat and especially using hybrid
heating technologies can result in great reductions in GHG
emission of electricity and heat sectors and reduce conventional
generation peaks [21], which in turn contributes to the secure
operation of the IEN. It was suggested that a mixed combination
of technologies at district and local levels needs to be considered
to meet techno-economic and environmental performance targets
as well as to minimise the influence on the constraints of the local
network [22].
(ii) Storage: Adding storage in weak parts of the networks is an
effective way to decrease operational insecurity and strengthen
the system [23].
(iii) Correlations of pipeline parameters: Accurate estimation of dis-
tributions and correlations of pipeline parameters need to be
considered in order to reduce potential risks to operation of IENs
[24].
(iv) Operation mode of coupling components: The operation mode
influences the energy flows and IEN operation [25]. For example,
at the conditions of high heat demand, a CHP unit in the heat load
following mode may generate great amount of electricity and
export it to the grid and hence negatively impact the security of
supply of the IEN [26]. Considering all the above points can avoid
load contingency, which produces frequency deviation and gen-
eration re-dispatch that in turn changes the energy flow profiles
in the IEN that could cause critical operational condition for the
IEN [27].
3.1.2.3. Aspects impacting the energy trilemma
3.1.2.3.1. Interdependencies. Interdependencies and coupling com-
ponents of IEN allow for IEN to have interactions through these com-
ponents [28]. Disturbance from one network transmits to the other
coupled networks through these conversion components [17]. Also, high
loads in one network influence the energy flows of the coupled networks
through these coupling components [26]. Additionally, conversion
technologies at different levels in IEN significantly affect the
multi-energy flows in all the coupled networks, and subsequently the
IEN emissions and operational costs [22]. The influence of in-
terdependencies on the reliability of the IEN has been studied. Imple-
mentation of gas Demand Response (DR) provided additional resilience
for the gas network and the IEN, or gas storage increased the IEN reli-
ability [21]. Increasing the number/capacity of coupling components
will increase the impacts of failures in the gas network on the electricity
network in terms of decrease in the reliability of the electricity network
[29]. Moreover, failures in the electricity network can affect the reli-
ability of gas network when the power supply of facilities in gas network
is satisfied by electricity. Also, decrease in generation capacity in the
electricity network can significantly reduce the reliability of the coupled
gas network [29]. Failure in the electricity network was shown to
eventually have an effect on the gas system. However, due to the slow
dynamics of gas system, the failure in gas system will not be transmitted
to the coupled electricity network rapidly [30]. The heat generation
technology mix affects both gas and electricity networks as well as their
interaction and has impact on the overall IEN operational costs and
carbon footprint [31]. CHP units reduce the impact of voltage drop on
electricity networks [28] and CHP units with lower heat to electricity
ratios have a greater impact on the reduction of IEN losses, due to the
fact that the reduction in electric losses that CHP units provide out-
weighs the increase in gas losses [32].
3.1.2.3.2. Uncertainty. Uncertainty within IENs was another topic
investigated in the papers. There are several sources of uncertainty
within IENs including renewable energy output, the random outage of
generation units, and the random fluctuation of the loads. It was shown
that increase in the uncertainty range of these factors leads to great
fluctuation of the state variables of the nearby buses/nodes [33]. One of
the most important sources of uncertainty is uncertainty of wind, which
was studied in several papers. It was found that uncertainty of wind
leads to significant risks to the operational security of natural gas sys-
tems [34,35] and consequently to the security of real-time operation of
IENs [36].
3.1.2.3.3. Integrated modelling. Investigation of modelling aspects of
IENs in the papers have shown that all the networks need to be simulated
and analysed in an integrated manner. This is due to the fact that a
disturbance in one system can influence the other coupled systems.
Moreover, the disturbance propagates back to the system where it
initially occurred. Thus, a combined analysis of an IES is required, even
if the aim is to investigate the influences on the system itself where the
problem occurred [37]. Therefore, there is a need for integrated IEN
studies and energy system assessment for the evaluation of system
resilience [18] or system flexibility [13]. Additionally, it was shown that
in short term operation, steady-state gas flow models could provide less
accurate results due to the real-time imbalances of gas flow [36].
Therefore, dynamic or quasi-dynamic simulation needs to be imple-
mented especially in order to quantify the process of interactions be-
tween IENs [17].
3.2. Optimal dispatch
3.2.1. Problem formulation and solving
The problem of optimal dispatch in IENs is normally formulated as a
classical optimisation problem with the objective of minimisation of
operational costs or equivalently maximisation of social welfare subject
to operational constraints as shown in Equations.(1–3).
A summary of the popular methods used by the research community
for problem formulation and problem solving for optimal dispatch of
IENs is presented in Table 4.
3.2.2. Key findings
3.2.2.1. Flexibility of operation. Flexibility of operation of IEN was the
most important and frequent aspect investigated in the references. It was
shown that co-optimisation of IENs rather than optimisation of indi-
vidual networks separate from the other network(s) improves the
economy and flexibility of the IEN [101,104]. Also, different types of
energy storage devices in IENs contribute to the flexible operation of the
networks [87]. Inclusion of multi-type storage has also the benefit of
reducing the operational uncertainty and the operational costs of IENs
introduced by network constraints [105]. Additionally, comparison of
impact of P2G and DR in terms of impact on improving the flexibility has
shown that P2G has a greater impact compared to DR in terms of
improving the flexibility of IENs [106]. Furthermore, it was observed
that multi-directional compressors provide additional flexibility to the
gas network and hence reduce total operational costs and gas load
curtailment as a result of the network flexibility they offer [77]. One of
the options for GHG emission reduction that is gaining great attention, is
electrification of heat. However, it was shown that increasing flexibility
of the electricity network is necessary for electrification of heating for
GHG reduction purposes [57]. This flexibility can be provided by inte-
gration of heating system and gas system with the electricity system in
order to accommodate fluctuations in the electricity system [74]. The
dynamic of gas and line pack capabilities in gas networks can provide
flexibility and reliability for electricity networks in IEN short-term
operation [59].
3.2.2.2. Security of supply. Security of supply, and matching the supply
and demand, are important aspects discussed in the papers. Importance
of the integration of systems in terms of operation, planning, security
and reliability was investigated [95]. It was shown that combining gas
and electricity networks is important to evaluate energy conversion
between networks to minimise risk, ensure IEN security, and match
supply with demand whilst minimising cost for transmission [107].
Also, the security constraints of the gas network place limitations on the
IEN operation [108]. Additionally, it was found that while dynamic gas
system control and increased coordination of IEN each provide benefits,
both are essential for security and reduced operational costs in high
stress conditions [91]. Furthermore, it was shown that a natural gas
shortage will limit electricity generation of gas-fired units and increase
the total operational cost of the electricity network significantly [109].
Moreover, dual-fuel electricity generation units will consume an alter-
native and even more expensive fuel such as coal in order not to violate
the operational security constraints [110].
3.2.2.3. Aspects impacting the energy trilemma
3.2.2.3.1. Storage. Storage in IENs was another topic discussed in
several papers. In addition to provision of flexibility to the IEN, as dis-
cussed previously, storage in IEN decreases load curtailments and im-
proves IEN resilience against contingencies [100]. Gas storage in
particular is more effective in managing congested IEN [70] and could
greatly reduce shedding and operational costs [85], enhance system
resilience [71], and support the operation of IEN [85]. Additionally, it
was shown that gas storage in an IEN heavily dependent on gas imports
for electricity generation has the ability to hugely reduce the operational
costs of the coupled electricity network [56]. Furthermore, it was shown
that thermal storage has the benefits of reduction in total operational
cost, improving IEN reliability and capability in accommodating more
renewables [111]. Integrated analysis rather than analysis of individual
system separate from the other(s) is beneficial since it showed gas
storage led to decrease in electricity imports in extreme weather con-
ditions [56]. Also, storage could offer greater and safer use of the
hydrogen in the gas network [10].
3.2.2.3.2. Line pack. Line pack was another topic discussed in the
papers. Apart from being an intrinsic property of gas networks, which
contributes to the flexibility of gas networks and consequently to IEN as
discussed previously, it was shown that line pack also plays an important
role in balancing the production and consumption [90] and contributes
to reduction of costs and more economical dispatch [112]. The higher
the line pack of the system, the lower operational costs to meet the
demand [59]. Additionally, it was demonstrated the lower the line pack,
the more the gas supplied from other sources and storages, and hence
the more IEN operational costs [64]. In order to simulate the line pack
storage and investigate the time evolution of it, dynamic gas flow
models instead of steady state models need to be implemented [113]. In
this way, it is possible to assess the amount of line pack storage in each
single pipe [90]. This also provides the basis for studying security of
supply [113], which can be carried out by varying the gas supply profile
over the day and hence controlling and simulating the line pack and
pressures [114]. Another benefit of application of dynamic gas models is
in studying coordinated scheduling since steady-state gas models ignore
gas inertia and line pack. This may result in suboptimal or less accurate
short-term co-ordinated scheduling of IEN [112].
3.2.2.3.3. P2G. P2G was the most frequent coupling component
that was investigated in the papers. It was found that P2G facilities have
several benefits including mitigating congestion of gas network as well
as the IEN [10,90], avoiding wind curtailment [115], supplying gas
demand and improving wind accommodation [74,90], reduction of
operational costs of gas networks [115,116] and IEN [98,108,117,118],
reduction of IEN emissions [108,115] and enhancing the capability of
the IEN in comprehensive utilisation efficiency of energy [98,108]. The
above benefits allow for additional resilience in ability of the gas
network to provide the minimum required offtake pressure [10]. The
location of these P2G facilities has impact on their role [90]. P2G units
can support more cost-effective unit commitment scheduling by avoid-
ing the curtailment of surplus wind generation and converting it into
natural gas [96] and hence decrease revenue deficit at higher P2G ef-
ficiencies [102]. Furthermore, combined operation of the P2G and gas
turbine units can effectively weaken the under/over voltage issues
[102].
3.2.2.3.4. Renewable  generation. Although  different  types  of
renewable generation, including wind and PV, have been investigated,
wind generation has been more frequently explored in the papers
compared to other types of renewable generation. Wind generation is a
highly accessible option for decarbonising electricity and heat demands.
It was shown that the electricity generation costs and eventually IEN
operational costs reduce as the penetration of wind generation increases
[89,119]. Also, increased levels of uncertainty of wind generation lead
to increased levels of uncertainty of operational costs of IEN [60] and
hence to the higher system generation costs [120]. The disadvantages of
low wind penetration levels include more supply to gas turbine power
plants, more import of expensive electricity, less line pack and flexibility
in the gas network, more compressor power consumption and eventually
higher IEN operational costs especially in high demand conditions [61].
Several solutions were suggested for reduction of curtailment of wind
generation including: (i) P2G units can convert most of the surplus wind
into gas and heat [74]. However, due to high operational cost of P2G, to
ensure the benefit for both the wind farm and the IEN, cost-benefit
trade-off needs to be carried out between wind generation level and
IEN operational economy [103]. (ii) Producing hydrogen from elec-
tricity reduces wind curtailment in a high wind case and decreases
operational costs, GHG emissions and gas flow from gas sources [63]. It
was suggested to produce hydrogen at high wind regions by locating
electrolysers in those regions [63]. (iii) Increasing the heat pump ca-
pacity gradually decreases the rate of wind curtailment [121]. Also,
analysis of impact of correlation coefficients between different wind
farms on the operational costs showed that if the correlation coefficients
between wind farms are larger, the IEN operational costs will be larger
too [87].
3.2.2.3.5. Co-ordinated scheduling. Co-ordinated scheduling of inte-
grated networks was investigated in several papers. Comparison of co-
ordinated scheduling with co-ordination of networks separate from
one another showed that the case of co-ordinated scheduling leads to
lower emission and operational costs [122] and stays within the oper-
ational constraints especially with higher cost savings at higher load
levels [67]. A study with DR found that co-ordinated gas-electricity DR
achieves a better IEN economy than single electricity DR or single nat-
ural gas DR [60]. Additionally, it was shown that co-ordinated sched-
uling, which can better handle the inaccuracy of forecasting [73], could
help the IENs relieve a contingency in one network, which also impacts
operation of the other coupled networks, with lower operational costs
compared to non-co-ordinated scheduling [68]. To ensure all the
aforementioned benefits of co-ordinated scheduling, gas and electricity
Transmission System Operators (TSOs) need to closely collaborate and
co-ordinate [113], which is further discussed in section 4.
3.3. Optimal planning
3.3.1. Problem formulation and solving
The mathematical model for optimal planning of IENs is similar to
traditional planning of gas, electricity or heat networks, which can be
formulated as a typical optimisation problem. However, compared with
the traditional approach, there are more decision variables, more
comprehensive objectives, more complex constraints, and higher
uncertainty levels in planning models of IENs.
A problem of optimal planning is formulated with the target of
minimisation of Net Present Value (NPV) of sum of all the expansion
costs consisting of investment costs (or capital costs, CAPEX) and
operational costs (OPEX), or maximisation of NPV of social welfare of
IEN over the planning horizon subject to operational and investment
constraints, as follows:
Elements of the energy trilemma are considered either in the
objective function or in the constraints to link the models to the analysis
and scenarios.
A summary of the popular methods used by the research community
for problem formulation and problem solving for optimal planning of
IENs is presented in Table 5.
3.3.2. Key findings
3.3.2.1. Flexibility of operation. It was shown that all the energy storage
systems contribute in the long term to avoid energy deficiency and
hence to increase economic savings [177]. Among different storage
types, additional gas storage facilities could enhance gas availability and
line pack during periods of high gas load levels [165]. Also, gas storage
devices are effective in mitigating the impact of uncertainty of the de-
mand on the long-term planning of IENs and in enhancing the security of
energy supply [169]. It was found that in the co-planning, line pack
varies and decreases more in high load conditions [167]. Also, P2G units
can support IENs to benefit from more RESs penetration, delay elec-
tricity transmission lines construction [190], decrease costs and emis-
sions, and increase heat profit of IENs [198]. Additionally, compressors
can postpone building gas pipelines and hence decrease IEN costs [190].
3.3.2.2. Aspects impacting the energy trilemma. It was found that heat
demand needs to be considered in operational planning of the flexibility
of IENs [13]. Also, in order to consider the uncertain future demand and
generation, stochastic solutions need to be investigated, which present
the lowest expected cost for this purpose [176], and hence does not
result in any load curtailment [164]. It was also found that demand
uncertainty impacts the operational cost and not the investment cost
[160]. Moreover, the more the reliability of satisfying the stochastic
demand, the more the investment cost. Especially in IENs where the
electricity network is highly dependent on the gas system, increasing the
reliability of the gas system significantly increases the reliability of the
whole IEN [169]. Also, increase in the level of stress within the IEN
increases both the cost of operation and investment [184]. It was found
that N-1 contingency of natural gas transmission network has large
impact on planning of IENs, especially in the regions with limited nat-
ural gas resources [170]. Also, it was shown that installation of new
pipelines or incorporation of fuel switching capabilities are effective
solutions to relieve fuel supply constraints and reduce the total opera-
tional costs [191].
3.3.2.2.1. Co-planning. Co-planning of IENs has been compared
against sequential or separate planning in several papers and several
benefits in terms of above aspects around the energy trilemma have been
investigated. It was found that co-planning of IENs finds a solution with
the following characteristics compared to sequential or separate plan-
ning, due to the fact that it considers strategically all the networks and
their interactions:
lower total costs (investment and operational) [158,167,168,170,
171,174,179–183,186,188,190,193,197], or equivalently higher
social welfare [194–196]. Especially the higher load growth rates,
the higher benefit to cost ratios [167]. This is due to the fact that IEN
has an infrastructure capable of levelling the loads in different vec-
tors and hence it can relieve transmission stress and postpone con-
struction investment and operational costs [172,174].
lower carbon emissions [167,189].
less load curtailment [171,181].
higher robustness and reliability levels while accommodating
different uncertainties in the IEN [167,172,179,185,189,195,196].
Also, if operational constraints are neglected, e.g. power imbalance is
allowed, then the total cost decreases since certain investments can
be avoided [190].
more flexible [196] and enhanced resilience levels [186].
less transmission and energy losses [181,196] and hence higher ef-
ficiency of the energy grids and better infrastructure utilisation ef-
ficiency [162,181,189,195,196] due  to maximised transfer
capabilities within all the vectors [196].
3.3.2.2.2. Networks interactions. Natural Gas-fired Power Plants
(NGPPs) have been investigated more compared to other possible
coupling components for the operational planning of IENs in the liter-
ature. It was found that installation of NGPPs reduces investment [158]
as well as the total operational costs, since this guarantees security of the
electricity network [170] and NGPPs and system transmission capacity
are efficiently utilised [167] (despite the growth in operational costs of
the gas network due to building more infrastructure [192]). Also, it is
necessary to integrate more NGPPs to increase the reliability over the
planning horizon [159,179]. Additionally, the sensitivity of the inte-
grated networks design to the confidence level is dependent on the
quantised size of the candidate NGPPs and the level of the electricity
demand [179]. It was found that the constraints of transport of natural
gas has influence on the planning of the electricity system, especially in
regions where the candidate NGPPs depend massively on a constant
supply of natural gas [191].
4. Research needs, gaps and future research direction
recommendation
The systematic review has identified the following gaps for future
research in operation, dispatch and planning of multi-vector energy
networks.
4.1. Smart energy management framework
There is a need for development of advanced control and manage-
ment frameworks in order to perform online optimal dispatch of IENs
based on historical and online data that are further processed by Arti-
ficial Intelligence (AI) and Machine Learning (ML) techniques. This
framework needs to be deployed through Internet of Things (IoT)
framework since there is a growing attraction in online acquisition and
storage of data in IoT frameworks. Additionally, energy networks are
becoming more integrated through several coupling components. As a
result of vector integration using ICT systems and data collection
through IoT frameworks, cyber attacks are considered as a potential
threat to the operation of multi-vector energy networks. Therefore, IoT-
driven control and management frameworks for IENs considering AI, ML
and cyber security assessment needs to be investigated and deployed on
multi-vector energy networks with a single owner for test and validation
purposes (Fig. 8). In this way, the studies that have been carried out on
security and resilience of future electricity grids [204] can be further
extended to future IENs.
4.2. Organisational and regulatory barriers for the future integrated
control rooms
The benefits of integrated optimal planning and dispatch of energy
networks have been investigated in several publications and have been
summarised in this paper. However, in reality these networks, which are
coupled at several locations, are planned and operated separate from
each other in transmission level all the way to medium level distribution
level since they are owned, operated and governed by different com-
panies/governments/regulators. As a consequence, if a fault/contin-
gency occurs in one of the networks, the operator of that network
attempts to find the solution within that network, whereas, the other
coupled network(s) might be able to provide some support/service to
relieve this affected network (Fig. 9). However, integrated control and
management of coupled networks in one central location has not yet
been investigated, due to the following facts: (i) each energy network
has its own control room separate from the control room of the other
coupled energy network(s); (ii) organisational policies, procedures and
regulation of each network operator is different from the other one. In
order to benefit from the advantages of integrated planning, dispatch
and control of coupled energy networks, the operators of the networks
need to be in one central location and communicate with each other in
order to find a practical solution for planning of the networks or for
fault/contingency relief in case of occurrence of any fault/contingency
in either network.
Hence, some relevant questions need to be considered in the phase of
designing the control rooms of future such as:
• How well do the tasks and roles in different control rooms
correspond?
• Are there different levels of urgency or extent of active intervention
needed?
• At what scale of production does gas-for-storage become significant
on either side?
• What regulatory, market or institutional factors might affect co-
operation?
• How could control room practices be adapted to enable new possi-
bilities for energy storage?
4.3. Impact of uncertainties on the trilemma evaluation of performance of
IENs
Impact of uncertainty of renewable generation and loads on the
energy trilemma have been investigated in several publications. How-
ever, uncertainties of the following have not yet been evaluated:
(i) The physical characteristics of the energy networks and coupling
components normally deteriorate over time. No models have yet
been developed to correlate the impact of aging of the physical
assets on deterioration of physical characteristics of the networks
and hence on the operation and energy trilemma;
(ii) Dynamic behaviour of the networks have been investigated in
several papers. However, ideal networks with perfect assump-
tions have always been considered for this purpose. These model
simplifications and perfect assumptions help a model to run and
produce results faster. However, this happens at the expense of
some uncertainty in the results based on which the operator of the
network wants to make the right decision at the right time.
To conclude, there is a need to evaluate the extent of impact of these
modelling uncertainties on the energy trilemma performance evaluation
of IENs to obtain a tolerance range when reporting the performance of
IENs. This especially applies to high level integrated networks such as
transmission or high level distribution networks, where great amounts
of energy carriers are transmitted over long distances and even minor
miscalculations can have great consequences on the energy trilemma
performance evaluation of IENs (Fig. 10).
4.4. Economics of energy system integration
Several technical benefits around the elements of energy trilemma
have been investigated for integrated operational analysis of coupled
networks by different researchers, which have been summarised in this
paper. However, networks operate under the constraints of market,
regulatory, and policy frameworks through price signals, taxes, and
other incentives [205]. The more integration of the networks, the more
complexity is introduced in the linkages between the operators of the
coupled networks. There is, therefore, a need to investigate which level
(i.e., transmission or distribution) and to what extent is best to have
more integration in the networks in order to better resolve the energy
trilemma challenges and to explore whether the economic network
integration benefits can be improved through amendments to market,
regulatory and policy constraints. If one of the levels was found as the
best level to have more integration, then what would be the added value
of integration of operation in the other level in terms of the marginal
benefits for the whole energy system.
4.5. Deployment of smart multi-energy regions
With increasing linkages between gas, electricity and district heating
networks, deployment of smart multi-energy in regions rather than
districts is gaining attention in the energy research community [10,
105]. The very first challenging step to deploy these regions is devel-
opment of models for optimal planning , dispatch and operational
analysis of these regions. However, there are several challenges associ-
ated with this step, which need further investigation and are summar-
ised as follows:
(i) Developing dynamics of the gas and district heating networks [4];
(ii) Convergence and optimality of the solutions of the models
without compensation for accuracy [4];
(iii) TEE impact of renewable energy generation at the building level,
e.g. PV and solar thermal, on the IENs [28];
(iv) Development of control strategies that can be used for close-to
real-time simulation [28];
(v) TEE impact analysis of the flexibility offered by P2G on operation
of the IENs [22,57];
(vi) Forecasting the demand (heat and electricity) and optimum uti-
lisation of the flexibility of the multi-energy system [13];
(vii) Quantification and evaluation of the TEE impact of natural di-
sasters on the resilience of the smart multi-energy IENs [18]; and
(viii) TEE evaluation of benefits of seasonal storage and the impact on
the IENs while transporting the gas produced by P2G to the
seasonal storage facilities [10].
5. Conclusions
This paper summarises the most important findings of recent litera-
ture on analysis of multi-vector energy networks, rather than energy
hubs, in relation to the elements of energy trilemma: flexibility of oper-
ation, security of supply and affordability. The reviewed papers have been
classified into three groups: (i) Operational analysis; (ii) Optimal
dispatch; and (iii) Optimal planning. The focus of this paper has been on
energy networks since the main parts of the whole energy system that
connect the natural resources and import to the final energy user are
energy networks including gas, electricity and district heating/cooling
networks along with the couplings and interactions that occur between
them both at transmission and distribution levels. Also, the focus of this
paper has been on key findings rather than the energy models since the
models developed for simulation and analysis of Integrated Energy
Networks (IENs) have been reviewed in several other literature. The key
findings of the most recent papers have been explained in relation to the
elements of energy trilemma. The main driver for this paper was to
identify the gaps in the state of the art in the area of multi-vector energy
networks research and to articulate this through the lens of the energy
trilemma.
The paper makes recommendations for future research priorities
including: development and demonstration of cyber resilient smart en-
ergy management frameworks, ways to overcome organisational and
regulatory barriers for future increased energy network integration,
uncertainty analysis of the future performance of IENs, potential eco-
nomic value of energy systems integration and deployment of smart
multi-energy regions.
The paper provides useful insights for the energy research commu-
nity by presenting several novel ideas for future research and facilitating
the path to a decarbonised economy, due to the comprehensive sys-
tematic review that has been carried out.

ABSTRACT
Within the context of the Smart City, the need for intelligent approaches to manage and coordinate the diverse
range of supply and conversion technologies and demand applications has been well established. The wide-scale
proliferation of sensors coupled with the implementation of embedded computational intelligence algorithms
can help to tackle many of the technical challenges associated with this energy systems integration problem.
Nonetheless, barriers still exist, as suitable methods are needed to handle complex networks of actors, often with
competing objectives, while determining design and operational decisions for systems across a wide spectrum of
features and time-scales. This review looks at the current developments in the smart energy sector, focussing on
techniques in the main application areas along with relevant implemented examples, while highlighting some of
the key challenges currently faced and outlining future pathways for the sector. A detailed overview of a fra-
mework developed for the EU H2020 funded Sharing Cities project is also provided to illustrate the nature of the
design stages encountered and control hierarchies required. The study aims to summarise the current state of
computational intelligence in the field of smart energy management, providing insight into the ways in which
current barriers can be overcome.
1. Introduction
Although cities occupy only 3% of the earth’s land area, they con-
sume 75% of natural resources and produce 60–80% of global green-
house gas emissions [1]. Their impact on the environment will grow as
urbanization increases from 54% of the world population today to 66%
by 2050 [2]. Increasing resource efficiency at city scale would enable a
more environmentally sustainable solution and help to transition to a
low carbon economy while meeting the challenges of increasing po-
pulation in such areas. Multiple UN, EU and US projects have been
developed in the recent past which are targeted towards solutions for
such sustainable cities. These include the GI-REC (Global Initiative for
Resource Efficient Cities) [1] by the United Nations, European In-
novation Partnership on Smart Cities and Communities (EIP-SCC) [3]
by the European Union and the Smart Cities Initiative [4] by the United
States among others.
Reliable, efficient and low carbon energy supply is one of the key
requirements for next generation smart cities [5]. The close proximity
of multiple energy vectors like electric power, heat and gas, introduces
opportunities for energy systems integration and real time management
of multiple energy vectors [6]. The vision for the future smart energy
system is to have automated algorithms which are able to learn user
demand and usage profiles from historical data [7], leverage on that
information to optimise, coordinate and control the interconversion
and distribution of different energy vectors, while simultaneously
taking into account the engineering constraints of the system and the
business and policy objectives. As embedded algorithms become more
pervasive at different levels of the energy network (from smart meters
[8] at the usage end of customers to real-time phasor measurement
units along the transmission lines), Computational Intelligence (CI) and
Machine Learning (ML) would become increasingly important to take
automated decisions and react to changes in system dynamics [9].
This review seeks first to provide an overview of many of the main
areas in which CI and ML strategies are currently implemented in smart
energy systems. By the nature of the topic, application areas are di-
verse, however, the push towards system integration necessitates a
holistic perspective. Firstly, data management challenges are in-
troduced, followed by applications specific to the built environment
and transport sectors. The integration of different supplies and demands
is then covered, with an additional section devoted to the user’s role in
this technological landscape. After this review of current application
areas, the focus shifts in Section 3 towards the future potential of CI and
ML in the smart energy domain, targeting key challenges that can be
overcome through the further implementation of current techniques. It
also highlights the challenges and short-comings of the present CI/ML
tools and presents potential future developments in these fields that
would be beneficial in the context of smart energy systems. In Section 4,
implementation considerations are addressed, leading to a specific case
study in Section 5 in which many of the key smart energy aspects are
present. The project, an EU Horizon 2020 Smart City project (called
Sharing Cities) is one the authors are working, which incorporates
multi-vector energy systems and integration of different energy assets,
encountering the associated challenges in real time demonstration of
the same.
2. Review of existing concepts and applications in smart energy
management systems
Intelligent solutions for control and operation of the various in-
dividual components that comprise an urban energy system have be-
come increasingly prevalent [10]. Often driven by the goals of reducing
energy consumption, emissions or cost [11], while maintaining ro-
bustness to uncertainty [12], such approaches seek to leverage the key
enablers of increased computational power and data ubiquity, as well as
artificial intelligence (AI), machine learning and advanced control
techniques [12]. These approaches need not be solely characterised as
‘Smart City’ related interventions and are often considered in an iso-
lated manner. Nonetheless, the overlapping objectives, challenges and
enablers mean that the application of computational intelligence to the
challenge of Smart Cities must consider the amalgamation, adaptation
and integration of these various solutions.
In this section, some such approaches are summarised, particularly
focussing on the building and transport sectors as energy demands,
followed by methods for integrating the disparate sources and sinks.
2.1. Urban data handling
The definitions of Smart Cities are varied, with examples to be found
in [13]. Though a large number of themes and concepts arise under the
Smart City umbrella, a central and common aspect across almost all
solutions and domains is the incorporation of Information and Com-
munications Technology (ICT) [14] and the Internet of Things (IoT)
[15].
2.1.1. Big data & the internet of things
Across all domains, Cisco have estimated that 50 billion devices will
be connected to the IoT by 2020 [16]. As more objects are connected to
the IoT, the ability to exploit an ever-expanding volume of data, now
given the moniker Big Data, has been a key driver behind many new
approaches [17]. Developments in the management, utilisation and
analysis of such large data-sets have emerged to facilitate this, with a
shift away from relational database technologies to distributed Not
Only SQL (NoSQL) database technologies such as Cassandra and Mon-
goDB due to increased flexibility and scalability [18]. Many works
evaluate the characteristics of big data management in terms of the Vs
of data management, which can include [19] Volume, Variety, Velocity,
Variability and Value. Others have been referred to, including Veracity,
Volatility and Validity [20]. A particular challenge in this regard is in
the use of soft-sensing approaches, whereby data from sensors not as-
sociated with specific pre-defined tasks (e.g. data from mobile devices)
is harnessed. In [14], these challenges are explored, with particular
attention paid to the veracity, volume and velocity of data. ML and
probability based approaches for data fusion [21] can improve things,
exploiting the commonalities across multiple data sources.
Without appropriate strategies, wastefulness is inevitable. With this
in mind, inefficiencies resulting from improperly managed data is
covered in [11]. Capturing the data while avoiding inefficiencies re-
quires appropriate platforms to be developed with the power to process
and integrate a large number of data streams from different sources,
potentially with varying time-scales and data formats, while also pro-
viding the necessary APIs and storage capability. A generic architecture
incorporating layers for sensing, transmission, data management and
specific applications is proposed in [22], while, more generally, the
protocols and architectures suitable for such an application of IoT in a
Smart City context are surveyed in [23].
Targeting applications for this data processing capability, a detailed
overview of the energy saving opportunities of using ICT as an enabling
technology is provided in [24] in which a framework is developed to
identify opportunity hotspots for applying ICT to reduce energy use. The
authors conclude, in agreement with a study in [25], that the installa-
tion of intelligent heating systems and the use of ICT for production
process control and supply chain management are areas of particular
potential. The availability of data coupled with CI and ambient in-
telligence techniques can allow for more accurate forecasts of future
loads [26] which is especially useful in the domain of energy systems.
These can be used to aid in the selection of suitable operational ap-
proaches and control decisions, automation and visualisation. A review
of the main CI approaches for this purpose is provided in [27]. The
accuracy of hybrid approaches (whereby multiple techniques are
combined) are shown in particular to outperform standalone ap-
proaches in terms of load forecast accuracy.
The suitability of any specific method, however, depends on the
target data-set. Inevitably, reliance on data in complex networks of
systems can introduce large uncertainties - the importance of which
should not be underestimated. Effective methods for characterising this
uncertainty are needed to handle this issue, with a review of possible
approaches suitable to the integrated energy domain presented in [28].
The authors allude to the common over-reliance on non-empirical as-
sumptions in many approaches as well as the often underestimated
need to consider the interrelation of uncertainties across multiple
parameters. Furthermore, apart from parametric uncertainty, effective
exploitation of data in an energy management scheme requires the data
quality to be suitable for the task-at-hand, with adequate excitation,
granularity and trustworthiness.
2.1.2. Communication networks
The vast increase in sensors and the need for interconnectivity be-
tween networks of sensors and actuators has lead to an evolution in the
communication infrastructure used for the implementation of smart
grids. The traditional state of the power grid is summarised in [29] as a
one-way pipeline with clear demarcations between generation, trans-
mission and transformation, where system capacities are often sized for
worst case scenarios, due to an inability to incorporate DSM. The smart
grid alternative however, as discussed in [30], can instead be viewed as
hierarchy of three newtworks: the Home Area Network (HAN), the
Neighbourhood Area Network (NAN) and the Wide Area Network
(WAN). The HAN layer typically operates wirelessly over small areas,
useful for sensors and smart meters in buildings, where low date rate
technologies (following Zigbee or Z-wave standards for example) are
well suited due to security, low cost and low power requirement. For
NAN, spanning groups of buildings or districts in an urban environ-
ment, Wi-Fi and cellular technologies can be employed. Ethernet net-
works can be well suited to the WAN layer due to the higher data rate
requirements. Suitable protocols and standards for implementation of
these networks are crucial however, with different companies and
countries approaching the problem separately. Common standards from
organisations such as the Institute for Electrical and Electronics En-
gineers (IEEE) (e.g. IEEE P2030) have been developed to counteract
this.
2.1.3. Cyber-security
The reliance of the smart city on communication networks and
connectivity inevitably leads to significant cyber-security concerns with
adversarial threats and an increased risk of system misuse (externally or
internally) impacting a large number of people. The increased digital
surface in a smart city, as referred to in [31], results in more opportu-
nities for security breaches. The interrelations between different net-
works can also introduce the possibility of cascade failures and fault
propagation. Furthermore, in the energy domain, traditional systems
are often incorporated within the wider smart city environment, but
may provide inadequate security protocols. Security issues in Super-
visory Control and Data Acquisition (SCADA) systems commonly used
to manage energy systems are detailed in [32] for example. A com-
prehensive summary of the different threats faced across different smart
city sectors is provided in [33], including malicious threats and in-
secure protocols in smart-grids, home automation systems and cloud
storage. The authors also focus on the forensic data requirements to
effectively investigate incidents protect against future incidents.
The volume of data and the vast number of connected devices can
make effective mitigation of these issues a serious challenge. In [34] for
example, the difficulty of cryptographic key management for smart
meters is highlighted in a scenario in which keys are updated periodi-
cally - an unprecedented need for support staff and time results. To
understand the threats faced and assess vulnerability, threat modelling
approaches are developed in [35]. This modelling and assessment is
generalised further in [36] where the concept of the Safe City is dis-
cussed, whereby, the effectiveness of safety and healthcare processes
are improved. In [37], security solutions are discussed for smart grids,
once again highlighting the need for adaptive key management solu-
tions in smart meters such as the privacy preserving aggregation
scheme with adaptive key management (PARK) proposed in [38].
2.2. The built environment
The prominence of buildings as significant energy consumers in the
global energy landscape is well understood, with 40% of global energy
consumption typically assigned to the general building sector [39].
Despite including complex heating, cooling and electrical demands,
traditional rule-based approaches for managing relevant components
and determining operational decisions dominate, leading to un-
necessary inefficiencies [40]. In anticipation of the future dec-
arbonisation of the electricity grid, a desire to move towards electricity-
based heat sources (primarily through heat pumps) has emerged in
recent times [41], furthering the case for integrated energy approaches
[42]. Additionally, more efficient data handling through suitable
Building Information Modelling (BIM) methods introduces opportunity
for change [43], with the possibility for improved monitoring and di-
agnostic techniques, such as the Artificial Neural Network (ANN) al-
gorithms developed in [44]. Through the use of intelligent modelling
and control techniques, enabled by the increased access to measured
data from the buildings and external environment, significant reduc-
tions in energy consumption have been reported in [45], with potential
savings of approximately 30% achieved through automation. Some of
the more common approaches proposed for achieving these savings are
presented here, with comprehensive reviews available in [46,45].
2.2.1. Demand-side management in smart cities
The term Demand Side Management (DSM) can be used to represent
a wide range of interventions in the built environment, from permanent
actions (e.g. retrofit) to short-timescale actions like Demand Response
(DR) [47]. Smart city concepts and intelligent control can have a par-
ticularly significant impact on the shorter-timescale events, with real-
time data providing the basis for automated decisions. Key to the
suitability of the built environment for this is the flexible nature of the
demand. The slow thermal dynamics and imprecise nature of the
thermal objectives in buildings (occupants typically require ambient
temperatures to be within a band, rather than matching a specific single
set-point) introduces the opportunity for wider system optimisation.
Without diminishing comfort in the building, loads can be temporally
shifted [48,49], allowing (with appropriate control) for the satisfaction
of supply constraints and an improvement in the performance of the
overall energy system. A multi-objective optimisation approach to do
just this is introduced in [50] for example.
Furthermore, with an increased push for the electrification of heat
[51] and district heating networks [52], coupled with the relative
cheapness of thermal storage [53], the link between building thermal
loads and the wider energy system becomes stronger. As such, the
ability to coordinate and aggregate demand-side thermal loads in an
urban environment underpins much of the potential benefits of energy
system integration [46] (discussed further in Section 2.4).
2.2.2. Advanced control strategies
For thermal management, advanced control approaches such as
Model Predictive Control (MPC) have been regularly reported in lit-
erature to provide improved heating and cooling system control over
more traditional strategies [45]. In such approaches, the operation of
the system (typically Heating, Ventilation and Air Conditioning systems
(HVAC)) is optimised over a receding horizon using predictive dynamic
models, often to minimise energy consumption subject to comfort
constraints. Many variations have been proposed, such as a prioritised-
objective approach in [54] to avoid the need for excessive tuning
parameter selection and the stochastic approach of [55], formulated to
better incorporate uncertainty.
Aside from MPC, other non-traditional approaches can be ad-
vantageous. Fuzzy-logic controllers (FLCs) are proposed in [56,57] to
achieve an improved performance with less of a design burden when
compared to MPC. To incorporate user-behaviour in the automated
decision process, a multi-agent based approach is proposed in [58] to
handle the control of the different regions of a multi-zone building, with
a further incorporation of Particle Swarm Optimisation (PSO) in [59]
and reinforcement learning in [60]. A more detailed review of such
approaches can be found in [61]. While performance benefits of more
advanced approaches are clear, the initial cost and expertise require-
ment is as yet prohibitive [62]. Within a wider smart city context
however, in which data platforms and sensor networks become more
commonplace, such approaches should become more attractive.
On the electrical side, load shifting through appropriate control of
heat-pumps and thermal storage is discussed in [49]. Such load shifting
approaches can reduce required installed capacity as well as reducing
stress on the wider system though monetising, incentivising and fairly
distributing the financial gains from this wider system operational
benefit is an open challenge. In [63], the impact of such DSM strategies
on comfort levels is assessed, particularly noting the importance of the
building fabric properties on the load-shifting potential - a better in-
sulated building has a greater potential to store energy in the building
fabric. Looking at the wider system behaviour, DR strategies can be
implemented for active and reactive power balance [64], frequency
regulation [65] and power factor correction [66]. Appropriate me-
chanisms for achieving these aims have been proposed [67], with
success reliant on appropriate pricing models which must achieve
system flexibility with appropriate compensation for the providers of
this flexibility, without compromising clarity and transparency. In [68],
many of the current pricing scheme options are summarised.
2.2.3. Modelling the thermal behaviour of buildings
Supporting many of these proposed control strategies is the ability
to utilise data to develop more accurate models of the thermal and
electrical behaviour of the buildings [69]. An ANN-based approach
proposed in [70] is shown to be effective for forecasting electricity
consumption, while in [71], a Deep Neural Network (DNN) approach is
similarly applied. Black-box system identification approaches (such as
the Auto-Regressive Moving Average model with Exogenous input
(ARMAX)-type approach) are covered in [72,73], whereby standard
linear relationships between the heat inputs and resulting temperatures
are assumed. The resulting model formulations are suitable for ad-
vanced control, while accuracy over the relevant time horizons can be
achieved, but only with sufficiently excited training data [74]. To
capture the behaviour and interactions of users with the energy systems
and thus better predict energy demands, agent-based approaches can be
implemented, such as in [75]. Time-series approaches for forecasting
building energy consumption are reviewed in [76], while electrical load
forecasting techniques reviewed in [77].
Appropriate methodological choices are case-specific. Black-box
approaches can reduce the need for deep knowledge of the underlying
system to be incorporated a priori, however, many design choices (such
as system order and structure) are still required. While any of the above
approaches can be integrated into a smart energy management strategy,
success is very much predicated on sufficient data quality. This requires
a good degree of excitation and a low-level of unmeasured disturbance -
as highlighted in [40]. Hybrid approaches that can blend expert
knowledge with empirical data effectively are an attractive prospect.
Gray-box [78] and white-box approaches, in which an underlying
physics-based model structure is assumed, can increase the likelihood
of sensible outcomes, though the incorporation of this information can
become intractable as system scale and complexity increases.
Data availability and quality as well as system complexity are key
considerations in the selection of a suitable approach. In cases where
very little data is available (even coarse consumption data, though
measured, may not be accessible due to privacy concerns), it can still be
possible to characterise the energy performance of a building. This may
be insufficient for advanced control, however it can still be useful to
inform decisions surrounding future interventions and to understand if
energy consumption is following expected patterns. A benchmarking
approach for information poor buildings is provided in [79] to identify
deviations from monthly energy bill data with some additional HVAC
measurements. In scenarios of little historical data it can also be pos-
sible to exploit the fact that vast quantities of data may be available for
similar buildings which may display similar dynamics. A transfer
learning method for energy prediction is proposed in [80] for example,
using time-series multi-feature regression with an allowance for sea-
sonal and trend adjustments. In [81], a Deep Belief Network (DBN) (for
feature extraction) is incorporated into a reinforcement learning-based
approach to enable knowledge transfer to buildings without historical
data. Apart from lacking in historical data, there may also be gaps and
missing values potentially affecting predictive performance. This is
acknowledged in [82] in which the authors build an interpolation
scheme into a wider approach for building energy prediction using deep
Recurrent Neural Networks (RNN). This is a less researched area and
more focus needs to be given to developing modelling methodologies
addressing these issues.
2.2.4. Fault detection and diagnosis
As data-driven techniques can be used for prediction of system be-
haviour, it follows that similar techniques can be employed to detect
when the energy systems (e.g. HVAC) behave in a manner deviating
from expected operational bounds in times of component fault or
failure. Many techniques for Fault Detection and Diagnosis (FDD) have
been studied, with approaches classified in [83] as process-history
based (models inferred from measured data), quantitative model based
(detailed physical models), and qualitative model-based (expert-de-
signed rules). When detailed models are not available, grey-box or
black-bock approaches can be attractive. A wide variety of statistical
and ML techniques have been explored for such data-driven strategies
with popular approaches including Principal Component Analysis
(PCA) [84] to detect anomalous divergence between interrelated vari-
ables, ANNs and Support Vector Machines (SVMs) [85,86] to classify
faults based trained on historical data and fuzzy logic [87] to more
easily incorporate human knowledge in the classification procedure. A
detailed summary of the main approaches can be found in [88].
Though new opportunities have emerged with more pervasive
sensing, historical data tends to be lacking in data describing critical
events or attacks [89]. If a wide range of failure modes are possible,
corresponding training data may be difficult or impossible to obtain in
sufficient quantity. To allow for this, suitable probabilistic approaches
must be considered. A probabilistic neural network method was shown
in [90] to out-perform more standard back-propagated neural networks
for fault detection in refrigeration systems for example. Parameter-in-
variant approaches have been developed for classification in which
constant false positive rates are obtained regardless of parameter values
[89]. These approaches have been taken in networked systems [91] and
for HVAC systems in the building energy domain [92]. Hybrid ap-
proaches which can combine the benefits of physical knowledge, expert
knowledge and historical data can be harnessed to better tackle the
complexity present in a multi-tiered set of urban energy networks [88].
The ability of Bayesian approaches to incorporate subjective informa-
tion enable them to fit easily in a hybrid framework. The benefits of
hybrid approaches have been long understood (a grey-box approach
that incorporates physical knowledge with Bayesian techniques is
proposed for example in [93]), however such approaches come with a
significant design burden and off-the-shelf solutions may not match the
specific need.
2.2.5. Zero-energy and nearly-zero-energy buildings
As reported in [94], Zero-Energy-Buildings (ZEBs) and Near-Zero-
Energy-Buildings (NZEBs) are expected to play a significant role in
achieving the goals of the European “Smart Cities & Communities In-
itiative” [95]. Conceptually, ZEBs should meet all their energy re-
quirements from nonpolluting renewable sources, however, specific
definitions vary in strictness [96], typically depending on the allowance
of off-site renewable generation. The slightly less stringent concept of
NZEBs has become more common, though inconsistencies in definition
are present across different countries [97]. Nonetheless, the concept fits
in with other smart energy approaches, as intelligent coordination is
required to maximise the utilisation of any renewable energy sources
through management of the supply and demand. Any successful NZEB
will tend to require a mix of energy generation and storage technologies
which must be optimally managed. From an operational perspective,
advanced control approaches and optimised scheduling play a sig-
nificant role [98]. In [99] for example, a model is developed to handle
the stochastic nature of renewable generation while minimising costs
through optimised scheduling based on Genetic Algorithms (GAs),
teaching learning-based optimisation (TLBO) and enhanced differential
evolution (EDE). In [100] game theoretic approaches and adaptive
fuzzy control strategies are used to manage the demand and supply
respectively.
From a design perspective, CI and ML approaches can also be le-
veraged to approach zero energy. In [101], the need for suitable
optimisation approaches for evaluating different design options is un-
derlined (including geometry, fabric construction, ventilation strategies
and shading, as well as energy system design). Evolutionary algorithms
are put forward by the authors as an attractive option. Technology
selection is also crucial. In [102] for example, Monte Carlo simulations
are used to evaluate hybrid energy system solutions.
2.3. Transportation energy and mobility
The potential impact of CI in the transport sector has been well
reported, often from an infrastructural perspective, with solutions di-
rected towards challenges such as congestion management [103] and
space utilisation (e.g. parking) [10]. The energy perspective must also
be considered.
2.3.1. Electrification of the sector
As with the heating sector, the anticipated de-carbonisation of the
power grid coupled with technological advances in the electric-vehicle
(EV) sector [104] has led to EVs becoming a lower carbon option for
personal transport than traditional fossil fuel-based approaches [105].
Furthermore, e-bike and e-scooter rental schemes may become more
common in the coming years [106]. This is all likely to lead to increased
stress on the electricity grid at times of peak load [107], necessitating
the introduction of more intelligent energy management strategies. In
[108], the potential to use different CI-based algorithms (including PSO
and Genetic Algorithms) to achieve this is explored. As with any opti-
mised approach, suitable definition of the objective or fitness function
is crucial to achieve a behaviour that suits all involved actors. Apart
from stressing the grid, on a positive note, EVs can allow for increased
grid flexibility, particularly if Vehicle-to-grid (V2G) power flow is
permitted. Estimation of the flexibility introduced through EVs using
agent-based modelling techniques is discussed in [109]. Once again, the
need to incentivise and appropriately monetise V2G type power flow,
considering the longterm impact on the EV batteries (in [110] it is re-
ported that in an extreme case of full daily battery discharge results in a
3.2–4.4 year reduction in battery life if used for V2G) is a challenge.
Aside from V2G, to achieve flexibility, a variable Time-of-Use (ToU)
cost for charging electric vehicles is proposed in [105]. The purpose is
to optimise the ToU in such a manner as to discourage EV charging at
times of high grid-stress to better manage the overall grid load. To
enable the use of renewable sources with the high loads of fast-charge
EV stations in an off-grid fashion, a decentralised control formulation
integrating renewables, storage and fast-chargers is developed in [111].
Further to this, to manage the introduction of EV charging to a smart-
grid context while handling the uncertainties of renewable generation
and the associated system constraints, a hierarchical MPC approach is
proposed in [112]. For any such scheme, the challenge of predicting
short-term EV demand profiles is key to success. Markov-chain traffic
models are proposed to achieve this in [113] using real-time CCTV data.
Using public vehicle fleets with pre-specified schedules (such as buses
[114] and waste disposal fleets) could greatly simplify the problem of
predictable coordination.
2.3.2. Fuel diversification
Apart from EVs, the ability to integrate different energy supplies can
enable alternative fuel sources to be incorporated more easily into the
transport sector. In [115] for example, ethanol produced for transport
in a multi-energy system is discussed, with a decrease in transportation
costs of 30% projected [116]. Similarly, the use of fuel-cells for EVs as
energy generators and distributors (whereby hydrogen acts as the en-
ergy carrier) to encourage the transition to 100% renewable energy is
outlined in [117] with a control methodology for such a scheme out-
lined in [118]. More generally, this concept of integrating different
energy sources while simultaneously aggregating, shifting and mana-
ging energy demands is central to the topic of smart energy manage-
ment systems and is discussed further in Section 2.4.
2.4. Integrated systems
The high population density and correspondingly large demands
across the different energy vectors found in urban environments leads
to opportunities for more efficient operation by combining and con-
verting supplies and temporally redistributing demands through energy
storage and intelligent control [119]. Renewable energy sources can
have significant environmental benefits, with technological improve-
ments and reducing costs (in [120] for example, a drop in installed cost
of up to 50% was recorded for solar energy between 2010 and 2015
alone) ensuring that they will play a crucial role in the urban energy
landscape. Through appropriate energy management, an increased pe-
netration of renewable and low-carbon sources can be achieved, as well
as a more efficient consumption of generated power, heating and
cooling [116].
2.4.1. Distributed generation
Smart meters can provide a high degree of granularity for mea-
surement and control of spatially distributed actors [121]. This, along
with distributed ledger technologies [122] introduces the potential to
include a wider array of energy producers in the energy system, leading
to concepts in which domestic buildings and data-centres can be seen as
prosumers [123], generating heat and/or power. This changes the urban
energy sector from a set of centralised supplies and decentralised de-
mands to an interconnected set of distributed supply and demand net-
works. The central part to be played by IoT in the future development of
these interconnected systems and smart grids is emphasised in [124].
Based on a Malaga use-case, [125] highlights the importance of voltage
regulation strategies with a large renewable penetration. This is an area
where demand response [126] and coordination [65] can play a sig-
nificant role.
A further benefit to this increasingly diverse, distributed approach is
system resilience [127], which is a critical concern in an energy dense
urban environment with limited space. The definition of resilience,
once again, tends to vary, but it can generally be seen as the ability of a
system to survive a shock and return to equilibrium [128]. In a system
reliant on a single centralised energy source, a single shock or un-
expected change can have a significant system-wide impact when
compared with a smart integrated system with multiple sources. Some
of the key factors influencing energy system resilience are highlighted
in [127], including spatial distribution of generation, collocation of
supply and demand, diversification of fuel mix and generation type,
back-up energy stocks, storage and spare generation capacity. Dis-
tributed generation capacity is insufficient without suitable distributed
energy trading mechanisms however, such as the approach for trading
between multiple islanded micro-grids in [129]. A cooperative energy
trading scheme is developed in [130] in which the two way trade be-
tween local renewable generation and the grid is optimised to minimise
the total energy cost subject to quality of service constraints. In con-
trast, the problem is considered in the form of non-cooperative actors in
[131], whereby a game theoretic framework is applied to the problem.
2.4.2. Multi-vector generation
The close proximity of electrical, heating and cooling demands can
make the city environment particularly suitable to co-generation and
tri-generation technologies, utilising heat that would otherwise have
been wasted [119]. As previously mentioned, the continued de-carbo-
nisation of the electrical grid has brought increased attention to heat
pump technologies. Furthermore a push towards renewable technolo-
gies such as biomass and solar [132] for both heating and cooling has
resulted in a changing energy landscape. To incorporate different sup-
plies, emphasising the lower carbon options, storage technologies and
district energy networks can be implemented as discussed in
[133–135]. In [136], the introduction of a hydrogen grid to integrate
with electrical and thermal networks is discussed. This could in turn
promote the increased adoption of fuel cells for energy storage.
Appropriate selection of technologies and designs is needed here, with
data science and modelling techniques once again playing an important
role. Concepts and evaluation models for such Multi-Energy Systems
(MES) are reviewed in [116], with methodologies incorporating Mixed
Integer Linear Programming (MILP) techniques developed in [137],
further incorporating evolutionary techniques in [138].
The move towards distributed generation facilitates a greater range
of technology options. Given the spatial constraints of an urban en-
vironment, wind energy potential may be limited, however the use of
small turbines is not uncommon [15]. Similarly, solar energy uptake
has increased rapidly in recent years [120]. The need for adequate
storage levels to enable temporal shifting of variable renewable sup-
plies to match demands is key. Many storage technologies exist, with
time-scales ranging from milliseconds to seasons [139]. Some less
conventional options include mechanical flywheels (as reviewed in
[140]) and supercapacitors [141], while in thermal networks phase
change materials have a much higher energy density than water (as
they exploit latent energy storage) [53]. Battery technology has ad-
vanced in recent years with lithium-ion batteries comparing favourably
in terms of lifespan, energy density and power with more traditional
lead-acid batteries [142]. The mix of electrical supplies alone can then
include baseloads (large-scale nuclear and fossil fuel-based generation),
co-generation plants, small scale diesel generators, solar PV, wind
power, run-of-river hydro [143], biofuels, fuel cells and any of the other
electrical storage options mentioned above. On top of this, the in-
creasingly common interconnections with the thermal and transport
networks must also be considered. Numerous possible combinations
and supply mixes can be chosen from (see for example the flywheel -
wind turbine - diesel generator approach of [144] or the integration of
renewables and nuclear in [145]). As such, design and operation stra-
tegies that can handle this new level of complexity, while delivering on
environmental, financial and resilience goals must be established [146].
2.4.3. Coordination, control and operation
Beyond design and analysis, operating distributed and multi-vector
energy systems in real-time with optimal decision making requires in-
novative control structures and software architectures. The energy hub
concept [147] has emerged with the intention of explicitly accounting
for the couplings between the different infrastructures. Decentralised
energy system integration through this concept is discussed in [148],
with case studies illustrating the benefits of suitable integration ap-
proaches (e.g. a combination of Photo-voltaic (PV), Solar thermal and
hydro produced 46% lower emissions than a PV-biomass combination).
Modelling and optimisation considerations are covered in [149], in
which economic benefits of 11%–29% and 11% emissions reductions
are predicted with optimised operation of a network of three energy
hubs.
From a control perspective, as with building energy, micro-grids and
smart-grids can benefit from the application of MPC due to an ability to
explicitly handle system constraints while incorporating predictions
and forecasts of system outputs and environmental variables that can
all impact the optimal decision making process [35]. The application of
MPC to MES is carried out in [150] with a particular focus on the choice
of suitable prediction horizon. In [151], active demand response ap-
proaches in electric heating systems coupled with storage for achieving
these impacts are introduced, with the authors presenting modelling
approaches which attempt to reduce computational effort without
losing validity. With such advanced strategies, the need to manage the
different actors and their respective objectives makes game theoretic
frameworks a natural fit. Such approaches are discussed in [152,153],
with the application to energy hubs illustrated in [154].
2.4.4. Interaction with the wider environment
The smart city concept can be seen as the application of ICT to all
aspects of urban life [155], far beyond energy. Energy networks will
tend to interact with and impact other aspects of the urban landscape,
most notably with water [156] and waste management [157]. These
interactions can be exploited for their potential synergies, with the
water-energy nexus being a prime example [158]. The high population
density and correspondingly high water demand in a city mean that
water systems must be carefully designed to ensure security of supply
(reflecting the challenges of the energy sector). Strategies that can re-
duce water demand can also reduce energy consumption. In [159],
rainwater harvesting and gray-water reuse is studied to this end. Such
advances will benefit from intelligent approaches [160] and increased
data acquisition [161]. Waste-water treatment must also be considered
here, particularly given the push towards resource recovery in the
sector [162] (fuel diversity can be improved through on-site generation
of biogas for example [163]). The design and analysis of the benefits of
such multi-domain schemes requires techniques that can handle the
system complexity [164], such as the network perspective proposed in
[165]. Additionally, full life-cycle assessments should be carried out to
understand the impacts of any interventions across all environmental
vectors. This involves consideration of the materials used, supply
chains, operation, waste and emissions from any intervention, such as
that carried out for EVs in [166].
2.5. User-centric considerations
As noted in [31], the success of a smart city depends on the parti-
cipation of its citizens. Intelligent solutions may often correspond to
increased automation, while many interventions seeking to improve
environmental and economic goals on a system-wide perspective may
not necessarily benefit all users within the system. Given the reliance of
a smart city on user engagement however, the human perspective must
be considered in any actions taken.
2.5.1. Participation and engagement
User participation can be passive or active. Data can be passively
gathered from users by soft-sensing or crowd-sensing [167] (using
sensors in phones for example) to understand the patterns of movement
around a city. Clearly, all citizens may not desire such a use of personal
data. To encourage active engagement, a ‘Smart Citizen’ app is pro-
posed in [168], enabling a smart city to take advantage of distributed
soft-sensing. Another platform has been developed in [167] to select
individual participants that can be best combined for effective crowd-
sensing. To incorporate users more directly in the operational decisions
of the city, control schemes can be formulated to accept feedback from
people, particularly useful for subjective measures (e.g. thermal com-
fort). Such a scheme is proposed for building energy in [169]. Inter-
active dashboards can be implemented to further encourage participa-
tion, displaying key metrics and statistics associated with the city’s
energy systems [170]. Users are central to any strategy [171] and as
such, all approaches must be carefully developed to ensure that users
supplying data and partaking in smart energy initiatives (e.g. prosu-
mers) are appropriately compensated, particularly given the potentially
conflicting objectives of energy suppliers. The topic of a smart city
energy marketplace is well explored in [172], considering the per-
spectives of the energy retailer, the distribution system operator, re-
sidential and commercial prosumers, market operator and service
providers. Full participation cannot be assumed however, and scenarios
in which access to user data is not permitted must still be considered,
such as energy meter data at an individual dwelling level or electric
vehicle usage. ML techniques such as transfer learning [173] can exploit
previously-acquired knowledge from similar problems (e.g. accessible
data from similar users) to estimate such data gaps.
2.5.2. Privacy concerns in data-rich landscapes
Given the data-driven nature of the smart city concept, user privacy
considerations must be prioritised in any form of participation. Changes
and improvements beyond the smart city domain such as the EU
General Data Protection Regulation (GDPR) have recently brought
these concerns to the fore, however, a greater understanding of the
challenge faced is needed to attain the benefits of interconnectivity
without infringing on peoples rights or misusing their trust. In [174] for
example, the importance of understanding the data types (personal or
impersonal) and the purpose for which they are collected (e.g. service
provision is more favourable than surveillance) to the perception of
privacy violation is underlined. To that end, the authors developed a
framework to better capture peoples privacy concerns by incorporating
such classifications. The problem of data over-collection is discussed in
[175], with the authors highlighting the difficulty of the problem as the
data collection can be within the permissions authorised by the user yet
can increase security risks. Active approaches for mitigating the pro-
blem are also introduced. Approaches for maintaining privacy in energy
trading schemes is considered in [176], specifically using multi-sig-
natures, Blockchain and anonymous messaging streams.Future direc-
tions for research in privacy solutions are detailed in [37], stressing the
potential for trusted third party monitoring and auditing to detect im-
proper access to data and the need for collaborative efforts across
public, private and academic actors to develop adequate policies and
regulations.
3. Further integration of CI and ML techniques to different aspects
of smart energy system design and operation
The previous section provided an overview of the different concepts
and application areas relating to energy systems in the smart city en-
vironment. In this section, the ML and CI perspective is expanded upon,
detailing suitable techniques for the above applications, while detailing
some of the key future opportunities and challenges for the field.
3.1. Key areas of integration of CI/ML tools for design and operation
The basic premise of smart energy systems is centred on the inter-
actions among multiple entities (end users, generation and distribution
companies, energy markets etc.) and multiple energy vectors and the
use of automated algorithms for control and real time decision making.
Therefore online real time learning from data (e.g. sensor data from
electrical transformers, transmission lines etc. to human behaviour data
like occupancy of buildings from IoT devices) and adaptation of the
decision making and control strategies accordingly, is one of the key
pillars in smart energy management where ML/CI algorithms would
play a significant role. Another broad area where rule-based CI or ex-
pert system approaches are useful for this is in the context of encoding
domain knowledge of the engineering operation, expert judgements for
setting reliable process parameters of adaptive engineering systems.
They would also be able to facilitate more interpretable rule based
automated frameworks which can fuse data from multiple sources and
take actions honouring system constraints and also simultaneously meet
user set objectives. Meta-heuristic optimisation methods from CI are
also useful for designing optimal rating and capacity of smart energy
systems, subject to different system constraints and multiple objectives.
We elaborate these key points and how they can impact the design of
smart energy systems in the next sections.
3.1.1. CI/ML for energy generation/conversion and demand predictions
Energy generation/conversion and demand depends on multiple
factors and data driven ML/CI approaches are useful for modelling and
prediction of these quantities [177,178]. Renewable energy generation
like wind and solar are highly stochastic and dependent on geo-
graphical location (e.g. latitude, elevation, distance from sea etc.) and
temporal aspects of weather (e.g. season of the year, day or night time
etc.) among other variables [179]. These variables also affect energy
demand and usage along with other social and economic factors like
country (developed nations consume more energy per capita than
poorer nations), family size and income, traditions and culture (e.g.
festivities generally increase energy usage) etc. Energy demand can not
only be for heat, gas and electricity usage at homes, it would also be for
other new applications like charging of electric vehicles, trading with
the grid or other peers etc. In such situations, other more complex
variables (say journey plans, congestion in the city etc.) play a sig-
nificant part in the predictions. The key points from the modelling
perspective are that multiple diverse factors are involved, historical
data of many co-variates are available and the inter-dependence of
these factors to the final energy demand or generation is complex. All
these make the usage of ML/CI techniques very attractive. Data driven
learning techniques like neural networks, adaptive neuro fuzzy in-
ference systems (ANFIS), support vector machines (SVMs) etc. are
useful for such cases of regression to generate predictions which can be
integrated in an overall energy management framework. Ensemble
learning [180] which leverages on multiple weak learners to give better
predictive performance than each of the individual learners by boot-
strap aggregating or boosting is also useful in such a context.
As IoT devices become cheaper, sensing will become more pervasive
at homes or end use points of energy [181]. Additional data about
building occupancy, user behaviour etc. would be available in real time
streaming format which can be incorporated into the energy demand
prediction models to improve their predictive accuracy. Data from IoT
enabled home devices like washing machines, microwaves, dish
washers and fridges would also contribute to richer datasets. Sensor
data fusion algorithms would be useful for combining data derived from
diverse sources and CI tools like fuzzy set theory can play a significant
role in such applications.
For most of these CI/ML algorithms, there needs to be a mechanism
for online learning and adaptation to account for the changing patterns
in user behaviour and the load demand profiles. For example, as more
and more renewable generation and EV demand get added to the grid,
both the generation and demand load profiles would look drastically
different from the present scenario. Depending on how fast these
changes would occur, the strategy might be to train the models offline
on past data and update the production level ML/CI models regularly
(e.g. every week or month). If changes are on a more rapid time-scale,
the ML/CI algorithms have to be equipped with online learning in real
time to respond quickly to these changes and have good predictive
accuracy in their forecasts, noting that in many control approaches
(including MPC), the system states are continually updated.
3.1.2. Rule/logic based frameworks for smart energy system operation and
decision making
Conventional ML methods have been successful in a data rich en-
vironment and use cases where the underlying mechanism of the system
is not well understood [182]. Recommender systems in online shopping
stores or video/music content based sites where it is important to
predict what the user might like next based on his current preferences
are two examples. Note that since our present understanding of beha-
vioural choices is limited and there is no exact answer for what a person
would like, it is acceptable for a black box ML/CI algorithm to predict
something which can approximate the user’s preferences.
Contrary to these examples, engineering systems are built and de-
signed in a bottom-up fashion and the working principles are governed
by laws of physics that are generally well understood. There is almost
always a well defined objective and a set suite of methods based on
engineering logic for operating the system under any circumstance. Any
ML/CI algorithm which tries to make automated decisions for operating
engineering systems must do so with this engineering judgement built
into its system. Therefore interpretability regarding the underlying
workings of the ML/CI algorithms and their behaviour in contingency
situations is important to gain faith in such systems [183] and their
consequent uptake and deployment by energy utilities. We therefore
believe that CI/ML techniques which are based on an interpretable set
of rules or logic (say Mamdani fuzzy inference systems, Bayesian net-
works etc.) would be useful in forming the core of a successful energy
management architecture. These systems might leverage other black
box ML/CI techniques (e.g. neural networks) by having their predic-
tions as inputs into the system, but the core decision making system
should never be a black box ML/CI algorithm. A high level review of the
short-comings of such black box deep learning methods has been cov-
ered in [184] and interpretability of the ML methods is one of the key
issues (along with others like being data hungry, poor incorporation of
prior knowledge, trust concerns etc.).
Another way in which CI/ML based control of energy infrastructure
differs from that of popular ML applications mentioned above, is that of
reliability and the significant costs/penalty associated with malopera-
tion of such systems. Reliable supply of energy is an important public
utility service and shutting down such systems due to some automated
ML/CI black box algorithm is detrimental to the revenue, consumer
satisfaction and public confidence in the company. Moreover, if the
algorithm operates the engineering system in such a way that it induces
mechanical or thermal stresses into the system (say multiple hot start-
ups of a power plant within a short interval) it would lead to reduced
life time of the system and consequent increase in overall power cost.
Other cases of maloperation, (e.g. running power plant milling equip-
ment or turbines at high speeds) might actually lead to mechanical
breakdowns and property damage leading to significant business in-
terruption.
Such interpretable rule based systems can also be deployed as em-
bedded smart solutions for end users which can leverage on smart meter
data and real time energy pricing from decentralised generation sources
to take optimal decisions on behalf of the user depending on user
specified objectives like cost minimisation or comfort maximisation.
3.1.3. Meta-heuristic optimisation methods for smart energy system
planning and design
Evolutionary and swarm based optimisers can be useful over other
traditional methods (e.g. convex optimisation, mixed integer non-linear
programming etc.) especially in cases where the function to be opti-
mised is discontinuous, noisy, multi-modal or cannot be expressed in
closed mathematical equations and requires simulation based tools.
Given the stochastic (noisy) renewable generation and load profiles of
these systems, complex interactions between multiple stakeholders
(requiring agent based or other simulation based approaches to model)
and uncertainties in how the interactions and energy system landscape
would pan out in the future, the use of meta-heuristic optimisers is an
attractive proposition. These optimisers are useful at the design and
planning stage for optimal design of smart energy systems given the
current energy infrastructure, future demand and other objectives like
cost and reliability of operation [185]. Robust meta-heuristic optimi-
sation methods are useful in taking into account the model un-
certainties and ensuring that the designed system gives similar levels of
performance even if the real system variables are slightly different from
the simulated system. Evolutionary and swarm based multi-objective
optimisation methods are also suitable for design of systems with
multiple contradictory objectives, such as system efficiency, cost, re-
liability etc. Pareto optimal solutions (whereby no further improvement
to one objective can be obtained except at the detriment of at least one
other objective) obtained from these optimisers would help quantify the
best possible trade-offs under a given set of engineering constraints and
objectives.
Such optimisation methods could also be useful in the context of
real-time decision making and control in an energy management
system. These might be used as add-ons to a rule-based system with the
requirement to deliver results in real time. Other applications would be
the adaptive update of controller parameters based on varying process
parameters, online system identification etc. However, evolutionary
algorithms are typically not suited to real time implementation and
need to be run multiple times as convergence is not guaranteed. If the
objective function can be written down using a simple set of linear or
non-linear closed set of form mathematical equations then its best to
use fast iterative techniques such as convex optimisation or sequential
quadratic programming. If the objective function can only be expressed
in the form of computationally expensive simulation code, then surro-
gate model based optimisation [186] or Bayesian optimisation can be
useful to obtain optimised values in a short duration. Even then, it
might not be suitable for real-time update at each time step. These
techniques might be used to compute the optimal parameters of the
system offline and then update the online algorithm after specified time
intervals (e.g. a few hours) using, for example, adaptive look-up tables.
3.2. Challenges and shortcomings of present CI/ML tools
Most ML/CI techniques are designed to be trained offline and then
the trained model is deployed in production mode. There are fewer
research methods that look into efficient and fast methods for recursive
online updating of these model parameters for real-time streaming data.
One way to circumvent this problem is to make near real-time updates
by batch processing historical data offline to re-train the models and
deploying the retrained models at regular (e.g. daily or weekly) inter-
vals. This method would capture medium to long-term trends but might
miss out on shorter-term trends in energy demand and prediction.
Consequently, the automated decision making and control algorithms
might be sub-optimal.
Most ML models are black boxes and lack interpretability.
Introducing prior judgement in terms of the model structure or rules
might make them interpretable but this almost always results in a
consequent decrease in predictive accuracy of these models as the data
might not actually conform to the intuition or judgement of the experts
as encoded in the model structure. Some research has been geared to-
wards training neural networks and then trying to extract rules algor-
ithmically from the trained model. In general this tends to produce
superfluous rules or ones which are not necessarily logically consistent
or make practical sense. A more comprehensive review of different
approaches to interpreting such black box models can be found in
[187]. More research needs to be done on the interpretability aspects if
CI/ML methods need to be tightly integrated in smart energy systems.
The recent trend in ML is to move from point predictions of outputs
(either class labels for classification tasks or real valued predictions for
regressions etc.) to predictive distributions which explicitly capture the
uncertainty in the output [188]. This is especially important as part of a
larger decision making framework where uncertainty propagates across
multiple levels (as can be the case in the hierarchically structured en-
ergy domain). Computing uncertainties in predictions as opposed to
maximum likelihood point estimates is also computationally expensive.
Numerical methods like Markov Chain Monte Carlo (MCMC) can be
used in a Bayesian framework to quantify parameter uncertainties for
models but MCMC is inherently a sequential algorithm and parallelising
it or scaling it up for high volume of data is difficult. Various approx-
imate methods like variational inference exist but they might not al-
ways give realistic estimates of the uncertainties [189]. Other non-
parametric techniques like Gaussian Processes might be used to have
good predictive accuracy along with uncertainties, but they do not scale
very well with large data-sets [190]. Advances in Sequential Monte
Carlo (SMC) techniques [191] would be especially useful for energy
management applications due to the nature of the streaming time series
data in such systems.
3.3. Future research directions in CI/ML relevant to the context of smart
energy systems
One of the key concerns for smart multi-vector energy systems is the
risks associated with integrating all these different energy networks
together. There might be cascaded failures where failure of one of the
energy vector networks might trip the whole system and cause cata-
strophic damages. Appropriately designed CI/ML tools should be able
to detect such failures and take corrective action in real time to limit the
extent of damage, for example, by islanding a particular area or
network. The energy management system should also be able to suggest
and operate the network in fall-back option mode in the event of such
exigencies. The other key desirable functionality of such a system is to
ensure how to recover fully or restore to a partial state, a working
energy network after a blackout or failure has occurred. CI/ML algo-
rithms would play a significant role in detecting when and what failure
has occurred, what parts of the energy infrastructure has been affected
and what is the best possible alternative to restore the system.
Since much of the system dynamics can depend on the actions taken
by the smart algorithms themselves, the design and planning of such
smart energy systems should explicitly implement these algorithms in
simulation and stress test different scenarios to understand the asso-
ciated risks in the operation of such systems. Game theoretical ap-
proaches, for example, can then be used to uncover vulnerabilities.
Another potential hazard in interconnecting energy networks is that
there would be increased risk of damage due to cyber-attacks and the
consequent losses would be more severe. This would be aggravated by
linking up these networks to the internet for essential services, such as
data sharing platforms (common in smart city applications), cloud
storage or high performance cloud computing back-ends used for run-
ning the algorithms in real time. Research also needs to be done to
ensure that CI/ML algorithms can detect network intrusion, understand
unauthorised access and can take appropriate actions to mitigate cat-
astrophic consequences of such attacks.
As embedded smart energy systems become more and more perva-
sive, the energy network would become more decentralised with de-
velopments such as peer-to-peer trading of energy and sale of energy
from vehicle to grid. CI/ML algorithms must be present at multiple
levels/scales (building scale, community scale, district scale, nation
scale etc.) and should be able to ensure stability and reliable operation
of the grid in spite of such uncertain operating conditions.
One common push-back against the wide adaptability of such in-
telligent systems for fully automated real time operation and decision
making is the reliability and confidence in CI/ML algorithms, especially
in abnormal situations. Formal verification methods or automated
model checking for CI/ML systems might be an avenue to explore to
ensure that smart energy systems designed with CI/ML components
would not go into unsafe states or modes of operation even in abnormal
situations. On the other hand, these methods should provide support for
detecting incipient faults prior to the manifestation of an abnormal si-
tuation.
4. Real world implementation cases and considerations for Smart
Cities
Though the benefits of exploiting the increased smartness of cities to
achieve efficient energy system integration have been well established,
with techniques, applications and opportunities summarised in detail in
the previous sections, the real-world implementation of such strategies
requires the navigation of many practical barriers. These include the
range of stakeholders required from both private and public sectors, the
level of expertise needed, the conflicting objectives of different actors,
the capital cost of such large scale interventions and the uncertainty
surrounding the expected benefits (particularly from a financial per-
spective). Furthermore, as noted in [192], effective smart cities emerge
as a result of a variety of interacting components, which can differ
between cities. A comprehensive overview of the different barriers can
be found in [193]. Nonetheless, a large number of projects have been
carried out or are currently in progress that seek to demonstrate the
potential in the sector - a white paper categorising many of the main
examples globally was compiled by the UK-based Future Cities Catapult
for example, in [194]. In this section, considerations for implementing
smart integrated energy solutions are discussed based on real world
examples.
4.1. The replicability challenge
The holistic nature of Smart Cities has led to a diverse spectrum of
characterisation approaches. In [195], 181 cities are ranked in terms of
performance across 10 metrics to provide an impression of overall
smartness. The rankings show that the current state of progress varies
significantly with geographical location, with many high-ranking cities
clustered in Europe. Despite this, global urbanisation trends and the
consequential need for infrastructural development ensures that growth
in the sector is not confined to the current European and North
American centres - the Indian context is outlined for example in [196],
while the growth of Smart Cities in China is described in [197]. As
noted in [198] however, the evolution patterns of smart cities highly
depend on the local context, affected by variables such as population
density, economic development, cultural priorities, political structures
and climate to name but a few. Even within individual cities, require-
ments and barriers can vary across different districts and neighbour-
hoods depending on building age, infrastructural make-up and demo-
graphic profiles. As such, district-level interventions are often favoured
over city-wide approaches [199,200]. A single template or one-size-fits-
all set of solutions is unlikely to be achievable. Acknowledging this, a
theoretical framework for analysing business models through the use of
qualitative indicators is developed in [201]. This can allow adminis-
trators to avoid potential pitfalls and shape decisions to fit a particular
context. In [202], the cities of Montreal, London and Stockholm are
studied to examine the approaches for strategizing smart cities. The
authors stress the need to integrate many multidisciplinary resources in
any strategy developed.
4.2. Data handling & system integration
One common theme that tends to cut across all contexts is the need
for appropriate platforms to manage the data streams from the various
assets. The London data store [203] is an example of such a platform,
allowing open access to statistics and datasets. An example in which
such platforms have been leveraged to enable smart management of the
underlying systems can be found in Padova [23], where an open data
and ICT platform initially used to measure environmental variables was
extended to include a smart lighting strategy. In Santander, transpor-
tation issues such as congestion, parking and driver guidance were
tackled using this integrated data [204,22]. Similarly, the Smart Brain
strategy developed by Alibaba [205] was implemented in Hangzhou to
take various social media and traffic information to reduce average
travel time by 10%.
Focussing on consumption, monitoring of water and electricity
consumption with smart meters in homes enabled guidelines and in-
centives to be provided to residents in Dubuque [206]. Building further
on such concepts, a smart solar neighbourhood was developed in Nice
[207,208,22] to integrate distributed electricity generation and solar
power forecasts using thermal storage and intelligent demand man-
agement. In addition to this, the city introduced smart transportation
and smart waste management strategies as well as smart lighting.
4.3. Demonstrators and living labs
Identifying a definitive business case for integrated energy solutions
has proven to be a challenge, with the involvement of many actors and
benefactors required. As such, many recent Smart City projects have
sought to act not solely as technical test-beds, but as starting points for
wider scale-up. From a technical point of view, this requires quantifi-
able performance indicators, while also providing guidelines for
handling the financial and governance challenges involved, as well as
the wider social impact. In the UK electricity industry, this has led to
demonstration projects centred on the development of large-scale smart
grids while piloting innovative consumer services and commercial ar-
rangements [209–211]. In several European cities such as Helsinki
[212,213], and Barcelona [214], multi-purpose test-beds have similarly
been established to implement and evaluate various solutions to over-
come specific city challenges. Such Living Labs enable a better under-
standing of the challenges faced prior to full-scale roll-out of a solution,
as well as providing more concrete performance figures and business
cases. In [206], the development of similar demonstrators globally
(including in Seoul and Rio de Janeiro) with IBM is discussed. Given the
purpose of these living labs as generators of insight, the importance of
understanding the holistic outcomes of such projects (particularly be-
yond the technical perspective) where private ownership is involved is
discussed in [215].
4.4. EU Smart Cities and Communities call projects
A number of ambitious projects centred on the development of
scalable Smart City interventions are currently in progress, as part of
the EU H2020 call for Smart Cities and Communities. To encourage
large-scale replication of the demonstrators, interventions in each
project are first developed and implemented in a different set of light-
house cities, while the methods and learnings are taken on by a set of
follower cities with the intention of replicating similar projects. The
focus of all such projects is on integrated commercial-scale solutions
with high market potential in the fields of energy, transport and ICT
[216]. Once again, integrated ICT forms a central part of all these
projects. Beyond this, wide ranging topics and solutions encompassing
smart grids, district heating and transport are proposed (as well as
numerous other branches of the smart city concept). Of particular sig-
nificance is the focus on implementation and scalability, illustrating the
current state of the sector, moving from a conceptual phase to one of
more tangible realisation. The projects in the call are listed in Table 1.
All of the above give a generic overview of the on-going and recent
projects and applications in the context of smart energy systems for
smart cities. To better illustrate the challenges faced in real time im-
plementation, the following section details the energy management
strategy development for one of these smart city projects that our group
is involved in, outlining the current status and vision for the future of
the work.
5. Overview of energy management in the ‘Sharing Cities’ project
and vision for the future
In this section, an overview is provided of the approach to be taken
for managing an integrated set of energy networks in the Borough of
Greenwich, developed as part of the London branch of the “Sharing
Cities” project. The energy management strategy developed is referred
to as the Sustainable Energy Management System (SEMS). The over-
view highlights some of the main control and data handling con-
siderations and challenges faced in such a project.
5.1. System overview & architecture
A number of assets are included in the proposed strategy, with the
overall scheme encountering many of the key challenges of the domain,
including the electrification of both heating and transport, the inclusion
of renewable generation and the provision of storage. It is proposed that
a set of apartments are to be fed by a low-temperature heat network
supplied by a combination of water source heat pumps, thermal stores
and additional boilers (acting as a back-up). Open-access EV charge-
points are to be installed in the region, as well as an additional e-bike
scheme. Photovoltaic panels are also to be installed to increase re-
newable generation in the borough. The apartment block considered in
the strategy consists of 95 apartments of various shapes, ranging from
two-person to five-person in size and built of solid brick. The building,
constructed in 1937, is to be retrofitted with radiators suited to a low
temperature heating system. The proposed retrofit includes the in-
stallation of apartment-level smart controllers to which user-specific
set-point schedules can be applied. These communicate through a Z-
wave protocol with Thermostatic Radiator Valves (TRVs) to manage the
radiators. Energy meters, heat meters, sensors associated with the
supply-side equipment and the smart control gateways are hardwired to
the central energy centre, with all data passed to the SCADA system.
The various system components and assets are shown in Fig. 1. The
increased use of electrical power to satisfy thermal and transport de-
mands is likely to increase the stress on the local power network, par-
ticularly if load peaks from the different demands coincide. The un-
certainty associated with the increased renewable penetration must also
be handled by the power grid. Coordination of the different supplies
and demands, in particular, exploiting the energy storage capability
available in the heat network and the apartments themselves is there-
fore paramount.
5.2. Control strategy
The SEMS seeks to guide the operation of the different individual
system components in a manner that is, in some sense, optimal (whereby
optimality is dependent on the definition of the specific objectives
chosen), while providing intelligent coordination strategies to ensure
the overarching system constraints are satisfied. The potential exists for
objectives across different components to conflict at times and so the
global objective (insofar as a single objective can be identified) may not
correspond with individual local objectives. Nonetheless, due to the
desire for optimal operation coupled with the presence of environ-
mental forecasts and large numbers of inputs, outputs and constraints,
solutions based on MPC are a natural fit.
5.2.1. Hierarchical structure
Allowing for communication latency and computation time, 15min
time-samples are chosen for the MPC-layer. Such a sample-time is too
long for robust operation of the lowest control layer, associated with
proportionalintegralderivative (PID) and rule-based controlled actua-
tors, valves, pumps etc. In the control hierarchy, the MPC-layer gen-
erates set-point trajectories which are fed to the autonomous lower-
layer, which is designed and operated in a standard fashion. Such an
arrangement is typical in process control to ensure robust operation in
cases whereby the predictive layer misbehaves [226].
5.2.2. System constraints
Each individual component has associated physical operating con-
straints, while additional constraints are imposed at an overarching
system-level (for example, the total electrical capacity of the local
distribution network). In this case, some of the main constraints are:
•comfort in each apartment is maintained as requested
•the electricity grid is not over-stressed
•EV charge-points are operated according to the owners specifica-
tions
•all individual assets are operated within specific physical limits
(including outputs, ramp-up rates, operating time etc.)
5.2.3. Objectives
From a whole-system perspective, multiple objectives are present.
Once again these objectives appear at multiple scales, with each dif-
ferent actor representing a different purpose. Some of the key objectives
are as follows:
•minimise financial operating cost for all actors (building occupants,
government, EV users, asset owners, etc.)
•minimise the overall environmental impact of system
•minimise EV-charger curtailment
•maximise renewable penetration
Naturally, these objectives are not mutually exclusive and conflicts
can exist, necessitating a suitable coordination approach.
As alluded to, the goal of the control framework is to optimally
coordinate the different energy assets in the region, operating the in-
dividual components so as to satisfy the wider system constraints, while
minimising environmental and economic impacts (by exploiting
varying energy prices). A challenging feature of the energy sector at this
scale however, is the diverse range of stakeholders, impacted actors and
beneficiaries with different objectives. Considering each asset in-
dependently can lead to higher level system constraint violation, for
example, if EV charging peaks coincide with those of the heat pump
heating load, the power network may become over-stressed.
Furthermore, even if all components and system constraints are han-
dled within a single centralised framework, the different objectives can
lead to conflicting requests, for example, maximising the PV panel
output on the supply-side while minimising power consumption on the
user-side may lead to an unstable power grid.
5.2.4. The overall SEMS control architecture
Bringing together these aspects, the complete architecture of the
proposed control strategy - the SEMS - is shown in Fig. 2. To ensure
robust operation, the lowest control layer (the actuation layer) follows a
typical classical approach based on PID and rule-based controllers. This
layer includes the Thermostatic Radiator Valves (TRVs) in each apart-
ment, the pumps and valves used to actuate the heat pumps, boilers and
stores, and the switches associated with all other electrical assets.
The set-point trajectories for the actuators are provided by the
middle, MPC-based layer. In this case, two separate MPC controllers are
implemented, however additional controllers can be added in a mod-
ular fashion if further assets are to be included. This modular form
enables the system scale to be increased without requiring a complete
reformulation and without excessive complexity. The MPC controllers
take measurements, weather forecasts, energy prices and user-defined
set-point schedules at fifteen minute intervals, calculating at each time-
step an optimal set-point trajectory for the following of 12h. The
models used by these MPC controllers are typically in a standard linear
state-space form, with quadratic objectives. The model parameters and
form can then be chosen to ensure convexity with a reasonable solution
time - open-source solvers from the CVXOPT package in Python for
example can be used to solve the optimisation problems. A further
important consideration here due to the use of historical data to gen-
erate future predictions is the data storage requirement. While data
storage is available here (specifically through the London Data Store),
accumulation of excess data over-time is to be avoided by the use of
recursive parameter estimation approaches.
In the top layer of the formulation, the projected heating and
electricity loads for the different assets are taken from the MPC con-
trollers. The purpose of this layer is to ensure that the combined loads
recommended by the multiple MPC controllers do not lead to constraint
violation at a system-wide level. The challenge of coordinated control
and the relative merits of centralised and decentralised formulations
have been well established [227]. Objectives can be balanced through
an appropriate selection of weights [228] or prioritised frameworks
[54], while higher-level coordination layers and game theoretic ap-
proaches [152] can allow for interaction between otherwise isolated
controllers to ensure all actors are satisfied in a manner that can be
considered fair. Depending on user-preference, two options are ex-
plored here. Firstly, an approach can be implemented in which addi-
tional hard constraints are applied to the MPC layer in the event of
high-level constraint violation following pre-defined rules. In the
second approach, an iterative procedure is used in which excessively
large power requests are penalised in the objectives of the MPC layer in
an increasing manner until system-wide constraints are fulfilled. The
former option is more robust, however, the need for pre-defined rules
inhibits the scalability of the overall scheme.
5.3. Multi-vector energy system simulation environment
5.3.1. Pre-implementation control design
Though the central purpose of the project is to implement the de-
rived algorithms in the real world, a necessary prior stage in the de-
velopment of the proposed control strategy is the creation of a simu-
lation environment or digital twin that can accurately represent the
temporal dynamics of the real-world systems. This is important for
several reasons. Firstly, any formulated algorithms must be rigorously
tested to ensure a smooth transition to the real environment. Given the
innovative nature of the proposed solutions, the specificity of the
challenges faced and the potential impact that would arise in the event
of error in the real system, this testing is paramount. Secondly, the long
time-scales and high expense associated with the system prohibits any
excessive periods of reformulation or tuning - preferably this should be
done in advance of implementation. Additionally, to assess and com-
pare the performance of different strategies in an unbiased manner, all
external factors and covariates should be identical from one experiment
to the next. In an environment such as this, whereby the weather plays
a conspicuous role, this is only possible in simulation. Finally, to inform
potential stakeholders of the possibilities and benefits of various control
strategies in a quantifiable manner, simulated case studies can be car-
ried out without many of the barriers associated with real-world im-
plementation. This next section provides detail of the digital twin de-
veloped for this Sharing Cities project.
5.3.2. The digital twin
The purpose of the digital twin is to combine the various generation,
storage and demand units in a single platform that can communicate
with an external control framework in the same manner as the real
system. To allow for different assets to be included in a modular
fashion, a suitable co-simulation environment is required. In this case,
the Building Controls Virtual Test Bed (BCVTB) environment is used
[229], which itself is based on the Ptolemy II environment [230]. In this
case, the simulated energy systems comprise of a heating network, the
apartment block it supplies, a set of EV charge stations, a set of PV
panels, a number of e-bikes and lamp posts, along with the electricity
grid and the gas grid. Profiles representing external variables such as
weather (temperature, solar irradiance etc.) and energy prices (gas and
electricity) are also included.
To represent the thermal dynamics of the apartment block,
EnergyPlus building energy simulation software is used. EnergyPlus is a
whole building simulation package [231], capable of incorporating the
physical properties of the building as well as the heating system or
cooling system configurations and settings with weather data and oc-
cupancy schedules to produce temperatures, heat flows and water flows
throughout the building envelope. Elaborating on work carried out as
part of the EU H2020 funded MOEEBIUS project [232], the dimensions
of the apartment block and an appropriate set of building materials are
set to enable accurate simulation of the thermal dynamics of the
building, down to a granularity of 60 s time samples.
The heating network consists of water-source heat pumps, boilers,
thermal stores, pumps, valves and the PI-controllers needed to instigate
all of the above in the same manner as the real system. These are
modelled in Ptolemy II based on standard heat transfer and mass
balance equations (including heat losses) with Coefficient of
Performance (COP) curves taken from manufacturers’ data used to re-
present the efficiencies of the boilers and heat pumps. The flow tem-
perature and flow rate of the heat network are passed to the EnergyPlus
model, which then sends back the return temperature. Other compo-
nents, such as EV chargers, e-bikes and environmental variables are
instigated as time-varying profiles, generated based on typical uptake
values with pseudo-random variations added.
All sensors present in the real system are represented in simulation,
with all relevant measurements pushed to the control layer via a web
application framework (in this case, using Flask [233]). The purpose is
to directly replace the digital twin with the real system, thus the control
layer, developed here in Python, is equivalent to that of the real system.
In the same manner as the real system, it pushes set-point trajectory
recommendations back to the digital twin. The complete architecture of
the digital twin and its link with the SEMS is shown in Fig. 3.
5.4. Additional challenges for the SEMS
The design and implementation of the outlined scheme in the real
system introduces number of technical challenges. This section briefly
summarises some of those that have been encountered or are antici-
pated.
The development of suitable Application Programming Interfaces
(APIs) to enable different actors access and view the behaviour and
performance of the installed systems is required. This must include
appropriate methods for visualisation, specifically illustrating the de-
cisions made by SEMS and the impact of these decisions to any actors
affected (positively or negatively) by these decisions. Crucially, this
must be achieved without infringing on the privacy of the individuals
behind the data (in this case, the occupants of the apartments are an
important example).
An issue that arises in the control of multi-vector energy systems is
the need to consider multiple time-scales. This is largely due to the
differences present in the thermal behaviour of systems such as build-
ings and the far higher-frequency dynamics of electrical networks. The
time-scales needed to provide frequency response services to the elec-
trical network for example, are clearly different to those needed to
manage the thermal store. The strategy and its associated functions and
objectives must be appropriately designed with this in mind, while all
discrete-time models used must be derived to ensure stability at the
relevant sample-times.
Though the potential improvements available through the im-
plementation of strategies such as this have been enabled by the pro-
liferation of data (as covered in Section 2 of this paper), the need to
effectively manage a large number of data streams is in itself a sig-
nificant challenge (an overview of this area of the smart cities domain is
provided in [234] as a guide). Data platforms and suitable APIs must be
developed and implemented to handle the various formats and inter-
faces associated with each of the assets. Historical data must be stored
to enable system design, modelling and analysis to be carried out while
privacy concerns must also be understood and mitigated.
Suitable KPIs and metrics must be developed to assess the perfor-
mance of the designed approach. Once again, these must consider the
various complementary and conflicting objectives present in the multi-
stakeholder landscape. To provide insight into the real-world perfor-
mance, extended monitoring periods are required, long enough to
capture typical seasonal variations while ensuring that temporal
anomalies do not dominate the results. In addition to monitoring the
performance of the scheme, a similarly rigorous approach must be
taken to monitor the system behaviour without coordination to enable
valid baseline performances to be understood, though a certain amount
of this burden can be carried by the digital twin.
Scalability is key to the long-term success of any Smart City solu-
tions. As referred to in the previous section, the scheme is designed to
enable modular expansion if additional systems are to be incorporated.
Despite this, time and expertise are needed to carry out the required
modelling and control design for any new assets brought in.
Furthermore, all impacted actors must be considered in any further
system integration, as well as any spatial changes. In [235] for example,
the need to consider the impact of regionally varying energy pricing
when planning energy conservation measure implementation is ex-
pounded. Such issues can affect any performance metrics determined.
As system connectivity and complexity increases, ensuring system
resilience can become more challenging. Cascade failures [236] and
communication faults must now be mitigated, with suitable failure
modes designed. As previously discussed, the control architecture is
founded on a layer of standard controllers, which in the event of a loss
of communication, can operate the various assets in an isolated manner.
Aside from this however, measures must be included to prevent in-
dividual faults (sensor, actuator or model faults) from propagating
throughout the system [228]. Once again, the digital twin can play a
role in the required analysis for this.
6. Conclusion
This paper surveys the current directions in smart energy systems
for smart cities and draws on our insights in developing a large scale EU
pilot project for real time implementation of the same. Current appli-
cation areas are first addressed, including data handling and
technological advances in the built environment and transport sectors,
as well as cooridnation and integration methods for exploiting sym-
bioses across different energy domains. The potential use of ML and CI
approaches to tackle specific future challenges in the area is also ex-
panded upon. Along with theoretical developments, we also look at the
practical aspects of real time implementation and understanding the
way the smart city concept is formalised for these real world cases, we
can envision how and where different CI and ML paradigms can be
integrated. This would help to inform other researchers working on
more theoretical aspects and help understand which are the high im-
pact areas to focus on and also possibly help in guiding the budding
research community in this area to challenges motivated by real world
case studies.Spatially resolved mean and unsteady surface pressure in swept SBLI using PSP
Abstract 
Strong crossflow and swept separation are key aspects of the flow dynamics of 3-D shock wave/boundary layer interactions. This study explores the global surface pressure field beneath the canonical interaction produced by a sharp fin with deflection angle 15◦ with a turbulent incoming boundary in a Mach 2 freestream flow. This corresponds to an interaction of moderate strength, and the unsteady pressure distribution captures pressure fluctuations associated with separation shock motion upstream of the interaction. Details of the PSP calibration are also described where the calibration process combines both a priori (with separately painted test coupon) and in situ calibration (with pressure tap measurements during test). Flow features are identified directly from the quantitative pressure distribution and compared to qualitative surface oil flow visualizations. The technique facilitates measurement of the pressure distribution on surfaces that have been difficult or impossible to instrument, such as the face of the shock-generating fin. The unsteady paint response is captured simultaneously with unsteady pressure transducers on the surface underneath the interaction, and a frequency response function based on this comparison is presented. As the results discussed herein demonstrate, the use of PSP allows one to capture significantly more information about this complex, highly three-dimensional interaction with details that are not easily obtained using traditional sensors, while also providing a more informed global view of the interaction. The utility and limitations of the technique in application to supersonic wind tunnel experiments are discussed. 

1 Introduction 
Experiments to capture surface pressure fields in shock wave/boundary layer interactions are motivated by the ubiquity of the phenomenon in supersonic external aerodynamics and its direct application to the development of next-generation high-speed air transport systems (Urzay 2018). Strong shock wave interactions are accompanied by adverse pressure gradients that can separate the incoming boundary layer, producing highly unsteady pressure forces and heat flux that have been historically challenging to predict, even in relatively simple geometries (Dolling 2001). The complex 3-D flows in swept interactions are the focus of many computational studies, which require experimental validation through wind tunnel or flight testing (Panaras 1996; Gaitonde 2015). With recent advances in diagnostics, significantly more instantaneous measurements are becoming feasible while also producing new validation datasets including plenoptic (Jones et al. 2018) and tomographic PIV (Humble et al. 2009; Threadgill and Little 2018) for volumetric flowfield measurements. When coupled with surface pressure fields, these provide rich insight into flow physics. A relatively new technique for acquiring spatially resolved surface pressure fields is pressure-sensitive paint (PSP), which is an alternative to arrays of traditional pressure taps. PSP has demonstrated improved spatial resolution over individual pressure taps, but with additional expense, complexity of light and camera setups, and potentially greater error. It is not yet clear which experimental configurations and flow conditions benefit from the inherent trade-offs. Recent reviews (Gregory et al. 2008, 2014) track the maturation of PSP in wide-ranging experimental applications, which include impinging jets (Davis et al. 2015), transonic swept wings (Sugioka et al. 2018), and cavity resonance (Casper et al. 2018). Experimental configurations that use PSP to study supersonic flows include blunt fin-generated SBLI (Raju and Viswanath 2005), jets in crossflow (Crafton et al. 2015) , swept fins on a cone (Turbeville and Schneider 2018), swept impinging SBLI (Padmanabhan et al. 2019), and curved inlets (Funderburk and Narayanaswamy 2019). The development of reliable, short-response-time paints for capturing unsteady pressure fluctuations remains an active area of research and is becoming increasingly specialized for different flow regimes (Sugioka et al. 2018; Sugimoto et al. 2017). Unsteady tests offer additional challenges in illumination, since paint excitation for each frame decreases with higher camera frame rates. Because the paint emission decreases with higher static pressure, experiments with subatmospheric surface pressures result in increased signalto-noise ratios and can take advantage of shorter exposure times. Temperature sensitivity, which can be effectively mitigated in steady paint response using a dual-luminophore technique described in Sect. 2, remains a source of error in unsteady pressure measurements. The benefits afforded by the high surface resolution of this image-based measurement are balanced by relatively higher uncertainty due to influences of camera noise, temperature, light source nonuniformity, and paint degradation. Additionally, the associated costs of the camera, lights, and paints remain higher than a typical low-noise static pressure transducer system used for time-averaged measurements, and so the value of higher spatial resolution must be justified. Sharp fin interactions produce a number of complex flow features that are of particular interest to researchers seeking to better understand flows experienced by real vehicles (Alvi and Settles 1992; Dolling 2001). An oblique shock generated at the leading edge of the fin sweeps over incoming boundary layer, and sufficient pressure rise separates the boundary layer, shown in Fig. 1. Early models of this flowfield describe the helical separation vortex, deduced largely from surface oil flow visualizations (Korkegi 1975; Kubota and Stollery 1981). Detailed observations of the flowfield structure and recent quantitative studies of flow properties across a wide range of Mach numbers and fin angles have been completed (Alvi and Settles 1990; Kim et al. 1991; Lu and Settles 1990; Arora et al. 2018). The single sharp fin-generated swept interaction produces high unsteady pressure forces and heat fluxes in the mid- and high frequency (St> 0.05) related to shear layer dynamics (Garg and Settles 1996; Arora and Alvi 2016; Adler and Gaitonde 2017), but it is not clear whether models used to describe dynamics of two-dimensional interactions exhibiting “closed” separation are applicable to flows displaying “open” separation, such as the current interaction (Dolling 2001). The inherent three dimensionality of swept interactions makes well-resolved measurements that elucidate flow physics more expensive, requiring distributed measurements of surface pressure. The single sharp fin-generated swept SBLI is often simplified by assuming conical symmetry of flow features, which is observed some distance downstream of the fin leading edge (Lu and Settles 1990). The length required to attain conical symmetry, known as the inception length, has been shown to vary with incoming boundary layer properties and interaction strength. The PSP measurement resolution, which would be insufficient if traditional pressure taps were used, permits analysis of inceptive scales that are present within an interaction and hint at different physical mechanisms within the flowfield. The current effort details experiments in two supersonic testing facilities to evaluate steady and unsteady PSP response underneath a complex, 3-D flow. This paper strives to cover critical issues of calibration and application to supersonic experiments. A novel method of correcting the unsteady PSP response for temperature is also presented. 

2 Pressuresensitive paint (PSP) models 
The underlying processes for accurate pressure measurements using PSP are briefly described in the following section; a comprehensive history and theory are available in Liu and Sullivan (2005). The photophysical parameters that influence the measured radiative emission are limited to pressure and temperature, since nonuniformities in luminophore application and excitation light are corrected by calculating ratios of images at known pressure conditions. The chemical process is governed by oxygen quenching, whereby the intensity of light emanating from the paint under ultraviolet excitation is a strong function of the concentration of oxygen surrounding special luminophore molecules. By taking a ratio of images acquired at known pressure conditions and the unknown test conditions, nonuniformities in lighting and luminophore distribution are accounted for and the resulting ratio is modeled by the reduced Stern–Volmer equation (Liu and Sullivan 2005): where I represents the image intensity (of a single pixel), P is pressure of that point, and subscript ref signifies the intensity and pressure at the known reference condition. The coefficients A(T) and B(T) are found empirically, and several authors have found that quadratic fits requiring an additional coefficient are more appropriate (Sugioka et al. 2018; Schreivogel et al. 2012). The present experiments use an exponential fit for pressure calibration of the steady PSP and a quadratic calibration for unsteady PSP. The active molecule that is presently used in both time-averaged and unsteady PSP is a platinum porphyrin (PtTFPP), which is receptive to excitation near 395 nm and emits near 650 nm. The steady PSP, which is known as binary fluoride-isopropyl-butyl (FIB) PSP, contains PtTFPP and an additional luminophore suspended in the FIB binder. The second luminophore, which is relatively insensitive to pressure, is captured simultaneously to correct for the temperature dependence of PtTFPP; the emission is close to 550 nm and can be captured on separate channels (red and green) using a commercially available color CCD camera 

3 Experimental methodology Experiments were carried out in two intermittent supersonic blow-down wind tunnels with different test section dimensions and boundary layer properties. The application of PSP is used to better understand the properties of this flowfield and to discover scaling parameters due to the differences in model size and boundary layer thickness, among others. Specifications of the facilities, PSP technique, and data processing methods are described in the following sections. 
3.1 Experimental facilities The experiments are conducted at similar freestream conditions within two supersonic wind tunnels, but with different incoming boundary layer properties, test section dimensions, and model dimensions. Comparisons of test conditions between the two tunnels at Mach 2, including boundary layer properties, are summarized in Table 1. The compressible boundary layer profile of Sun and Childs (1973) is used to define the velocity profile in the wake region, which is matched to the Musker (1979) log layer and buffer layer velocity profile. The temperature profile for the turbulent boundary layer is obtained through the modified Crocco–Busemann relation described in White (2006). 
The test section of the pilot tunnel measures 101.6 mm wide, 76.2 mm tall, and 254 mm long (4′′ × 3′′ × 12′′). The shock-generating fin is placed at an angle of15◦ with respect to the freestream Mach 2 flow and is 37.5 mm (1.48 inches) tall and 76.2 mm (3 inches) long. The leading edge of the fin is machined to a sharpness of 7°. Total pressure is measured in the stagnation chamber using a Omega PX-305 pressure transducer. All tests were conducted at a stagnation pressure of 345 kPa, stagnation temperature of 289 K, and freestream velocity of 510 m/s. The boundary layer thickness at the leading edge of the fin that is estimated from PIV to be 3.5 ± 0.2 mm (Arora et al. 2015), and compressible shape factor is 2.99. The model is mounted on the test section ceiling so that cameras could be mounted underneath the test section, which reduces the effects of vibration on acquired images. Companion experiments are conducted in the Florida State University Polysonic Wind Tunnel (PSWT), also in a Mach 2 freestream flow and fin deflection angle of 15°, but with a larger model (101.6 mm (4 inches) tall and 127 mm (5 inches) in length) and fin leading edge angle of 25°. The test section has a square cross section measuring 304.8 mm on all four sides and measures 609.6 mm in length (12′′ × 12′′ × 24′′). The test model is mounted on the floor of the test section, and high-quality optical windows are installed in the ceiling and sidewalls for camera/light source access. Both pressure-sensitive paints were applied to the test article in the same manner, using a hobby-type airbrush for both basecoat and photo-active layers. Image registration, preprocessing, and the procedure for converting light intensity ratios to pressure values are performed identically between the two facilities. 
3.2 Surface oil flow visualization A common technique for characterizing flow topology is surface oil flow visualization. In current tests, equal parts light mineral oil and red fluorescent powder dye are thoroughly mixed and airbrushed onto the model to produce even coverage of small (< 0.3 mm diameter) droplets. This dye is illuminated with high-intensity LEDs with a wavelength of 400 nm (ISSI), and images are captured at 15 Hz using an AVT GigE monochrome camera with Nikon f/1.4 50 mm lens. A long-pass filter with a cutoff wavelength of 590 nm is used to isolate the painted droplets from the reflection of the LEDs, which enhances the contrast in the resulting images. Surface flow visualizations where the principal surface flow features have been marked are shown in Fig. 2 and demonstrate the flow similarity between interactions in the two facilities. Further details of the surface flow visualization technique can be found in similar studies (Baldwin et al. 2019; Arora et al. 2018). The images are normalized by respective incoming boundary layer thickness, which scales the surface flow fields by a factor of approximately four between the two experiments 
3.3 Steady PSP measurement To capture the mean pressure response with minimal temperature dependence, a binary FIB paint (ISSI product) is used (Peng et al. 2013). An RGB CCD camera (ISSI) with a Nikon f/1.2 50 mm lens records the emission of both platinum-porphyrin (temperature and pressure sensitive) and the reference (temperature sensitive only) luminophores, which are detected separately on the red and green channels of the RGB sensor, respectively. A 470 nm long-pass filter is used to filter the 400 nm excitation wavelength; the blue channel of the RGB images is not used. By ratioing each image’s red and green channels, the pressure field is corrected for temperature variation during the test. Pressure is calculated by taking the “ratio of ratios” with respect to a reference image acquired at known pressure conditions before starting the tests 
The final resolution of the images after splitting the red and green channels is 804 × 604 pixels, with a scale of approximately 26 pixels/mm. Images are acquired for the entire test run at 10 Hz, and the images corresponding to stable test conditions are averaged to produce a single intensity field. Typically 15 s of at-condition data are acquired, resulting in 150 images that are used for each average. In the pilot tunnel experiments, three static pressure ports located in the upstream flow region are used to validate the mean pressure in the freestream for each run. Limited comparison is available to the PSP response in the freestream; mean pressure has been extensively tested using arrays of static pressure transducers at the same test conditions, and the paint response validation is discussed in Sect. 4.1. The model temperature is monitored by two thermocouples and shows approximately 13◦C temperature change over the duration of the run and 3◦C change during the acquisition of the mean data (15 s). However, based on the quality of the fit to validation data from a previous test (Arora and Alvi 2016), no temperature corrections are introduced in the present analysis. The model plate used in the PSWT has 13 pressure taps within the interaction for in situ comparison with PSP. 
3.4 Unsteady PSP measurement A polymer-ceramic PSP with a highly porous base structure, which permits an oxygen diffusion time to the luminophore on the order of 100 μs, (Fang 2010) is applied to the plate surface. Three unsteady pressure transducers are each placed 44.5 mm from the fin leading edge at angles of 22°, 37°, and 57° from the incoming flow. The locations are identified from flow visualization as close to separation line, underneath vortical flow, and just upstream of reattachment, respectively. More details are provided in the subsequent discussion of unsteady pressure results in Sect. 4.3. 
A Phantom V2012 camera with a Nikon f/1.2 50mm and 12mm spacer is used with a 610 nm long-pass filter (Andover Optics) to isolate the PSP emission from reflected light from the excitation LEDs. The monochrome images are acquired at resolution of 768 × 480 pixels at 50,000 frames per second for 1 s. In order to correct for wind tunnel vibration, the wind-on images are registered to the initial reference image using a cross-correlation algorithm with accuracy up to 0.01 pixel shifts. The ratio of registered wind-on and reference images is then calculated, and masks are applied to remove pixels corresponding to the fin and unsteady pressure ports. In order to make post-processing less computationally intensive, the images are cropped and bicubic downsampling is applied to reduce the final size of each image to a resolution of 150 × 268 pixels and scale of 10.7 pixels/mm. 
3.5 Steady PSP calibration Prior to wind tunnel testing, pressure response and temperature response are independently calibrated using a small plate that is painted concurrently with the model plate to ensure applicability of the calibration. Pressure is adjusted in a sealed chamber with optical access for the RGB camera and LED illumination, while a Peltier heater affixed to the painted plate with embedded thermocouples permits temperature control of the plate. The calibration results, including a calibration fit provided by ISSI and the exponential fit derived from in situ calibration, reinforce the temperature insensitivity of the technique (Fig. 3). The paint was calibrated at pressures from 2 to 20 psia at 2 psi intervals and temperatures of 0, 10, 15, 20, and 25 °C. The chevrons are colored by temperature, with the darkest symbol at 0 °C and the lightest at 25 °C, and confirm the low temperature sensitivity of the dual luminophore steady PSP. 
3.6 Unsteady PSP calibration In order to convert variations in intensity to pressure fluctuations without applying a temperature correction, the steady pressure distribution obtained using the temperature-compensated PSP is used as the known pressure to fit the unsteady intensity data from fast-response PSP. This allows the recorded unsteady intensity data to be calibrated to an unsteady pressure field, as depicted in Fig. 4. First, the steady pressure distribution is mapped to the same image space as the unsteady intensity ratios. Next, the relationship of intensity ratio and pressure is obtained by plotting the intensity ratio on the abscissa and pressure on the ordinate axis, as in Fig. 5. A quadratic function is fit for all data within a specified region of the interaction using a leastsquares estimate. These values for averaged unsteady PSP are included in the comparison to pressure tap data in Fig. 6 and agree well with this validation data. Some discrepancy between the interaction and freestream flows is observed, but it is not yet clear whether this effect is due to the impact of temperature or results from low signal-to-noise ratio. The unsteady pressure is simultaneously measured using three Kulite XCQ-062-5D pressure transducers mounted in the model plate flush with the test surface. The locations were identified from flow visualization as close to separation line, underneath vortical flow, and just upstream of reattachment, described in Sect. 3.4. The sensors are recorded at 200 kHz using a PXI-4462 A-D converter with built-in antialiasing filter that provides an alias-free bandwidth of DC to 80 kHz. Pressure acquisition is triggered by the camera recording, which is manually triggered by the operator once stable test conditions are reached. The differential pressure line is attached to the tunnel floor static pressure located on the wind tunnel side wall. A tare of the Kulites is performed during the wind-off image acquisition for each test run. A sample of time history of pressure (low-pass filtered below 8 kHz) measured by each Kulite and small “patches” of adjacent PSP (Fig. 11) shows the strong correlation between the techniques. A frequency response function is then estimated from each pair of signals to characterize the paint’s response; further discussion is in Sect. 4.3 
4 Results and discussion 
4.1 Measurements in SBLI using steady PSP The mean pressure field of the pilot tunnel measured using steady PSP is shown in Fig. 6a. The mean pressure field demonstrates an initial pressure increase near upstream influence line (UI), a pressure plateau between the separation line (SL) and inviscid shock line (IS), and maximum pressure near the fin, close to the reattachment line (RL). Two circular arcs are included to show where previous experiments (Arora et al. 2018) measured static pressure with traditional transducers, at r∕훿 = 4.5 and r∕훿 = 14.3. One array is within the inception zone, while the other lies under conically developed flow. Due to localized disruptions in the paint surface near the fin beyond r∕훿 = 12, steady PSP data downstream of r∕훿 = 12 is not reliable due to possible flow disruptions present under the paint surface and this limit is used to compare to pressure tap data in the conically developed region of the flow. Uncertainty in the steady pressure distribution is estimated as two standard deviations (2휎) from the mean, which is calculated from 15 s (150 images at 10 Hz) of PSP acquisition. The conical coordinates are based on initial surface oil flow in Arora et al. (2018), which place the VCO nearly 15훿 from the fin leading edge. This estimate for the VCO obtained from surface oil flow visualization relies solely on the upstream influence and separation line, which appear to converge at a greater distance than flow features downstream of separated flow. This has been demonstrated in simulations of swept compression ramp and fin-generated SBLI and hints at a deviation from quasiconical similarity in low Mach number interactions (Adler and Gaitonde 2019). Since the fin pressure distribution is also captured, a full view of the interaction for the interaction measured in the pilot tunnel is available (Fig. 7). A high pressure feature appears on the fin surface, and though accompanying surface oil flow experiments were not performed on the fin surface, it is conjectured that this is the response to the formation of a second compression due to a bow-shock-like shock ahead of separation near the fin leading edge. This is explored in more detail in the discussion of the unsteady PSP fields. However, within the fully developed region, outside the inception zone and downstream of separation, the boundary layer influence is negligible and the agreement in the pressure distribution using PSP between the two facilities is remarkably good. The discrepancy between “inner” and “outer” scaling within the conically developed region in swept SBLIs has been noted recently in simulations (Adler and Gaitonde 2019) and is here discussed from experimental pressure fields. A global comparison between the pressure fields measured in the pilot tunnel and PSWT is shown in Fig. 9, with color contours extracted from the PSWT and dashed line contours from the pilot tunnel. In dimensional coordinates, it is obvious that the flowfields are quite similar near the inviscid shock (solid line). However, the upstream influence region (where surface pressure increases from freestream (P∕P∞ = 1) to P∕P∞ = 1.6) is much larger in the PSWT pressure field than in the pilot tunnel. Additionally, the region of peak pressure (above P∕P∞ = 2) near the fin interface does not appear until farther downstream than in the pilot tunnel, indicating that the downstream location of peak pressure scales with boundary layer thickness as well. The flowfield model described by Vanstone et al. (2018) for swept SBLI describes the outer scaling of the interaction, which is strongly influenced by the parameters of the incoming boundary. The highly swept separation, where the crossflow velocity component dominates, appears to collapse using different scaling parameters, termed the inner scaling. Based on studies of swept ramp and sharp fin SBLI (Settles and Bogdonoff 1982; Settles Gary and Lu 1985), a correlation for the interaction inception length can be expressed as the following function: The conclusion from these correlation studies at Mach 3 studies is that an average value of a = 1∕3 produces general convergence of the extent of upstream influence, with broad scatter for fins (swept and unswept) and semicones in freestream Mach numbers of 2.25–4 (Zheltovodov and Knight 2011). This indicates that when the normal Mach number, Mn, is identical as in the two cases studied here, the inception length and boundary layer effect can be scaled by the Reynolds number using the boundary layer thickness as the pertinent length scale with a common exponent. However, the present data scale with a different exponent on the Reynolds number, a = 0.7. This is applied in Fig. 10, which scales the pressure distribution from the two wind tunnels with very good agreement throughout the pressure field. The deviation in scaling parameter from the accepted ℜ1 훿∕3 for freestream Mach numbers above 2 is possibly due to local effects of a bow-shock-like structure formed at the leading edge of the interaction, which is absent (or greatly reduced in influence) at higher Mach numbers. This was observed in a study with freestream Mach number of 1.8 and fin angle of 15°, with upstream influence obtained from surface oil flow visualization and pressure shifting farther upstream than predicted by the correlation with a = 1∕3 (Mukund et al. 2003). The deviation from established scaling may be a persistent feature of interactions with very high sweep that are close to shock detachment, but it is not possible to generalize from the limited available data. 
4.3 Unsteady pressure The unsteady pressure field shows strong fluctuations throughout the interaction. Three unsteady pressure sensors are installed underneath the interaction in the following configuration: K1 is downstream of separation, K2 is near the inviscid shock trace (underneath the recirculating vortex), and K3 is located near the impinging jet upstream of reattachment. Figure 11 shows the time history of the pressure from Kulites compared to the time history from PSP by locating a region of approximately 25 pixels just upstream of the Kulite in order to avoid paint response contamination downstream of the sensor. Although care is exercised to ensure the transducers are flush with the surrounding painted surface, it is important to recognize potential localized thermal contamination and local flow perturbations if the transducer is not perfectly flush. The Kulite XCQ-062- 5psid transducers fitted with B-screens have been reported to exhibit a flat frequency response up to 16 kHz (limited by the acoustic resonance of the sensor packaging) (Hurst and Olsen 2014) and here provide a “true” pressure, or input, for the PSP frequency response function up to the filtered frequency of 8 kHz. As observed in the time histories in Fig. 11, the PSP signal appears to track the temporal undulations in pressure as measured by the Kulites. There is a, however, a notable difference in the amplitude. To examine this further, the frequency response function (gain and phase) between the Kulite and averaged paint response are plotted from 0 to 8 kHz in Fig. 12. The frequency threshold for unsteady PSP is selected based on comparisons of PSP response and unsteady pressure transducers installed in the plate at three points in the interaction region. Coherence indicates the degree to which the paint is responding linearly with the pressure fluctuations measured by the transducer—because the transducer and PSP “patch” are not exactly collocated in the highly turbulent flow, it is expected that coherence is less than 1. However, values above 0.5 provide confidence in the PSP technique up to the determined cutoff frequency. There is a marked difference in the Kulite/PSP comparison in the K3 position, especially in the low frequency (0–1 kHz) coherence, that does not match the comparison observed in other locations. It is not possible to ascertain whether this difference in the signals is due to discrepancies in the physical response of the paint. It is likely that since this region of the flowfield is near reattachment, dynamics are dominated by mid-frequency pressure fluctuations, and the response within this low-frequency range is too low in amplitude to be accurately captured by PSP (Adler and Gaitonde 2018). The following analyses all consider the PSP field, wherein the time history of each pixel has been low-pass-filtered to 8 kHz, which provides a resolvable St훿 = 0.055. Although this does not capture all of the important mid-frequency content relating to Kelvin–Helmholtz and shear layer dynamics (0.01 < St< 0.1) (Adler and Gaitonde 2018) or high-frequency unsteadiness originating in the incoming boundary layer (St훿 ≈ 1), the spatially resolved pressure fields permit new observations of global, low- and limited mid-frequency dynamics. The relative energy of pressure fluctuations is visualized by the root-mean-square pressure in Fig. 13, which is nondimensionalized by the freestream pressure (P∞) obtained from the steady pressure distribution. The footprint of the interaction is clearly visible by the sharp increase over the freestream pressure fluctuations at the upstream influence line. Peak rms levels are observed near the leading edge of the fin, both on the face of the fin and the plate surface. The amplitude of pressure fluctuations decreases as the interaction grows in size and reaches a steady value in the conically developed region of the flow. The first peak lies between the upstream influence and separation lines and is related to the separation shock foot motion. The strong unsteadiness upstream of the fin is similar to the unsteadiness in a 2-D separated SBLI. Several instantaneous snapshots of the 3-D pressure distribution on the fin and surface beneath the interaction are shown in Fig. 14. The selected sequence shows every 4th image, so that the time step between pressure fields is 80μs. Although the instantaneous images are shown at a time step smaller than the upper limit of the filtered frequency response to 8 kHz (훥tfilter = 125μs), motion of low-frequency structures is visible within the pressure field. A black contour line is added to P�∕P∞ = 0.01 to help identify highpressure structures present in the field. Colored arrows are added to identify features on the fin and plate surface and are discussed below. The sequence begins (Fig. 14a) with high pressure upstream of the fin leading edge observable on the plate surface (yellow arrow) and traces of high pressure on the fin surface (white arrow). This corresponds to the farthest upstream extent of the “quasi-2-D” SBLI present at the fin leading edge. Pressure near reattachment (magenta arrow) is positive in this frame, and although does not appear as a discrete moving pressure structure in the flowfield, it consistently correlates with high pressure immediately upstream of the fin leading edge. In the next frame (Fig. 14b), the high pressure near the fin leading edge (yellow arrow) appears to shift toward the interaction, and low-pressure zone at the fin leading edge grows. The high-pressure structure moves farther away from the fin leading edge in Fig. 14c on both the plate (yellow arrow) and fin surface (white arrow). The high-pressure structure tracked with the yellow arrow splits into two in Fig. 14d, where the downstream feature convecting away from the fin is marked by a gray arrow. Low pressure upstream of the fin leading edge is at a maximum from the observed frames, indicating downstream limit of the shock motion, nearly opposite to the region of high pressure marked by a yellow arrow in Fig. 14a. The next frame (Fig. 14e) shows a weakening of the high pressure on the fin (white arrow) and fast convection of flow from the swept interaction (gray arrow). The high-pressure structure marked with the yellow arrow remains relatively stagnant. Pressure near reattachment (magenta arrow) is low and reverses the pattern evident near reattachment observed in the first frame (Fig. 14a). The final frame (Fig. 14f) demonstrates pressure increasing on the plate upstream of the fin leading edge and weak pressure signature on the fin surface that is reminiscent of the first frame. The high-pressure structures that were upstream of the fin (yellow and gray arrows) convect quickly away once out of the vicinity of the fin leading edge. The presence of high-amplitude pressure structures near the fin leading edge is immediately obvious, and the pressure signature on the fin surface shows the presence of a curved structure with alternating high and low pressure, which is most evident in Fig. 14d. This is evidence of a sweeping shock produced by the separation upstream of the fin leading edge, similar to a bow shock. In addition to the oblique shock produced by the freestream flow deflecting in response to the fin, unsteadiness within the interaction is influenced by convecting flow features from the separation near the fin edge within the boundary layer. 
5 Conclusions Spatially resolved steady and unsteady surface pressure fields in swept SBLI are obtained using pressure-sensitive paint. The mean pressure distribution is obtained using an in situ pressure calibration and validated against pressure tap arrays within the interaction. In situ calibration can correct for changes in PSP luminophore response, which decay at different rates and affect the binary FIB calibration over successive wind tunnel runs. The key contribution of the current work is establishing a procedure for anchoring unsteady PSP results to static pressure distribution measured with temperature-corrected steady PSP. Once calibrated, the unsteady PSP is able to capture the unsteady pressure field up to at least 8 kHz (St훿 = 0.055). Future efforts to characterize the response can enable frequency-dependent calibration of the paint response and improved quantitative description of the pressure fields. Additionally, the PSP analysis can be further improved using modal analysis techniques in order to reduce noise (Sugioka et al. 2019) and isolate key physics within the unsteady data. The pressure fields between two facilities with fivefold difference in boundary layer height can be assimilated by scaling by 훿∕ℜa 훿, where a = 0.7 instead of the widely validated scaling of a = 1∕3. The scaling of upstream influence measured in the present experiments deviates from correlations developed for higher Mach numbers (M > 2.5), possibly due to the local effect of a bow-shock-like structure near the fin leading edge. Further investigation into the scaling for interactions with high sweep angle is necessary to determine the broader applicability of the present scaling law to low supersonic Mach numbers, since distinct physics in local recirculating flow near the shock generator is implicated Transonic Turbine Vane EndwallFilm Cooling Using the PressureSensitive Paint MeasurementTechnique 

Abstract
This work focuses on the parametric study of film cooling effectiveness on the turbine vaneendwall under various flow conditions. The experiments were performed in a five-vaneannular sector cascade facility in a blowdown wind tunnel. The controlled exit isentropicMach numbers were 0.7, 0.9, and 1.0, from high subsonic to transonic conditions. The freestream turbulence intensity is estimated to be 12%. Three coolant-to-mainstream mass flowratios (MFR) in the range 0.75%, 1.0%, and 1.25% are studied. N2, CO2, and Argon/SF6mixture were used to investigate the effects of density ratio (DR), ranging from 1.0, 1.5,to 2.0. There are eight cylindrical holes on the endwall inside the passage. The pressuresensitive paint (PSP) technique was used to capture the endwall pressure distribution forshock wave visualization and obtain the detailed film cooling effectiveness distributions.Both the high-fidelity effectiveness contour and the laterally (spanwise) averaged effectiveness were measured to quantify the parametric effect. This study will provide the gas turbinedesigner more insight on how the endwall film cooling effectiveness varies with differentcooling flow conditions including shock wave through the endwall crossflow passage 

Introduction
Film cooling is one of the effective approaches to protect the gasturbine components from being damaged by the high-temperaturemainstream inside the engine during operation. This protection isespecially important on the first stage vane since it has to withstandthe highest heat load from the combustor exit. The film cooling onthe endwall region of the first stage vane is, therefore, a challengingtask for the gas turbine designers due to the nature of the complicated flow field from the strong pressure gradient in the endwallpassage, and the secondary flows including the horseshoe vortices,passage, and corner vortices [1]. The complicated interactionbetween the coolant and the mainstream can make the coolant coverage on the endwall to deviate from the desired scenario.Even not considering the effect of leakage flow from the interfacebetween the combustor and vane, many studies about the possibleparameters affecting the film cooling performance on turbineendwall are available in the open literature [2–9]. However, thereare relatively few investigations focusing on the endwall heat transfer/film cooling under transonic conditions. Although there areseveral papers [10–15] on the film cooling studies under transonicflow conditions conducted on simplified geometries or even thecascade, the conclusions from these studies contradict each otheras the Mach number or the oblique shock wave is found to haveeither a positive/negative effect or no measurable effect on thefilm effectiveness. Among the limited endwall film coolingstudies under transonic flow conditions, Harasgama and Burton[16] performed a heat transfer experiment on the outer endwall of the nozzle guide vane with discrete holes film cooling under an exitMach number of 0.93. They found that placing the cooling holealong an iso-Mach line is a very effective approach to cool thevane endwall. In terms of the Nusselt number on the endwall,there is a reduction up to 75% near the hole exit due to filmcooling; around 50% reduction in most of the other areas with thecoolant-to-mainstream mass flow ratio less than 1.45%. Giel et al.[17] used the steady-state liquid crystal to study blade endwallheat transfer for exit isentropic Mach numbers of 1.0 and 1.3 in alinear cascade. Their results indicate that the shock in the supersoniccase breaks up the smooth Stanton number distribution seen in thesonic case. Also, the Stanton number for the sonic case is actually40–60% higher than the supersonic case. Nicklas [18] investigatedthe heat transfer and film cooling in a linear cascade under exit isentropic Mach numbers of 1.0 and 1.2 using an infrared camera. Hisresults about the influence of the Mach number show an interestingphenomenon. For the downstream of the throat area, there is a 20%heat transfer coefficient decrease in the supersonic case comparedwith the sonic case despite the higher local Mach number in thesupersonic case. He attributed the heat transfer reduction to theeffect of the shock in the throat to the endwall boundary layer,the shock leads to a fast increase of the endwall boundary layeror even separation. Later works from Jonsson et al. [19] and Salvadori et al. [20] studied the heat transfer/film cooling under an outletisentropic Mach number of 0.88. Nonetheless, similar to the aforementioned research, there is no clear clue how the film coolingeffectiveness on the endwall will change under the different transonic conditions. The interaction between the film and the pressuregradient in the endwall is already very complicated, not to mentionif the shock wave is present. Specifically, whether the shock waveaffects the film cooling on the endwall in a similar fashion as the flatplate test remains a question.Therefore, this study experimentally investigates endwall filmcooling by systematically varying coolant-to-mainstream massflow ratios (MFR) and density ratio (DR), with the mainstream conditions from high subsonic to transonic at a given endwall filmcooling design. The present work will help the gas turbine designerto have more understanding of the coolant behavior on the endwallunder the transonic flow conditions. 
Experimental Setup and Method
Five-Vane Annular Sector Cascade. The flow loop of the testrig is shown in Fig. 1(a). As shown in Fig. 1(b), the metal cascadeand the two reinforcement bars for the windows are made by eitherthe selective laser sintering process or aluminum machining. Thetest vane is an aluminum machined piece, while the endwall ismade from a stiff resin material called Somos® PerFORM usingthe stereolithography process. The test vane is of a threedimensional design with a solidity around 0.5 and an aspect ratioof around 0.7. The endwall is contoured to have the passage areacontraction before the leading edge (LE) plane and a step expansionafter the trailing edge (TE) plane, respectively. The current testsection has a wide Plexiglas window coverage extended toaround 1.6Cax both upstream and downstream of the centervane’s leading and trailing edge planes. The three-window designis intended to divide the windows into smaller pieces to reducethe equivalent force that each window has to sustain during thehigh-speed flow. Also, reinforcement bars are installed to provideadditional coverage between the window interfaces. A blowdownfacility is used as the source of the mainstream. It consists of a highpressure compressor, a dryer, and a reservoir. An electronic flowcontrol valve is used to control the mainstream velocity by monitoring and adjusting the pressure in the upstream of the flow loop.The mainstream condition during the tests was continuouslymonitored using the Pitot-Static probes instrumented at 3.81Caxupstream and 0.75Cax downstream of the vanes leading and trailingedge planes, respectively. The probe readings from the downstreamwere used to define the nominal flow condition (exit Mach number) as shown in Fig. 2(a). A turbulence grid was used to create an estimated turbulence intensity of 12% [21] and an integral length scaleof 1.5 cm on the vanes’ LE plane. The grid is located at 7.61Caxupstream of the vanes’ LE plane. It has an equivalent bar widthof 1.43 cm and 54% porosity. The light-emitting device (LED)light and charge-coupled device (CCD) camera are arranged atboth upstream and downstream of the test section for data capturingunder different views 
Film Cooling System. There are two cooling hole rows on theendwall inside the passage, five cylindrical holes at the first row, and three cylindrical holes at the second row as shown in Fig. 2(b).Since the typical upstream portion of the endwall is protected by theinjection from the slot between the combustor and the vane, thecurrent two-row holes are intended to protect the endwall withinthe passage only. It is noted that the hole close to the pressureside at the second row is actually underneath the vanes’ TE dueto the special vane geometry. The diameter (d) of the coolinghole is around 0.14 cm. The injection angle is around 29-deg and17-deg for the first and second row, respectively. They are of different lengths and exit areas due to the curvature of the endwall. Asingle plenum is used to supply the coolant to all the endwallcooling holes. A metal screen and a honeycomb are installedinside the plenum to make sure the coolant pressure inside theplenum is uniform. The uniformity is further confirmed with twopressure taps instrumented on both sides of the plenum walls. Dryair from the compressor loop or the foreign gas cylinders were usedas the coolant. Three different kinds of foreign gas including N2(DR = 0.97 ≈ 1.0), CO2 (DR = 1.52 ≈ 1.5), and Argon/SF6mixture (DR = 1.93 ≈ 2.0) were used. The flowrate of the coolantis controlled using a rotameter, in which the reading is correctedusing the pressure at the rotameter exit 
Experimental Method. The conduction-free pressure-sensitivepaint (PSP) technique is used to capture the film cooling effectiveness on the endwall. PSP is a mixture of luminescent moleculesand oxygen permeable binder. The luminescent molecules in thepaint can be excited by using the light of a specific wavelength(400 nm) to illuminate the painted surface. The correspondingemission intensity from the painted surface is of a longerwavelength (650 nm). It can be recorded by the monochromescientific grade CCD camera (The Cooke Corporation, SensiCam,with 320 × 240 resolution, 16-bit dynamic range). A long-passfilter is attached to the camera to prevent the excitation lightfrom being captured.The PSP’s emission intensity is inversely proportional to theoxygen concentration of the painted surface (oxygen quenching).So, a proper calibration to quantify the variation of the emissionintensity to the oxygen partial pressure on the painted surface isneeded. A typical calibration can be realized using a pressurecontrolled chamber with optical access and a test coupon coatedwith PSP. The test coupon is a small strip of copper sprayedwith black paint, painted 4–5 layers (30–40 µm thickness perlayer) of PSP, and then placed in the chamber. The coupon hasone T-type thermocouple embedded and is attached to a heatingmat. During the calibration, the PSP-coated coupon, LED, andthe camera were arranged to a similar scenario as the real test.The emission intensity variation within the expected experimentpressure range was captured correspondingly. The oxygen partialpressure adjacent to the painted surface is found to be correlatewith the surface emission intensity by a power fitting curve asshown in Fig. 3. In addition, the sensitivity of PSP emission intensity to surface temperature and view angle are described inRef. [9].The center endwall was painted with PSP using an airbrush bythe same procedures as described in calibration. The area in thevicinity of the center endwall including the test vane and the neighbor vanes are painted black to reduce stray reflection. Four imagesets are required to determine the film cooling effectiveness for agiven flow condition. They are the black image, the referenceimage, the foreign gas injection image, and the air injectionimage. The black image set (Ib) is used to eliminate the backgroundnoise from the camera sensor. The reference image set (Iref) is theintensity taken with LED switched on but without flow(wind-off). Two wind-on image sets including the air injectioncase (Iair) and the oxygen-free foreign gas injection case (Ifg) aretaken under the same coolant flowrate with mainstream presents.The corresponding oxygen partial pressures are PO2,air and PO2,fg .One hundred images were recorded in each image set to averageout the measurement noise. 
Test Conditions
To systematically study the parametric effect of varying MFR,DR, and the exit Ma on the endwall film cooling, 33 sets of experiments were performed. Five different MFRs from 0.5% to 1.5%were conducted at the lowest exit Ma case to capture the generaltrend of the MFR effect on this particular endwall film coolingdesign. Three different MFRs from 0.75% to 1.25% were later conducted at higher exit Ma cases. For all the MFR cases studied, DR isvaried from 1.0 to 2.0. The summary of the test matrix is shown inTable 1.Experimental UncertaintyThe fluctuation of mainstream velocity (±3%) and the fluctuationof coolant flowrate (±2%) will contribute to the uncertainty of themeasurement in this experiment. However, the fluctuation of thePSP emission intensity value captured by the camera is consideredthe dominant uncertainty source. By the method proposed by Kline and McClintock [23] based on a 95% confidence level, theuncertainties of film cooling effectiveness deduced from the calibration curve at η = 0.1, 0.3, 0.5, and 0.7 are 15%, 3.9%, 1.7%, and0.7%, respectively, in the present study. The reported uncertaintyrefers to the local values as it is estimated on the raw data basedon the corresponding value of each pixel in the data capturing region.Results and DiscussionPressure and Isentropic Mach Number Distribution. Thestatic pressure distributions on the endwall under different mainstream conditions are shown in Fig. 4. The corresponding isentropicMach number is calculated using the total pressure measured from the inlet pitot-static probe. The individual color bar range is used fordifferent exit Ma cases to find the local variation of the pressure/Maon the endwall. The line (no data) within the passage on the contouris the junction line between the data captured from the upstreamview and downstream view. This line will be included in all the following contour plots. The static pressure, in general, is the highestat around the vane’s LE and then gradually decreases toward theTE. The corresponding isentropic Ma is increasing from LE toTE in the passage. The pressure on the pressure side of thepassage is higher than the one on the suction side, this pressure difference is the source of the crossflow inside the passage and canaffect the coolant distribution. Since the downstream pressure monitoring point (Pitot-Static Probe) is actually out of the passage (flowdecelerates due to further channel expansion) as shown in Fig. 2(a),the isentropic Ma on the endwall especially at the downstream ofthe passage is found to be consistently higher than the nominalflow condition. For exit Ma = 0.9 and 1.0 case, there is a narrowsector-shaped static pressure recovery band (downstream of thedotted line in the close-up dashed box) that starts at right upstreamof the second cooling hole row, which is not observed in lowest exitMa cases. This abrupt Ma change is also revealed in Fig. 4(c). Thiscould be the oblique shock wave originating from the neighborvanes’ TE, as its location is considered similar to the previousstudy [24] based on the same vane’s geometry. However, whetherthe shock wave might affect the endwall film cooling or not willbe discussed in the Mach number effect.Blowing Ratio and Momentum Flux Ratio. For all the MFRsand DRs in this study, the row-averaged blowing ratio and the corresponding momentum flux ratio are calculated. A constant discharge coefficient is first assumed for all the cooling holes atthe first and second rows. With the plenum pressure measuredby the pressure tap, the endwall static pressure and isentropic Ma distribution measured by PSP, the discharge coefficient canbe calculated as 
The local coolant velocity can thus be obtained to calculate thetrue coolant mass flux. The row-averaged blowing ratio andmomentum flux ratio are shown in Table 2.As the coolant from the first and second cooling hole rows aresupplied by a single plenum, it is expected that there will be a relatively small amount of film that comes out from the first row compared with the second row due to the endwall static pressuredistribution. However, since the mainstream mass flux at thesecond row is higher than at the first row, the averaged blowingratio on the second row is found to be lower than the first row formost of the test conditions. 
Mass Flow Ratio Effect. 
DR = 1.5 at exit Ma = 0.7 cases areselected to demonstrate the MFR effect. As shown in Fig. 5,coolant turning can be found for both cooling hole rows as thetrace directions do not exactly follow the hole exit directions. TheMFR effect on the first and second cooling hole row is different.Overall, the current two-row design seems to provide sufficientcoolant protection for the majority of the current endwall passage.The film coverage from the first row demonstrates an increasingthen a decreasing trend when the MFR increases from 0.5% to1.5%. Best film coverage is found to be when MFR = 0.75%.When the MFR increases beyond this point, the shrinking of thecoolant trace right at the downstream edge of the cooling hole indicates the onset of coolant lift-off. Also, it is found that the coolantlift-off does not uniformly happen in every hole at the first row. Butthe surface static pressure does not demonstrate a similar nonuniformity at the vicinity of the first row. One potential reasoncould be that non-uniformity is actually the result of the timeaveraged unsteadiness of the film due to mainstream turbulence.This will require further investigation of the film’s unsteady behavior. On the other hand, the film coverage from the second rowdemonstrates an increasing trend when the MFR increases to1.0% but then gradually levels off for even higher MFR. This behavior difference between cooling hole rows can be attributed to threereasons. First, at the location of the first row, the suction side leg ofhorseshoe vortices from the vane’s LE and the corner vortex fromthe vane’s pressure side are still very close to the surface. So, thefilm from the first row is very susceptible to lift-off. Second, thesmaller surface angle of the second row can make the coolinghole performance less sensitive to coolant lift-off since the coolant’svertical (wall-normal) momentum is reduced compared with thecounterpart of the steeper surface angle (first row). Third, the longer cooling hole length and slightly larger cooling hole exit areadue to the shallow surface angle can lead to a more uniform exitvelocity profile where it is beneficial to have higher effectiveness.Besides, the coolant deflection due to the mainstream is reducedwhen MFR increases (higher coolant momentum). But this behavioris not prominent for trace from the second row which implies thestrong mainstream still dominates the local flow field.Figure 6 shows the laterally averaged effectiveness when DR =1.5 at exit Ma = 0.7. It is worthy to mention that in this study, thenoise level on the region upstream of the first row is not always negligible and does not correlate to the variation of any parameter inthis study. While this noise level varies from run to run, theauthors confirmed that it does not affect the results (magnitude)for the region covered with film traces (X/Cax ≥ 0.3). To preventconfusion in interpreting this noise as the true effectiveness, theupstream of the first row (X/Cax < 0.3) is excluded from all thelateral averaged effectiveness plots in this study. For the upstreamregion (X/Cax < 0.68) which the coolant only comes from the firstrow, it is clear that MFR = 0.75% case has the highest averagedeffectiveness. For higher MFR, the decreasing trend is alsovisible. For the downstream region (X/Cax ≥ 0.68), since the filmeffectiveness contributed by the second row is dominant, an increasing trend then being leveled off averaged effectiveness trend can bedetected similar to the observation in the contour plots. There aresome film (coolant) carryover even after the trailing edge plane,but the data there suffer from the reflection due to the step expansion, so the data after X/Cax > 1.1 are not included in this study.Density Ratio Effect. DR = 1.0 and 2.0 at exit Ma = 0.9 casesare selected to demonstrate the DR effect as shown in Fig. 7. Itshould be noted that, for a given cooling hole design under afixed endwall pressure distribution, the required differential pressure to deliver a given amount of coolant decreases when thecoolant density increases. This implies that the heavier coolant actually is less likely to being ejected from the holes where the mainstream static pressure is high. This is typically the case when thecoolant flowrate is low (so is the corresponding blowing ratio). Ata given coolant flowrate, the heavier coolant also possesses lowermomentum. This is great for better lateral coolant spreading, butthe correspondingly low streamwise momentum can lead to insufficient film coverage in the streamwise direction. However, in thisparticular endwall cooling design, a positive DR effect is observedwhen MFR is as low as 0.75%. This implies that the momentum ofthe heavier coolant is sufficient enough to have a satisfactorystreamwise film coverage. On the other hand, it is found that thewide coolant trace when DR = 2.0 does not demonstrate sufficientoverlapping, leaving the portion between holes unprotected. This could be easily improved by using the shaped hole in the firstrow. The short coolant traces from the second row when DR =2.0 simply implies that the low coolant momentum is inadequateto form a proper film coverage. When MFR increases to 1.0%,the film coverage from the first row reduces but the one from thesecond row enhances. The momentum of the heavier coolant islower compared with the lighter coolant, which is a great advantagein terms of having film stay adhered to the surface instead of lift-off.When MFR is further increased to 1.25%, the coolant lift-off isinevitable particularly for the film from the first row since thecoolant traces are shrinking for both DRs. Nevertheless, a widertrace is still observed when DR = 2.0 compared with DR = 1.0, indicating a positive DR effect.Figure 8 shows the laterally averaged effectiveness for DR = 1.0and 2.0 at exit Ma = 0.9. The trend of averaged effectiveness variation is found to be consistent with the contour plots. The behaviorof the DR effect on the averaged effectiveness from the first rowand second row is found to be the same. This reiterates the fact thatwhile the DR effect can be negative or marginal for the lowcoolant flowrate, within the tested MFR range, a positive DR effectfor the current endwall cooling design can be expected since thecoolant has sufficient momentum (flowrate is relatively high).Mach Number Effect. The performance of the different coolinghole rows is supposed to vary based on the local endwall surface pressure (subjected to the Ma). This will cause the coolant split (redistribution) and affect the film distribution between two rows. So, theobserved coolant behaviors when varying the Ma are considered ascombined characteristics instead of purely from flow compressibility. Three different MFRs when DR = 1.0 and 2.0 casesare selected to examine the Ma effect as shown in Figs. 9 and 10. Thefilm from the first row and second row shows different Ma effects. Inaddition, different MFRs are found to demonstrate different Maeffects as well. For the film from the first row, it is found that thefilm coverage in general increases with the increase of the exit Ma.As the exit Ma increases, the mainstream velocity at the regionnear the first row also increases. This higher mainstream velocityresults in a thinner boundary layer around the first row. The corresponding higher near-wall velocity facilitates the film bendingtoward the wall, relieves the coolant lift-off, and eventually leadsto better film coverage. The positive Ma effect on the film effectiveness is more prominent when the coolant flowrate is high. This phenomenon is consistent with the finding reported by Shiau et al. [24]and Saumweber and Schulz [25]. For the film from the second row,the film coverage variations when exit Ma increases are found to berelated to the corresponding MFR. When the MFR = 0.75%, theeffectiveness is found to decrease significantly when exit Maincreases. But when MFR = 1.25%, the film coverage at exit Ma =1.0 outperforms the Ma = 0.9 case. From Fig. 4(a), when exit Ma = 1.0, the local static pressure rises when flow across the shock wave is observed (narrow high-pressure band upstream of the secondrow). According to Ligrani et al. [13], compared with the casewithout the shock present, the flow deflection occurs across theshock can locally reduce the effective blowing ratio and forcelarger film concentration near the surface. From Figs. 9 and 10, thelarger film concentration near the surface is indeed observed as thefilm for higher exit Ma seems to diffuse and fills the spacingbetween the holes compared with the lowest exit Ma. In addition,as the traces from the second row do not seem to be affected significantly by the endwall flow field (passage vortex), the argument fromLigrani et al. [13] seems to be applicable to the scenario in the presentstudy as well. For low MFR (e.g., 0.75%), the shock reduces theeffective blowing ratio by increasing themainstream local static pressure, which leads to insufficient coolant momentum and thereforepoor film coverage. On the other hand, for the higher MFR (e.g.,1.25%), the effective blowing ratio reduction by increasing the mainstream local static pressure and deflecting the jet could indeedachieve improved effectiveness by relieving the coolant lift-off tendency. However, this might also lead to higher effectiveness in thevicinity of the hole as mentioned earlier, but the film is incapableto propagate much in the streamwise direction.Figures 11 and 12 show the laterally averaged effectiveness forDR = 1.0 and 2.0 at different exit Ma, respectively. In general, the averaged effectiveness is increasing when exit Ma increasesfor X/Cax < 0.68; for X/Cax ≥ 0.68, a decreasing then increasingeffectiveness trend is observed when exit Ma increases. It is alsofound that exit Ma = 0.7 cases have the highest averaged effectiveness for X/Cax ≥ 0.68, but this advantage is diminishing when theMFR increases since the coolant lift-off is weakened by the shockwave when MFR is high 
Matching the Momentum Flux Ratio. 
As the coolant momentum dominates the film behavior, the momentum flux ratio is beingconsidered as the quantity to combine the blowing ratio and density ratio for film performance comparison, especially in the flat platestudy. Based on the row-averaged momentum flux ratio inTable 2, selective cases with the matching momentum flux ratioare compared to evaluate if it can portray the resulting effectswhen the MFR, DR, and exit Ma are varying.For the first row, only two cases when exit Ma = 0.7 have thematching I ≈ 2.8 as shown in Fig. 13(a). It is found that while thefilm distribution seems similar, the traces in higher DR cases are relatively diffused. For the second row, eight cases for exit Ma rangingfrom low to high are having the matching I ≈ 0.3 as shown inFig. 13(b). It is found that except for the cases when exit Ma =0.9, the coolant traces in higher DR cases are wider which meansstronger lateral spreading. In other words, with the same momentumflux ratio, different DR results in different coolant diffusion scenarios. Moreover, it is found that the coolant spreading from the secondrow for high exit Ma is more prominent than the low Ma cases,which is considered due to the shock as discussed earlier. Therefore,it is reasonable to conclude the momentum flux ratio effect is unableto properly describe the Mach number and the DR effects observedin this study 
Area-Averaged Effectiveness. 
The area-averaged effectivenesson the endwall for all the test cases in this study is shown inFig. 14. The endwall is divided into two zones, Zone 1 is fromthe upstream edge of the first row to the upstream edge of the second row; Zone 2 is the region at the downstream of Zone 1.Since there is no coolant coverage upstream of the first row,those areas have been excluded from Zone 1, which means thearea-averaged value will be inherently higher than the laterallyaveraged one. For Zone 1, MFR = 0.75% seems to be anoptimum flowrate for exit Ma = 0.7, when exit Ma increases, adecreasing trend on effectiveness when MFR increases is observedfor exit Ma = 0.9, but the trend becomes to decrease then increasesfor exit Ma = 1.0. For Zone 2, an increasing than being level off oneffectiveness is observed for exit Ma = 0.7, but the trend becomesto increase then decrease for higher exit Ma. The trend for Zone 1and Zone 2 is found to be the opposite when just comparing theresults at MFR = 0.75%, 1.0%, and 1.25%. This could be attributed to the single plenum design as the film coverage variationwhen MFR changes is shifting between the coolant contributedfrom the first and second row.
Conclusions
A parametric study on the effect of MFR, DR, and exit Ma on theendwall film cooling effectiveness measurements was performed ona five-vane annular sector cascade. The film cooling performanceunder different conditions is compared and discussed with regardto detailed film cooling effectiveness contours, laterally averaged,as well as the area-averaged effectiveness. The key highlights ofthe presented study are as below:(1) For this particular endwall film cooling design, increasingMFR has an increasing then a decreasing effect on the filmeffectiveness from the first row; for the film from thesecond row, an increasing then leveled off trend on thefilm effectiveness is observed.(2) Within the tested MFR ranges, a positive DR effect on thefilm effectiveness is observed for both the first and secondrows on the present endwall cooling design.(3) The Ma effect is positive for the coolant from the first row,for the coolant from the second row, the Ma effect dependson the MFR. Negative Ma effect is observed when MFR islow and the positive effect is demonstrated when MFRincreases. Due to the cooling hole location, the film fromthe second row does not seem to be affected by theendwall flow field, so the effect of the shock wave on thefilm is similar to the flat plate test. Because of the plenumdesign, in addition to the flow compressibility, the Maeffect in this study includes the impact from the coolant split.(4) The momentum flux ratio is unable to correlate the film behavior for different flow conditions (varying MFR, DR, exitMa) as the case in the flat plate test.(5) The variation trend of area-averaged effectiveness on Zone 1and Zone 2 at different exit Ma is different. Especially forexit Ma = 0.9 and 1.0, a reversed trend is observed forZone 1 and Zone 2. Validation of a NumericalQuasi-One-Dimensional Modelfor Wave Rotor Turbines WithCurved Channels 

Abstract
A wave rotor is a shock-driven pressure exchange device that, while relatively rarelystudied or indeed, employed, offers significant potential efficiency gains in a variety ofapplications including refrigeration and gas turbine topping cycles. This paper introduces a quasi-one-dimensional (Q1D) wave action model implemented in MATLAB for thecomputation of the unsteady flow field and performance characteristics of wave rotors ofstraight or cambered channel profiles. The purpose here is to introduce and validate arapid but reliable method of modeling the performance of a power-generating wave rotorwhere little such insight exists in open literature. The model numerically solves the laminar one-dimensional (1D) Navier–Stokes equations using a two-step Richtmyer time variation diminishing (TVD) scheme with minmod flux limiter. Additional source termsaccount for viscous losses, wall heat transfer, flow leakage between rotor and stator endplates as well as torque generation through momentum change. Model validation wasconducted in two steps. First of all, unsteady and steady predictive capabilities weretested on three-port pressure divider rotors from open literature. The results show thatboth steady port flow conditions as well as the wave action within the rotor can be predicted with good agreement. Further validation was done on an in-house developed andexperimentally tested four-port, three-cycle, throughflow microwave rotor turbine featuring symmetrically cambered passage walls aimed at delivering approximately 500 W ofshaft power. The numerical results depict trends for pressure ratio, shaft power, and outlet temperature reasonably well. However, the results also highlight the need to accurately measure leakage gaps when the machine is running in thermal equilibrium. 

Introduction
Over the past decades, numerous research studies have beendedicated toward the investigation of dynamic pressure exchangemachinery, such as wave rotors. These devices use the energy carried in moving shock waves to transfer energy from one stream offluid to another without the need to incorporate additionalmechanical parts. This advantage, in combination with relativelylarge pressure ratio gains through shock wave compression and ahigh efficiency associated with this process, renders wave actiondevices an attractive technology for power generation.Opposed to crypto-steady flow devices, such as turbomachinery, wave rotors are inherently unsteady flow devices, whereshock and expansion waves travel along discrete channelsarranged around the circumference of a cylindrical drum. To eachside of the spinning rotor, there are stator endplates containingport openings, as shown in Fig. 1. Exposing the rotor channelsperiodically to the ports then triggers shock and expansion waves.The application range for wave rotors outlined by the literatureis diverse. The bulk of early studies focused on pressure exchangers with straight passage profiles for gas turbine topping cycles[1–8] and supercharging devices for internal combustion engines[9–16]. In recent years, the application to refrigeration cycles[17–19] and pressure-gain combustors [20–22] has come into thefocus of consideration.In comparison, little amount of attention has been given towave turbines with cambered passage walls aimed at acting bothas pressure exchangers while producing shaft power through momentum change of the flow [23–27]. Documentation of theseendeavors is unfortunately fragmentary. The best documented andmost successful example of a wave rotor engine was done byPearson [23,24] at the University of Bath. Initial tests were conducted on a single cycle, through-flow wave rotor with helicalpassage shape leading to a power output of around 26 kW at arotational speed of 18,000 rpm. Further research was suspendedafter the engine was destroyed due to overspeeding. Furtherexperiments by General Electric and General Power Corporationyielded insufficient shaft power generation and were not furtherpursued [25,26] 
One limitation of early efforts on wave rotor design was partially due to a lack in computational power rendering performanceestimation time consuming (in particular when done by hand calculations) and often inaccurate. To mitigate this and to accuratelycompute unsteady wave action in the rotor channels and steadyconditions in the ports as well as wave rotor performance parameters, one-dimensional (1D) codes have established themselves asswift and reliable tools. This has been pursued by a number ofinstitutions, ranging from the Naval Postgraduate School [28–32],University of Tokyo [6,33,34], ONERA [6,35,36], Michigan StateUniversity [37], NASA Glenn Research Center [38–42] as well asXiamen University/Beihang University [43,44] most recently.While most of the mentioned studies deal with straight channels, there is merely one study that incorporates passage curvaturein a one-dimensional environment, albeit at reduced order througha passage-averaged description. This was conducted at NASAGlenn Research Center by Welch and Paxson [42] and comparedport axial and tangential velocities as well as predicted power output to a two-dimensional computational fluid dynamics (CFD)simulation. However, no comparison with experimental data wasdone.Against this background, this paper aims at addressing this gapby introducing a one-dimensional model that allows a reliable andcost effective insight into the performance of wave rotors withstraight and arbitrarily shaped camber. To the authors’ knowledge,this paper will present, for the first time, a quasi-one-dimensional(Q1D) model that allows direct computation of torque output forcambered wave rotor channels without the need for further modelreduction. It is also the first time such a model has been comparedto experiments. Conventional 1D models for turbines and compressors employ either a map-based approach that requires a prioriexperimental data in the form of mass flow rate, efficiency, andpressure ratio [45] or model the rotor wheel through an adiabaticpressure loss that is calibrated over the anticipated flow rangeagainst experimental data [46]. The proposed model, however,differs to that approach as directly models the unsteady waveaction dynamics within the rotor while accounting for finite passage opening, friction, leakage, and inviscid forces.The structure of the paper is thus as follows: first, the governingequations and model source terms for viscous and inviscid forces,flow leakage, and wall heat transfer are introduced; second, steadyand unsteady validation of the code is done based on experimentaldata from the open literature on pressure dividers done by Kentfield [47] and NASA [48]. Further validation of the model isachieved through experimental data from a wave rotor turbineexperiment performed in the gas stand at the University of Bath.The wave rotor turbine features a symmetrical, arc-shaped passage design that was designed to produce a power output of up to500 W.Simulation Model
The model was implemented in MATLAB R2017 and follows asingle wave rotor passage of constant cross section as it passesports along the circumference. It consists of numerical routines tosolve the one-dimensional conservation equations accounting forchannel curvature for shaft power extraction, heat transferbetween fluid and rotor walls, flow leakage in the axial clearancebetween stator and rotor, as well as gradual passage openingeffects. The code can be applied to both through-flow and reverseflow wave rotor applications.Finally, the user has the option of defining the gas compositionfor the operating medium. Throughout this study, this is assumedto be air, composed of 79% nitrogen and 21% oxygen.For the derivation of the model, a number of assumptions aremade; first of all, in order to justify a one-dimensional formulation, channel length is expected to be an order of magnitude largerthan channel width and height. Furthermore, the flow path followsa single streamline through the passage. The quasi-onedimensional formulation assumes that all quantities are uniform across the cross section. This does not hold for wave rotors, ashigh rotational speeds due to centrifugal effects, vortices due tofinite passage opening effects, and shock-wave boundary layerinteractions create secondary flows and skewed air/gas demarcation surfaces, which cannot be captured using one-dimensionalwave action codes. While the effect of gradual passage openingon the primary shock strength can at least be qualitatively captured, interferences between neighboring channels and betweenthe channel and leakage cavity, which become particularly pronounced for larger axial clearances between rotor and stator, cannot be depicted directly.Governing Equations. The equations used to describe theunsteady, compressible, and viscous effects taking place within awave rotor turbine are the one-dimensional Navier–Stokes equations, which can be written in conservative form as 
The first term represents time-dependent variations, while the second term refers to advection. The source term S accounts for viscous as well as inviscid effects (i.e., friction and “blade” forces),as well as leakage losses and wall heat transfer. These will beexplained in more detail in the section Source Terms. The statevector U and the flux vector F are defined as 
Heat conduction within the fluid is addressed through Fourier’slaw of heat conduction. Furthermore, friction between particles isincluded through shear force. They can be more explicitly statedas 
The set of PDEs encompasses four primitive variables u, q, p, ande, which require the introduction of an additional equation toensure closure of the equation system. This is done by the idealgas equation relating pressure, temperature, and density 
The model further treats air as a calorically imperfect gas, wherespecific heat constant at constant pressure cp ¼ f(T) varies withtemperature. While this is less important for pressure exchangers,where temperatures can be close to ambient conditions, itbecomes more important at elevated temperatures exhibited in gasturbines, where the peak cycle temperature delivered from thecombustor exceeds 450–500 K [49]. Thermodynamic data for specific heat constant and internal energy used throughout this studystem from GRI-MECH [50] and the thermal database provided byVirginia Tech [51]. Finally, dynamic viscosity is modeled throughthe well-known Sutherland relation.Source TermsViscous and Inviscid Forces. The previous one-dimensionalmodels addressed merely viscous (friction) forces and did notdirectly address inviscid profile forces that account for shaftpower generation. Fluid viscosity determines wall friction andpromotes convective heat transfer. As a result, the correspondingsource term affects both the momentum and the energy equation.Friction force is defined as 
It features a friction multiplier Cf that accounts for additionalmomentum losses that cannot be captured with the relatively simple approach taken. As stated by Winterbone and Pearson [52],the friction factor f is a function of the Reynolds number withinthe channel and is given by 
where k denotes the surfaces roughness value, which was set to25 lm. Using the force diagram shown in Fig. 2(c), one can workout the tangential component of the friction factor and write thetotal source term as 
Torque generation in wave rotors is pulsatile in nature and is primarily generated when the channels are exposed to a high pressure inlet port. Throughout this study, tangential forces and thuspower generation are computed as a result of the momentumchange as the fluid travels along the cambered channel walls. 
Overall, this is exemplified by the change in velocity triangles atleading and trailing edge of a passage wall, as shown in Fig. 2(a).The corresponding blade force source term affects momentum andenergy equation and takes the form 
The tangential blade force per unit volume can be determinedusing the change in momentum, which is influenced by the difference in absolute tangential velocity Ch across leading and trailingedge, the mass flow rate through the channel m_ , and the channelvolume V. In the discretized domain, this force is calculated foreach element taking the difference in the tangential velocity vector from one cell to the next into account 
Friction forces are accounted for by the previously suggested viscous source term. Thus, the blade force source term acts in aninviscid way so that one can assume the blade force vector to beorthogonal to the relative velocity vector, as given in Fig. 2(b).Hence, the scalar product of blade force vector and relative velocity vector equals zero and the equation can be rearranged to givethe axial inviscid blade force per unit volume 
In reality, the port angles will be aligned at a different angle thanthe passage walls angle. This can take place at off-design conditions or if an additional momentum change is desired at the statorrotor interface in order to create more torque. In addition, flowseparation on the passage wall “suction side” can often be witnessed if the port angle is considerably larger than the passagewall angle. To account for such incidence losses, an additionalentropy-based loss coefficient is introduced and applied in theform of a distributed loss factor. It can be calibrated throughexperimental data or three-dimensional CFD simulations 
Flow Leakage. 
The effect of leakage is a crucial factor in waverotor performances and was modeled as a simple, nonlabyrinthleak in similar fashion as in the previous publications from Kentfield, NASA, and ONERA [35,39,53]. Leakage is modeled as alumped capacitance model assuming steady flow equations. Itconcerns both continuity as well as energy equation, so that theleakage source term may be stated as 
where CD is the discharge coefficient, p and q the pressure and thedensity in channel or cavity, respectively, h denotes enthalpy, dlthe axial clearance between rotor and stator, and Dz the cell sizeused for discretization. The leakage function U takes up the form,which allows to differentiate between flow entering and leavingthe cavity depending on the pressure ratio in cell and leakagecavity 
The source terms are only active at the extremities of the rotorchannel and are set to zero in the remainder of the domain. Aftereach cycle, the mass and energy balance are computed, and thecavity pressure and temperature updated from a first-order systemof differential equations as provided by Eq. (16). The equationsdesignate continuity and the energy equation derived from the firstlaw of thermodynamics and are integrated in time using anexplicit Runge–Kutta scheme. Cavity pressure can be found byapplying the equation of state for ideal gases 
The heat transfer term stands for the heat transfer from the waverotor wall temperature. For the respective heat transfer coefficient,a Nusselt number relation for annuli with inner cylinder rotationwas used [54] 
Wall Heat Transfer.
 The viscous source term given in Eq. (7)features the wall temperature to compute convective heat transfer.During operation and constant inlet temperatures from the ports,the rotor temperature settles at a constant value. In order toinclude the effect of wall heat transfer, a lumped capacitancemodel that follows the first-order differential equation given inEq. (18) was implemented 
Domain Discretization.
The explicit, second-order accuratescheme of Richtmyer in combination with a minmod flux limiterof Roe and Baines was selected for the discretization of the governing equations in space and time. This allows for an accurateresolution of flow discontinuities while obeying time variationdiminishing criterion and preventing spurious oscillations fromtaking place in their vicinity. The proposed scheme suggests atwo-step technique where additional half timesteps are introduced.The first step consists of a first-order accurate Lax-Friedrichsmethod, which can be obtained through the integration of Eq. (1)in space and time and assuming the intercell fluxes to be the average of two consecutive cells 
Boundary Conditions
For the implementation of boundary conditions, a cell-centeredapproach has been chosen. This involves the addition of an imagecell lying just outside of the domain. In general, one can distinguish between inflow, outflow, and wall boundaries. For each ofthese types, one needs to determine the flow variables on theimage cells. Walls are treated as reflective boundaries where pressure and density are equal to the neighboring interior cell node,while velocity is assigned the same value with the opposite sign.Assuming subsonic inflow, it is necessary to specify two characteristics entering the domain, while one characteristic leaves thedomain. Therefore, stagnation properties (temperature and pressure) are imposed, while velocity is extrapolated in the zerothorder. This approach further guarantees a simple way to compareresults with experimental conditions. When computing the flowvariables at the image cell, it is crucial to consider that the stagnation properties are defined in the absolute frame of reference,while the code operates in the relative frame of reference.For outflows, one characteristic enters the domain, while twocharacteristics leave the domain. Thus, a single physical conditionneeds to be imposed (static outlet pressure) and the remaining twoare extrapolated from the domain interior.Unfortunately, this is only valid for fully exposed channels.There are, however, periods where only a certain portion of thechannel is exposed to a port, as shown in Fig. 3(a). This gradualpassage opening is of paramount importance in the formation ofprimary shock waves [55]. To account for this, a function is defined that determines the amount of overlap based on port opening and closing positions, channel width, and the channel positionat each time-step.If this function is between zero and one, the port is only partially open and an alternative solution for the boundary conditionsis calculated. In the case of inflow, the principle is given in Fig.3(b). The gas enters the domain from a reservoir described bystagnation properties for pressure and temperature in the form of ajet through the cross section area at station 2. At station 1, the jetfurther expands to cover the entire width. The first cell in thedomain is at station i. Assuming steady flow conditions, one canformulate the three conservative equations for mass, momentum,and energy between stations 1 and 2 as 
giving three equations for six unknowns in total. The set of equations is closed using the energy equation and isentropic relationbetween stations 0 and 1 as well as wave and pathline compatibility relations [52]. In a similar fashion, one can formulate the problem for partially open outflows, as given in Fig. 3(c). Here, gasflowing through a cross-sectional area of A1 exits the domainthrough area A2. In the subsonic case, the pressure at station 2equals the reservoir pressure at plane 2. Together with the conservation of mass and energy as well as the isentropic relationbetween stations 1 and 2 and the corresponding compatibilityequations, all six unknowns can be determined.ProcedureThe solution procedure of the code shall be introduced usingthe example of a four-port throughflow wave rotor, as shown inthe unfolded view of a single wave rotor cycle in the z t/h planein Fig. 4. The ports are designated as high pressure gas inlet(HPG), high pressure air (HPA), low pressure air (LPA), and lowpressure gas (LPG), referring to high pressure gas and air and lowpressure air and gas, respectively.It follows the steps outlined in Fig. 5(b). Initially, geometricdimensions of the rotor and target rotational speed of the waverotor are specified. Furthermore, the number of cycles per rotationand the port solution needs to be provided. An initial guess andrough layout can be devised through the analytical tools given by Chan and Liu [43] and M€ uller and coworkers [56–58]. Subsequently, all matrices used throughout the computation are, alongwith the spatial domain, initialized. In terms of the time-step, aninitial estimation is provided. While a uniform grid is used, thetime-step size is allowed to vary depending on the maximumwave speed maxi_x0007_  jun i j þ an i in the domain at the previous timestep and a fixed CFL number of 0.7 [52]. This allows us to savecomputational time as otherwise a constant time-step would needto suit the largest velocities in the domain and would thus have tobe more on the conservative side.In open loop configuration (without a closed combustor loop),stagnation properties in the HPG port are fixed, while static pressure in the HPA port is varied automatically using a simple proportional controller based on a simple Bernoulli relation until themass flow rates are matched and within a 2% relative error. Thelow pressure inlet conditions generally involve ambient temperature of around 300 K, while the total inlet pressure again beingvaried in the same manner as for the HPA port and according tothe desired loop flow ratio 
In the low pressure gas port, exhaust gases are expelled to theambient and the static pressure is again automatically adjusted togive the desired inlet mass flow rate m_ HPG. At the end of eachcycle, combustor and leakage cavity properties are updated andport flow conditions need to be determined. This is done through mass flux averaging according to Eq. (25) and converting the values into the absolute frame of reference 
Before the next cycle commences, conditions within the channelat the cycle end are set as initial conditions for the subsequentcycle, thus guaranteeing periodicity. Finally, the simulation isstopped either when the maximum number of cycles is reached orthe relative changes of combustor and leakage temperature andpressures are below a predefined threshold 
Results and Discussion. 
The results presented in the sectionResults and Discussion seek to exhibit the code’s ability to predictresults from wave rotor experiments. This shall be done in twostages. First of all, performance data of two three-port pressure dividers are produced and compared with data from the open literature. These are Kentfield’s pressure divider from the 1960s [47]and NASA pressure divider from the mid-1990 s [48]. After that,laboratory experiments conducted at the University of Bath on al-wave rotor turbine are used to test the code when dealing withfour port, throughflow wave rotor turbines with symmetricallycambered profiles.Wave Rotor Pressure Dividers. The geometric dimensionsand operating conditions of the two investigated designs are givenin Table 1. The main characteristics of both designs shall bebriefly introduced. Kentfield’s device is characterized by relatively large channel width, while NASA’s three-port design hasfour times the number of channels and has, as a consequence, arather small channel width. Thus, it can be expected that finiteopening timing effects are of minor importance. In terms oflength, the NASA design is longer, so that viscous losses becomemore pronounced. Leakage plays a reduced role in Kentfield’sexperiments owing to tighter clearances between the rotor and stator endwalls.Rotational speed of both designs and total inlet temperature inthe medium pressure port are fixed in both simulations. Total inletpressure in the medium pressure port is variable for the Kentfieldexperiments. In the model, the total outlet pressure of the highpressure port is varied in both cases to give a desired mass flowratio m_ M=m_ H ranging from 0.1 to 0.6. The NASA experiment wasconducted at a constant mass flow ratio of 0.37. The unfoldedmidplane view for the two pressure exchangers with the anticipated wave pattern and relative positions of the three respectiveports is given in Fig. 6. Solid red curves denote shock waves,hatched blue areas expansion fans, and dashed gray lines (weak)pressure waves.Using the MATLAB code on Kentfield’s three-port pressuredivider yields the results depicted in Fig. 7. All data were produced with a grid resolution of Dz/L ¼ 0.0135 and a dischargecoefficient of 0.67 and a friction multiplier of 1.77. The plotsshow total pressure for the high pressure port on the ordinate andtotal pressure for the low pressure port on the abscissa for four different mass flow ratios. All values are nondimensionalized withrespect to the inlet total pressure and inlet mass flow rate, respectively. Figure 7(a) shows a comparison of the 1D-Navier–Stokesequation (that includes particle friction and heat conduction in thefluid but without wall heat transfer, friction, and leakage model)compared with the experimental data. Clearly, the pressure levelin the high pressure port is overpredicted with an increasing deviation toward lower pressure levels in the low pressure port. The full model results are then shown in Fig. 7(b) outlining the effectlosses impart. The main loss mechanisms responsible for the difference are leakage and friction, confirming the findings of theprevious studies [35,39]. Since maximum temperatures in bothcases are relatively low, wall heat transfer plays a minor role.The same model parameters were then applied to performunsteady validation using the NASA three-port wave rotor. Thisexperiment featured static pressure traces at three locations,namely at z/L ¼ 0.025, z/L ¼ 0.5, and z/L ¼ 0.975, within a waverotor passage as it travels through the circumference. Evaluatingthe numerical results and normalized static pressure at the channelends, namely at data against the numerical results, gives the distribution shown in Fig. 8. In comparison with Fig. 6(b), one can witness the initial expansion fan generated upon opening the lowpressure port on the right-hand side. This is well captured in themodel, although the expansion ratio at z/L ¼ 0.025 remainsslightly overpredicted. The subsequent shock discontinuity issharper than in the experiments and features a sharp peak for bothprimary and secondary shock waves. The overall shock pressureratio across all stations is, however, well depicted. The finalexpansion fan as well as the attenuation of the pressure wave takes place slightly sooner, as seen at around 
Bath l-Wave Rotor TurbineLayout and Operating Conditions. The third validation casedeals with a four-port, three-cycle throughflow wave rotor withsymmetrically cambered wall profiles. The rotor was designed toyield approximately 500 W of shaft power output at a target peakcycle temperature of 750 C and HPG total inlet pressure of285 kPa. The port solution is shown in Fig. 4 and can be dividedinto a high pressure and low pressure section in the bottom andtop, respectively. The former encompasses a HPG and a HPA outlet that would form the combustor loop in a gas turbine arrangement, while the latter houses the LPA inlet and a LPG outlet . Adetailed view of the symmetrically cambered wall profiles and aphotograph of the rotor and endplates with the corresponding portopenings are given in Figs. 9(a) and 9(b).Geometric dimensions and operating conditions that will beused for the simulations are given in Table 2. The rotor has a midwidth diameter of 60 mm, length of 30 mm, and features 46 symmetrically cambered channels with a maximum profile angle of21.5 deg. In total, three cycles are covered in a single rotation andthe design speed is set to 32,000 rpm, giving a tangential speed inthe midwidth plane of 100 m/s.The wave rotor test rig encompasses an open loop configuration, as shown in Fig. 10(a). A set of industrial compressors provides pressurized and dry air to the inlet side (HPG and LPA).The corresponding flow rates and thus the flow ratio k are controlled through a set of pneumatically actuated gate valves andmeasured through differential mass flow meters. On the HPG leg,the incoming air is additionally directed through an air-to-air heatexchanger to use hot exhaust gases from the outlet side beforebeing run through a set of 44 kW-electrical heaters that ensure thetarget inlet temperature is reached. Rotational speed and shaftpower output is measured through an eddy-current dynamometerthat modulates the load on the wave rotor. It is coupled to thewave rotor through a single-plane coupling. On the outlet side,further gate valves ensure the mass flow rates in the high pressurezone, i.e., between HPG and HPA, can be matched accuratelybefore being expelled from the test chamber through extractionfans. Figure 10(b) exhibits a photograph from the test chamberdisplaying the electrical heaters as well as the wave rotor anddynamometer units and outlet gate valves.The design of the wave rotor encompassed a variable nominalaxial clearance between 0.1 and 0.4 mm. Unfortunately, it wasfound during the experiments that the nominal clearance differedsignificantly from its design value due to uneven thermal expansion. As the exact values of the actual clearances could not bedetermined, it was decided to use the nominal values for throughout the validation study.Rotor Characteristics. Before moving on to the results, it isnecessary to classify the wave rotor performance with respect toother existing machines. Table 3 lists a comparison of nondimensional performance parameters for finite passage opening T, viscosity F, and leakage flow G for various wave rotor types, asdefined by Nagashima et al. [6] and Wilson and Fronek [48].Although comparison is somewhat difficult for pure pressureexchangers and wave rotor turbines operating at high temperatures, one can see that the Bath l-wave rotor performs similarlywith respect to finite opening timing effects despite its short rotorlength. As expected, due to the small size compared with largerdesigns, viscous losses become more pronounced and is similar tothe microwave rotor study conducted at the University of Tokyo& ONERA [6]. The most critical loss mechanism for the investigated design is without a doubt leakage effects, which is at bestsimilar to the ABB Comprex and the University of Tokyo. In reality, however, it is anticipated that uneven thermal expansion ofstators and the shaft-rotor assembly account for a considerably larger G-parameter approaching 0.2–0.23. This effect however,cannot be captured in a one-dimensional environment and needsto be taken into account through the leakage coefficient CD. Anadditional effect of leakage that cannot be identified using onedimensional code is that increasing leakage gaps promote theinteraction between neighboring channels effectively attenuatingin particular the reflected secondary shock extensively.Model Validation. For the first part of the experimental validation, rotational speed was swept from 24,000 rpm to the designspeed of 32,000 rpm in steps of around 2000 rpm. This is done fortwo different nominal leakage gaps, namely 0.20 mm on the inletand 0.25 mm on the outlet side as well as 0.3 mm on both in- andoutlet side, respectively. In addition, the peak cycle temperatureTt,HPG was varied from 500 C to 600 C. The loop flow ratio k ismaintained constant at 1.7. In the second part of the validationstudy, k is varied in steps of 0.5 between 1.7 and 2.7. The totalinlet pressure remains at approximately 270 kPa throughout.Variations in rotational speed lead to different velocity trianglesat both in- and outlet. To account for these variations and associated misalignment of the flow with the rotor profiles, a parameterfit was made for the loss coefficient floss. The linear relationshipdetermined from this is given in Fig. 11(a) and used throughoutthe validation study. The same procedure was applied to the leakage coefficient CD. Here, the situation becomes more complicated as different leakage gaps account for slightly different characteristics, as illustrated in Fig. 11(b). The reason for this lies most likelyin the fact that actual leakage gaps are not known and merely thenominal values were taken. Hence, the selected discharge coefficient partially accounts for this lack in information. The reasonwhy one sees a variation of the leakage coefficient with speed canbe explained by additional blockage or greater dynamic head created in the leakage gap through higher rotational speeds, similaras witnessed in turbomachinery [59]. Finally, the friction multiplier was set to 2.7 throughout the simulations. A similarly highvalue was witnessed in the studies by Paxson and Wilson [39].At the design speed of 32,000 rpm Fig. 12 gives the numericallydetermined unfolded view of both wave pattern in the left-handside contour plot and temperature distribution in the right-handcontour plot. The plot also shows the predicted velocity profiles ineach port and the extent of gradual passage opening. Marked instations S1 and S2 are primary (right running) and secondaryshock (left running) waves generated upon opening of the highpressure gas (HPG) inlet and high pressure air (HPA) outlet. Further shown are expansion fans E1 and E2 produced as a result ofHPG closing and LPG opening. The temperature distribution indicates inherent exhaust gas recirculation (EGR) characteristic toaxial throughflow machines as well as a small amount of fresh airexhaustion (FAE) through the exhaust port LPG.The effect of gradual passage opening is shown in Fig. 12(b),where mark T1 indicates the initiation of the primary shock wave before progressively increases toward the fully developed level atapproximately T2. The predicted instantaneous and cycleaveraged power output for a single channel is given in Fig. 12(c).It becomes apparent that solely the flow entering through the HPGport results in torque generation, while opening of the LPG portand inflow through LPA does not significantly contribute to overall power generation. This is of course expected as the flowthrough the HPG port signifies the flow with the highest enthalpy.The total expected power output is then evaluated by multiplyingthe number of channels with the cycle-averaged value. The plotfurther indicates that the proposed symmetric, arc shape is not anoptimum with respect to torque generation, which is outlined bypositive regions in the instantaneous power trace near openingand closure of the HPG port.Comparing power output, pressure ratio, and outlet temperaturefrom the HPA duct for the experimentally determined dataset withthe simulation model shows the group of plots displayed inFig. 13. The first plot on the top left denotes total pressure ratiodata obtained for a maximum inlet temperature of 500 C and anominal clearance of 0.20 mm on the inlet and 0.25 mm on theoutlet side. The model accurately predicts the total pressure distribution well across the entire speed sweep. As expected, increasedleakage gaps result in compromised energy transfer and thuslower achieved pressure ratio. This trend is reflected in the modelresults, although there is an overprediction in pressure ratio with arelative error between experiments and simulation of around10–15%. Furthermore, the model predicts a pronounced drop-offin pressure ratio as one approaches lower rotational speeds, whichhas not been witnessed in the experiments. Opposed to the validation on pure pressure exchangers of Kentfield and NASA, there isa more pronounced discrepancy between simulation results andexperimental data. This can at least partly be attributed to the factthat experimental work on pressure exchangers was done at nearlyambient temperature conditions. This renders thermal expansioneffects; minimal and axial clearances between stator and rotors are likely to remain unchanged from their nominal values. For thewave rotor turbine under investigation here, peak temperaturesdiffer significantly from ambient conditions and uneven thermalexpansion between rotor and stators that cause the axial clearancegap to increase in size, which severely affects in particular thesecondary shock wave strength and thus the achieved pressureratio.Raising maximum inlet temperature from 500 C to 600 Cresults in an increase in pressure ratio as a consequence of moreenthalpy being directed to the wave rotor. While the overall trendis also reflected in the simulation results, the model indicates thatthe increase in temperature appears to outweigh the penalty inlarger leakage flow, while in the experiments this is clearly notthe case. This shortcoming exemplifies the sensitivity of the system that arises when choosing model factors, thus limiting its predictive capabilities. 
The model yields good results with respect to predicted poweroutput for all simulated conditions, shown in Fig. 13(b). Power isslightly overpredicted throughout with a relative error of around5–10%. The power output increase shown in (b) stems from anincrease in HPG mass flow rate of 34 g/s compared to 32 g/s in thelow leakage case.Finally, Fig. 13(c) gives a comparison of the averaged totaltemperature at the HPA outlet with the experimental data recordedthrough thermocouples close to the port outlet. Across all testedconditions, the relative error varies between 1% and 6%. Whilethere seems to be moderate (left and right) to no (center) variationwith respect to rotational speed changes, the model predictssteeper line gradients. This is a consequence of the model predictions regarding exhaust gas recirculation at the HPA port. Thetemperature distribution shown in Fig. 12 implies some FAE.Reducing FAE by allowing more fresh air being directed to theHPA port before the port closes. This reduces EGR rates at theHPA port and accounts for a slightly lower mixed-out temperature. This effect is well shown in Fig. 13(a). The simulationresults in (center) and (right) almost show the same gradient withrespect to speed as in the low leakage case in (left). The discrepancy between the two stems again from the effects of leakage,where increased leakage promotes interaction and flow betweenone channel, another channel, and the leakage cavities around it.Thus more hot air is ingested into the cold air stream leading tomore or less the same average outlet temperature.Figure 14 exhibits the effect of a variation in loop flow ratiofrom 1.7–2.2 and 2.7 with rotational speed. The model correctlypredicts an increase in pressure ratio with increasing loop flowratio. However, while there is a relatively small difference at aloop flow ratio of 1.7 of less than 3%, the model reacts more sensitively to an increase in loop flow ratio resulting in higher pressure ratios and increased error. Higher loop flow ratio signifies areduced cold air mass being ingested into the wave rotor resultingin a higher average rotor temperature and thus altered thermalexpansion. The Q1D-model cannot account for this change inleakage characteristic, which results in a greater error between thesimulation results and the experimentally determined data. 
Predicted shaft power output captures the trends of increasingpower output with increasing loop flow ratio. The main mechanism for this is an increase in inlet mass flow rates to around 34 g/s at k ¼ 2.2 and 35.5 g/s at k ¼ 2.7. However, the increase indicated in the experiments is higher than shown for the simulationmodel, resulting at a maximum underprediction in shaft power atk ¼ 2.7 of approximately 8%.Looking at the averaged temperature in the HPA port, the maximum relative error between experiments and simulations is ataround 4%. The model is able to predict the effects of the decreasein fresh air flow well. Higher loop flow ratios increase EGR andthus mark a reduced effect of fresh air temperature on the mixedout average temperature. As a result, average temperaturesincrease as is both witnessed in the model and experiments. Furthermore, varying rotational speed has most influence on HPAoutlet temperature at low loop flow ratios, where EGR rate is generally lower. At higher flow ratios, EGR rate is high and the HPAtemperature less sensitive to a variation in rotational speed. Thiseffect is recorded in the simulation results through a reducedgradient.
Conclusions and Outlook. 
This paper presented a numericalmodel that extends the previous one-dimensional codes to allowcomputation of torque generation in curved rotor passages throughadditional source terms that compute inviscid blade forces. Themodel was furthermore validated through both literature data andexperimental data from a symmetrically cambered microwaverotor developed at the University of Bath. The main points of thepaper can be summarized as follows: The objective of the study was to perform model validationthrough literature data for straight channeled pressureexchangers run at low temperature and through experimentaldata obtained from a symmetrically cambered microwaverotor developed at the University of Bath. The comparison with literature data for pressure exchangersat constant speed and inlet conditions yields good agreementfor both steady and unsteady flow data. This gives confidence that the model is able to predict fundamental characteristics,such as wave patterns and port flow conditions. It furtheremphasizes the importance of loss mechanisms on the performance characteristics of wave rotors. A comparison of the model with sub-1 kW microwave rotorturbine data yielded promising results and showed that themodel is able to identify the variation of pressure ratio andtemperature with rotational speed. Predicted power outputwas also in very good agreement with the experimental data.Nonetheless, a few restrictions in the model persist and canbe listed as follows:– The model appears to react more sensitively to changesin inlet temperature and leakage gap than witnessed inthe experiments 
Increases in nominal leakage lead to a larger relativeerror between the determined total pressure ratio forexperiments and simulation. The issue is exacerbated ifonly the nominal, rather than the actual leakage gap for athermally stable operating point is known rendering thedetermination of leakage coefficients difficult and resultsmore deviating from experimentally determined ones.– Power predictions are slightly overpredicted throughout,but show similar trends as in the experiments and varyonly marginally with respect to changes in rotationalspeed. However, an additional loss factor is required toaccount for the (mis-)alignment of the flow at the port/rotor interface. For higher loop flow ratios, the increasein power output in the simulation results is lower compared to the experiments, resulting in a slight underprediction for k ¼ 2.2.The model is able to predict port outlet temperatureswell, but fails to identify the effects larger leakage gapsincur. The effect of reduced EGR rates and thus morefresh air being directed through the HPA port appears tobe more pronounced than witnessed in the experiments.Also, the ingestion of air from neighboring channels andthe leakage cavity seem to become more pronounced atelevated leakage gaps, which cannot be identified by themodel. However, trends showing the effect of increasedloop flow ratio on the outlet temperature are capturedwell. The introduced model facilitates wave rotor turbine designprocess, as it provides the means to investigate both the pressure exchange capabilities of the wave rotor as well as its use as a power turbine. It can thus be used for initial sizing andas an inexpensive design tool to vary channel camber beforemore expensive CFD modeling tools are applied.                                Dropwise condensation theory revisited: Part I. Droplet nucleation radius

Abstract 
     An improved thermodynamic model is developed for the critical radius for droplet condensation on subcooled surfaces having a promoter coating layer. In addition to heat conduction through the droplet, this model takes into consideration thermal resistances of the coating layer, the liquid vapor interface, and curvature depression of the equilibrium interface temperature, which are important in nanosize droplets. The critical radius for droplet condensation is obtained based on the availability criterion, and effects of contact angles, degree of subcooling, thickness and thermal conductivity of the coating layer, and saturated pressure on droplet nucleation radius are illustrated. Furthermore, the critical radius for condensation on a superhydrophilic subcooled surface without a promoter coating layer obtained from this new model recovers to the critical radius of droplet condensation in classical heterogeneous nucleation of droplets.

Introduction
     About 80 years ago, Schmidt et al. found that dropwise condensation heat transfer had a much higher heat transfer coefficient than that of dropwise condensation. Since that time, much experimental and theoretical work has been carried out on dropwise condensation heat transfer. In early years, the problem of onset of droplet condensation was the focus of study. In 1936, Jakob proposed that very thin layers of steam condensed initially on the subcooled surface, and the thin film fractured into droplets after a certain thickness was reached. Tammann and Boehme proposed a different droplet condensation mechanism, suggesting that droplet nuclei randomly existed initially on subcooled surfaces. This mechanism was later confirmed by McCormick and Baer＊s experimental observation, which showed that droplets nucleated on cavities on the condenser surface. Umur  and Griffith used an optical microscope to detect thin films of molecular dimensions, and no film greater than a monolayer thickness. Based on the equilibrium theory of Gibbs free energy and assuming that temperature of the vapor at a uniform temperature, Graham and Griffith gave the following expression for the critical radius for droplet condensation on a subcooled wall at Tw with the degree of subcooling given by DTsub, i.e. where ql is the density of the liquid, rlv the liquid每vapor surface tension, T is the saturated temperature of the vapor, and hfg the latent heat. Eq. (1) has been used for calculation of critical radius for drop- wise condensation heat transfer until recently. Most recently, Quan et al. obtained the critical radius for droplet condensation on a subcooled surface in a saturated vapor based on changes in Gibbs free energy and availability analyses, assuming a linear temperature distribution between the wall and the top  of the droplet at a saturated temperature. A review of literature on early work on onset of droplet condensation, droplet nucleation density, single droplet heat transfer, and nucleation mechanism from the molecular viewpoint is given by Khandekar and Muralidhar.
    In this paper, we will obtain the critical radius for onset of droplet condensation on a subcooled wall having a promoter coating layer based on the change in availability. This new model takes into consideration thermal resistances of the coating layer, the liquid vapor interface, and curvature depression of the equilibrium interface temperature, where the latter two effects become important in nanosize droplets. Thus, the new model for the critical radius for droplet condensation differs from previous models in two different aspects: (i) there is no need to assume temperature distribution in the droplet adjacent to the wall such as uniform temperature in or linear temperature distribution in, and (ii) effects of thermal resistance of coating, interfacial thermal resistance of the liquid vapor interface as well as curvature depression are taken into consideration. This newly developed model on the critical radius of heterogeneous droplet condensation will be applied to the study of dropwise condensation heat transfer on a subcooled surface in Part II of this paper series.

Thermodynamic analyses on onset of droplet condensation
    Consider the formation of a droplet with radius r on a smooth subcooled surface at Tw having a coating layer with thickness d in a saturated vapor at Ts and ps, as shown in Fig. 1. We now carry out a thermodynamic analysis to study onset of droplet condensa- tion on a subcooled surface by making the following two assumptions: The local temperature of vapor before onset of droplet conden- sation is the same as after onset of droplet condensation, i.e. This assumption is based on the local thermal equilibrium of the vapor and liquid before and after onset of droplet condensation. The vapor adjacent to surface before onset of droplet condensation is at ps(Ts) where ps(Ts) is the saturated pressure at Ts and at Tv (where Tv < Ts), i.e., at a subcooled state, or ps(Ts) > ps(Tv), i.e., at a supersaturated state.
    The change in Gibbs free energy from local supersaturated vapor to subcooled liquid is presented in a p每T diagram as shown in Fig. 2, where the supersaturated vapor initially at Point 2 is condensed into subcooled liquid at Point 3 undergoing a constant temperature process. The vapor before phase change is at Point 2, where ps(Tv) > ps(Ts) = pv, so that the vapor is supersaturated or subcooled initially. The liquid inside the droplet at Point 3 is subcooled because  pl > pv. We  now determine. The specific Gibbs function g=G/m is the change in specific Gibbs free energy from subcooled vapor to subcooled liquid undergoing a constant temperature process can be calculated from. To evaluate the second term in the right hand side of Eq. (4), we note that one of the thermodynamic relations gives where s is specific entropy, cp is specific heat at constant pressure and v is specific volume of the fluid. In order to obtain the specific Gibbs free energy of the liquid gl at Point 3 and the specific Gibbs free energy of the vapor gv at Point 2, one saturated state at Point 1 or Point 4 is chosen for simplification of dp = 0 and dT = 0, respectively. For the constant pressure process for vapor from a saturated condition at Point 1 at temperature Ts and pressure ps(Ts) to Point 2 at a subcooled state with temperature Tv and pressure ps(Ts), the specific entropy of vapor can be given by an integration of Eq. (5) from Point 1 to Point 2 to give. and the specific entropy of liquid at Point 3 can be obtained from an integration of Eq. (5) from Point 1 to Point 3 to give for liquid is used in Eq. (7). The change of specific entropy between the vapor before droplet nucleation and liquid after droplet nucleation by subtracting Eq. (7) from Eq. (6) to give. 
    To evaluate the first term in the right-hand side of Eq. (4), we note that another thermodynamic relation for the change in enthalpy between the supersaturated vapor at Point 2 and the subcooled liquid at Point 3 can be calculated accordingly. And the enthalpy of supersaturated vapor at temperature Tv and pressure ps(Ts) at Point 2 and the enthalpy of the subcooled liquid  at temperature T and pressure p at Point 3 is. Expanding the third term at the right hand of Eq. (11) with a Taylor expansion and neglecting small terms, the above equation gives. which is also given by Quan et al. Eq. (12) can also be obtained by the second approach, by way of Point 4 (a saturated state) at Tl (or Tv) and ps(Tv) to Point 2 at Tv and ps(Ts) for vapor, and to Point 3 at Tl and pl for liquid, which is an isothermal process with dT = 0. The change in chemical potential can be calculated from Gibbs每Duhem equation. where ps(Tv) is the saturated pressure at Tv and Rg is the specific gas constant. On the other hand, for the subcooled liquid phase at Point 3, Eq. (13) is used again to calculate its chemical potential from Point  4 to Point  3. Since  the  liquid  is  virtually  incompressible with vl  taken to be constant,  its chemical potential is determined as. where p (T ) is the saturated pressure at T. Eqs. (14) and (15) were obtained previously by Khandekar and Muralidhar. For a pure substance, the specific Gibbs free energy is equal with its chemical potential. With the help of the assumption Tv = Tl, the change in specific Gibbs free energy is. An integration of the Clausius Clapeyron equation gives rise to the saturated pressure equation as. The change of availability during droplet condensation is given by. where Alv, Asl and Asv is the surface area change of liquid vapor, solid liquid, and solid vapor interface after nucleation. When Young＊s equation is applied, and combining it with Eq. (12), then Eq. (19) can be simplified to. where Tl is the temperature inside the droplet and h is the contact angle. In order to perform the integration in Eq. (20), Graham and drop
    Griffith assumed that Tl is uniform and Quan et al assumed that Tl is linear with distance from the wall. In the following, we will derive Tl without making any assumption in its temperature distribution and then obtain the critical radius for droplet condensation.

Determination of temperature inside the droplet
    We note from Fig. 3 that the liquid temperature Tl in the droplet can be obtained from where Rg is the specific gas constant. The local differential volume where the right-hand side of Eq. (21a) can be obtained from Kim
and Kim  as follows. Where d is the thickness of the coating, and u is a shape fitted coordinate introduced by Kim and Kim for droplet which is related to x and h (see Fig. 3) by. Which is in terms of the local coordinate u. Substituting Eq. (22) for interesting to note that the change in availability is equal to zero for a subcooled superhydrophilic surface with h = 0. This implies that filmwise condensation occurs on a superhydrophilic subcooled surface without any nucleation work.
    We now obtain the critical radius rc for heterogeneous droplet nucleation from Eq. (26a) according to where the value of the critical radius rc is obtained from Eq. (26) numerically. Note that in all of previous droplet nucleation theories, the minimum radius for droplet was assumed to be Rmin=r0 but in this paper we will assume that rmin = rc, which is determined below.

Results and discussion
     In this section, we will present results of the critical radius rc for droplet condensation of water vapor on a subcooled wall based on Eq. (26). Effects of subcooled degree, contact angle, coating thickness, thermal conductivity of coating layer and saturated vapor pressure on the critical radius will be analyzed. Also, the variation of change in availability versus droplet radius will be discussed.
    The change in availability DW given by Eq. (26a) during forma- tion of an embryo droplet in a subcooled water under the condition of  DT sub = 4 ∼C and Ts = 100 ∼C for contact angles of h=5∼ and h=150∼ is shown in Fig. 4(a) and (b) respectively, where the critical radii rc are obtained based on the criterion of DW/or =0, where the change in availability is given by Eq.(26a). As shown from Fig. 4(a) and (b), the critical radii are 5.0 nm  for  h = 5∼ and 5.6 nm for h=150∼,  and the corresponding changes in availability are respectively.  Comparing Fig. 4(a) and (b), it can be seen that both the critical radius and the change in availability increase with an increase in the contact angle.
    The critical embryo radius for droplet condensation on a subcooled surface obtained from the above model at DW/or = 0 without a promoter coating layer is given in Fig. 5. The result from the classical heterogeneous droplet nucleation given by Eq. (1) is also presented. It is shown from this figure that the critical radius for droplet condensation decreases with increasing degrees of subcooling. At the same degree of subcooling, the critical droplet radius increases with increasing contact angle. This is because the base area of the droplet becomes smaller at larger contact angles. On the other hand, the contact angle plays an important role for the droplet nucleation radius on hydrophobic surfaces, specifically at small degrees of subcooling. It should be noted that the present model for critical radius of droplet condensation in the limit of h0 almost recovers to the Eq. (1). This is because at the limit case of h=0 (a superhydrophilic surface), the droplet is  in contact with the subcooled surface with only one contact point and therefore the droplet temperature is uniform at Ts which demonstrates the validity of the present model. To see clearly the results at h = 0∼, an enlarged graph is plotted at the top right corner of Fig. 5, and a comparison of critical radii computed from the present model with h=0 and those from Eq. (1) is also presented in Table 1. It should be pointed out that film condensation can be considered as dropwise condensation with small critical droplet radius, which occurs on a superhydrophilic surface with h0. Thus, the curves for h=0 can be considered as the occurrence of filmwise condensation on a superhydrophilic surface.
    Temperature drops due to four thermal resistances in a single droplet at onset of condensation with critical radius rc computed according to Eqs. (21c), (23c), (23d) and (23e) for a surface with no coating at DTsub = 4 ∼C and Ts=100 ∼C are listed in Table 2. In this table, xi is the location of the interface at the top of the droplet (where u = h) which was computed according to Eq. (21d). From this table, we can see that the most important part of the four thermal resistances is the temperature depression due to the curvature effect which is large compared with the thermal conduction resistance through the embryo droplet. This is because the radius  r of an embryo droplet is very small, so the thermal resistance due to curvature is relatively large as determined by  Eq. 
Temperature distributions Tl in the droplet with respect to both   of u and x are presented in Fig. 6. The interfaces for these two cases are at xi = 5.017 nm for h = 90∼, and xi = 9.617 nm for h = 150∼ respectively. It is shown from Fig. 6(a) that although temperature varies linearly with respect to u, it varies nonlinearly with respect to x Fig. 6(d) at large contact angles. Thus, the assumption of an uniform temperature distribution in the droplet in the classical model for heterogeneous condensation  was wrong (since  it is valid only in the limit of h = 0 when filmwise condensation occurs on a superhydrophilic surface as mentioned earlier), and the assumption of a linear temperature distribution in the droplet for heterogeneous condensation is not accurate at large contact angles.
    Condensers are usually made of copper or aluminum which is hydrophilic in contact with water. A coating layer is usually applied on the subcooled surface to increase the contact angle of the subcooled surface for the purpose of promoting dropwise condensation heat transfer, which is a common industrial practice. Organic polymers such as polytetrafluorethylene (PTFE, ＆Tefion＊), and dioctadecy disulphide etc., are usually  applied  as the coating. Although coatings can promote good hydrophobicity, their thermal conductivities are very small (about 0.25 W/m K) compared with metals. These promoter coating layers will lead to an additional thermal resistance to the dropwise condensation process as analyzed in Section 2.2. Moreover, since the embryo droplet is in the range of nanometers, the thermal resistance due to coating layer will be especially significant. The effect of pro- moter coating layer on the embryo droplet radius is investigated below.nucleation radius on a promoter coating layer with respect to degree of subcooled and contact angle are similar with those with- out a coating layer. The critical radius for droplet condensation increases with the addition of a coating layer. Effects of contact angle are more remarkable on a surface with coating (Fig. 8) than those without a coating layer (Fig. 5). Also, the critical radius for onset of droplet condensation will not recover to Eq. (1) for a con- tact angle close to 0 if a coating layer is added on the substrate. Fig. 7 shows the effect of coating on changes in availability during droplet condensation. It is seen that the change in availability becomes larger with the addition of a coating layer. The maximum DW occurs at a larger critical radius for droplet condensation on a surface with coating than with those without a coating layer. Fig. 8 presents the critical radius for onset of droplet condensa- tion on a coating layer with thickness d=0.1 lm and thermal conductivity kcoat = 0.25 W/m K. The variation trends of droplet. Heat transfer characteristics of a single embryo droplet on a coating layer with d = 0.1 lm and kcoat = 0.25 W/m K at DTsub = 4 ∼C and Ts = 100 ∼C is given in Table 3 and Fig. 9. It is shown in Table 3
that the most important part of the four thermal resistances is the temperature depression resulting from the curvature. And the thermal conduction resistance through the embryo droplet is very small compared to the curvature effect. Comparing Table 3 with Table 2, it can be seen that DTdrop with coating is bigger than that without coating. This is because at a given degree of subcooling (say DTsub = 4 ∼C), droplet nucleates on a coating with a larger radius, so DTcur decreases according to Eq. (23d). And DTi increases with more liquid每vapor interface at a larger radius according to Eq. (23e). The temperature distribution inside the droplet is presented in Fig. 9 where the top interface for these two cases is located at xi = 8.68 nm for h = 90∼, and xi= 58.4 nm for h = 150∼ respectively. As shown in Fig. 9(d), temperature distribution inside the droplet is not linear with respect to x at a large contact angle. Fig. 10 shows the effect of coating layer＊s thickness on the crit- ical radius for onset of droplet condensation. It can be seen that a thicker coating layer will give rise to a larger critical radius for con- densation, since a thicker coating layer will increase its thermal resistance.  For a 1 lm coating layer with kcoat =0.25 W/m K, h=90∼ and Ts=100 ∼C, the critical radius for droplet condensation
is about 8 times larger than that prediction by Eq. (1) without a coating  layer.  A coating  layer with  thickness d < 0.1 lm has little influence on the droplet nucleation radius, so an ultrathin coating layer is desirable for dropwise condensation. For this reason, much work has been done to fabricate monolayer promoters for the purpose of promoting dropwise condensation. Fig. 11 shows effects of coating thermal conductivity on droplet nucleation radius. It can be seen that a coating layer with higher thermal conductivity will decrease droplet nucleation radius, since coating with higher thermal conductivity will reduce the thermal resistance of the heat transfer process. So, a thin coating layer with high thermal conductivity can decrease the droplet nucleation radius and is favorable for promoting dropwise condensation. Fig. 12 shows the effect of saturated vapor pressure ps on the critical radius for droplet condensation. Since values of thermophysical properties, such as saturated temperature Ts, latent heat hfg, liquid vapor density ql and qv, and surface tension rlv, are all related to the saturated pressure ps(Ts), physical properties will affect the critical radius for droplet condensation. It can be seen from Fig. 12 that the critical radius for droplet condensation increases at higher saturated vapor pressures. This is because at high saturated pressure ps, Ts increases while hfg decreases, the critical radius for droplet con- densation will be enlarged according to Eq. (1).

Concluding remarks
    In this paper, a thermodynamic model based on Gibbs free energy and availability is developed for heterogeneous condensa- tion nucleation on a subcooled surface with coatings. This model is a modification of previous dropwise nucleation theories in which the thermal resistance of coating on the surface, the interfacial thermal resistance of the liquid vapor interface and curvature depression of the equilibrium interface temperature have been taken into consideration. Based on the nucleation criterion for change of availability oDW/or = 0, nanometer scale critical radius for dropwise condensation can be predicted. The following conclusions can be drawn from this paper:
    Droplet condensation radius is small (in nm) at large degree of subcooling. Thus, the interface effect of temperature drop from vapor to liquid is very large and cannot be neglected.
    For subcooled surfaces having large contact angles, the temperature inside the nanoscale condensate droplet is not linear with respect to distance perpendicular from the subcooled surface.
    At high contact angles, the base area of the droplet in contact with the subcooled surface is small, resulting in a high thermal resistance and large droplet nucleation radius.
    At zero contact angle (i.e., a superhydrophilic subcooled surface), nano droplets with a uniform temperature at Ts are formed with a point contact with the subcooled surface at Tw. Film condensation occurs on a superhydrophilic surface at (h = 0∼) without nucleation work and condensation heat transfer.
    Although the addition of a coating layer can promote dropwise condensation, it will also increase the thermal resis- tance of the condensation process. A thin hydrophobic coating layer with a high thermal conductivity will decrease droplet nucleation radius and is desirable in practice.
    At higher vapor pressures with higher saturated temperature and smaller latent heat, the droplet nucleation radius will be enlarged.         Dropwise condensation theory revisited Part II. Droplet nucleation density and condensation heat flux

Abstract  
    An improved dropwise condensation heat transfer model modified from previous models is proposed in this paper. The critical radius for onset of droplet condensation is determined in the preceding paper (Part I), leading to a more accurate determination of droplet nucleation density and the coalescence radius in this paper (Part II). Effects of subcooling, contact angle, thickness and thermal conductivity of the coating layer on droplet nucleation density, condensation heat flux, and critical condensation heat transfer rate for onset of droplet condensation are illustrated. The predicted droplet nucleation density and dropwise condensation heat flux are shown in excellent agreement with existing experimental data.

Introduction
    The study of dropwise condensation heat transfer has attracted a great deal of attention since Schmidt performed his experiment on dropwise condensation heat transfer in 1930. Le Fevre and Rose developed the first dropwise condensation heat transfer model in 1966, in which the average condensation heat transfer was obtained by an analysis for heat transfer through a single droplet combined with the drop size distribution. In this single droplet model, three thermal resistances including conduction resistance, vapor liquid interfacial resistance and surface resistance were considered in series. Wu and Maa obtained an expression for calculation of dropwise condensation heat transfer by dividing the droplets into two groups: small droplets before coalescence and large droplets after coalescence; they used a population balance model to obtain drop size distribution of small droplets before coalescence which grow mainly by direct condensation. Abu-Oriba improved Wu and Maa¡¯s model by taking account of the thermal resistance of promoter coating based on a single droplet heat transfer model given by Le Fevre and Rose. A dropwise model similar to Abu-Oriba was recently proposed by Kim and Kim, who included the effect of contact angle on heat conduction in a single droplet. Although Kim and Kim¡¯s model for dropwise condensation heat transfer is a most comprehensive dropwise condensation model, it has a few short- comings: (i) the critical radius was determined from the critical radius for classical heterogeneous droplet nucleation condensation, which did not take into consideration effects of contact angle and coating layer, (ii) the value of droplet nucleation density is, and (iii) the model is unable to predict the critical heat transfer for onset of dropwise condensation.
    In this paper, we propose an improved dropwise condensation model which is based on the modification of Kim and Kim¡¯s model by using the more accurate expression for the critical radius for onset of droplet condensation  derived  in the  preceding paper to determine (i) the minimum droplet nucleation radius and  (ii)  the droplet nucleation density. Effects of contact angle, degree of subcooling, contact angle hysteresis, thickness and thermal conductivity of the coating layer and the saturated vapor pressure on dropwise condensation heat flux are analyzed. The predicted droplet nucleation density and dropwise condensation  heat flux are shown in excellent agreement with existing experimental data.

Previous models for dropwise condensation
    In this section, we will briefly review previous models for dropwise condensation on a subcooled surface. Wu and Maa assumed that droplets condense on a subcooled surface can be divided into two groups: a group of small size droplets (with radius from rmin to re) having a number density n(r), and a group of large size droplets (with radius from re to rmax) having a number density N(r) and gave the following expression for dropwise condensation heat transfer: where rmin is the minimum droplet radius, re is the coalescence radius, and qdrop is the heat transfer rate through a single droplet given by Kim and Kim as where h is the contact angle; d is the thickness of the coating; hcont is its thermal conductivity, and hi is the interfacial heat transfer coefficient is given by where a is the accommodation coefficient. Graham and Griffith assumed the minimum radius rmin of the droplets to be equal to the critical radius r0, which can be obtained from the classical heterogeneous droplet nucleation theory as. where DTsub = Ts-Tw is the degree of subcooling with Ts being the saturated temperature of the vapor; ql is the liquid condensate density, and rlv is liquid vapor surface tension, and hfg is the latent heat. It should be noted that Eq. (2a) has taken into consideration of the thermal resistances of the promoter, the liquid vapor interface, and the curvature depression of the equilibrium interface temperature. In the first integral of Eq. (1), n(r) is the drop size distribution of small droplets with radius from r0 to re, which is given by. The lower limit of the first integral is the critical radius rmin for droplet nucleation, and the upper limit in the first integral is the. Coalescence radius r is given by. Rose has derived a theoretical expression for droplet nucle- ation density which is given by. Where rmin = r0 is in all previous models. Although Eq. (4b) was derived in 1976, it has seldom been used in practice since Eq. (4b)  is known to be overestimating the droplet nucleation sites if rmin =  r0, where r0 is given by Eq. (2c). On the other hand, experimental values of droplet nucleation density Ns were found in the range from 109 m2 to 1015 m2, and this value was used in previous dropwise condensation models. In the second integral of Eq. (1), N(r) is the number density of large drops with radius from re to rmax which is given by. where rmax is maximum drop radius, which can be obtained according to the force balance between surface tension and gravity as with hr being the receding contact angle and has the advancing contact angle. Eqs. (1)-(5) with rmin=r0 is Kim and Kim¡¯s model for dropwise condensation heat transfer. It should be noted that the critical condensation heat flux from  Eq. (2a)  gives qdrop ro=0,  which is unrealistic since the critical heat flux is not equal to zero for bubble nucleation

The improved dropwise condensation model
    As mentioned earlier, we now propose an improved dropwise condensation model which differs from Kim and Kim¡¯s model in two aspects: the evaluation of (i) the critical radius for onset of droplet nucleation and (ii) the droplet nucleation density Ns, which will now be discussed. And the critical radius for onset of heterogeneous dropwise condensation rc is obtained by submitting Eq. (2a) with qdrop to Eq.(6a) and letting
where the value of the critical radius rc can be obtained numerically. The results of critical radius for nucleation of droplet condensation are presented in Part I in details, where effects of contact angles,degree of subcooling, thickness and thermal conductivity of the coating layer, and saturated pressure are discussed.
The condensation heat transfer rate at the onset of droplet condensation qdropercT can be obtained from Eq. (2a) by substituting r=rc to give where hi is given by Eq. (2b) with a=0.3 being chosen for computation in this paper. Eq shows that: and qdrop is not zero in the present model because latent heat is released when droplet is formed and heat is transferred from droplet to the subcooled wall. It is interesting to note that for h = 0(a superhydrophilic surface), rc=r0 and therefore i.e, film condensation occurs on a superhydrophilic
r is computed for different contact angles, subcooled degrees, coating thickness and thermal conductivity of the coating and the results are presented in Fig. 1. Fig. 1(a) shows the effect of contact angle on critical condensation heat flux at the onset of droplet condensation. It is shown that (i) the critical condensation heat flux qdrop (rc) is small at small contact angles (i.e., on hydrophilic surfaces), (ii) qdrop (rc) increases as the contact angle becomes larger. The latter is because rc increases at larger contact angle (see Fig. 5 in Part I). As a result, we can see that the numerator of Eq. (7) increases, and this effect will play the major role. Similarly,  rc  becomes larger at lower subcooled degrees (Fig. 1(b)), bigger coating thickness (Fig. 1(c)), and lower thermal conductivity of coating (Fig. 1(d)), and under these conditions, qdrop(rc) given by Eq.(7) will also increase.
    As mentioned earlier, Eq. (4b) is known to be overestimating  the droplet nucleation sites if r0 is given by Eq. (2c).This implies that r0 given by Eq. (2c) may be too small especially when the subcooled surface has a coating layer. In this paper, we propose  that the analytical expression by Rose for droplet nucleation density Ns given by Eq. (4b) is to be calculated according to. where rc is the critical radius for onset of heterogeneous droplet nucleation given by Eq. (6), which has taken into consideration effects of contact angle, the thickness and thermal  conductivity  of a coating layer and liquid vapor interfacial thermal resistances. Effects of contact angle, the thickness and thermal conductivity of the coating layer on droplet nucleation density based on  Eq.(8) are presented in Fig.2. It can be seen from this figure that the values of the droplet nucleation density computed from Eq. (8) are in the same order of magnitude as those obtained from experimental observation. This confirms the validity of Eq. (8) indirectly. Fig.2(a) shows the effect of contact angles on droplet nucleation density on surfaces without a coating layer is small except on a subcooled surface having a large contact angle and at small degrees of subcooling. It is shown that the droplet nucleation density decreases as the contact angle increases. At large contact angles, the droplet nucleation radius become large, indicating that the onset of droplet condensation is more difficult to occur. Conse- quently, the number of activated condensation sites decreases. For large degree of subcooling, the nucleation potential aggrandizes,the radius of condensation droplets becomes smaller and more droplet nucleation sites are activated. It should be noted that for contact angle smaller than 90¡ã, droplet nucleation density curves shown in Fig. 2(a) are close with each other. This is because the droplet nucleation radius for h < 90¡ã is close to each other, which was explained in the preceding paper. Fig. 2(b) shows the effect of contact angle on droplet nucleation density on a coating layer having a thickness of d = 0.1 lm and a thermal conductivity  kcoat = 0.25 W/m-K. A comparison of Fig. 2(a) and (b) shows that addition of a coating layer decreases the droplet nucleation density greatly. Effects of thickness and thermal conductivity of coating layers on droplet nucleation density is presented in Fig. 2(c) and (d), respectively. The results show that droplet nucleation density decreases on a thicker coat- ing layer with a smaller thermal conductivity.
    The proposed improved model for dropwise condensation heat transfer rate is given by Eq. (1) with qdrop given by Eq. (2a) with rmin being replaced by rc, i.e., where n(r) and N(r) are given by Eqs. (3a) and (5a) respectively with rmin being replaced by rc, the critical radius rc determined from Eq. (6b), the coalescence radius re determined from Eq. (4a) and with droplet nucleation density Ns determined from Eq.(8), respectively. Fig. 3 is a comparison of the predicated condensation heat transfer based on the improved dropwise condensation model with existing experimental data obtained by different investigators. Since information on the contact angle was not reported in their experimental data, three contact angles (h=60¡ã, 90¡ã and 120¡ã) were chosen for computations in the proposed model and the results are compared with the experimental data. In compari- son with the present theory, the mean absolute percentage error of the experimental data are 34.8%, 17.9% and 21.9% if the contact angle of  h=60¡ã, 90¡ã, or 120¡ã, were used for computation respectively. Fig. 4 shows the effect of contact angle on the predicted drop- wise condensation heat flux  at a specific degree of subcooling of 4 ¡ãC. It can be seen that the condensation heat flux firstly increases with the contact angle to a maximum value at about h=120¡ã and then decreases afterward. This behavior with respect to h can be seen from the three terms in downstairs of Eq. (2a) for a hydrophilic surface (h < 90¡ã) and a hydrophobic surfaces (h > 90¡ã) respectively. For a hydrophilic surface (h < 90¡ã), all of the three terms decrease with the contact angle, and thus, the condensation heat flux increases with the contact angle. For a hydrophobic surface (h > 90¡ã), heat transfer rate qdrop decreases because the  increase in first two terms (i.e., thermal resistance of coating, and conduction in the droplet) in downstairs is larger than the decrease of the third term (i.e., the interfacial thermal resistance) and consequently the condensation heat flux decreases. For the above reason, Fig. 4 shows that an optimal contact angle exists for max- imum dropwise condensation heat flux for a given degree of subcooling. The contact angle hysteresis, defined as ha hr, also influences dropwise condensation heat flux because it affects the movement of the droplet. With small contact angle hysteresis, the maximum drop radius decreases as shown in Eq. (5b), suggesting that droplets are easier to move. The effect of contact angle hysteresis on dropwise condensation is given in Fig. 5. In the calculation, the receding contact angle hr and advancing contact angle ha is estimated as. Fig. 5 shows that condensation heat flux is much larger at low contact angle hysteresis than at high contact angle hysteresis. Fig. 6 shows the effect of contact angle on dropwise condensa- tion heat flux on subcooled surfaces having a coating with thick- ness  d = 0.1 lm  and  thermal  conductivity  kcoat = 0.25 W/m-K. It can be seen that the dropwise condensation heat flux firstly increases with the contact angle, then decreases for hydrophobic surfaces, a trend similar to Fig. 4 (without a coating). It is shown from Fig.6 that the maximum condensation heat flux occurs at about h = 90¡ã with a promoter coating, which is smaller  than at h= 120¡ã on a surface without a coating layer as shown in Fig. 4. This is because the first term (i.e., thermal resistance ofcoating) in Eq. (7) decreases for 0 < h < 90¡ã  but  increases for 90¡ã < h < 180¡ã, leading to the shift of optimum contact angle to be smaller from h = 120¡ã to h = 90¡ã at maximum qdrop. Fig.7 shows the effect of thicknesses and thermal conductivities of the coating surface on condensation heat transfer. The results show that thermal resistance increases with a thick coating surface having a low thermal conductivity, so that dropwise condensation heat flux is reduced under such situations. Fig. 8 shows the effect of saturated pressures on predicted dropwise condensation heat flux on subcooled surfaces without a coating layer at h = 90¡ã. It can be seen that dropwise condensation heat flux increases with the saturated pressure. This is because the nucleation radius is smaller at higher saturated pressures as shown in the preceding paper. When the nucleation radius is small, the nucleation density is high, so dropwise condensation heat flux increases at high saturated pressures.
Fig. 9 shows that predicted condensation heat fluxes are in good agreement with Stylianou and Rose¡¯s experimental data at different saturated pressures. The contact angle of h = 90¡ã was chosen for computation in this model because the contact angle was not reported in their paper.

Concluding remarks
     In this paper, an improved model is proposed for dropwise condensation heat flux on a subcooled surface with coatings, where embryo droplet nucleation radius and droplet nucleation density are evaluated differently from existing models. This improved model differs from Kim and Kim model in two important aspects: (i) the droplets nucleation radius is evaluated from the preceding paper instead of the critical radius given by Eq. (2c). (ii) The droplet nucleation density is calculated from the analytical expression given by Rose with the critical radius computed from the preceding paper instead of experimental values. Effects of contact angle, degree of subcooling, contact angle hysteresis, thickness and thermal conductivity of promoter coating layer as well as saturated pressure on dropwise condensation heat transfer are presented. The following conclusions can be drawn from this paper:
     An analytical expression for critical condensation heat transfer rate at onset of dropwise condensation is obtained in this paper for the first time. At a given degree of subcooling, this condensation heat transfer rate increases with the contact angle.
    The validity of Rose¡¯s analytical expression for droplet nucleation density is confirmed by a more accurate calculation of critical radius for onset of dropwise condensation, with effects of coating, interfacial temperature drop, and curvature depression of equilibrium interface temperature taken into consideration.
    Increasing degree of subcooling within certain limits can diminish embryo droplet nucleation radius, magnifying droplet nucleation density and increasing the dropwise condensation heat flux.
    The dropwise condensation heat flux increases with the contact angle of hydrophilic surfaces at first, then decreases at high contact angles of hydrophilic surfaces. An optimal contact angle for dropwise condensation heat transfer exists for a given degree of subcooling.
    Although a hydrophobic coating layer can enhance dropwise condensation heat transfer, it also introduces an extra thermal resistance to the condensation heat transfer process. A thin hydrophobic coating layer with a high thermal conductivity can aggrandize droplet nucleation density and enhance dropwise condensation heat transfer greatly.
    Both the condensation heat flux and droplet nucleation density increase at higher saturation pressures.Effects  of wall  temperature on slagging and  ash  deposition of Zhundong coal  during circulating fluidized bed  gasification
Xiaobin Qi a,b, Guoliang Song a,⇑, Weijian Song a,b, Shaobo Yang a,b, Qinggang Lu a
a b s t r a c t  ：
Zhundong coal  with a large reserve is faced severe ash-related problems including slagging and fouling during pulverized-coal boiler combustion. The  gasification in  circulating fluidized bed is a good way to use Zhundong coal  due to the low  reaction temperature, while it is  unknown for  the slagging and ash deposition mechanism of Zhundong coal  during circulating fluidized bed gasification. In this paper, the gasification of  Zhundong coal  was tested in  a 0.4 t/d CFB test rig  simulating the real industrial device. Ash  deposition probes were installed along the gas  flow  direction to characterize its  slagging and ash deposition at different wall temperatures. These probes were treated with three cooling methods (no cooling, air  cooling or  water cooling) to reach different wall temperatures (882–737, 665–429 and 81–45 °C). The  results reveal that deposition was affected greatly by  wall temperature.  Mineral elements including Na, Ca, Fe, Al and Si were likely to accumulate in  deposits on  high-temperature  surfaces, and existed in  the forms of  silicates and sulfates at wall temperature  above 882 °C, resulting in slagging. Although the contents of most mineral elements in  the deposits on  cooled surfaces were always low,  a large gas-wall temperature gradient triggered the condensation of  gaseous species, such as  Na,  Cl and K. The  CaSO4  and CaO in  gases were condensed and restructured in  an  ordered crystalline array on  the surfaces of ash particles, forming a coating layer that made the ash particles stickier. Chemical composi- tion and crystallographic analyses show that the properties of deposits on the probes and gas-borne ashes were different. More mineral phases as well as sodium were present in deposits. Additionally, due to the Cl enrichment on  air-cooled surfaces, the corrosion of the metal surfaces should be  focused.
1. Introduction
The Zhundong coalfield, which is a very  large coal  deposit still under exploration, is potentially an important coal mining resource in  West China  [1].  The  coal  reserves in  this coalfield were esti- mated to  be  390  Gt. The  high volatile content, low  ash  and sulfur contents, and high reactivity make Zhundong coal  a potential sup-ply   of  gas   fuels,   liquid  fuels,   hydrogen, electricity, and  other chemical materials. However,  due to  the insufficient understand- ing of its characteristics, the utilization of Zhundong coal  is limited by  the occurrence of  severe slagging and fouling [2,3].  To better understand  and  resolve these  two  problems,  researchers  have tested Zhundong coal  extensively, but mostly in  laboratory-scale furnaces [2,4–8]. The main cause for slagging and fouling of Zhun- dong coal is attributed to the richness of alkali-alkaline earth metal (AAEM) elements, particularly Na.
Na  affects slagging and fouling mainly in  two aspects: (1)  its high chemical reactivity with other minerals leads to  formation of  eutectics  [3,4,9] and  (2)  it  is  strongly prone to  sublimation [2–4,8]. Na in  the coal  may also  be  involved in  complex transfor- mation reactions and partially ends up  in  the form of  silicates and sulfates. At melting temperature below 1000 °C, sodium sili- cates would mix  with iron,  calcium, or magnesium oxides to form eutectics. These eutectics easily adhere  to  and slag  in  the coal burners of industrial pulverized coal-fired boilers [10].  Moreover, they cover the surfaces of bed  materials or  ash  particles to  form adhesive coating layers, leading to  agglomeration or  even de- fluidization [9,11,12]. Additionally, during thermal coal conversion, Na is easily released through fast  de-volatilization. The Na vapors will nucleate/condense to form sticky fine particles, thus accelerat- ing the ash deposition in the boilers [2,4]. In Zijing bituminous coal, a  type of  Zhundong coal  containing 12.77 wt% Na2O  in  coal  ash, volatilized Na content was  maintained at 20–55 wt% during com- bustion at 400–1000 °C, in  which Na  volatilization was  kept at a high  level   within  600–900 °C  [8].   Volatilized  Na   will   adhere, through  sulfation [10]   or  condensation, to  the  heated surfaces and other ash  particles to  form a  sticky inner layer on  the bare heating surfaces or to form a sticky coating surface of bulk ash  par- ticles, which are  considered as two necessary aspects of ash  depo- sition [4].  Therefore, the existing research is  focused on  the Na release and transformation during the high-Na coal  thermal con- version [8,13–15]. To date, the most effective resolution in indus- trial  boilers  is   co-combustion  with  other  coals  [2,16].  Some additives, such as clay [11,17], kaolin [6] and gibbsite [17], are used to  restrain agglomeration during the fluidized bed  combustion  of high-alkali coal.  These Al-rich additives could react  with  Na  to form high-melting-point aluminosilicates, partly restraining foul- ing  and slagging.
Recently, there are  some studies about the slagging and fouling of Zhundong coal.  The sintering experiments of Zhundong coal  ash under an  O2/CO2   atmosphere in  a  laboratory-scale sintering furnace (over 1200 °C) show that all sintered coal  ashes were layered in different colors [5].  The  relative contents of Si, Ca, Al and Fe in the oxyfuel combustion atmosphere were all different compared with the air-firing atmosphere. During pulverized coal  combustion of high-Na lignite in  a 25 kW  down-fired furnace, Zhundong coal slagged significantly around 1150 °C on  the walls near the burner inlet due to  the formation of  low-melting-temperature  wollastonite and other eutectic mixtures [4]. The mechanism how AAEM species  accelerate  ash   deposition  can   be   summarized  in   two aspects: (1) formation of a sticky inner layer on the bare tube surface and (2) formation of a sticky coating surface of bulk ash  particles.  Wang et al. investigated the mechanisms underlying the Ca/ Na/S/Cl  transformation and ash  deposition in  both full-scale and laboratory-scale furnaces, and found sulfate condensation at about
850  °C was  pivotal in ash  deposition [2]. Wu et al. reported the ash deposition behaviors of  Zhundong coal  in  a  3 MWth   pilot plant facility, and found the Fe-/Ca-bearing minerals and sulfates promoted slagging and ash  deposition [18].  However, among these studies, the most recent work was  carried out  at high temperature (over 1000 °C) during combustion of Zhundong coal.  As reported, ash   deposition  on   convective heat  transfer  surfaces could be cleaned using the conventional soot-blowers by  controlling the flue  gas  inlet temperature [19].  Due  to  low  reaction temperature of  CFB, the reduction of  low-temperature eutectics might remit the slagging and ash  deposition of  Zhundong coal.  Thus,  CFB is highly potential  for   utilization of  high-sodium  Zhundong  coal. We  previously investigated the Na transformation during gasification/combustion [20]  and compared the slagging behaviors among three types of Zhundong coal containing different sodium contents in a 0.25  t/d CFB test rig [21].  It was  found Na behaved differently under these two reaction atmospheres: Na  migrated from Zhundong coal  to deposit mainly in the form of Na2SO4  during combustion, but in the form of NaCl during gasification. The properties of Zhundong  coal  exploited  from  different  coalfields  vary  consider-ably.  Such  variation is contributed to the formation of morpholog- ically  different slags,   in  which the Na/Si  ratio in  Zhundong coal plays a very  important role.  Zhundong coal  with high volatile con- tent is a good  gasification fuel. Gasifying Zhundong coal by CFB and gradient  utilization  of   the  gas   and  gasified  fly   ash   are   new attempts. A series of studies are  needed to  realize these attempts. To date, there is  rare research about Zhundong coal  gasification and even less  report about the effects of wall  temperature on slagging  and deposition during CFBG of Zhundong coal.
In this work, the CFBG of a type of Zhundong coal  was  tested in a 0.4 t/d CFB test rig. The slags  and deposits were sampled by ash deposition probes, whose temperatures were controlled by  water or  air  in  order to  reach different wall   temperatures. To  further reveal the mechanisms of  slagging and fouling, we  analyzed the morphology, chemical compositions and crystalline phase of both slags  and deposits. The aim  of this work is to characterize the slagging  and ash  deposition during CFBG of Zhundong coal  and to pro- vide  guidance for  industrial application of  Zhundong coal  in  the future.
2. Experimental

2.1. Materials

The proximate and ultimate analyses as well  as ash  composition of Zhundong coal  are  presented in Table  1. The relatively low  ash and high volatile contents make Zhundong coal  suitable for gasification, but the high Na2O content in ash  (up  to 7.28%) indicates the possible occurrence of slagging and ash  deposition during gasification. The  experimental coal  was  sieved to  the size  of 0.1–1.0 mm before each experiment.
The important role  of Al2O3  in inhibition of de-fluidization has been reported  extensively [6,11,17]. Thus,  Al-enriched bed  mate- rial containing 90.05% Al2O3  and 7.69% SiO2  was  used in this study, which was  sieved to  0.18–0.71 mm.


2.2. Test system

The  gasification experiments were undertaken in  a 0.4 t/d CFB test rig (inner diameter 150  mm; height 4.4 m).  The test rig  (brief schematic showed in Fig. 1) has  a coal  feeding rate of 0.4 t/d.
The  test system is equipped with thermocouples and differential  pressure sensors along the riser height. Its  main pipelines are coated with high-temperature-resistant cotton in  order to  minimize heat loss during operation. The fuel was  ignited at the startup stage by the external heating provided by heating wires.
All of the data, including temperature, pressure and air volume- flow  rate, were collected and displayed real-time by  a Programmable Logic Controller (PLC) data acquisition system.

2.3. Sampling system and  analyses

For a better comparison, the T1-measured bed  temperature was stabilized at ~935 °C for  ~8 h  in  each test. The  details of experimental conditions are  shown in Table  2.
Ashes  were sampled from positions P1–P9 as  shown in  Fig. 1. Ashes  in  positions P1–P4 were sampled by  sampling cans, which were each controlled by a valve independently. Ashes  in positions P5–P8 were  collected by  temperature-controlled ash   deposition probes (schematic diagram illustrated in Fig. 1) cooled by different media (e.g.  air,  water or  nothing), and were used to  characterize the coal slagging and fouling. The inner and outer surfaces of probe walls were each installed with a  thermocouple to  measure wall temperatures.  The   two  thermocouple-measured temperatures were averaged to  be  the wall  temperature of  the corresponding probe. The  probes are  composed of a fine  heat-resistant material– stainless steel Cr25Ni20 (GB/T20878-2007).
Ashes  in position P9 were collected by  an  ash  sampling probe, which was  purged by  N2  in  advance. This  operation ensures the sampling is valid.  Before  the condition stabilized, the ash  sampling probe contained a large amount of unwanted ash.  To collect more valid  ash,  purging the sampling probe must be  carried out.  Mean-while, N2, an inert gas, has  no effect on the atmosphere around the ash. The gas composition in position P4 was  also  detected by a portable infrared gas  analyzer (Gasboard-3100P).
The  microtopography of the samples and element distribution at some interest  points were analyzed on  a  scanning electron microscope with an  energy dispersive X-ray  spectrometer  (SEM- EDX;  S-4800, Hitachi,  Japan). The  compounds of  samples were detected on  an  X-ray  diffractometer (XRD; PANalytical, Nether- lands). The  chemical components of  some samples were deter- mined via  an  X-ray  fluorescence meter (XRF, XRF-1800,  Japan).

3. Results and discussion

3.1. Deposition tendency at  different wall  temperatures

According to the gas  properties shown in Table  3, it is believed that the ash  deposition probes for the three tests were maintained at almost the same reducing atmosphere.
Since   the experimental conditions were similar among the three tests, the temperature curves along the rise  (T1–T5) coincided  perfectly  (Fig.  2).  The   wall   temperatures  were  different among the adiabatic, air-cooled and water-cooled probes. Under the action of  thermal radiation, the thermocouple-measured gas temperatures were affected by  the cooled probes. Thus,  the temperature  difference was   enlarged gradually from position P5  to P8 (i.e. T6, T7, T9 and T10).
The  wall  temperatures varied among the adiabatic,  air-cooled and water-cooled probes owing to the different thermal conductivities of the cooling media. For the uncooled probe which could be considered as adiabatic, the wall  temperatures in positions P5–P8 ranged  from 882   to   737  °C  (Fig. 3a),   which were  only   several degrees to  dozens of  degrees lower than the corresponding gas temperatures  (891–774 °C).  The   wall   temperatures  of  the  air- cooled and water-cooled probes were 665–429 °C and 81–45 °C, respectively, which were largely lower compared with the adiabatic case.
The ash  deposition probes along the gas flow direction are  illustrated in Fig. 3b. The amount and morphology of deposits both differ among probe types and change with wall  temperature. A small amount of slags  is found only  on the adiabatic probe in position P5, but fine  ash  particles are  evenly distributed on  the surfaces of the two cooled probes in positions P5 and P6. It is indicated the cooled probes can  capture more ash  particles than the adiabatic one.  The deposited amounts on each probe increase gradually from position P5 to  P8.
Slagging and ash  deposition are  affected by  the adhesion force between  heating surfaces and  ash   components,  especially  Na- based compounds [22].  The  ash  particles on  the probes certainly contain viscous materials that melt easily at ~900 °C, such as silicates and sulfates [9,23]. The  gaseous alkali metal elements after approaching the cooled surfaces would be  condensed to  be  adhesive   among ash   particles,  thereby  promoting deposition [4].  In the two cooled cases, fine  ash  particles were transported by  thermophoresis, a phenomenon in which fine or aerosol particles move against the temperature gradient and ultimately deposit on  low-temperature surfaces.
The formation of a fouling layer on a probe aggravated the thermal  resistance and thereby decreased heat flux.  The  heat flux reduction was  reflected by  the temperature change. However, because of the small amount of deposits on the probes, the temperature changes were not  obviously different among the three cases. Therefore, the temporal variation of temperature only  in  position P8  under the air-cooling condition is  illustrated in  Fig. 4,  where G/WT  is  the gas/wall temperature around/of the ash  deposition probe in position P8, and DT is the temperature difference between them. Generally, the temperatures  in  position P8  including gas temperature and wall  temperature varied slightly, which is consistent with the small thickness of  the fouling layer. At  the initial deposition stage, the wall  and gas  temperatures tended to  rise  to a thermal equilibrium with smaller difference between them. As ash  deposition continued, the thermal resistance was   enhanced, leading to  the decrease of  wall  temperature  and the increase of temperature  difference. With the progression of  deposition, the contact area between ash  particles was  enlarged and the deposits became dense. In  consequence, the thermal conductivity was increased [24].  Thus,  at the final  depositing stage, the wall  temperature rose  slightly again and the temperature  difference was diminished.
3.2. Microstructure and  crystalline phase  of deposits

Fig. 5 shows the micro-morphology and element distribution of deposits on the probes in position P5. Granular slags  appear on the adiabatic probe (Fig. 3b)  and are  covered with obvious melting as indicated by  SEM image. EDX shows that large amounts of Ca, Si, Na, Fe and S are  distributed on  the slag  surfaces, indicating these elements are  fluxing materials [4,5,25,26]. The  agglomerated and melted particles are  primarily formed from eutectic compounds, silicates and (composite) sulfates, which reduce the melting point of deposits. Special attention should be  paid to  iron,  which functions as a flux agent during the deposition. In addition, no chlorine is found on the particles, indicating it is still  gaseous at 882  °C. The major mineral phases of  the slags,   detected by  crystallographic analysis, are  SiO2, Al2O3, Ca2Al2SiO7  and Na2Al2Si3O10 (Fig. 6). The Al-rich material  used in  these experiments effectively captured sodium  to   form  high-melting-temperature  refractory  minerals [27,28]. However, the further increase of sodium atop the Al-rich grains would convert these refractory minerals to fluxing minerals. It could be  deduced that an  interaction between sodium and Al- rich  bed  materials occurred from surfaces inward to the inner part, just like  the interpenetration between Na2O in Zhundong coal  and quartz sands (SiO2) [21].  The dominant component inside the gran- ular salgs  was  still  Al2O3. Thus,  the presence of Al2O3  in the granular  slags  indicates it served as a skeleton of slagging. Ca2Al2SiO7, a common fluxing material in the slags  of Zhundong coal,  tended to appear at high temperature [5,24]. Na2Al2Si3O10 resulted from the reaction among Na-based compounds, SiO2  and Al2O3.  Formation of Al-containing compounds indicates that high sodium in  Zhundong coal  could consume Al gradually. In addition, many sodium compounds were still   not   detected by  XRD because of  the low crystallinity.
Na-  and  Si-rich particles  also   appear in  the deposits of  the air-cooled probe  in  position  P5  (Fig. 5b).   These particles were considered to  reserve strong viscosity at the gas  temperature  of
903  °C, like  under the adiabatic condition. However,  no  slagging occurred, indicating slagging was  inhibited by  the low  wall  temperature (~665 °C). One  explanation is  that those agglomerated and melted particles became less  viscous once they contacted with the low-temperature walls. The  weakened viscosity is  unable to produce enough adhesion force  that can  prevent the fall-off of particles from the  probe induced by  the  airflow collision or  self- weight. As  reported, the deposited ashes became heavier with the rise  of  probe temperature through a  drop tube furnace [25], which was  attributed to  the higher viscosity of the ashes on  the higher-temperature  surfaces.
A large amount of  fine  particles were adsorbed onto the surfaces  of both the air-  or water-cooled probes due to  the temperature gradient (Fig.  5).  These cooled surfaces were a  good   home for  volatile components. Fig. 5c  shows the SEM-EDX results of an ash  particle sampled from the water-cooled probe in  position P5. Despite small amounts of Ca, Si, Fe and S on  the surfaces, the Na and Cl contents are  still  very  high (9.05% and 13.54% respectively) compared with Fig. 5a.  Volatilized-Na content  was   up  to  50% at 900 °C,  and Cl  was   more  volatile [8].  It  is  believed that  large
amounts of Na and Cl are  still  gaseous at about 900 °C. These gaseous components were condensed on  the surfaces of the fine  particles absorbed by the cooled probes. The Na/Cl molar ratio on the particle surfaces is 1.03,  meaning a high possibility of Na  and Cl being condensed to  NaCl. The  Cl enrichment is a potential threat to  the metal heating surfaces [29].  The  condensed Na  severs as an  adhesive between fine  particles. 
Fig.  7  shows an  ash   particle with a  coating layer. The  local amplification shows that the smaller particles lean on  each other orderly and closely to  form a coating layer. EDX shows abundant Ca and S appear on the layer surfaces, indicating a higher probability that the smaller particles are  composed of CaSO4. Moreover, the Ca/S atomic ratio is 1.80  in area 2 and 1.65  in area 3, which means still a large amount of Ca exists in other forms. CaO is the most possible form in  addition to  CaSO4.  Chemical fractionation analysis shows that 66.6% of Ca in  Zhundong coal  is organically bounded [4] and this part of Ca is decomposed to oxides at low  temperature (<900 °C) [30].  At the reducing condition, another possible form of Ca is atom, which, together with CaO and CaSO4, is released from the raw coal  into gases. These components, when meeting low- temperature  surfaces or  when at low  gas  temperature, are  condensed to an ordered crystalline array on the surfaces of ash  particles.  These calcium species deposited  on  the surfaces of  fly  ash particles, making the ash  particles on  the heating surfaces stickier [2].  Under the combustion conditions, the clusters of  fine  CaSO4 particles  were  also   observed at  the  flue   gas   temperature   of 507  °C, in which the CaSO4  was  considered to  result from the sulfation of CaO [3].
3.3. Property changes of deposits along  the  gas flow direction

The morphologies of slags  and deposits along the gas flow direction are reflected by chemical compositions, which are related with the decreasing temperature [2].  As shown in  Fig. 8, the chemical compositions of the deposits change with temperature in different ways among the three conditions. For the adiabatic condition, the top  two elements in most deposits are  Ca and S, corresponding to the Ca- and S-rich raw coal.  This  indicates that CaSO4  contributes to  the deposition. With the temperature drop, Na,  Fe, Ca, Si and Al contents were all reduced from position P5 to  P8, especially in the interval from P5 to P7, i.e. the wall  temperature decreases from
882  to 804  °C. In contrast, the contents of other elements such as Cl and K were the lowest (~0) in position P5, and increased with the decreasing wall  temperature, which indicates Cl and K were gaseous at 882  °C and began to  condense below 804  °C. The  S, Ca, Fe and Mg contents decreased, but were still  higher than those under the air/water cooling conditions in  the corresponding positions.
Thus,  it is believed that S, Ca, Fe and Mg are  more likely  to  enrich on  the high-temperature surfaces.
Additionally, under the adiabatic condition, Na, Al, Si, S, Ca and Fe  contents in  the deposits were all  maximized at position P5 under all the three conditions, which explains the distinctive morphology. As reported, the ash  richer in Ca, Fe and Mg melted more easily [25].  When approaching high-temperature surfaces, these elements formed surface-adhering eutectic compounds which then combined with the strongly affinitive S to form low-melting- temperature sulfates.
Under the air-cooling condition, the Fe and Al contents declined with the decreasing temperature, but were maintained at lower levels compared with Ca, S, Si and Mg. Moreover, the contents of Na, Cl and K all increased much with the decreasing temperature, probably because they tend to  condense on  cooled surfaces [2]. Fe, Ca, S, Si, Al and Mg contents under the water-cooling condition were all lower compared with the other two conditions, suggesting there  is   no   temperature-dependent   regularity  below  100 °C.
However, the wall  temperature below 100 °C could still  stimulate the condensation of Na, Cl and K.
The above discussion suggests that the properties of the deposits changed significantly along the gas flow  direction. Especially at high  wall    temperature   (882–737 °C),  mineral  elements  were enriched more easily on  these surfaces. With the decrease of wall temperature,  Na,  Cl and K contents increased modestly, but the contents of most other mineral elements decreased evidently. In a  biomass furnace, medium-heated  surfaces below 900 °C were the major regions for  deposition [31],  which is  consistent with the fact  that the largest changes of mineral contents occurred at
882–804 °C.
The  changes of mineral phases in ashes with the gas  temperature were investigated via  XRD. Due  to  small number of samples, no   deposits  in   positions  P5–P7 were  detected  by   XRD.  Thus, Fig. 9 only  shows the XRD patterns of ash  samples from positions P4 and P9, whose major mineral phases are  CaSO4  and CaO, which is consistent with Fig. 7. Na4SiO4  and MgO were only  detected in  ash   samples from position P4,  indicating these mineral species were probably captured by  metal surface to s from position P4  P9. Since  Na, Ca, and Mg were more inclined to  reside in fine  particles [32],  the fine  ash  particles escaping from the cyclone were rich  in alkali metal compounds. These fine  particles adhered onto metal surfaces to  form the initial ash  deposition layer [33].  Especially   among  minerals,   the  presence of  alkali sulfates  greatly enhanced the adhesion force  between ash  and metal surfaces [22].
3.4. Differences  between ash  and  deposits 
Deposits originated from the fly ash  particles in  gases. To further understand the mechanism of deposition, we  compared the properties between deposits and fly  ash.  The  ash  at position P4, which fell  into the sampling can  gravitationally, was  representative  of  the fly  ash  in  the flue  gas.  The  chemical compositions  of ash  and deposits are  shown in  Fig. 10.  In these experiments, the contents of S, Ca and Fe all increased with the rise  of wall  temperature.  Especially, the  contents  of   these  elements  in   deposits exceeded those in  ash   only   under  the adiabatic condition. This indicates  that  S,   Ca   and  Fe   migrated   easily  to    the  high- temperature surfaces, but hardly to the low-temperature surfaces. As  reported,  alkali metal  compounds selectively adhered  and deposited onto metal surfaces [33].  Na,  K and Cl in  all  deposits (especially the air-cooled deposits) were more sufficient than those in  ash,  which proves that the corresponding gaseous components or fly ash  particles enriched in Na, K and Cl would gather on metal surfaces. Since  the air-cooling condition is  similar to  the actual industrial boiler, attention should be  paid to  the Cl-caused corrosion   of  deposits. Additionally,  alkali-metal-sulfate-caused  corrosion  might occur during the combustion of Zhundong coal  [34].
XRD reveals more mineral phases in deposits than in ash  (Figs. 9 and 11).  Besides CaSO4  and CaO detected in ash,  all deposits contain Ca2SiO4,  Ca3Mg(SiO4)2,  MgO  and NaCl.  The  presence of NaCl in deposits further proves that Na and Cl are  condensed in the form of  NaCl.  Ca3Mg(SiO4)2,  which resulted from the reaction among SiO2, MgO and CaO, was  also  detected in Zhundong coal  ash  under an oxyfuel  combustion  atmosphere  [5].   Ca3Mg(SiO4)2     would decompose to  Ca2SiO4  at ~1200 °C.
At 600 °C, the CaCO3   in  coal  would decompose to  CaO, which reacts with other mineral species to  form more complex minerals[2,5].  CaO in adiabatic deposits shows a lower XRD peak and thus is very  reactive with other mineral compared with that in  cooled deposits. Additionally, low  wall  temperature prevented the participation of SiO2  and Al2O3  during other chemical reactions, resulting in  their high XRD intensities in  water-cooled deposits. The  presence of Na4SiO4   in  deposits on  both the adiabatic and air-cooled probes indicates that high temperature surfaces could easily capture sodium silicates.
In  general, more mineral phases are  present in  deposits, and mineral elements such as  S, Ca  and Fe  are   prone to  enrich on high-wall-temperature  surfaces. Attention should be  paid to  chlorinous corrosion under the air-cooling condition.

4. Conclusions

The effects of wall  temperature on  slagging and ash  deposition during circulating fluidized bed  gasification of Zhundong coal were investigated. The main conclusions are  listed below.

(1)  Deposition was  greatly affected by  wall  temperature.  Slagging  occurred on  the surfaces at above 882  °C. As the gas- wall  temperature gradient was  enlarged, fine  particles were captured and deposited onto cooled surfaces. Deposition affected wall   temperature in  return and thus destabilized the heat flux.
(2)  The enrichment of Ca, Si, Na, Fe and S was  the main cause of slagging on  the high-temperature  surfaces. Volatile species such as  NaCl were condensed on  cooled surfaces and thus contributed to  deposition. Ca was   condensed and restructured in an ordered array to form a coating layer on ash  particle  surfaces, making ash  particles stickier.
(3)  The  contents of  mineral elements decreased along the gas flow  direction. The largest content changes occurred within the wall  temperature of 882–804 °C. The  contents of these elements were always low  at below 737  °C. However, gaseous Na,  Cl and K were condensed to  deposits along the gas  flow  direction.
(4)  The properties of deposits and ash  were very  different. More mineral phases were present in  deposits. Mineral elements such  as   S,  Ca  and  Fe  tended  to   enrich  in   deposits  on higher-wall-temperature  surfaces. Attention should be  paid to  chlorinous corrosion under air-cooling condition.


      Effects of electric fields on onset of dropwise condensation based on Gibbs free energy and availability                                                                                       analyses

Abstact
    A thermodynamic model based on changes in Gibbs free energy and availability is developed in this paper for onset of condensation of supersaturated or saturated vapor under electric fields. Both homogeneous condensation and heterogeneous condensation on subcooled surfaces are considered. Effects of contact angle, subcooled wall temperature and the strength of the electric field on heterogeneous condensation are illustrated. The results of these analyses show that an electric field can decrease the critical radius and promote onset of droplet condensation in a supersaturated or saturated vapor.

Introduction
    As early as in 1871, Thomson performed an analysis on thermodynamic equilibrium of vapor liquid interface with curvature and gave the initial form of Kelvin's equation, which describes the critical radii for onset of droplet condensation. Based on a grand potential analysis, Vorob'ev and Malyshenko concluded that external electric fields stimulated both bubble nucleation and onset of condensation. Warshavsky and Shchekin studied uniform electric field's effects on droplet shape and analyzed the change of chemical potential and the work required for droplet formation, and they proved that electric fields enhanced onset of droplet condensation. Gao et al. Used Monte Carlo method to simulate onset of condensation under two conditions: they found that an electric field promoted onset of condensation under the condition of fixed vapor chemical potential, but it prohibited onset of condensation under the condition of fixed temperature or pressure. All of these papers focused on effects of electric fields on onset of condensation in an environment with uniform temperature. 
    In this paper, we study the effect of an electric field on heterogeneous droplet condensation on a subcooled surface in a supersaturated or saturated vapor based on thermodynamic analyses, taking into consideration the wall temperature gradient and the contact angle of the subcooled surface. The critical radii for droplet condensation and bubble nucleation under the same degree of subcooling and superheat are first field is found to promote droplet condensation, which is in agreement with previous analyses. 

Onset of homogeneous condensation under an uniform electric field
    Consider the most simple case of homogeneous condensation in which droplet is condensed in a supersaturated vapor at a uniform temperature Tv and at a pressure pv, (which is higher than the saturated pressure ps corresponding to Tv) under an external imposed electric field.The chemical potential in a fluid under an external electric field with strength E can be written as. Where E is the strength of an imposed electric field, ¦Å is the dielectric constant of the fluid, ¦Í, ¦Ì, h, s are respectively the specific volume, the chemical potential, specific enthalpy and the specific entropy of the fluid. The change in specific Gibbs free energy after and before condensation is. where ¦Å and ¦Å are the dielectric constants of liquid and vapor respectively; El and Ev are the electric field strength imposed on liquid and vapor, respectively. The first term on the right hand of Eq. (2) can be calculated as: where Ts is the saturated temperature corresponding to vapor pressure pv; cpl and cpv are respectively specific heats of liquid and vapor at constant pressure, hlc and hvc are respectively the critical specific enthalpy of liquid and vapor at an equilibrium state where Tl = Tv. The second term on the right hand side of Eq. (2) can be calculated as. where hgf=fhfg is the latent heat from vapor phase to liquid phase; pl and pv are respectively the pressure inside and outside the liquid droplet.
Substituting Eqs. (3a), (3b), (3c) and (4) into Eq. (2) and setting El = where ¦¤T = Ts fTv N 0, ¦ÅL and ¦ÅG are the relative dielectric constant of liquid and vapor respectively, for which ¦Ål = ¦ÅL¦Å0 and ¦Åv = ¦ÅG¦Å0, with ¦Å0 = 8.85 ¡Á 1012 F/m being the vacuum permittivity. The relative dielectric constant of vapor ¦ÅG is regarded as a constant (which is listed in Table 1) and is independent of temperature. However, the relative dielectric constant of liquid is temperature dependent, which can be expressed as. It follows that the change in Gibbs free energy for the whole droplet is:where pl and pv are determined from the modified Young Laplace equation in an electric field. We can numerically calculate the critical radius for homogeneous condensation of droplet (rc1) at different electric field strengths by setting Eq.(7) equal to zero, i.e. The results of calculations, based on physical properties of water and water vapor listed in Table 1, are presented in Fig.1, where it can be seen that the critical radius for onset of homogeneous droplet condensation decreases with the imposed electric strength. If there is no electric field, i.e., E = 0, Eq.(10) with the aid of the Clausius Claperon equation gives, which is the Kelvin equation for critical radius for onset of homogeneous condensation of droplet. It is interesting to note that if Tv, pv, ¦Ñl in Eq. (11a) is replaced by Tl, pl, ¦Ñv, respectively, the resulting expression is identical to the critical radius for homogeneous bubble nucleation. Thus, expressions of critical radii for onset of homogeneous condensation of droplet and for homogeneous nucleation of bubble can be interchangeable after a simple substitution if the temperature of the environment is uniform. It is interesting to note from Eqs.(11a) and (11b) that the critical radius is inversely proportional to its initial phase's density respectively. The initial availability of the system (i.e., vapor at Tv and pv) can be expressed as. After droplet condensation occurs, the availability of the system with liquid, vapor and interface can be calculated as follow:It follows that the change in availability during homogeneous con- densation is the sum of Eqs. (13)¨C(15) minus Eq.(12) to give. Eq. (16) is presented in Fig. 2, where it can be seen that the peak value of availability change decreases with the electric field's strength. The peak value of availability represents the energy barrier of phase change because nature process will always tend to decrease the availability. Accordingly, the critical droplet radius (rc2) for onset of condensation of droplet can also be calculated as,  It can be seen from Fig. 2 that the condensation will have smaller energy barrier and smaller critical radius in a stronger electric field. This means that an electric field promotes homogeneous condensation of droplets. Fig. 3 shows changes in Gibbs free energy given by Eq. (7) and avail- ability given by Eq. (16) versus droplet radius during homogeneous condensation of droplet under the condition of Tv = 90 ¡ãC, Ts = 100 ¡ãC, and E =1 ¡Á 107 V/m. The two critical radii rc1 and rc2 in homogeneous condensation of droplet obtained respectively by Gibbs criteri marked in Fig.3. It can be seen that the critical radius obtained from ability criterion (rc2). It follows that Gibbs criterion is the necessary con-dition for onset of dropwise condensation, and the availability criterion is the sufficient condition. This conclusion is similar to those given by Dong et al for bubble nucleation. which is higher (or equal to) than the saturated pressure ps(Tv). It is assumed that: (i)The external electric field is uniform. (ii)The liquid droplet's temperature at height x is the same with the vapor temperature at the same height, i.e., Tv(x) = Tl(x) and temperature changes linearly from Tw to Ts, i.e. where Tv(xd) = Ts corresponds to the saturation temperature of the atmosphere vapor pressure. So, the temperature gradient k is: where xd is the height of the droplet, and ¦È is the contact angle of the substrate.
The temperature on top of the condensation droplet equals to the saturation temperature Ts corresponding to the environment vapor pressure at the onset of condensation; Next, consider the problem of onset of heterogeneous condensation of droplet on a wall (at x = 0) at a subcooled temperature Tw and with contact angle ¦È. Next to the wall is a semiinfinite extent of supersaturated (or saturated vapor) at a temperature Tv, and at a pressure pv
    The change in Gibbs free energy during heterogeneous condensa- tion of droplet on a subcooled wall can be obtained by integrating the specific Gibbs free energy change for the droplet's mass to give: where ¦¤T(x)= Ts Tv(x), ps(Ts) and ps(Tw) are the saturated pressures of water vapor corresponding to temperature Ts and Tw respectively. It should be noted that for the special case of ¦È = 180¡ã, Ts = Tw = Tv, ps(Tw) = ps, and ps(Ts) = pv, Eq. (20) will be reduced to the case of homogeneous condensation given by Eq. (7). According to Eq. (10), we can determine the critical radius (rc1) numerically. The results of this calculation for Tw=90 ¡ãC, Ts=100 ¡ãC and ¦È = 60¡ã are presented in Fig. 4, which shows that an electric field also promotes onset of heterogeneous condensation of droplets.
    The availability of initial state consists of the vapor phase and interface is: After condensation occurs, the availability of the system consisting of vapor, liquid and interface are given by: where the volume of the droplet and areas of interfaces are: and we can get the value of ¦Òlv¡ä from Eq. (9) for which the contact angle of the droplet under an electric field can be calculated as: Substituting Eqs. (25)¨C(29) in Eqs. (21)¨C(24), we can obtain the total availability change during heterogeneous condensation of droplets:
    Fig. 5 shows the change in availability during heterogeneous con- densation for Tw = 90 ¡ãC, Ts = 100 ¡ãC and ¦È = 60¡ã in different electric fields with strength of  0, 1 ¡Á 107 V/m and 4 ¡Á 107 V/m. It can be seen that the onset of condensation in a stronger electric field has a smaller energy barrier and a smaller critical radius, so an electric field also enhances onset of heterogeneous condensation of droplets. Fig. 6 shows the changes in Gibbs free energy and availability given by Eqs. (20) and (30) respectively during heterogeneous condensation of droplets for E =1 ¡Á 107 V/m, Ts = 100 ¡ãC, and Tw = 90 ¡ãC. Comparing this figure with homogeneous condensation under same conditions (E =1 ¡Á 107 V/m, Ts=100 ¡ãC, and; Tv = 90 ¡ãC, i.e., subcooling degree
to be 10 ¡ãC) shown in Fig. 3, it can be seen that the peak value of availability change in heterogeneous condensation of droplets in this figure is about 5 ¡Á 10^20 J, which is smaller than that of homogeneous condensation of droplets (about 4 ¡Á 10^19 J) shown in Fig. 3, and the corresponding critical radius of heterogeneous condensation is about 1.2nm as compared to homogeneous condensation critical radius of 1.3nm. Thus, under an electric field, homogeneous condensation of droplets is more difficult to occur than heterogeneous condensation of droplets under the same temperature conditions.

Onset of heterogeneous condensation without electric fields
    We now consider the special case of heterogeneous condensation of droplets without an electric field. For this purpose, we set E = 0 in Eqs. (20) and (30) to give the changes in Gibbs free energy and availability for heterogeneous condensation of droplet on a subcooled substrate with temperature Tw and contact angle ¦È: Eqs. (31a) and (31b) versus the droplet radius are presented in Fig. 7, which shows changes in Gibbs free energy and availability during het- erogeneous condensation of droplets in a supersaturated (or saturated vapor) without an electric field. Setting Eq. (31a) equal to zero, the critical radius for the simple situation of onset of heterogeneous condensation of droplet with no electric field (E = 0) is simplified as. Setting ¦È = 180¡ã, Ts = Tw = Tv, ps(Tw) = ps, and ps(Ts) = pv in Eq. (32), the result will be reduced to the most simple case for homogeneous condensation given by Eq. (11a). The critical radius rc2 can be obtained from Eq.(31b) according to Eq. (17). It can be shown from Fig. 7 that the critical radius rc1 is less than the critical radius rc2 although the difference is small. It should be noted that Quan et al. have derived the following expression for critical radius for heterogeneous nucleation of bubble with no electric field (E = 0). Under no electric field, a comparison of critical radii for droplet condensation (with a subcooling of 10 ¡ãC) given by Eq. (32) and bubble nucleation (with a superheat of 10 ¡ãC) given by Eq. (33) is presented in Fig. 8. It is interesting to note that the critical radius for onset of droplet condensation is much smaller than the critical radius for onset of bubble nucleation because the density of liquid is much larger than that of vapor. Fig. 8 also shows that onset of droplet's critical radius increases with the contact angle, which means that condensation is more prone to occur on a hydrophilic surface. However, the critical radius for bubble nucleation decreases with the contact angle, meaning that boiling bubble is more prone to nucleate on a hydrophobic surface. From Eqs. (32) and (33) we note that the two expressions of critical radii for onset of  dropwise condensation and bubble nucleation are not identical, because the two processes of bubble and droplet formation are not exactly opposite. This is reflected in the changes in specific Gibbs free energy during heterogeneous condensation of droplets and heterogeneous nucleation of bubbles are not exactly the opposite. For the onset of heterogeneous condensation, the specific Gibbs free energy change is where pl is a function of radius and calculated by Young Laplace equation, ps (Ts) equals to the vapor pressure pv. For heterogeneous nucle- ation of bubbles, the specific Gibbs free energy change is: where pv is a function of radius and calculated by Young Laplace equation, ps (Ts) equals to the liquid pressure pl. The difference in specific Gibbs free energy's expressions for onset of condensation and boiling nucleation will lead to a difference in total Gibbs free energy changes, so the expressions for critical radii in these two cases are not exactly opposite.

Conclusions
    In this paper, we have developed a thermodynamic model to explore the electric field's influence on onset of condensation. Gibbs free energy analysis and availability analysis have been applied to obtain critical radii of dropwise condensation and the energy barrier for onset of condensation. From this analysis, the following conclusions are obtained:
    An electric field can promote onset of homogeneous heterogeneous condensation of droplets, reflecting in a decrease of both critical radius and nucleation energy barrier.
    Critical radii for onset of dropwise condensation have been obtained from Gibbs free energy criterion and availability criterion respectively. It is found that the former is always smaller than the latter, so Gibbs free energy criterion is the necessary condition while the availability criterion is the sufficient condition for onset of condensation. This conclusion for droplet condensation is similar to those given by Dong et al for bubble nucleation.
     Under the same temperature conditions and the same electric field, it is found that onset of homogeneous condensation is harder to occur than onset of heterogeneous condensation, reflecting in a higher energy barrier and a larger critical radius than heterogeneous condensation.Hydrogen production by  Zhundong coal gasification in supercritical water
Hui Jin*, Yunan Chen, Zhiwei Ge, Shanke Liu, Changsheng Ren, Liejin Guo
a b s t r a c t
The high sodium content of  Zhundong coal restricts its application in  direct combustion and supercritical water gasification is  a promising clean and efficient coal conversion technology. To obtain the gasification and sodium distribution characteristics of Zhundong coal in  supercritical water, experimental investigations were conducted in  autoclaves and supercritical water fluidized bed reactors. The experimental results showed that as the concentration of  Zhundong coal slurry decreased or  the residence time increased, more sodium was released from the solid phase in  autoclaves. Carbon gasification efficiency of more than 95% was obtained with the coal slurry of  30 wt%~40 wt%  in  the fluidized bed reactor. Less  than 0.07% sodium was released from the reactor, in other words, most of the sodium remaining could be  collected and caused no  pollution and fouling problems for downstream process. Supercritical water fluidized bed reactor appeared to  be a promising reactor for  supercritical water gasification of Zhundong coal.
Introduction

Coal  accounts for  the main part of energy resources in China and will  play a dominant role in  the next decades [1]. Direct combustion of coal in  the gas  phase has caused serious environmental pollution and safety problems. Zhundong coalfield has huge exploitable reserves of  164  Gt  [2,3],  how- ever, direct combustion of Zhundong coal is restricted for  its high alkali metal content. Because alkali metal lowers the ash fusion temperature and leads to  serious fouling and slagging on  the boiler water wall quickly [4e8].
Efficient and clean Zhundong coal utilization method is urgently needed and advanced coal conversion methods such as pyrolysis or  gasification were investigated  [9,10].  Much attention was paid to  the effect of sodium upon the gasification results and solid foundation was built for  the clean con- version  of   Zhundong   coal  [5,11e13].However traditional gasification and pyrolysis were conducted in  gas  phase, and sodium containing fly ash may exist in the raw gas. Therefore, special fly  ash separation device is  necessary to  prevent slagging problems. Supercritical water gasification technology attracts people's  attention due to  the unique chemical and physical properties of supercritical water [14e17]. Supercritical water provides a homogenous and rapid reacting medium for   clean  coal  conversion  [18e26]. The  main  conversion product is hydrogen, which is believed to  be  one of the most promising fuel in this century [27,28]. Moreover, the solubility of  inorganic matter in  supercritical water is  quite limited, which makes the  separation of  unwanted inorganic slats easier [29e33]. MODAR SCWO  [29] reactor used reverse-flow vessel to  recover the slats with a small steam at the bottom of the reactor. Vogel  [34] investigated the separation of inorganic salt in the process of biomass gasification to recover the salts as fertilizer and also avoid catalyst poisoning. Tester [35] discussed several commercial approaches that have been developed and/or used to control salt precipitation and solids buildup in SCWO systems. Cao [36,37] conducted supercritical water gasification of black liquid, and investigated the alkali recovery from supercritical water fluidized bed reactor. Therefore, supercritical water gasification might be  a good choice for  coal with high sodium content.
The element transformations were examined by  re- searchers. For example, the transformations of Pb, Mn, Ni, Cu, Zn, Cd and Cr during the supercritical water treatment of used tires were studied by  Chen [38]. Nitrogen [39e41] and sulfur [42e44] transformation characteristics were investigated in the supercritical water environment from the point of view of pollution reduction. Su  [3] conducted the gasification of Zhundong  coal in   supercritical water  with a  quartz  tube reactor to  omit the unwanted catalytic effect of  the reactor wall and obtained a kinetic model for  Zhundong coal gasification. However, the distribution characteristics of  sodium were beyond the discussion. This manuscript investigated Zhundong coal gasification in  supercritical water with auto- claves and fluidized bed reactor, and attention was paid to the sodium transformation characteristics.
Materials and experimental method
Coal sample
Table 1  showed the elemental analysis and the proximate analysis of Zhundong coal. It can be  observed that the moisture content is  as high as 17.1%,  which also restricts the widespread application of Zhundong coal [15,45,46]. The coal ash analysis is  seen in  Table 2 and it  can be  seen that the sodium content is 3.40 wt%  and potassium content is 1.05 wt %. The coal particle size used in experiments was in the range of 75e150 mm.Apparatus  and experimental procedures Supercritical water gasification reactions were conducted in high-through autoclaves for  batch experiments and in  fluidized bed reactor system for  continuous experiments. The autoclave material  is   Hastelloy  C276.   The depth and the effective volume  of  the autoclave are 20  mm and 10  mL respectively. The designed temperature  and pressure of are 750℃ and 30  MPa  respectively. The average heating rate inside the reactor was about 70 o C/min. More details of  the autoclave were noted in the reference [17].
The fluidized bed reactor has supercritical water as fluid- izing agent. A  distributor made of  sintered metal with an average pore diameter of 50 mm is located in the bottom of the reactor. The sintered metal is provided by Northwest Institute for   Nonferrous  Metal  Research.  Water  preheated   to   the desired temperature flows through the distributor to  form a fluidization state [47,48].  Zhundong coal-water-slurry  flows into  the  reactor  above the  distributor.  The fluidized bed reactor is  constructed of  316  stainless steel and the design temperature and pressure are 700 o C and 30 MPa respectively. The inner diameter of the reactor is 40 mm, and the height of the reactor is 1650 mm.

Analytical methods and data interpretation

The composition of the gaseous products was analyzed by gas chromatography  (Agilent 7890A)  with thermal conductivity detectors (TCDs) and high-purity argon was used as the carrier gas. The yield of gas  was measured by wet gas flow meter. The determination of the sodium content in  the aqueous or solid sample was conducted in inductively coupled plasma-atomic emission spectrometry (ICP-AES, IRIS Intrepid II, Thermo Scientific). Carbon gasification efficiency and hydrogen yield were selected to  evaluate the gasification characteristics and defined as follows [17]:
CE (Carbon gasification  Efficiency) ¼ (total carbon in  the gaseous products)/(total carbon in Zhundong coal)  × 100(%)
Gas  yield ¼ (molar amount of a certain component of the gaseous products)/(mass of Zhundong coal)  (mol/kg).
Results and discussion

Effect of concentration

High concentration means high handling capacity of Zhundong coal in reactors, however, high concentration of feed- stock may be  overloaded for a certain reactor. The effects of concentration upon Zhundong coal gasification results were investigated in autoclaves as seen in Fig. 1. When the concentration of Zhundong coal was 5 wt%, the main gas fraction (fraction is based on volume in the paper) was H2 and CO2 and their fractions were 56.86%  and 40.75%  respectively.  As the concentration increased from 5 wt% to 15 wt%,  the fraction of H2 decreased to  40.74%  and the fraction of CO2  increased to
36.29%.  It was  worth  mentioning  that  the  fraction  of  CH4
increased obviously from 7.54%  to 21.26%.  It was probable that high concentration of  coal means  low  content of  supercritical water molecules surrounding reactants, therefore the methanation reaction (R-1) was enhanced for  CH4  production. The increase of CH4 also increased the lower heating value of  the gaseous product for utilization based on combustion.
4H2  þ CO2 /CH4  þ 2H2 O 	(R-1) As  the concentration of  Zhundong coal increased  from
5 wt%  to 15 wt%,  the carbon gasification efficiency decreased sharply from 45.72%  to  29.85%,  and the hydrogen yield decreased from 28.62  to  9.75 mol/kg. It meant that high concentration of  Zhundong coal decreased the amount  of supercritical water molecules around reactants. It can be deduced that the enhancement of heat transfer and effective catalyst were possible solutions to enhance the gasification process.
When the concentration of  Zhundong coal was 5  wt%,76.74%  of the sodium content in  coal was found in  the liquid phase and 23.25% of the sodium was found in the solid phase. The sum of sodium in the liquid and solid phase was not equal to  unity due to  the experimental errors. When the concentration increased from 5 wt% to 15 wt%,  the sodium measured in  the liquid phase decreased and the sodium in  the solid phase increased. It proved that low concentration was in favor of complete gasification and the release of sodium from coal into liquid phase.
Effect of residence time

Fig. 2 showed the influence of residence time on  gasification results. The fraction of CO was still within the range of 1%~3%. It was may be due to the catalytic effect on  the water gas  shift reaction as R-2 caused by sodium in Zhundong coal [49,50]. As the residence time increased from 10  min to  30  min, the fraction of H2 increased from 41.76%  to  45.29%  and hydrogen yield increased from 11.53  to  14.92  mol/kg and it proved that long residence time facilitated H2 production. As the residence time increased, the carbon gasification efficiency increased from 33.89%  to  37.96%,  meanwhile, the sodium content remained in solid phase decreased from 65.89%  to 29.47%.

Effect of temperature

Fig.  3 showed the effects of  reaction temperature upon the gasification results. It can be seen that high temperature was in favor of hydrogen production reaction. As the temperature increased from 650 to 750 o C, the carbon gasification efficiency increased from 19.89% to 31.46%. Because Kruse proposed that supercritical water gasification product distribution indicated two competing reaction pathways: One  pathway consisted of ionic reaction steps preferred at higher pressure and/or lower temperature. The second reaction pathway was a free radical degradation dominating at lower pressure and/or higher temperature [51,52].  Therefore, in  the case of  high temperature, more sodium in  coal were released from coal matrix because the gasification was enhanced. However, high temperature also led to low dielectric constant so as to restrain the release of  sodium from coal particles. To  sum up the above factors, the experimental results appeared that the distribution deviation was not obvious within the temperature range investigated.

Effect of potassium  carbonate

The experimental results above showed that the sodium in Zhundong coal  had  certain  catalytic  effect  upon  the gasification process in supercritical water. The effect of extra potassium carbonate was investigated and the experimental results were seen in  Fig. 4. It can be  seen by  comparison  of Figs. 4(a) and 1(a) that the fraction of gaseous distribution was quite similar. It proved that the amount of sodium in  Zhundong coal might be adequate for the catalyst for homogeneous reactions in  gas  phase. It can be  seen by comparison of Figs.
1(b) and 4(b) that when the concentration of Zhundong coal was 5 wt%,  the carbon gasification efficiency increased from 45.73  to  78.68%,  and the hydrogen yield increased from 28.62 to  48.86  mol/kg. It  proved that the sodium originally inside Zhundong coal was not adequate for  the heterogeneous car- bon gasification reaction, thus extra addition of catalyst was necessary.

Zhundong coal gasification  in supercritical  water fluidized bed

It can be  seen from the gasification result that the catalytic effect of  potassium carbonate was limited and the carbon gasification efficiency was  41.30%  when the concentration was 15 wt%.  The enhancement of heat and mass transfer is necessary for  further enhancement of  gasification reaction. Supercritical water fluidized bed reactor was famous for  the advantages for  preventing the reactor from shut-down by plugging [53]. Supercritical water fluidized bed reactor system was used to investigate the gasification results. It can be seen from Fig.  5(a)  that the gaseous products are mainly H2 and CO2, and the fractions were 49e52% and 32e35% respectively. The CO fraction was within the range of 2e3%. The gaseous product distribution was quite similar to  the experimental results by  the autoclaves. When the concentration of Zhundong coal was 40%,  the carbon gasification efficiency was 95.7%  and the hydrogen yield was 42.22  mol/kg. When the concentration was 30 wt%,  the carbon gasification efficiency was 97.3%  and the hydrogen yield was 48.56   mol/kg. The concentration of  the coal water slurry upon the gasification results of  supercritical water fluidized bed reactor was observed in  Fig. 5. It is emphasized that concentration for autoclaves referred to  the concentration in  the autoclaves. The concentration  for  the supercritical water fluidized bed reactor referred to  the concentration of  the feedstock (coal- water-slurry), and the concentration in  the reactor can be calculated by considering the dilution effect of the preheated water.
The carbon gasification efficiency was obtained in  the re- action temperature range of 620e660 o C. It can be  seen that carbon gasification efficiency of  Zhundong coal in  fluidized bed was obviously much higher than that in  autoclaves, and the reasons might be  as follows: (1)From the point of view of thermodynamics, higher reacting temperature causes higher hydrogen yield [47]. And  the heating rate is the most important factor for  supercritical water gasification of coal particle for a certain reactor temperature. When the heating rate of the coal particle is slow, the time for  the coal particle in  the low temperature  condition would be  long, afterward  unwanted intermediates will  be  produced to  form char, once formed, is difficult to  be  gasified [54e56]. In the autoclave, the temperature has almost the same heating rate with the fluid inside the reactor, while in supercritical water fluidized bed reactor, the temperature of  the coal particle was increased to  the bed temperature  rapidly. (2)The coal gasification reaction in  supercritical water is endothermic, and the water temperature around coal particle may be decreased, which has a negative effect upon the gasification process. While in  supercritical water fluidized bed reactor, the heat transfer around a coal particle is vigorously enhanced by the random motion of the coal particles. (3)In  the process of  the coal gasification, the products and intermediates are needed to be diffused and the reactant supercritical water is needed to  be  diffused towards the coal particles, otherwise, the unwanted polymerization reaction results in the formation of by-products such as char. The diffusion coefficient of organic matter or  gas  is not high enough in  autoclave compared with supercritical water fluidized bed reactor [48,57,58]. So it  is  obviously seen that the carbon gasification efficiency was far  higher than that of autoclaves. Besides, supercritical water has the potential ability to  separate the ash from the reactor, and the bed material is apt to  support catalyst [59]. Therefore, it is  believed that supercritical water fluidized bed reactor is a promising reactor type for  coal gasification in supercritical water.
Sodium distribution was also studied in supercritical water fluidized bed reactor. The sodium distribution was only measured in  the liquid phase since Figs.  1e3(c) showed good sodium balance to  prove the rationality of  the sodium measuring method used in this paper. It can be deduced from the  former experiments that  the  system was believed to operate stably when the accumulated gas   yield was linear with the running time [60]. When the system operation was in stable condition, the liquid effluent from the low-pressure gaseliquid separator  [60]  was collected with a measuring flask. The liquid effluent was diluted by 50-fold and measured by  ICP.  The experimental result showed that  only a small amount of  the sodium was carried away from the fluidized bed reactor and vast majority of  sodium remained in  the fluidize bed reactor due to  the limit solubility of  inorganic salts. In the case of 30 wt%  concentration case, 0.061%  of sodium was released from the  fluidized bed reactor, thus 99.939% of sodium remained in fluidized bed reactor. While in the case of 40 wt% concentration case, 99.932% of sodium was remained in the fluidized bed reactor. The deposited inorganic slats might be deslaged by lock-hopper online and the online deslagging device is  developing and related details will  be published later.
Conclusions

The gasification results  of  Zhundong coal and the sodium distribution characteristics in supercritical water were inves- tigated. The following conclusions were drawn:

1) Low Zhundong coal concentration and long residence time were in favor of the release of sodium from Zhundong coal. Reaction temperature  had  great influence upon carbon gasification efficiency but had no  significant effect upon sodium distribution.
2) Zhundong  coal slurry  with  40  wt%   concentration  was gasified  within   the   reaction   temperature    range   of
620 o Ce660 o C, the carbon gasification efficiency was 95.7%
and the hydrogen yield was 42.22  mol/kg. 99.932%  of  so- dium in Zhundong coal was remained in the fluidized bed reactor and could be removed as inorganic salts.
3) Supercritical water  fluidized bed reactor is  an effective reactor  for    Zhundong  coal  gasification  and   sodium removal.




The ash  deposition mechanism in boilers burning Zhundong coal with high contents of sodium and calcium: A study from ash  evaporating t℃ondensing

Xuebin Wang a, *, Zhaoxia Xu b, Bo Wei  a, Lan Zhang c, Houzhang  Tan a, *, Tao Yang a, Hrvoje Mikulˇci´c d, Neven Dui´c d
a b s t r a c t  

The  high contents of sodium and calcium in Zhundong coal  induce severe slagging and ash deposition in boilers. In this study, the ash deposition mechanism was investigated based on  the results obtained from a full-scale boiler (350 MW) burning Zhundong coal,  and a ﬁxed bed reactor used for  ash evaporating- condensing. In  the full-scale boiler,  the condensing and depositing of  sodium and calcium sulfates play an  important role on  ash depositing on  convection heating surfaces. Sulfates start to signiﬁcantly condense and deposit at the ﬂue gas  temperature of about 850 ℃ on the medium and high temperature reheater surfaces. Ash  evaporating tests proved that, with the increasing in  temperature from 400 ℃ to 1200 ℃, the ash evaporating process is  divided into three stages: 1)  400e800 ℃, 80% of  sodium,  and100%  of  chlorine are released; 2)  800e1000 ℃, all  the left sodium evaporates and sulfur starts to be released with the formation of partial aluminosilicates; 3)  1000e1200 ℃, all  the left sulfur is released through the decomposition of calcium sulfates and then calcium starts to evaporate, while silicon oxides disappear due to the formation of new complex silicates. Ash  condensing tests further proved that, the sodium in Zhundong coal  was released mainly in the forms of atom, oxide, and chloride, in which sodium chloride account for  about 50%.  When the evaporating temperature  increased higher than 1000 ℃, partial alkali and alkaline earth metals were released as  gaseous sulfates, and afterward condense and deposit on the heating surfaces. At last, a temperature-dependent  ash deposition mechanism in  Zhun- dong coal  combustion was proposed.
1.  Introduction
Zhundong, located in the east of Junggar Basin,  Xinjiang, China, with the forecast reserves of 390  billion tons is the largest intact coalﬁeld in the world. According to the current coal consumption in China, the Zhundong coal could meet the needs for coal consumption in China for the next 100  years [1,2]. Zhundong coal is characterized by high ignition quality and low environmental threat due to the high volatile content  and ultralow ash   and sulfur contents.  However, similar with the brown coal in Australia [3e6], the content of sodium in Zhundong coal ash  is very high, generally more than 5%, far more than the normal sodium content level in typical Chinese coal, usually between 1 and 2% [7,8].  Consequently severe problems of slagging, fouling, and corrosion on boiler heating surfaces are induced, which deteriorate the heat ﬂux, efﬁciency, and lifetime of a boiler [9]. Xi'an Thermal Power Research  Institute  (TPRI) investigated the slagging and fouling status quo  of dozens of boilers burning Zhundong coal in Xinjiang, China.  It indicated that boilers would be under great threat from slagging when the sodium content in coal ash exceeded 2.5 and
3% for 100  MW  and 225e350 MW  boilers, respectively. The  boiler adaptability of even larger capacity 600e1000 MW on high sodium coal is still unknown [7].
In this study eight kinds of Zhundong coal were compared with twenty-seven kinds of non-Zhundong coal very commonly used in China,  including bituminous, sub-bituminous, and lignite coal. The inverse values of  sodium, calcium and sulfur content  in  ash  are plotted in Fig. 1, showing that Zhudnong coal  is astricted to a very limited region. It can  also  be observed that the calcium and sulfur contents in Zhundong coal ash  were much higher than the average contents in  non-Zhundong coal  ash.  The  contents of calcium and sulfur in  certain types of Zhundong coal  ash  are  higher than 30%, and in some cases even 40%. Harbin Boiler Company has  conducted a test in a 300 MW boiler co-ﬁring Zhundong coal, and after a dozen days operation, the convection heating surfaces were blocked by severe slagging and ash  deposition. In this test, a high amount of calcium sulfates was found in  the slags  and deposits [10].  These observations indicated that beside sodium, calcium and sulfur species also  play  an  important role  in the slagging and ash  depositing process.
Recently,  only   a  few  studies investigated the Zhundong coal slagging. He et al. [11] used Laser-Induced Breakdown Spectroscopy (LIBS) to measure atomic sodium emissions in  Zhundong coal combustion under low  O2  concentration (3.9e10.6%,  oxygen-fuel combustion).  The   study  showed  that  very  little  sodium  was released from the Zhundong coal's volatile matters. 20e38% of sodium was released from the char content while 60e78%  of sodium was released from the ash  content. The  study also  showed that the ratio of  potassium released from the char content was enhanced by  O2  but   inhibited by  CO2  concentration. Zhou  et al. [12,13] developed  an  ash  deposition probe with charge-coupled device camera and heat ﬂux monitoring system to  on-line record the deposition thickness and heat ﬂux in a 300  KW test furnace. It was found that the deposit growth process was composed of four stages and the slagging deposits showed layer structures. However, the enrichment of alkali and alkali-earth metal was not found in each deposition layer.  Xu  et al.  [1]  compared the ash  properties from Zhundong coal co-ﬁring in a lab-scale drop tube furnace and a power plant boiler,  which agreed well.   The  study showed that particles smaller than 10  mm  had higher Ca, Fe, and Mg  content, while the weight fractions of sodium and potassium were not that high. In another study on Zhundong coal slagging, the ash  deposits were collected from different positions in  a  30  MW  subcritical boiler.  The  chemical analysis showed that the ash  fouling on  the superheater tubes was formed with a thinner Fe-rich layer followed by  the deposition of  Na2SO4  liquids.  The  slagging in  the water
cooled wall  was mainly induced by the low-temperature eutectics
from the interaction of CaO and Fe2O3  on the receding char surface, and the eutectic CaeAleSi containing Fe2þ-bearing oxide [9]. The other studies on  Zhundong coal  slagging were mainly focused on the ash  fusion and melting by  using lab-scale furnaces and ash melting  apparatus.  Meng [14]   found that  the low-temperature eutectics of ferrum and aluminosilicate played an  important role in  the high-temperature sintering of  Zhundong coal  ash  and its slagging near the water cooled wall.  It  was also  found that this phenomenon is comparatively affected by O2 concentration. Wang et al. [15]  following the State Standard of GB/T 212-2008 and GB/T
219-1996, measured the ash  melting temperature for  different Zhundong coal  co-ﬁring tests. The  study showed that there was a worst co-ﬁring ratio for the lowest melting temperature. However, the most recent work [16]  pointed out  that the two named stan- dards were not suitable for  the evaluation of Zhundong coal  ash melting, since the  softening temperature  decreased by  100   ℃, when the ashing temperature was lowered from 815  to 500  ℃.
Summarizing the previous results, the authors note that the slagging problem in Zhundong coal  combustion is mainly ascribed to two reasons: (1)  the formation of  low-temperature eutectics containing ferrum at high furnace temperatures in the water cooled wall  region, and (2) re-condensing of evaporated alkali and alkaline earth metals (AAEMs) at lowered gas  temperatures in the convection heating surface region. The  melting or  fusion of eutectics containing ferrum has   been widely reported, regardless of  coal types, and it mainly depends upon the primary components in coal ash  like  silicon, aluminum, and calcium. However, the trans- formation of AAEMs is more associated t℃oal types, for Zhundong coal  as newly found low-rank lignite with extremely high sodium and calcium content, the understanding of AAEMs transformation and ash  deposition is still  insufﬁcient. Therefore, this study aims to investigate ash  deposition mechanism in  the convection heating surface region of  boilers burning Zhundong coal, via incorporating the tests in a full-scale boiler and in a lab-scale ﬁxed bed reactor. In the full-scale 350 MW boiler, the slags and deposits were collected and analyzed at different convection heating surfaces along the gas  ﬂow direction. Moreover,  a special temperature-controlled condensing probe, above the fuel stack, was designed to capture the released AAEMs vapor in a ﬁxed bed  reactor. The  residual solid   ash  and the captured condensed species were analyzed by using X-ray ﬂuorescence (XRF), X-ray diffraction (XRD), scanning electron microscope (SEM),  and energy dispersive spec- trometer (EDS). The  research goal  is to build a full  transformation mechanism of sodium and calcium from the Zhundong coal, through evaporating and condensing, to deposits in boilers.

2.  Experimental section

2.1.  Fuel properties

Since  there is no  boiler that could completely be  operated on Zhundong coal,  in  the full-scale test Zhundong (ZD) coal  was co- ﬁred with Zhunnan (ZN) coal,  while in lab-scale test only  ZD coal was used. The  fuel  properties of ZD and ZN coal  are  compared in Table  1. The  water content in  ZD coal  is twice of that in  ZN coal, while the ash  content in ZD coal  is lower than in the ZN coal. Note that the ash  fusion temperatures (DT, ST, and HT) of ZD coal  are
higher than that of ZN coal by 120  ℃, even if the slagging of ZD coal
is much more severe. The  contents of calcium and sodium in  ZD coal  ash  are  over 40% and 6%, respectively. These two contents are much higher in ZD coal  than that in ZN coal,  however the ferrum content in ZD and ZN coal  ash  is almost the same. The comparison of fuel  properties indicates that the key  factors inducing slagging should be  high sodium and calcium contents, rather than melting and ferrum content.

2.2.  Slag and  deposit sampling in a full-scale boiler

Slags  and deposits were sampled in a 350  MW  tangential-ﬁred boiler (Model, DG1211/17.4-II22) after  continuous  operating  for six months. As shown in the Fig. 2, ﬁve layers of burners (a, b, c, d, and e) were running, in which three layers (b, c, and e) were used for ZD coal  combustion and the other two layers (a, d) were for ZN coal. Because the coal feeding quantity in each layer is the same, the co-ﬁring ratio of ZD coal  is about 60%, and the remaining 40% if of ZN coal. The sampling positions are  also  marked in the Fig. 2. They are  located on  each convection heating surface along the gas  ﬂow direction: wall  reheater (A-WR),  separation platen superheater (B- DPS), platen superheater (C-PS), medium and high temperature reheater (D-MHR),  high temperature superheater (E-HS),  low temperature superheater (F-LS), economizer (G-Eco),  air preheater (H-Pre).

2.3.  Evaporating and  condensing of ash  species

The  evaporating and re-condensing test was operated in  the schematic experimental system in Fig. 3, mainly including evaporating unit and condensation sampling unit. The  crucible contain- ing samples was heated in a tube furnace for ash  evaporating. An S- type thermocouple was inserted from furnace bottom to  measure the evaporating temperature. The  temperature-controlled condensation sampling unit was 5 cm  above the crucible, in order to capture the evaporated metal species for 1 h. The sampling unit was cooled by  air  and the sampling substrate at  the bottom was dismountable for  further analysis. A K-type thermocouple was welded on  the substrate inner surface to  measure the condensing  surface temperature,  which  was  controlled  at   500   ± 5  ℃  by adjusting air  speed. This condensing temperature range was used, since in the previous work by Schoﬁeld [17] it was reported that the deposition rate of sodium salts was almost unchanged with probe temperature  changing, when  the  probe  temperature  is  below 550  ℃
The fuel  sample was heated from ambient temperature to three
targeted temperatures: 500, 800, and 1000 ℃ at a constant heating rate of 5 ℃/min. The  entire evaporating process was divided into three stages and the condensed metal vapor species were continuously sampled at each stage by changing a new substrate.
The evaporation and condensation sampling were continuous in this apparatus, thus it  was impossible to  obtain the residual ash sample in  the crucible for  analysis. Therefore, the ash  was again prepared in an  atmosphere-controlled mufﬂe furnace (KSL-1700X, made by Hefei  Ke Jing Materials Technology Co., Ltd., China) by the heating curve shown in Fig. 3, at 400, 600, 800, 1000, 1100, 1150 and 1250 ℃ under air atmosphere. The total ﬂow rate passing through
the furnace and samples was controlled at 500  ml/min (O2, 100  ml/ min; N2, 400  ml/min). In this paper, only  the data at 400e1100 ℃ were presented, because at the high temperature of 1150e1250 ℃,the ash  samples melted and were tightly adhered on  the crucible surfaces, and it was impossible to carry out  the analysis.

2.4.  Characterization of slag  and  deposit samples

X-ray  ﬂuorescence (XRF, S4 PIONEER, Germany) and X-ray diffraction (XRD, X'pert MPD  Pro,  PANalytical, Netherlands)  were used to  analyze the element distribution and the chemical com- ponents of the deposits from full-scale furnace, and of the residual ash  from the mufﬂe furnace. The  micro-morphology and the element distribution in  the condensed deposits on  sampling sub-strate surfaces were analyzed by  using scanning electron micro- scopyeenergy  dispersive  spectrometry   (SEMeEDS,   JEOL  JSM-6390A,  Japan).

3.  Results  and discussion

3.1.  Morphology and  chemical components of deposit samples in a full-scale boiler

The  ﬂue gas,  steam, air,  and wall  temperature at  each heating surface were  collected from power plant  control system, and plotted in Fig. 4(a).  In this power plant burning Zhundong coal, the wall-temperature was mainly monitored at  the heating surface of super heaters and reheaters. At each stage of heating surfaces, there were  twelve  temperature  monitors, and  the  wall-temperature value in Fig. 4 was obtained after averaging. The water-steam and ﬂue gas  temperatures were obtained by directly reading from the monitor system of the power plant.
The  morphologies of slags  and ash  deposits along gas  ﬂow direction are  shown in Fig. 4(b).  It can  be seen that all the slags  and deposits from A-WR  to E-HS have a dark-red color,  which can  be associated to the presence of the ferrous oxides. In the region of high ﬂue gas temperatures (A-WR and B-DPS, Tﬂue gas > 872 ℃), there are a certain amount of dark porous slags.  In medium gas temperature regions (C-PS, D-MHR, and E-HS, 872  ℃ > Tﬂue  gas>630 ℃), there were no dark porous slags and these compacted deposits looked like blocky red  soil.  At last  three heating surfaces (F-LS, G-Eco, and H- Pre,  630  ℃ > Tﬂue  gas  > 141  ℃), no  blocky deposit was found, and there was mainly the loose ash stacking on horizontal tube surfaces. The samples in Fig. 4 can be divided into three groups: dark porous slags,  compacted dark-red blocks, and loose ash  stackings.
The  elemental  compositions  for   all   the  deposit  samples collected on  each surfaces are   displayed as  stacked columns in Fig. 5(a). It can be seen that the top  three elements in most deposits are  silicon, calcium, and sulfur, corresponding to the co-ﬁring ratio, ash  and inorganic elemental content in raw fuels.  The ash  content in ZN coal is more than twice of that in ZD coal, and silicon content in ZN coal ash  is almost 45% while that in ZD coal is below 18%. The calcium content ranks the ﬁrst (~40%) and third (~13%) in ZD and ZN  coal  ash,   respectively.  The  sulfur contents in  these deposits range from 11% to 24%, much higher than the ash  sulfur contents in Table  1. This indicates that sulfur oxides released from volatile and char react with inorganic ash species, and form sulfates that deposit on  the ash  or heating surfaces. The  sodium content is comparably low  and stable ranging from 3 to 5%.
In Fig. 5(a),  it can  also  be observed that in the region of D-MHR and  E-HS,  the  contents of  calcium and  sulfur were  suddenly enlarged while the contents of silicon and aluminum were lowered.
This  indicates that  calcium and sulfur species have higher concentrations in these two regions. To show it more clearly, the mass ratios of  S/(Si  þ Al), Ca/(Si  þ Al), and Na/(Si  þ Al) are  shown in Fig. 5(b).  In Fig. 5(b),  the signiﬁcant increase of sulfur, calcium, and sodium content in D-MHR and E-HS regions can be observed. These three element contents in E-HS region are  2.5 times than that in A- WR, B-DPS, F-LS, G-Eco, and H-Pre regions.
Comparing the  ﬂue  gas   temperature  in   Fig.  4(a)   with the elemental composition in Fig. 5(b), it can be seen that from A-WR t℃-PS, the ﬂue gas  temperature decreases from 1316 to 872  ℃, and there is no  apparent increase of sodium, calcium, and sulfur con- tent. However, from C-PS to D-MHR, with the ﬂue gas temperature further decreases from 872  to 772  ℃, a sharp increase in sodium, calcium, and sulfur content can be observed. This proves that in the temperature range of 872e772 ℃, amount of gaseous sodium and calcium sulfates condense into sulfate aerosols and then deposit on ash  particle and heating surfaces. The  condensing and depositing quantity of sulfates reaches the highest value at  695  ℃, in  E-HS region. Most sulfates condense and deposit in  D-MHR  and  E-HS region (872e630 ℃), since at the last  three heating surfaces (F-LS, G-Eco,  and H-Pre, 630  ℃ > Tﬂue  gas  > 141  ℃), sulfate content in deposits and ash  is almost constant.
The  observed phenomenon is  very similar with our  previous studies related to  the ash  deposition in  a full-scale grate furnace burning biomass with high potassium content [18].  In biomass furnaces, the gaseous sodium sulfates in  ﬂue gas  also  started its condensing and depositing on  medium  heating  surfaces below
900  ℃ [19].  This indicated that, when combusting fuels  with high
contents of AAEMs, there is a critical temperature around 850  ℃, at which most of the AAEMs sulfates start to condense, form aerosols, and to deposit.
XRD analysis was conducted to qualitatively identify the major and minor mineral components in  slags  and deposits shown in Table  2. In this table the pane color depth stands for mineral con- tent level.  The eight samples were divided into two groups: in the ﬁrst group (AeE) CaSO4 ranked the ﬁrst, while in the second group (FeH) SiO2  ranked the ﬁrst. This  is  corresponding to  the macro morphology and elemental composition results. The  samples in ﬁrst group are  mainly porous slags  and blocky deposits, due to the melting at high temperatures and condensing at medium temperatures. In  the second group, since already most of  sulfates have condensed and deposited in previous regions, CaSO4 content in this group of deposits decreases, SiO2  and Al6Si2O13 contents increase. Therefore these samples look  like  loose ash  containing more aluminosilicates. Moreover, the authors tried to ﬁnd sodium sulfates in the samples, however,  no  Na2SO4 was  found, only   some minor minerals like  NaAlSiO4,  (Na,Ca)(Si,Al)4O8, and  Na2S.  The  named minerals were also  mainly detected in the regions of C-PS, D-MHR,and E-HS, at  medium gas  temperatures. The  possible patterns  of sodium  sulfates might  be   covered  by  that of  calcium sulfates, because the calcium content was 40.69%  that was almost seven times of sodium content 6.07% as shown in Table  1.


3.2.  Chemical component of coal  ash  at  different evaporating temperatures

The  full-scale co-ﬁring test gives  the chemical components of slags   and deposits on  heating surfaces in  boilers, which clearly showed the  enrichment and depositing of  calcium sulfates on heating surfaces. However, due to the co-ﬁring of ZD and ZN coal, and relative lower content of sodium, 1/8  of calcium content, the enrichment of  sodium species in  deposits was not observed. To further demonstrate the evolution of  sodium from the raw Zhungdong coal  to  deposits, the experiments of  ash  evaporating and re-condensing were conducted.
Zhundong coal  ash  was mainly prepared in a mufﬂe furnace at
temperatures ranging from 400 to 1400 ℃ in air atmosphere. At the temperatures  of1200 ℃ and 1400 ℃, the ash   samples melted, ﬂowed, adhered to  the alundum plate, and were impossible to  be analyzed. The SEM micro morphologies of ash prepared at 600, 815,1000,  and 1100 ℃ are shown in  Fig.  6.  With the temperature
increasing from 600  to 1000 ℃, the ash  particles size  decreased with no  conspicuous sintering. However,  when the temperature was continually increased to 1100 ℃, signiﬁcant sintering occurred, ash  particles melted,  and aggregated into large porous pieces. In this study,  the heating time in  mufﬂe furnace is 1 h  at  targeting temperatures, while the practical residence time of coal  and ash particles in boilers is only  3e6 s, which seems incomparable. However,  those ash  particles depositing and adhering on  heating surfaces indeed undergo a much longer period. With the increasing in  deposits thickness on  heating surfaces, the heating transfer ef- ﬁciency is signiﬁcantly reduced, and the wall  or  deposits temperature can  increase from 500  to 1200 ℃ [13].  At such a high local wall  temperature (>1100 ℃), ash  particles on  wall  surfaces melt, aggregated, and tightly adhered on  unclean heating surfaces.
The  elemental composition of ash  at  different temperatures  is shown in Fig. 7(a).  With the increasing in ashing temperature, sodium and potassium contents  decrease, and at  temperature  of 1000℃, there is almost no  alkali metal. In  contrast, for  the elements like  silicon and aluminum, their contents in  ash  increase with temperature increasing. The change of oxygen content is also draws attention. Due  to  its  decreasing with temperature, this phenomenon indicates partial minerals release as oxides. More- over,  what should be  noted is  that sulfur content decreases at 1000 ℃ and becomes zero at 1100 ℃.
Multiplying the elemental content (%) with ashing ratio (%), and raw sample mass (g),  to  reveal the elemental evaporating more clearly, the absolute residual mass of key elements per  gram ZD coal can  be calculated. These values are  plotted in Fig. 7(b).  The right Y axis  scale  of calcium and sulfur is 10  times of that of sodium and chlorine  to  keep  them  in   one    ﬁgure  comparably.   With the increasing in ashing temperature from 400  to 1200 ℃, the evaporating process can  be  divided into three stages identiﬁed by  the curves of ashing ratio and residual element mass. During the ﬁrst stage, from 400 to 815 ℃, 80% of sodium, and 100% of chlorine were released. During the second stage, from 800  to 1000 ℃, all the left sodium evaporated and sulfur started to  be  released. During the third stage, from 1000 to 1100 ℃, all the left sulfur was released and calcium started its evaporating.
The release of chlorine in Zhundong coal  was very similar with our  previous investigation for  biomass fuel,  in  which all  chlorine was also  released at around 800  ℃ [20].  A previous XAFS study showed that all the chlorine in  brown coal  was likely  to  be  associated with sodium [3].  Here it can  be  noted that in  Fig. 7(b)  the released mass quantity of chlorine was much less  than that of so- dium, which indicated that only  partial sodium was released as
chloride and the other might be  released as  atom, oxide, and hydroxide. The calcium in coal  ash  was very stable and released only when the temperature was higher than 1100 ℃. Most of alkaline earth metals (Ca and Mg) existed as ion-exchangeable carboxylates in raw coal,  which decomposed and produced oxides at  low  temperatures   (<900  ℃)   [21].    However,   when  the  temperature increased, partial calcium oxide evaporated, reacting with silicon and aluminum oxides.
XRD patterns were measured to  further illustrate the trans- formation of minerals, and are  shown in Fig. 8. Fig. 8 shows that in the original ash  at  400  ℃, the major minerals include anhydrite (CaSO4),  quartz (SiO2)  and calcium carbonate (CaCO3) with a very strong signal peak, respectively.  There is  also  a  small amount  of gehlenite  (Ca2Al2SiO7) at this temperature. As the heating temperature increased to  600   ℃, the XRD peak intensity of  CaCO3 greatly decreased, referring to the decomposition of CaCO3:CaCO3 / CaO þ CO2(g) (R1) It was also  found that at 600  ℃, the XRD peak intensity of SiO2decreased but  that of Ca2Al2SiO7  increased, which indicated partial silicon oxides and aluminum oxides might react with calcium ox- ides  to generate gehlenite at a lower temperature through:2CaO þ SiO2  þ Al2O3  / Ca2Al2SiO7 (R2) When  the  heating  temperature  was  increased the  heatingtemperature to  815  ℃, it  was observed that CaCO3  disappeared,SiO2    content  further  decreased,  while  the  peak  intensity  of Ca2Al2SiO7   exceeded that of  SiO2.  This  even more afﬁrmed the possibilities of reactions (R1) and (R2) indicated  above. Besides of the reaction (R2) from CaO t℃a2Al2SiO7, CaSO4 was also  proposed
to react with SiO2 and Al2O3 to produce Ca2Al2SiO7  at around 800 ℃
[22].
When the ash  was prepared at 1100 ℃, the XRD patterns have been totally changed. In the temperature ranging from 400  ℃ to
815  ℃, CaSO4  and SiO2  were always the most abundant mineral species in  ash,   while at  the temperature of  1100 ℃ these two
mineral  species almost  totally  transferred  to   other  phases.  At
1100 ℃ the XRD peak intensity of  Ca2Al2SiO7    increased  again, proving that more gehlenites were formed at higher temperatures by reaction (R2). Moreover, it can  be noted that at the temperature of 1100 ℃ CaMgSi2O7 and CaSiO3 were formed, indicating that with the decomposing of CaSO4, CaO reacted with SiO2  to form wollastonite (CaSiO3).  Akermanites  (CaMgSi2O7) and  augites  (Ca(Mg,Fe) Si2O6) were also found with the participation of magnesium oxides and ferrum oxides. Our  previous results have proved that SiO2  is
highly efﬁcient to  promote the decomposition of  CaSO4.  Consequently, the evolution mechanism of Zhundong coal  ash  with high calcium and sulfur content at higher temperature than 1000 ℃ can be concluded as following:CaSO4  / 

3.3.  Micro-morphology and  elemental component of Re-condensing deposits after  evaporating

In the previous section, the residual ash  properties at  different evaporating temperatures have been declared clearly. However, the evolution of sodium was still not clearly demonstrated by XRD, due to its lower content, compared to other major mineral elements in ash.  In this study, a moderate quench method was used to  obtain
re-condensed sodium species at  the wall  temperature of 500  ℃.
Since  the distance from the evaporating unit to the quench probe bottom is short, the condensed and deposited species on the probe bottom are  capable to  represent the forms of  released inorganic elements from ZD coal  combustion.
The  micro-morphology of  re-condensed species from ZD coal combustion at  500  and 815  ℃ is shown in Fig. 9 (left).  When coal was burned at 500  ℃, there was no  remarkable crystal deposition on  the probe bottom, while at  815  ℃, cubic shaped crystal parti- cles  were observed.  The  element content of  deposits on  probe bottom was analyzed by using EDS for more than ten target points. It was found that there were mainly sodium and chlorine in  the deposits, besides of  the basic  elements of ferrum and chromium for  all  the tested points. The  atom mole contents of sodium and chlorine  in   crystals were  collected and  are   plotted  in   Fig.  9 (right). Fig. 9 (right) shows that most of points deviate from the line  of Na:Cl ¼ 1:1,  but  are  adjacent to  the line  of Na:Cl ¼ 2:1,  or are  in the area of Na:Cl > 2:1.  This indicated that only  50% or less of sodium was released in the form of NaCl, and the other sodium
might be  released in  the form of atoms. van  Eyk et al. [5]  investigated sodium release from Loy Yang  brown coal  by  using quantitative planar laser-induced ﬂuorescence measurement and equilibrium calculation. The study showed that besides of sodium chlorides, the  most likely   sodium compound in  ash   is  sodium oxide. This leaded to the release of atomic sodium from ash  at high temperatures. However, sodium was hardly to be released into the ﬂue gas  in  the form of Na2O  or NaOH,  because even if NaOH  was produced by  Na2O  and H2O  at high temperatures, it  would also equilibrate to produce atomic Na  [23].   The  results of  chemical equilibrium calculation for  sodium compounds by Takuwa and Naruse [24]  also  indicated that there was no  gaseous NaOH at  the temperature below 1100 ℃. However, another work on particulate matter emission from brown coal  combustion found NaCl mainly contributed to  the emission of particles with aerodynamic diameter less  than 0.1  mm  [25].  Li [3]  reviewed the existing forms of sodium in  Victorian brown  coal   with  high content of  AAEMs, which is very similar to Zhundong coal. In this brown coal, sodium species can  mainly be  found in  two forms: as  ion-exchangeable cations  associated  with  the  carboxyl  groups  forming  part  of organic coal  substance or as NaCl associated with the moisture in coal  [3,4,26]. Consequently, in this study, the cubic crystal of NaCl on  probe surfaces should be  from the NaCl  combined with the
moisture in Zhundong coal,  and the other atom sodium should be from the sodium associated with the carboyl groups in Zhundong coal.
After  the ash  was heated to  815  ℃, the third clean deposition probe, also  cooled and with the surface temperature of  500  ℃, was  inserted, and  the  heating temperature  was  increased to 1000 ℃ and kept constant for 1 h. The global micro structures and element contents in deposits are  shown in Fig. 10(a). It can  be seen that there was no  chlorine in the deposits, agreeing well with the results in Fig. 9(b),  since all the chlorine has  been released before the temperature of 815  ℃. As shown in  Fig. 10(a), sulfate species were  observed in   the  deposits at   evaporating  temperature   of1000  ℃.  Also   in   this  ﬁgure  a   small  amount  of   silicon and aluminum can  be  observed, indicating the depositing of  aluminosilicates. Furthermore, a selected measurement on  the element distribution of different kinds of deposits is shown in Fig. 10(b). At evaporating temperature  of  1000 ℃, the deposits on  the probe were mainly divided into two types: (1)  the sulfates of alkali and alkaline earth metals (SAAEMs) containing sulfur, sodium, potassium, and  calcium; (2)   the  aluminosilicates containing  silicon, aluminum,   sodium,   potassium,   calcium,  and   magnesium.   As shown in  Fig. 10(b), the ﬁrst type was gray  and noted as 026  and 028,  while the second type was light white and noted as 025  and 027.  Most of the light white particles were embedded into the gray layer at the bottom. This proved that in the ash  depositing process of  Zhundong coal   combustion, the  released or  formed gaseous sodium chlorides, sodium sulfates, calcium sulfates, and potassium sulfates  ﬁrstly  deposited  on   the  cooling surfaces, and  then partial aluminosilicates were captured and embedded into the ﬁrst layers.
3.4.  Temperature-dependent ash  deposition mechanism in
Zhundong coal  combustion

Based  on the results from the full-scale industry boiler and lab- scale  experiments, a temperature-dependent ash  deposition mechanism cor  the combustion of Zhundong coal,  with high sodium and calcium content,  in  pulverized coal  furnaces was pro- posed in Fig. 11. At a lower temperature of 800  ℃, more than 80% of the sodium in Zhundong coal was released into ﬂue gas in the form of atom sodium, sodium oxides, hydroxide, and chlorides; SO2 was mainly from the oxidation of organic sulfur and pyrites. With the temperature  increased to  1100 ℃, the  left sodium was mainly released in  non-chloride form, while all the left  sulfur in  calcium sulfates was released by the decomposition of calcium sulfates with the formation of  calcium oxides. Starting  from 1000 ℃, partial calcium has  been released as gaseous forms of calcium oxides and atom calcium. As the temperature increased, as high as the ﬂame temperature in pulverized coal furnace, all the sodium and most of the calcium should be  released as  gaseous species, which quickly reacted with sulfur oxides in the ﬂue gas, forming a large amount of gaseous calcium and sodium sulfates [27,28].  At  the temperature ranging from 1100 to 1450 ℃, Zhundong coal  ash  melts and was easy to be adhered on  cooling surfaces.
After the ﬂue gas passes through the convection heating surface, its  temperature is reduced. With the decreasing in  ﬂue gas  temperature, the aerosols of calcium and sodium sulfates started forming. When the  ﬂue  gas   temperature  decreased to   around
850  ℃, a large number of sulfate aerosols are  formed, which very
easily deposit on heating surfaces. These aerosols could also deposit on the surfaces of ﬂy ash,  aggravating the adhering of ash  particles on  heating surfaces. In the meantime, the gaseous sulfates around the cooling surfaces directly condense and deposit on the surfaces. The   depositing  of  sulfate aerosols and  condensed  sulfates on heating surfaces is especially important, since after the formation of such a layer, more and more ﬂy ash  particles were adhered and the depositing layer would grow up  quickly.

4.  Conclusions

In this work, the Ca/Na/S/Cl transformation and ash  deposition mechanism in  boilers burning Zhundong coal  with high calcium and sodium contents were investigated. The main conclusions are as following:
(1)  In  a full-scale boiler,  the condensing and depositing of sodium and calcium sulfates play  an  important role   on  the slagging and ash  depositing on  convection heating surfaces. Sulfates start to signiﬁcantly condense and deposit at the ﬂue gas  temperature of about 850  ℃ (at  medium and high temperature reheater).
(2)  With the increasing in temperature from 400  to 1200 ℃, the
ash  evaporating process is divided into three stages: a) from
400   to 800   ℃, 80%  of  sodium, and 100%  of  chlorine are released; b) from 800  to 1000 ℃, all the left  sodium evapo- rated and sulfur starts to  be  released with the formation of partial aluminosilicates; c) from 1000 to 1200 ℃, all the left sulfur is  released through  the  decomposition of  calcium sulfates and then calcium starts to  evaporate,  while silicon oxides disappears due to the  formation of  new complex silicates.
(3)  The  condensing quenching test further proved the detailed forms of  released sodium and calcium. The  sodium in Zhundong coal  was released mainly in  the forms of  atom, oxide, hydroxide, and chloride, in which sodium chloride accounts for less  than 50%. When the evaporating temperature increased higher than 1000 ℃, partial alkali and alkaline earth metals were released as  gaseous sulfates, and after- ward condense and deposit on  the heating surfaces.

The paper presents an investigation of flow and noise produced by the generic fan of an environmental control system installed in a circular duct, with a focus on the evaluation of the effect of mean flow distortions and elevated turbulence levels upstream of the fan, which are here created by replacing a baseline smooth bell-mouth duct inlet by inlets with a rectangular-to-circular transition of the duct cross section and with a T junction formed by two circular pipes. The study includes both experiments, which are aimed primarily at supplying data for a validation of simulation approaches and numerical simulations based on a hybrid Reynolds-averaged Navier–Stokes/large-eddy simulation approach. An acoustic modal decomposition of the measured and computed unsteady pressure fields is then carried out, which allows extracting “pure” (with filtered out hydrodynamic/turbulent fluctuations) acoustic modes propagating upstream and downstream under reflection-free conditions. A comparison with the experiment is performed for both the “raw” pressure spectra on the duct walls and the extracted individual acoustic modes. It is shown that, in terms of acoustics, a replacement of the clean inlet by the rectangular/circular one is almost neutral, whereas the T-junction inlet causes a strong noise penalty.
Noise produced by the environmental control system (ECS) constitutes a significant part of the overall noise inside an airplane cabin, and so it has a substantial impact on the passengers and crew comfort. Reliable prediction of this noise and revealing its main sources and mechanisms are essential for designing better noise-mitigating ECSs. Of significant interest in this respect are various installation and interaction effects: particularly, the effect of elevated level of turbulent fluctuations caused by nonuniformities of the flow in the ECS ducts on the noise generated by a blower (fan) ingesting such flow distortions. This effect is investigated exactly in the present study, which was carried out in the framework of the European-Union-funded project titled “Integrated Design of Optimal Ventilation Systems for Low Cabin and Ramp Noise” (or IDEALVENT).
The alteration of the noise produced by a rotating machine when it is subjected to different time-averaged and fluctuating inflow properties is well known. Helicopter noise research groups in the 1970s already considered the ingestion of atmospheric turbulence by the rotor and the resulting acoustic emissions [1]. The essential inflow distortion mechanism here, which is also encountered for aeroengines tested on static stands, is the intense longitudinal stretching of the turbulent structures sucked into the rotor that is induced by the contraction of the streamlines [2]. In many other applications reported in the literature, the inflow distortion is induced by local installation effects, e.g., in the presence of a hull with control surfaces for an underwater propulsor [3] or upstream obstructions in ducted axial subsonic fans [4–7]. In those cases, the time-averaged radial and azimuthal nonuniformity of the inflow is a key element affecting tonal noise generation. In [5], obstructions of different sizes and shapes were placed in front of the inlet of a ducted fan in an anechoic chamber. The fan was placed at the duct entrance, permitting free-field acoustic measurements. Conceptually similar free-field investigations were presented in [6] using cylindrical rod obstructions and in [8] for a free propeller placed in the core of a jet stream passing through a modal distortion generator.
A distinctive feature of the present work, with respect to the aforementioned investigations, is the inclusion of the duct response as part of the installation effect by means of in-duct microphone measurements. The coupling between the Fourier–Bessel modal decomposition of the source excitation and the duct response was studied in [9], which established the so-called Tyler and Sofrin rule for noise control, exploiting the duct cutoff properties through suitably chosen blade/vane counts. This is, however, hardly applicable in the case of a generally uncontrolled inflow distortion, which may contain complex and/or unknown Fourier–Bessel modal content and thereby defeat a strategy aimed at decoupling the source from the duct response [5]. Yet, the modal decomposition of the in-duct microphone measurements remains quite useful to extract the component of the pressure field corresponding to acoustic waves propagating away from the source, from the fluctuations associated with the boundary layer turbulence, or from acoustic reflections at the duct terminations [10,11]. The last point is actually essential to perform an unambiguous analysis of installation effects caused by different duct inlet geometries as described in the following because different inlets are associated to different inflow qualities and different acoustic reflection properties simultaneously.
A specific fan considered in the study is a generic ECS blower installed in a circular duct and having a rotor with 15 blades and a stator with 10 vanes (see Fig. 1). The duct inner diameter D is equal to 0.15 m; the rotation speed is around 11,200 rpm; and the bulk flow velocity is approximately 32 m∕s, resulting in a Reynolds number of the flow close to that in practical applications. To analyze the effect of the flow distortions upstream of the fan, three different cases are considered. In the first (baseline) case, the duct has a smooth bell-mouth inlet, resulting in a “clean” flow upstream of the fan with turbulence confined within a thin boundary layer on the duct wall. Note that, even using a well-designed inlet bell mouth, it is rather difficult to completely eliminate residual large-scale motions in the room where the inlet is located, which by acceleration and turbulence stretching can still induce significant tonal noise [12,13]. In the other two cases, a significant nonuniformity of the flow and elevated levels of turbulent fluctuations in the upstream part of the duct away from its walls are created by replacing the bell-mouth inlet by inlets with a rectangular-to-circular transition of the duct cross section and with a T junction formed by two circular inlet pipes (see Fig. 2). In this work, it is assumed that a reasonably designed ECS will guarantee that the blower will operate around its maximum efficiency point, even for complex geometrical layouts. For this reason, some care is applied to not significantly change the operating point of the fan, unlike in other studies, as described in [4], where some distortions induced sufficient blockage to lead to nearly choked conditions.
The study includes both a set of experiments aimed primarily at supplying data for validation of different simulation approaches and numerical investigations based on a hybrid Reynolds-averaged Navier–Stokes (RANS)/large-eddy simulation (LES) approach. This approach offers a computationally affordable scale-resolving simulation strategy with a low level of empiricism potentially capable of an accurate prediction of both tonal and broadband components of the noise generated by ducted fans at practical (rather high) Reynolds numbers. An alternative numerical technique, having received an increasing interest over the past decade, is based on the lattice- Boltzmann approach, and it was recently applied to fairly complex installed configurations such as a low-speed axial fan [14], an automotive cooling module [15], or a centrifugal ventilation system [16], albeit at considerably smaller Reynolds and tip Mach numbers than in the present study.
Amajor goal of this work is to check whether the developed numerical system really possesses the capability to simulate geometrically complex systems, with particularly small rotor–stator axial separation, as can be seen in Fig. 1, including capturing the relatively subtle effect of the fan noise alteration caused by the inlet distortions.
The paper is organized as follows. Section II contains an overview of experiments carried out at the von Kármán Institute for Fluid Dynamics. Then, Sec. III presents a description of the developed simulation approach, the numerical system employed in the calculations, and the computational setup. Section IV discusses results of the simulations and their comparison with available experimental data. Finally, some conclusions are formulated in Sec. V.
Considering that the present paper is mainly concerned with the simulation aspects of the ducted-fan flows, the characterization of the experimental part of the study will only be a brief overview.
The experimental rig for the various ducted-fan test cases is installed in the anechoic room of the von Kármán Institute for Fluid Dynamics. The duct is composed of modules corresponding to a fan or/and various flow obstructions and instrumented sections (static pressure probes, a radial and azimuthal hot-wire traversing system, microphone multiports, and loudspeaker arrays; see Figs. 2 and 3). The modules are made of acrylic glass. As described in the Introduction (Sec. I), to create flow distortions upstream of the fan, the bell-mouth inlet module shown in Fig. 3 is replaced by inlet modules with a rectangular-to-circular transition and a T junction, presented in Fig. 4. Both inlet geometries result in azimuthal inhomogeneity of the mean flow and the turbulence intensity being ingested by the fan, causing an extraneous tonal and broadband noise generation.
The aerodynamic measurements carried out for the three considered ducted-fan flows include the pressure rise across the fan module using two sets of four cross-connected pressure taps, and hot-wire measurements of the maps of the mean streamwise velocity and its root-mean-square fluctuations in several duct cross sections upstream of the fan. The in-duct pressure fluctuations are measured with two flush-mounted microphone arrays: one upstream and one downstream of the fan module.
For the hot-wire measurements, a single probe was used with a motorized radial–azimuthal probe traversing system designed to map the fan inflow over the entire cross section of the duct and quantify the effect of the different inflow modules. A map of the scanning grid is shown in Fig. 5, illustrating that the measurements were concentrated close to the duct wall, down to a minimum wall distance of about 0.002 m, with a total of 385 measurement locations over a crossplane. At the smaller distances to the wall, the velocity needed there for computing the mass flow rate was obtained by interpolation, assuming zero value at the duct wall. The hot-wire anemometer signal was low-pass filtered at 22,500 kHz and acquired at a frequency of 65,536 Hz. At each point of the map, data were acquired for a duration of 15 s, generating around one million time samples, which are considered largely sufficient to obtain mean flow and turbulence intensity with uncertainties well below the intrinsic measurement accuracy.
The simulated operating points of the fan with the three different inlets are superimposed with the experimental performance curve in Fig. 6, demonstrating a good agreement. Furthermore, the figure shows that the fan is operating in a realistic and stable regime, far enough from stall. Also important, it is verified from the simulations that the operating point is not significantly affected by the upstream distortions. This permits relating the extraneous noise generation to the inflow quality, and not to an increase of the steady load exerted on the blades.
Due to the fact that the microphone arrays and loudspeaker modules must be located between the blower unit and the inflow module in order to extract the modal content of the acoustic source, the minimum distance between the blower inlet and the inlet module has to be larger than four duct diameters. As a result, the degree of mean flow inhomogeneity that is ingested by the fan is noticeably attenuated with respect to what the same inlet geometry would generate for a shorter duct. This is illustrated by Fig. 7 which shows cross planes just downstream of the inlet geometry (top row) and just upstream of the fan inlet, approximately four duct diameters downstream of the duct inlet (bottom row) for bell-mouth inlet (first column), rectangular-to-circular inlet (middle column), and T-junction inlet (last column).
Regarding the in-duct acoustic measurements themselves, an elaborated system involving multiple-microphone and multiple- loudspeaker arrays was developed with the aim of extracting the noise produced by the fan, irrespective of the changing acoustic reflecting conditions induced by the different inlet geometries, as the objective of this study is indeed to investigate the inlet aerodynamic installation effect, and not the acoustic one. The approach was based on a modal decomposition of the acoustic field measured at a number of wall-mounted microphone locations, as well as on the prior measurement of the modal reflection matrices of the duct terminations and of the modal scattering matrix of the fan itself. This permitted extraction of the noise that would be radiated by the fan placed in an anechoic duct (the so-called active or source component of the acoustic field), and it compared the measured and simulated fan noise without ambiguity (in the present work, this is done only for the case of the bell-mouth inlet; see Sec. IV.C). More details about the modal extraction, which was based on an extension of a general linear multiport approach [10–11], were provided in [17–20].
In general, the approach developed for the solution of the considered problem consisted of direct computation of the noise produced by the fan based on a compressible scale-resolving simulation of turbulence with the use of a hybrid RANS-LES method of the detached-eddy simulation type, namely, the improved delayed detached-eddy simulation (IDDES;[21]). This method is currently well established for unsteady simulations in both aerodynamic and aeroacoustic applications, including engine and airframe noise (see, e.g., [22–30]).
In the presence of “inflow” turbulent content, IDDES performs as a wall-modeled LES (WMLES). To “inject” such turbulent content, specially designed distributed volumetric source terms are introduced into the momentum and turbulent kinetic energy transport equations within a confined region located upstream of the fan. This approach, called the volume synthetic turbulence generator (VSTG) is based on the earlier proposed efficient “surface” synthetic turbulence generator (STG) [25]. Its formulation and some results of validation can be found in [31]. Thus, within the developed computational procedure, the IDDES is used as a zonal method that functions in the RANS mode upstream of the VSTG area and in the WMLES mode downstream of this area. Similar to the STG [25], the VSTG parameters are set based on the RANS solution in the corresponding duct section and ensure that the “created” turbulence matches the RANS Reynolds stresses right downstream of the VSTG region. A major advantage of the VSTG over the STG [25] and other surface generators of synthetic turbulence is that, with a sufficiently stretched source region, it does not create any noticeable spurious noise, which is of primary importance for aeroacoustic problems.
Note that the IDDES version used in the present study employs the shear-stress transport with curvature correction (SST-CC) model of Smirnov and Menter [32] as a background RANS turbulence model. This model presents an adaptation of the curvature–rotation correction suggested in [33,34] for the one-equation Spalart–Allmaras model [35] to the two-equation k-ω SST model [36]. This model was chosen because unlike the model [33] it provides a modeled turbulent kinetic energy which is an input parameter of the VSTG technique.
In principle, the outlined approach is capable of predicting both the tonal and the broadband components of the noise produced by the fan and its propagation over the ventilation duct and into the surrounding ambient air. However, such computations would require nonaffordably large computational resources. For this reason, in the present work, the subdomain in which the turbulence-resolving (IDDES) computations are performed is limited to a relatively small part of the duct neighboring the fan module.
As a result, the whole computational procedure, in addition to the main (“production”) IDDES computation in such a truncated domain, involves two auxiliary precursor stages necessary for setting inflow and outflow boundary conditions for the production simulation. Specifically, the inflow profiles used in the IDDES stage of the computations are obtained from the steady RANS computation of the duct inlet flow. The information needed for imposing the outflow boundary conditions is obtained from a precursor unsteady RANS (URANS) computation of the fan flow (a more detailed description of the boundary conditions is presented in the next section).
Another consequence of carrying out scale-resolving simulations in the limited computational domain is that the extraction of the acoustic signal from such simulations is far from trivial. Indeed, in the vicinity of the noise-generating region (at least downstream of it), the acoustic signal is strongly covered with pressure fluctuations caused by turbulence and, in particular, by impingement of turbulent structures on the duct walls. Furthermore, despite employing “nonreflecting” boundary conditions at the inlet and outlet of the IDDES computational domain, some inlet and outlet reflections of the sound waves are still possible. So, in order to extract acoustic signals that are free of reflections and hydrodynamic fluctuations, specialized supplementary acoustic techniques should be used. In the present work, for this purpose, a methodology for acoustic mode extraction based on a linear multiport model [10,11,17–20] is adapted.
As mentioned, the extraction of the acoustic signal from the experimental data faces similar difficulties because the in-duct microphones also cannot distinguish between the acoustic and hydrodynamic/turbulent fluctuations of the wall pressure. In addition, as will be shown in the following, in the experiment the in-duct acoustic field is contaminated by reflections of sound waves by the duct terminations much more significantly than in the simulations. Therefore, postprocessing of both the measured and the computed pressure fluctuations with the use of the modal decomposition technique [10,11,17–20] is needed to ensure a consistent comparison of the predicted acoustic characteristics with the experiment.
The locations of the inflow and outflow boundaries of the computational domains and boundary conditions used in the three stages of the fan flow computations outlined previously are presented in Fig. 8.
A precursor steady RANS is performed in the computational domains, which include the inlet modules and an adjacent part of the ventilation duct of the length of 5D. At the RANS inflow boundary, the total pressure and the temperature are prescribed. At the outflow, the static pressure is assumed to be a constant adjusted in the course of the computations to match the experimental mass flow rate (MFR) through the duct equal to 0.636 kg∕s for all three considered cases (this corresponds to the bulk flow velocity around 32 m∕s). The computational domains used in the precursor URANS stage and in the main (production) IDDES stage are identical and include the fan module and the upstream and downstream parts of the duct of the lengths of 3D and 6D, respectively. The inflow/outflow boundary conditions used in these stages are as follows.
The inflow profiles are prescribed based on the known solution of the precursor steady RANS in the corresponding duct section. To prevent reflections of the sound waves from the URANS/IDDES inlet, the imposed boundary conditions are of a characteristic type. Namely, they prescribe the profiles of the incoming one-dimensional (1-D) Riemann invariants, whereas the outgoing invariant at the boundary is computed by extrapolation from the interior of the computational domain.
At the outflow boundary, in the precursor URANS stage, the value of the static pressure pout is tuned to ensure the given MFR. All the other flow quantities at this boundary are obtained by extrapolation from the interior of the domain. Then, in the final IDDES stage of the computations, a mitigation of reflections from the outflow boundary of the domain is ensured by using a “sponge” (“absorbing”) layer technique [37]. The width of the outflow sponge layer shown in Fig. 8 is around 3D,and a “target” solution within this layer is defined as the time average of a precursor URANS solution. The animations of unsteady pressure fields from the simulations suggest that these inflow/outflow boundary conditions prevent any noticeable reflections of sound waves from the boundaries. Yet, postprocessing of the results with the use of the modal decomposition technique [17–20] (see Sec. IV.C) reveals that, in fact, at few frequencies, these reflections are far from negligible.
Note, finally, that both the URANS and the IDDES computations are performed in the full azimuthal domain (360 deg) with the use of a rotating “rotor” grid block and sliding interfaces between the rotating and stationary grid blocks (see the following). Therefore, they do not rely on any assumption of the azimuthal periodicity of the flow, which is widely used in the fan flow modeling but is not justified in the considered cases of the “distorted” duct inlets.
To facilitate the generation of the grid for the IDDES and reduce its size, the real geometry of the fan shown in Fig. 1 was simplified based on a set of preliminary RANS computations carried out in the framework of the IDEALVENT project by A. Wohlbrandt of DLR, German Aerospace Center.§§ The simplifications are as follows:
1）Some geometrical details (such as screws, supply lines, etc.) were ignored. 
2）Cavities present at the front and back of the fan hub were replaced by solid walls. 
3）Small “steps” on the fan casing wall were smoothed.
4）The rotor tip clearance ( 1% of the rotor blades span) and fillets on both the rotor and the stator blades were ignored.
According to RANS, these simplifications do not significantly change the aerodynamic characteristics of the considered flow and, in particular, the pressure rise over the fan module; although, as will be discussed in Sec. IV, their effect on the acoustic characteristics may be substantial. Furthermore, the computational setup did not include a rectangular cavity on the casing wall downstream of the fan (see the far- right frame in Fig. 1). In principle, this cavity may cause acoustic scattering, resulting in a redistribution of the modal content of the acoustic waves. A quantitative evaluation of this effect, as well as of the effect of ignoring the tip clearance on the acoustics, is planned for future work.
The fan flow computations were carried out with the use of a general purpose in-house NTS code [38] capable of the simulation of turbulence in the framework of a full range of turbulence-resolving approaches. This was a structured finite volume high-order computational fluid dynamics code accepting multiblock overlapping grids of the Chimera type. The code has been intensively used for a wide range of aerodynamic and aeroacoustic problems, and it was shown to be reliable and accurate. It was adapted to the computation of the flows with rotating elements with the use of the approach based on employing different reference frames in different grid blocks combined with the sliding interfaces technique for imposing the boundary conditions at the interfaces between rotating and stationary grid blocks (see [39] for more details). A compressible branch of the code used in the present study was based on an implicit Roe-type flux-difference-splitting numerical scheme. In the URANS computations, the inviscid fluxes were approximated with fifth-order upwind-biased differences, except for the rotating grid blocks adjacent to the rotor blades (see below), where switching to the third-order upwind-biased differences turned out to be necessary in order to ensure stability of the simulations in the presence of very large gradients of velocity near the stationary casing wall. In the IDDES computations, in the major part of the computational domain, the approximation of inviscid fluxes was performed with the use of the low-dissipation weighted fourth-order centered/fifth-order upwind-biased scheme, with a fixed weight of the upwind differences equal to 0.1. The exceptions were, again, the grid blocks adjacent to the rotor blades, as well as the VSTG region. Within the former, as in the URANS, the third-order upwind-biased scheme was used. Within the latter, the weight of the upwind differences was set zero in order to ensure a rapid development of resolved turbulent structures.
The viscous fluxes in all the simulations were approximated with the use of a second-order centered scheme.
Finally, the time integration was based on the dual time stepping (with subiterations in pseudotime) and the backward second- order approximation of the time derivatives (three-layer scheme). The simulations were carried out with fixed number of subiterations per time step; Niter 10. This ensured a decrease of the maximum residuals of the governing equations by at least two orders of magnitude.
The grid used in the URANS and IDDES stages of the computations consisted of 34 overlapping blocks. Its structure and topology are illustrated by Figs. 9 and 10, in which the parts of the grid that belong to the rotating blocks treated in the noninertial (rotating) reference frame are shown in red.
The grid upstream and downstream of the hub is axisymmetric, with a Cartesian grid block inserted near the axis of the duct in order to avoid numerical difficulties caused by singularity near the axis arising in computations of three-dimensional (3-D) flows in the cylindrical coordinates. These axisymmetric blocks have Nϕ 124 uniform cells in the azimuthal direction and Nr 90 cells in the radial direction with clustering near the duct wall. The azimuthal grid in the axisymmetric grid blocks over the rotor and stator of the fan is refined by a factor of 3 relative the duct grid and has Nϕ 360 cells, with “intermediate” neighboring blocks with Nϕ 240 (shown in blue in Fig. 9) inserted in order to avoid too-strong abrupt changes of the azimuthal resolution between the rotor/stator and the duct grids. Around the rotor and stator blades, O-type grid blocks are built, which have, respectively, 100 and 150 cells in the circumferential direction (i.e., along the surface of the blades), with grid clustering in the vicinity of the leading and trailing edges of the blades. The radial grid in the passage between the fan hub and casing has Nr 60 cells and is clustered toward both walls. A total number of cells in the streamwise direction Nx is around 750, resulting in about 20 million cells total.
The streamwise grid is refined in the region starting from 2D (0.3 m) upstream and ending by 2.5D (0.38 m) downstream of the fan module, with additional clustering near the forward and rear edges of the hub and in the fan passage. The maximum grid step in this area is Δx m, and then is gradually increased, reaching the value of Δx m. With the high-order numerical scheme, this guaranties an accurate resolution of sound waves propagating along the duct up to a frequency of 6000 Hz (at this frequency, around 10 grid cells are used per the wavelength of the upstream propagating waves).
Finally, the time step was set as Δt s, which resulted in Courant–Friedrich–Lewy values based on the sound velocity of around 1.0 in the fine-grid part of the computational domain. This value of Δt was equal to around 1∕70th of the period corresponding to the rotor blade passing frequency (BPF) of fBPF ≈ 2800 Hz, thus ensuring a sufficiently accurate representation of the flow unsteadiness caused by rotation.
A time sample in the production IDDESs that were started from the flowfields obtained in the precursor URANS was around 35 convective time units; D∕U0. It included a transient period of ≈20 convective units to reach a statistically mature state of the flow and a period of ≈15 convective time units (≈14 full rotor revolutions) used for accumulation of turbulence statistics and instantaneous wall pressure for the computation of the pressure spectra.
Sample visualizations from the IDDES of this flow are presented in Fig. 11 in the form of instantaneous fields of vorticity magnitude and streamwise velocity in the meridian plane of the duct combined with the fields on axisymmetric grid surfaces inside the passage between the hub and the fan casing. In this figure, results are shown for three such surfaces, with the radius in the cylindrical part of the passage rsurf = 0.06 m (close to the hub; first row), rsurf = 0.07 m (middle of the passage; second row), and rsurf = 0.08 m (close to the casing wall; third row). Other than that, Figs. 12 and 13 present a 3-D view of the instantaneous vorticity and two 3-D views of the instantaneous isosurfaces of the swirl (magnitude of the second eigenvalue of the velocity gradient tensor) “colored” by the value of the streamwise velocity. In addition, in order to give an idea about the difference in the flow representation in the framework of scale- resolving simulations and RANS, Figs. 14 and 15 show sample comparisons of instantaneous fields obtained in the IDDES and in the URANS.
These figures provide a visual representation of turbulence resolved in the simulations. In particular, they show a drastic difference of the scale of the vortical structures present in the IDDES and URANS solutions. Naturally, the URANS is capable of resolving only the large-scale vortices originating from the turbulent wakes of the rotor blades and stator vanes (even these large-scale vortices in the URANS solution rapidly dissipate behind the hub base because of the very large turbulent viscosity predicted by the SST-CC turbulence model in this region; see Fig. 14). The IDDES reveals much “richer” vortical activity. The small-scale vortices in the IDDES are formed in the intervane stator area due to disintegration of the rotor blade wakes, in the stator vane wakes, and in the separation regions behind the hub base and on the suction side of the stator vanes (a shape of the latter separation region is shown in Fig. 16, which presents the time- averaged surface streamlines: “oil flow” visualization). As far as the quality of turbulence resolution reached in the LES regions of the IDDES is concerned, it can be assessed based on Fig. 17 presenting the fields of resolved and time-averaged modeled turbulent kinetic energy (TKE) from the simulations. As seen in the figure, the resolved energy amounts to 80–90% of the total TKE, which shows the quite acceptable accuracy of the simulation with the grid and numerics used.
Note that the flow over the rotor blades remains attached, except for a thin separation near the hub surface, which is probably induced by the separation (also thin) from the forward edge of the hub (the latter may well be caused by the simplification of the hub geometry in the computational model). As a result, despite the presence of the resolved turbulent content in the incoming duct wall boundary layer, with the current grid, the boundary layers forming on the rotor blades are treated effectively in the RANS mode of the IDDES. This does not allow capturing the noise originating from interaction of the fine-scale vortical structures populating these boundary layers with the leading edge of the stator blades, which could be achieved only on the basis of full LES with transition prediction. However, considering that the boundary layer on the rotor blades is very thin, this would require unaffordable computational grids. Nonetheless, despite this evident limitation of the current simulations, quite an acceptable agreement of the broadband component of the predicted and measured wall-pressure spectra in a wide range of frequencies is achieved (see Sec. IV.B). This provides indirect evidence that the simulations do capture a major part of turbulence activity within the noise-generating area. This is partly explained by some peculiarities of the considered typical ECS fan: particularly, the very small rotor/stator separation distance and the presence of multiple separation regions. An application of the approach used in the present study to other designs, for example, to a fan stage of modern turbofan engines [40] might be not that successful, at least on grids of similar density. Figure 18 illustrates a structure of the time-averaged field of the streamwise velocity predicted by the IDDES (the same three axisymmetric surfaces as in Fig. 11 are shown). It reveals the separation region near the suction side of the stator vanes, for which the length decreases while moving along the vane from the fan hub to its casing. This is consistent with the stator oil flow visualization presented in Fig. 16.
Finally, Fig. 19a gives an idea about the unsteady pressure field predicted by the IDDES inside the duct with an installed fan. The complexity of this field originates from a multimodal content of both the tonal and broadband components of the noise generated by the fan and propagating along the duct. In addition, the instantaneous pressure field is strongly affected by the pressure fluctuations caused by an impingement of turbulent vortical structures on the wall. The latter effect explains why the noise signature is seen much more clearly upstream of the fan where it is not contaminated by turbulent fluctuations. As for the wall-pressure field predicted by the URANS (see Fig. 19b), it completely misses the broadband noise component and has a regular pattern that is typical of the tonal noise.
This effect is illustrated by Figs. 20–23.
Particularly, Fig. 20 presents snapshots of the vorticity magnitude and the streamwise and radial velocities in the duct cross section upstream of the fan (recall that these fields are created by the VSTG based on corresponding RANS solutions slightly upstream of the VSTG region). The figure reveals a significant difference in turbulence impinging on the front surface of the hub and on the rotor blades for the three considered inlets. In the case of the clean (bell-mouth) inlet, only fine-grained turbulent structures confined within a narrow boundary layer on the duct wall are observed. With the T-junction inlet, the presence of large-scale turbulent structures upstream of the fan is evident, and the turbulence “covers” the whole duct cross section. For the inlet formed by the rectangular-to-circular transition, an intermediate situation takes place.
However, as seen from Fig. 21, this difference in the upstream turbulence does not cause any visible alteration of the large-scale or fine- grained turbulence within the fan passage and behind the hub base. This can be explained by a generation of intense vortical structures inside the fan and in the shear layers separated from the trailing edge of the hub. The effect of the shape of the duct inlet on the instantaneous and time-averaged velocity fields (not shown) turns out to be rather weak as well.
The turbulent statistics are more sensitive to the presence of the inlet disturbances. This is seen, for example, in Fig. 22, showing the predicted fields of the resolved TKE in the meridian section of the duct and on a surface of revolution located inside the fan passage; as well as in Fig. 23, presenting distributions of the root mean square of the pressure fluctuations over the fan surface. For the two disturbed inlets, the flow distortions result in increased levels of both TKE in the core of the flow upstream of the fan and the wall-pressure fluctuations on the “front face” of the hub and on the leading edge of the rotor blades. The effect is much stronger in the case of the T-junction inlet.
This, in turn, results in a noticeable alteration of the acoustic pressure field inside the duct, which is illustrated by Fig. 24. Particularly, the figure and (more visibly) corresponding animations of the wall-pressure reveal a noticeable increase of the amplitude of the acoustic waves propagating in the upstream direction in the ducts with the distorted inlets, which is consistent with the higher level of the root mean square of the pressure fluctuations on the fan surface. Again, the effect is more pronounced for the duct with the T-junction inlet.
A direct comparison of the raw wall-pressure spectra obtained in the IDDES of the three considered flows and in the experiment for the microphones located both upstream and downstream of the fan module is shown in Figs. 25 and 26, which present the major outcome of the current work. Note that in these figures, for the upstream microphone (mic.), predicted spectra are corrected with the use of empirical correlation (corr.) of Goody [41] to account for contribution of turbulence of attached boundary layer on the duct wall.
As far as the tonal noise is concerned, an analysis of these spectra reveals the striking difference between the predicted and measured amplitude of the first blade passing frequency order harmonic (f = 2800 Hz). Indeed, in the experiment, a strong tone at this frequency is observed; whereas in the simulations, this tone is almost completely cut off. Note, however, that for the considered ducted fan, the first BPF harmonic cutoff is exactly what has to take place (at least for the case of an axisymmetric bell-mouth inlet) according to the rule of Tyler and Sofrin [9]. So, the presence of this tone in the measured spectra is, most probably, explained by violation of the flow periodicity in the experiment, which may be caused by irregularities of the geometry ignored in the numerical problem setup (first of all, by the presence of the thick power supply cable and the rectangular cavity on the casing surface downstream of the fan).
Leaving aside this difference, the agreement of the simulation results with the data can be assessed as acceptable. Within the resolved frequency range, for all the three considered duct inlets, a significant discrepancy is observed only in the low-frequency end of the spectra of the upstream propagating noise (see Fig. 25). At least partly, this may be caused by stronger reflections of the low-frequency sound waves by the duct inlets in the experiment than in the simulations. Note also that the effect of the shape of the inlet (Fig. 26), which is of primary interest in the present study, is predicted more accurately than the absolute sound power.
Overall, the achieved agreement of the simulation results with the raw experimental spectra of the wall-pressure fluctuations in the considered ducted-fan flows supports credibility of the developed numerical tool. Still, as already mentioned, the comparison of the spectra presented previously cannot be considered as fully consistent because some reflections of the sound waves from the inlets and outlets of the ducts are present in both the experiment and the simulations. For the case of the bell-mouth inlet, a more consistent comparison is performed in the next section based on the linear multiport technique [17–20], which allows extracting the pure (with filtered out hydrodynamic/turbulent fluctuations) acoustic modes propagating upstream and downstream of the fan under reflection-free conditions.
The methodology [17–20] employs a projection of the sound field in the considered duct on a set of cut-on (propagating) acoustic modes that are the eigensolutions of the convective wave equation for the duct. The reflection and transmission of these modes are characterized by a scattering matrix of the test object (fan) and reflection matrices of the duct terminations in experiments or of the computational inlet/outlet boundaries in simulations. As applied to the experiment, the procedure includes two steps.
In the first step, in order to define the scattering and reflection matrices, a set of special experiments is carried out, which use sound fields produced by external sound sources uncorrelated to the sound field of the aeroacoustic source (a detailed description of this step can be found in [17]). Corresponding measurements for the fan with a bell-mouth inlet were performed in [17]. In the second step, the pure modal acoustic signal emitted by the fan is extracted based on the known scattering and reflection matrices and cross correlations of the pressure fluctuations at pairs of distant points sampled without external sources. Details of the postprocessing procedure were presented in [18–20].
As far as postprocessing of the simulation data is concerned, in principle, computing the scattering and reflection matrices could be performed with the use of effectively the same approach as in the experiment, i.e., by means of a set of simulations with known external acoustic sources (this methodology was described in [42]). However, in the present work, a simplified (and computationally much cheaper) approach is adapted. It employs the fact that, thanks to the nonreflecting boundary conditions, the computational inlet/outlet reflection coefficients are small in the whole range of frequencies, except for the lowest frequencies and the vicinity of the cut-on frequencies of the acoustic modes (see the following). This makes neglecting the actual scattering of the fan an acceptable approximation. Hence, both the inflow and outflow reflection coefficients and the amplitudes of the acoustic modes of the source can be extracted from the IDDES unsteady pressure fields with the use of postprocessing, which is basically the same as that used for the experiment [18]. The procedure, however, is adapted to much shorter time samples available in the IDDES as compared to the experiment (∼0.1 s of physical time in the IDDES versus 45 min in the measurements). Namely, in order to reduce the higher “noise” caused by a lower number of averages for computation of the cross spectra because of the short time samples, the procedure employs a much larger set of sampling points available in the simulations. In the postprocessing of the IDDES results, the spectra of the acoustic modes are computed by averaging 50 sets of the spectra obtained based on cross-spectral matrices of the size of 200 × 200; whereas in the experiments [17,19], only a matrix of the size of 16 × 16 is available.
Figure 27 presents the frequency dependencies of the inlet/outlet reflection coefficients for different acoustic modes propagating upstream and downstream of the fan in the duct with a clean bell-mouth inlet obtained based on the IDDES and the experiment. Results are shown for six modes with the lowest cut-on frequencies, namely, the plane wave [mode (0, 0)] propagating at all frequencies; four azimuthal modes (m, 0), with m 1; 2 with cut-on frequencies at 1330 Hz (m 1) and 2220 Hz (m 2); and the first radial mode (0, n), with n 1 (cut-on frequency at 2780 Hz, which is very close to the blade passing frequency of the considered fan). Dashed vertical lines in the figure correspond to these cut-on frequencies. Note that, for the two other inlets, the computational reflection coefficients (not shown) are very close to those for the bell-mouth inlet, whereas the advanced measurements allowing a modal decomposition were performed only for the bell-mouth inlet at frequencies between 500 and 3500 Hz.
The figure demonstrates that the computational reflection coefficient is small everywhere, except for a close neighborhood of cut-on frequencies of corresponding modes and the lowest frequency range (f < 350 Hz) for the outlet reflections. Note that the stronger reflection at the cut-on frequencies is most probably explained by changing the propagation angle: it is normal to the duct axis at cut on; for this reason, the 1-D nonreflective boundary conditions fail, as does the entire modal decomposition. Thus, in general, the results shown in Fig. 28 provide a posteriori justification of neglecting the scattering of the fan in the modal decomposition of the IDDES unsteady pressure field, except for the longest sound waves propagating downstream. For the latter, the reflections by the outflow sponge layer turn out to be very strong, which may result in an inaccuracy of the amplitudes of extracted downstream propagating plane waves (at low frequencies, all the other duct acoustic modes are cut off). Possible reason(s) for this deficiency of the simulations are yet to be understood. As for the sound wave reflections by the duct terminations in the experiment (both at the inlet and at the outlet), they are rather strong over the whole range of frequencies and for all the considered acoustic modes, which suggests that accounting for both the reflection and the scattering effects in the modal decomposition of the measured pressure signals is necessary.
Figure 28 compares the computed and measured amplitudes of the extracted acoustic modes for the bell-mouth inlet and illustrates the effect of the inlet shape on these amplitudes predicted by the IDDES. Major observations based on this figure are as follows.
First, the agreement of the predictions with the experiment for all the acoustic modes is consistent with the raw wall-pressure spectra discussed in Sec. IV.B above. Namely, similar to these spectra (Fig. 25), the major discrepancy of the simulation and experiment resides in the tonal noise of the fan corresponding to the first blade passing frequency order harmonic, which shows up in the experiment and is absent in the computed spectra. The computations also reproduce the spectral shapes and levels of the broadband part of the individual acoustic modes, which is encouraging when considering that these are rather subtle characteristics of the sound field of the source. Unfortunately, the lack of the experimental data at frequencies lower than 500 Hz does not allow suggesting any specific reasons of the discrepancy between the measured and computed raw pressure spectra of the upstream propagating noise observed in the low-frequency range (see Fig. 25).
As far as the effect of the inlet shape predicted by the IDDES is concerned, for the individual modes, it turns out to not be exactly the same as for the wall-pressure spectra (Fig. 26). In particular, for the individual modes, a replacement of the bell-mouth inlet by the T-junction inlet, in a wide frequency range, results in a significant (3–5 dB) increase of the noise propagating both upstream and downstream. In contrast to this, for the wall-pressure spectra, the effect for the downstream direction is almost unnoticeable, which is explained by “contamination” of the wall-pressure signal at the downstream microphone location by turbulent pressure fluctuations.
Finally, at low frequencies (f < 300 Hz), the noise penalty of the T-junction inlet for the upstream direction is very large ( 20 dB) for both the individual acoustic modes and the total wall pressure. For the rectangular/circular inlet, the noise penalty is marginal, except for the low- frequency noise propagating upstream, which is up to 10 dB stronger than in the system with the bell-mouth inlet.
A combined experimental/numerical study of the ducted-fan noise performed in the present work demonstrates the high potential of the hybrid RANS-LES scale resolving approaches to the turbulence representation as a tool for such noise prediction. Furthermore, it shows a high value of invoking a technique of modal decompositions coupled with the acoustic multiport approach. This technique is essential for extracting pure (with filtered out hydrodynamic/turbulent fluctuations) acoustic modes propagating under anechoic conditions, thus facilitating an adequate comparison of predictions and measurements.
It is found out that the effect of the inlet distortions caused by the use of the rectangular/circular transition as an inlet instead of the clean bell-mouth inlet results in a marginal alteration of the fan noise, except for the low-frequency noise propagating in the upstream direction, which increases by up to 10 dB. A noise penalty caused by replacement of the bell-mouth inlet by the inlet with the T junction formed by two circular pipes turns out to be much more severe. It ranges between 3 and 5 dB in a wide frequency range for both upstream and downstream propagating noise, and it reaches about 20 dB at the low-frequency end of the spectra.
In terms of numerical modeling approaches, further work should be directed, first of all, at a substantial mitigation of reflections of the long (comparable with the domain size) waves from the outlet of the computational domain observed in the present simulations. On the experimental side, definition of the scattering and reflection matrices in a frequency range considerably extended toward low frequencies and conducting experiments with different types of inlets are desirable.
We thank also A. Wohlbrandt (DLR, German Aerospace Center) who provided a cleaned up CAD model of the fan and performed a series of preliminary steady Reynolds-averaged Navier–Stokes computations justifying the geometry simplifications used in the present study.

A wall-modeled statistically converged Large Eddy Simulation (LES) of the turbulent ﬂow in the NASA Source Diagnostic Test turbofan has been successfully performed for the ﬁrst time. A good agreement with aerodynamic measurements is observed for both Reynolds Averaged Navier-Stokes and LES results, although the LES provides better results in the tip regions where large coherent structures appear and no ﬂow separation on the stator vanes is observed. In the LES the boundary layer naturally transition to turbulence on the blade suc-tion side but remains quasi laminar over most of its pressure side. The rotor-wake turbulence yielding the stage broadband noise is then seen to be quasi isotropic. Transition on the down-stream stator vanes is not triggered by the wake impingement but rather occurs at mid-chord. Finally, acoustics are investigated using both Ffowcs Williams & Hawkings’ and Goldstein’s analogies from the recorded LES noise source on the stator vanes. The latter analogy provides levels closer to the measurements especially at high frequencies, although the results are most likely still inﬂuenced by too coherent rotor tip secondary ﬂow at low frequencies.
Turbofan architecture has evolved toward an increasing contribution of the fan to the thrust. At approach conditions, the fan noise has become one of the main contributors with a main broadband content mostly in the forward arc of the engine [1–3]. The noise generated by the fan will increase even more for future turbo-engines that will involve Ultra-High By-pass Ratio (UHBR) turbofans. For these architectures, fan noise is projected to become the dominant source overall. In a classical turbofan as the present conﬁguration, the interaction between the wake of the rotor and equally-distributed Outlet-Guide Vanes (OGV) has been found to be the dominant mechanism that contributes to fan noise [4,5]. On the one hand, tonal noise can be more easily controlled by clever choice of blade counts according to Tyler & Sofrin rules [6] and taking advantage of the duct ﬁltering to have the ﬁrst Blade Passing Frequency (BPF) cut-off for instance. Moreover proper nacelle and exhaust duct liners can also damp both forward and backward propagating tones [4]. On the other hand, broadband noise has a signiﬁcant contribution much harder to control especially at high frequencies given the high number of cut-on modes for modern engines with low hub-to-tip ratio. Yet, with future noise regulations for commercial aircrafts [7,8], it becomes mandatory to get a better understanding of the noise generation and propagation for fan and OGV interaction broadband noise in order to be able to properly guide further noise reduction technology development.
Progress in High Performance Computing (HPC) has allowed for the correct prediction of complex ﬂows around isolated parts of turbomachines with Reynolds Averaged Navier-Stokes (RANS), unsteady RANS, Large Eddy Simulations (LES) and high-ﬁdelity Direct Numerical Simulations (DNS) as shown in reviews from Tucker [9] and McMullan et al. [10]. However, only the latter two methods (LES and DNS) can provide the multi-scale noise sources that yield broadband noise. A recent review from Gourdain et al. [11] gives an overview of LES industrial compressor ﬂows and reports simulations with full stages highlighting the cost of LES for multistage compressors [12,13]. But all these simulations involved low-pressure compressor with small blade spans, reduced rotor-stator distances and moderate Reynolds numbers based on the chord. Only de Laborderie et al. [12] considered the noise generation, but stressed the diﬃculties of predictions associated with limited passage simulations. More recently high-pressure turbines have been considered that involve both higher Mach and Reynolds numbers [14], and dealt with their noise generation and the propagation of combustion noise through them [15].
In that context, the “Fan Noise Source Diagnostic Test” (SDT) experimental set-up was investigated by NASA providing an extensive database of aerodynamic sources and aeroacoustic diagnostics of the broadband fan/OGV interaction mechanism [16–18]. This comprehensive study is dedicated to the validation of numerical and analytical methods and has recently become an AIAA benchmark case for fan broadband noise.
The present work focuses on the only Large-Eddy Simulation (LES) of this conﬁguration at the moment, in combination with acoustic analogies for noise prediction to investigate the rotor/stator interaction mechanism. It provides an in-depth analysis of the instantaneous aerodynamic features of this turbofan conﬁguration and the comparison of two acoustic analogies for noise propagation. This investigation is the continuation of the preliminary work presented by Leonard et al. with converged ﬂow and acoustic statistics [19]. In the latter, the streamwise turbulent length scale was extracted from the LES results and used in Hanson’s [20] and Posson’s models [21] to predict the far-ﬁeld noise. The prime focus has been on the noise generated by the fan/ OGV pressure ﬂuctuations and its link with the impingement of the passing turbulent rotor wakes. A precise description of the state of turbulence reaching the stator vanes leading edge is therefore mandatory, which strongly justiﬁed the LES. The NASA experiment is summarized in the next section. The complete numerical model and parameters are then provided. Finally both aerodynamic and acoustic results from the LES are shown.
As part of the NASA advanced Subsonic Technology Noise Reduction Program, several experiments [16–18,22,23] have been carried out on the Source Diagnostic Test rig presented in Fig. 1(a) involving different geometries and counts of the rotor blades and stator outlet vanes [22]. The conﬁguration studied here is the baseline conﬁguration designed by General Electric Aircraft Engines which is composed of 22 rotor blades (R4) and 54 radial stator vanes (Fig. 1(b)). This 1/5th scale model fan stage is representative of the secondary stream of current High Bypass-Ratio turbofan engines. Note that the experimental set-up does not involve a core ﬂow.
The fan stage has been tested in take-off and approach conditions, but only the approach condition that has the most exten- sive experimental data set has been simulated numerically. Free-stream Mach number of 0.10 is imposed, to achieve acoustic ﬂight effect and to provide far-ﬁeld acoustic data representative of real take-off and approach conditions [18]. The rotational speed at approach is 7808 rpm corresponding to 61.7% of the rotor design speed.
The overall test program had many phases, including detailed ﬂow ﬁeld diagnostic measurements using laser Doppler and hot- wire anemometry [17,23] and acoustic power measurements [18].
 
To decrease the computational costs of the simulations, only a periodic portion of the fan stage is simulated. Since the rotor- stator interface condition used for the LES simulation requires to have domains of the same angular extent in the rotor and stator parts, the number of outlet guide vanes has been modiﬁed from 54 to 55 allowing a 2𝜋∕11 angular periodicity in both
domains. The geometrical transformation has been performed as in Refs. [14,24] adjusting the vane aspect ratios to maintain the same solidity and stay as close as possible to the experimental operating point (see Table 1). Using the OPTIBRUI platform for the analytical prediction of axial fan noise [19,25], it was veriﬁed that similar broadband noise levels should be expected as similar ranges of cut-on modes are found for both conﬁgurations. The computational domain shown in Fig. 2 not only contains the fan stage but also the nacelle and outside ﬂow. The deﬁnition of the domain, avoids the need of an inlet condition inside of the turbofan which may affect the acoustic propagation in the nacelle and allows the ﬂow to establish itself from the free ﬂow and the fan rotational speed like in the experimental wind tunnel. Downstream the exiting jet and the mixing layer will also be accounted for in the simulation which also prevents from modifying the acoustic transmission at the end of the nacelle.
The domain is divided in two parts: the stator domain is composed of the outside region and the vane row, while the rotor domain is limited to the rotor row and the rotor wake development region. The interfaces between the two parts are shown by the brown surfaces in Fig. 2.
The unstructured mesh used in the present study for both RANS and LES simulations is shown along a blade-to-blade view at mid-channel height in Fig. 3. The spatial discretization around the blades and in the wakes of the blades has been a prime focus: 8 layers of prisms have been imposed on the blades and vane skin in order to have a good discretization of the near-wall region. The tip region is discretized with 8 layers of prisms and an average of 7 tetrahedral elements to reach the casing (about 15 overall cells) which is more than twice as ﬁne than the extra ﬁne-tripped LBM simulation on this conﬁguration [26]. The minimum size of the ﬁrst prism layer reaches 0.0007 rotor chords (0.05 mm).
The cell size in the rotor wake is progressively increased until reaching the rotor-stator interface where a constant mesh size of 0.02 rotor chords is used. Upstream of the rotor blades the mesh is uniform and coarsen up to a mesh size of 0.05 rotor chords at the entrance of the nacelle. For the outside domain, the nacelle nearby region and the mixing layers are reﬁned. However, the grid is still relatively coarse compared to the inside, the goal being to obtain the mean effect of the mixing layer and not necessary to resolve the turbulent structures precisely. The resulting mesh is composed of 75 million cells (15 million nodes) of which over 95% are used to discretize the inside domain. The domain cut-off frequency is estimated to be about 10 kHz at the inlet interface (upstream of the rotor) and 20 kHz 1 stator chord downstream of the stator. The minimum cut-off frequency at the blade skin (used for far-ﬁeld propagation) is about 35 kHz and 30 kHz for the rotor and stator respectively.
parallel code coupler OpenPALM [28] which interpolates and exchanges the residuals at each iteration over overlapping grids [29].
The numerical scheme for the present LES is an explicit Lax-Wendroff second order scheme both in space and time. The time step for the simulation has been set to approximately 2.5 × 10−5 ms to have 14, 000 iterations per blade passage. The sub-grid scale model used is the Wall-Adapting Local Eddy-viscosity model (WALE) that provides the proper turbulence decay toward the walls. As for the turbine cases [14], a log-law model (which switches to a linear model for y+ < 11.445) is applied on all
walls to be consistent with the above y+ values and to ensure a physical friction on the walls. The dimensionless wall-normal distance y+ of the ﬁrst cell at the wall computed from an average solution from the LES is presented in Fig. 4. The Thanks to the prisms layers, the y+ is under 50 on the whole blade skin, whereas it is coarser on the hub and casing surfaces where prisms have not been used. At the inlet and outlet, Navier-Stokes characteristic non-reﬂective boundary conditions are used combined with a sponge layer to avoid spurious reﬂections from vortical ﬂuctuations [30].
The end of the transient period and the beginning of the statistics recording has been chosen according not only to the stabi- lization of the operating point but also to a method based on local quantities from Mockett et al. for statistical error estimation of ﬁnite time signals [31]. To assess this local convergence, several pressure probes have been used in various regions to ensure that the turbulence actually reached a statistically converged regime everywhere. Three convergence plots of those probes are shown in Fig. 5.
After a transient regime of about 4 rotation times, statistics have been extracted from the simulation during approximately 9 rotations (about 70 ms or 18 blade passages). The statistical error estimated on the mean and the standard deviation are around 0.2% and 3.0% respectively in the fan stage and around 0.2% and 6.0% in the jet mixing layer. Overall the convergence of the
statistics is very good on both the mean and the Root Mean Square (RMS) of the ﬂow variables. For acoustic post-processing, wall-pressure ﬂuctuations have also been extracted during that time every 20 time steps of the simulation (which corresponds approximately to a 2, 000 kHz sampling rate) on the rotor and stator blades.
The full simulation, including the transient regime, took 15 days on 3000 CPUs on the supercomputer Mammoth-MP2 from Université de Sherbrooke (1500 CPUs for the static domain, and 1500 CPUs for the rotor domain).
The mass-ﬂow rate and total pressure ratio computed from the two simulations are compared in Table 1 with measurements obtained by Hughes et al. [16]. The mass-ﬂow rate differs from experiments by less than 3% whereas the pressure ratio differs less than 1%. Similar RANS results were obtained by the various participants of the benchmark with different codes and turbu-lence models. As expected, predicted performance is different because the simulated tip clearance corresponds to the sideline “hot” geometry with maximum loading that does not account for the same blade deformation in rotation. Moreover, the slight geometry modiﬁcations performed on the stator vane count have a minor impact on the fan stage operating conditions.
Iso-contours of Mach number are presented on blade-to-blade surfaces at three channel heights in Fig. 6. Flow is moving
from left to right. Instantaneous LES ﬁelds are shown in the left column. The mean ﬁelds for the LES and the RANS simulations are seen in the center and right columns respectively. The instantaneous LES snapshots clearly illustrate the turbulent nature of the ﬂow in the stage. At all heights, a quasi laminar ﬂow is seen on the pressure side of the rotor blade and a turbulent boundary layer on its suction side, becoming thicker with increasing radius (increased relative speeds). The turbulent rotor wakes are then convected toward the stator vanes. As was found in the CME2 compressor simulations in similar ﬂow conditions with three different codes [12,32,33], the wake impingement does not trigger the transition to turbulence at the stator leading edge (too low Reynolds number), which rather occurs at vane mid-chord. Moreover from the instantaneous snapshots at 50% channel height, an intermittent ﬂow detachment at the leading edge of the rotor is seen to shed vortices that graze along the blade suction sides. A complex turbulent ﬂow behavior is seen at 97% channel height caused by the tip gap vortices with some intermittent ﬂow separation on one stator-vane pressure side. Note that similar ﬂow behavior has been reported by Casalino et al. using a hybrid Lattice-Boltzmann/Very Large Eddy Simulation (LBM/VLES) model of the baseline SDT conﬁguration [26]. Yet, in this hybrid model a tripping device needed to be introduced close to the blade leading edge over the whole span to trigger transition to turbulence and yield the proper turbulent levels in the wake. No tip gap caused by the voxel resolution could also be captured in the coarsest simulations.
The mean ﬂow described by both numerical approaches is very similar at midspan. Nevertheless, the LES boundary layers are thinner on the rotor blades (no recirculation bubble as in the RANS) yielding slightly thinner rotor wakes with less velocity deﬁcits than in the RANS results. This effect is emphasized at the hub where the wakes are clearly thinner in the LES than in the RANS simulation. For both spanwise locations, the mean ﬂow in the stator domain is very close in the RANS simulation to the one found in LES in spite of the very different approach used for the rotor-stator interface. Close to the tip, however, the mean ﬂow is signiﬁcantly different in the two simulations. On the one hand, the RANS calculation predicts a large ﬂow detachment on the rotor blade leading edge which interacts strongly with the tip leakage ﬂow. On the other hand, the deviation induced by the rotor tip ﬂow has an effect on the stator ﬂow resulting in important ﬂow separations at the tip of the outlet guide vanes that do not appear in the LES, which is again quite similar to what was found in the CME2 compressor case [12,32,33]. It should be emphasized that such ﬂow separations predicted by the RANS simulations were neither observed experimentally nor in the tripped LBM/VLES case.
Fig. 7 presents the ﬁeld of the normalized component of axial velocity in an axial cut at mid-distance between the rotor trailing edge and the stator leading edge where hot-wire (HW) measurements have been performed by Podboy et al. [23]. The values are normalized by the respective (experimental, RANS or LES) mean axial velocity at the axial plane of interest in order to take into account deviations from the experimental operational point given in Table 1. The twisting effect on the rotor wake in the interstage by the differential transport of the wake vorticity by the mean swirl is well captured by all simulations. On the upper part of the channel, the radial oscillations (S-shape) observed in the RANS wake proﬁle (seen in all benchmark RANS simulations) are the consequence of the tip ﬂow detachment from the leading-edge and is not present in the LES. Both in the LES and RANS the top 10% of the channel are strongly affected by the tip leakage ﬂow. The strong axial velocity deﬁcit results in a higher ﬂow blockage. The ﬂow blockage is illustrated in Fig. 9 by measuring the mass-ﬂow rate m˙ for several segments of span h/H% and computing it in experiments assuming that the density proﬁles follow the same linear trend from hub to casing as in the computations and that experimental mass-ﬂow rate of 26.44 is recovered when adding all segments.
The mass-ﬂow rate from the computations closely follow experiments over most of the duct section. In the top 10% the mass-ﬂow rate drops about 20 and 10% for RANS and LES respectively which may still suggest an insuﬃcient mesh resolution in the tip clearance or in the casing boundary layer. Yet as mentioned above, the different geometry used in the simulation does not take into account the same blade deformation in rotation and thus, the gap is different than in the experiments. Finally the LES normalized axial velocity outside the wake agrees well with experiments, even though the wake deﬁcit is under-predicted by 5%.
The unsteady nature of the LES also allows to obtain statistics of the resolved turbulent velocity ﬂuctuations: the three normalized RMS % components of the velocity are presented in Fig. 8 along with the experimental results. The turbulence pattern obtained by LES, for both the wakes and the tip secondary ﬂow, shows a good agreement with the experiments. In fact, not only the turbulence intensity is accurately reproduced but also the distribution on the three components is very similar all across the channel height with hardly any turbulence near the hub and transition to turbulence at mid-span.
Moreover in the wake, the three components have similar levels in most of the channel height, the tangential component being the lowest of the three as highlighted in another turbofan stage studied by Bonneau et al. [34] with a ZDES approach. Close to the casing, the tip clearance ﬂows cause a noticeable decrease of the radial ﬂuctuations in favor of the axial and tangential components as already shown by Leonard et al. (Fig. 14 in Ref. [19]). Similar results have also been reported by Casalino et al. with a LBM/VLES approach (Fig. 7 in Ref. [26]). The anisotropy invariants I2 and I3 are computed in the rotor wake at several radial and axial positions and depicted in the Lumley triangle in Fig. 10 [35]. The invariants illustrate preferential directions of the turbulent stress ﬁeld. They are computed as
where tr denotes the trace, det its determinant and A is the non-dimensional form of the anisotropy tensor with components deﬁned as
where the overline denotes the temporal average, u is the velocity ﬂuctuation of the respective component and 𝛿 is the Kro- necker delta. As expected from the levels of the RMS components, all values are close to the isotropic limit.
One advantage of LES is to give access to detailed information on the turbulence development and can help understand its inﬂuence on the noise sources of the turbofan operating at approach condition. The turbulent structures are identiﬁed using an iso-surface of Q-criterion computed in the rotor reference frame in Fig. 11. Q stands for the second invariant of the velocity- gradient tensor. Each blade side is shown from its leading edge.
As already mentioned for the instantaneous Mach number contours, while the boundary layer on the rotor-blade suction side is turbulent starting from the leading edge, the favorable pressure gradient on the pressure side limits the development of the turbulence (quasi laminar along the whole blade span): the boundary layer is initially laminar and some turbulent spots can be localized by the appearance of vertical vortex structures on the upper part of the blade only. On 30% of the blade span close to the hub, hardly any turbulent structures are visible on the pressure side revealing that the boundary layer stays in a quasi laminar state. The temperature contours coloring the iso-Q further stress that high ﬂow shearing and skin friction typical of turbulent ﬂows only occur on the upper part of both sides of the rotor blades. In the lower part of the channel shown in Fig. 7, the blades generate a very thin wake and low velocity deﬁcit. The laminar to turbulent boundary layer transition cannot be captured by the present RANS approach as it assumes the boundary layer to be turbulent right from the leading edge. The early boundary layer development yields larger wake width in the lower part of the channel in Fig. 7. The tip leakage generates large coherent structures that interact with the pressure side provoking the large velocity deﬁcit in the upper part of the channel.
Dynamic Mode Decompositions (DMD) has been performed on the pressure signals on the rotor blade skin in the rotating reference frame [36]. This signal processing method allows to detect and extract a set of dominant modes in a given complex ﬂow [15]. Fig. 12(a) shows the mode associated with the relative vane passing frequency (VPF). Note that the rotor blade is again seen from its leading edge. In the lower part of the blade pressure side where the boundary layer is laminar, a periodic wave going upstream can be seen, representing the acoustic pressure wave generated by wake interaction on the stator vanes traveling upstream. On the suction side a similar pattern can be guessed but it is hidden in the hydrodynamic ﬂuctuations and turbulent vortices.
Further evidence of the above ﬂow and acoustic features is provided by the wall-pressure power spectral densities (PSD) presented in Fig. 12(b). These PSD are computed from the pressure signals of two probes in both the laminar (15% of blade span) and turbulent parts (85% of blade span) of the pressure side of the rotor blade. On the one hand, the spectrum of the probe located in the laminar region has much lower levels with an early and quick roll-off at mid-frequency. It also presents a hump around the stator passing frequency. On the other hand, in the turbulent region the spectrum is broadband with no particular tonal frequency. This acoustic tone is again masked by the turbulent pressure ﬂuctuations.
The unsteady pressure recorded on the stator vane skin has been used for far-ﬁeld noise prediction with two different analo- gies, the Ffowcs Williams & Hawkings’ (FWH) analogy in free ﬁeld, and the extension of Goldstein’s analogy to annular ducts [37].
The FWH analogy allows to directly propagate the stator wall-pressure ﬂuctuations to the far-ﬁeld without any duct reﬂec- tions in order to then compute the acoustic power. Indeed from the resulting acoustic pressure p′(x, t) a surface integration over an upstream half-sphere S− and a downstream half-sphere S+ is performed to evaluate the intake and exhaust power:
dropped from the duct eigenfunction Em,𝜇 as it is a real function. In Eq. (10), the subscript I represents the reference coordinate system I , the coordinate vector xI0 = (rI0, 𝜃I0, xI0) constitutes the coordinates of the cell-centers on the surface elements.
Both analogies can be used following two different approaches. The ﬁrst one is obtained by computing the acoustic power
from Eq. (3) or Eq. (6) for FWH and Goldstein’s analogy respectively) independently for each blade, and then doing a summation over all of them. This calculation does not take into account any blade-to-blade correlation as the phase is lost when computing the norm in Eqs. (3) and (6). The second approach takes into account blade-to-blade correlations. For the FWH free-
ﬁeld analogy, the acoustic power S± (𝜔) in Eq. (3) is directly computed from the pressure ﬂuctuations p′ from Eq. (4) from all the
the blades in the computational domain and then calculating the norm of the expected value. Last, the pressure amplitudes are scaled by the ratio of total vane count and computed vane number. The phase shift of the vanes that are not directly computed is not yet taken into account in this work.
The results from the FWH analogy (noted as FWH Uncorr. and FWH Corr. for the uncorrelated and correlated summation respectively) and for Goldstein’s in-duct analogy (noted as Goldstein Uncorr. and Goldstein Corr.) are compared with the mea- surements [18] in Fig. 13(a) and (b). In the experiments, sound power is measured upstream by means of an acoustic screen to hide the downstream contributions. Then this contribution is subtracted from the acoustic power measured without the screen to get the exhaust information. The mock-up was also run without the stator row to measure the contribution from the fan only. The latter is then subtracted from the acoustic power measured with the fan-OGV conﬁguration to isolate the OGV contribution. Unfortunately, the experimental rotor-alone broadband noise levels eventually overtake the fan-OGV noise lev- els at some frequencies. This behavior is potentially caused by the strong swirling ﬂow at the nacelle exit interacting with the pylon.
The numerical predictions using the pressure ﬂuctuations on the stator as equivalent dipoles for both analogies provide an overall good agreement with experiment both in terms of levels and shape. The largest differences in levels (up to 10 dB higher than the fan-OGV measurements) are seen at low frequencies which highlights that additional noise mechanisms may be present in the simulation. Leonard et al. [19] already showed that the rotor-alone predictions were overpredicted because of too large coherent structures present in the tip gap region (under-resolved region in the LES). Such structures typically yield low-frequency humps observed in low-speed fans [39], which could correspond to the hump above 2 kHz in Fig. 13(a) and (b). Moreover, the differences with experiments are higher toward the intake since the reﬂections on the rotor blades are not taken into account. The effect of the reﬂections was shown analytically for the present conﬁguration by Posson and Moreau [40] and numerically by Casalino et al. [26].
The results obtained with Goldstein’s analogy extended to annular ducts are closer to the experimental data, especially toward the exhaust demonstrating the limitation of using a free-ﬁeld acoustic propagation for this conﬁguration. The nacelle plays a crucial ﬁltering role on the wave propagation: different duct-modes with distinct energy levels are cut-off yielding the sharper spectral decay at high frequencies seen both experimentally and numerically compared to the free-ﬁeld case. According to the number of blades and vanes, the BPF and its ﬁrst harmonics should be cut-off. However, the peak at 2863 Hz which corresponds to the BPF is visible with both approaches because only 5 vanes have been simulated and the Tyler & Sofrin rules [6] only apply for a full (axisymmetric) conﬁguration. Nonetheless, the peak is reduced by 10 dB in the second approach when the correlation of the 5 vanes is taken into account. The spurious peak also contributes to the above low-frequency over-prediction which shows the very strong sensitivity of the destructive interference in tone cut-off conditions. Finally the current agreement
with experiments at the exhaust is similar to what Casalino et al. obtained with the hybrid LBM/VLES approach (Fig. 12 in Ref. [26]), where the LES yields a better high-frequency decay because of the higher resolution of the vane noise sources.
The Large Eddy Simulation of the scale-model simpliﬁed NASA SDT turbofan has been successfully achieved for the ﬁrst time at approach condition for the reference stator-vane case. This simulation has been performed on the complete nacelle and fan- OGV experimental set-up with a slightly modiﬁed vane count to reduce the computational domain in the azimuthal direction.
The simulation was initialized from a preliminary steady RANS simulation of the same set-up using a k − 𝜔 SST turbulence
model. This conﬁguration is of particular interest since it provides a set of detailed experimental data not only for aerodynamics but also for acoustics. It is then a good test case to evaluate the ability of LES to reproduce the turbulent ﬂow in a fan stage but also to test different acoustic models for noise emission and propagation.
Both RANS and LES results have shown a good agreement with experiments on the mean aerodynamic ﬂow properties. However, the RANS simulation presents a leading-edge recirculation bubble at the tip of the rotor blade yielding a downstream S-shape rotor wake, which is not observed in the LES that only presents some intermittent vortex shedding grazing on the suction side. Similarly some ﬂow separation is observed at the tip of the stator vanes in the RANS results, which is neither found in the experiments nor in the LES. Noticeably the LES transitions to turbulence on the rotor-blade suction side close to the leading edge because of the sharp local acceleration, which could not be obtained in less resolved hybrid LBM/VLES results without tripping. The comparison of turbulent velocity ﬂuctuations with hot-wire measurements has shown that the LES also gives an accurate representation of turbulence in the wake. Noticeably, the bottom part of the wake is essentially laminar and much thinner in the LES than in the RANS that assumes turbulent ﬂow over the whole passage. From the RMS components and the anisotropy invariants, the turbulence is shown to be mostly isotropic in the wake which conﬁrms the validity of this commonly made hypothesis in most analytical models such as Hanson’s or Posson’s. Similarly to what was found in low-pressure compressors at similar Reynolds number based on the chord, the transition on the stator vanes is not directly triggered by the rotor wake impingement but rather occurs at mid-chord.
Finally, the LES has the advantage to provide unsteady wall-pressure signals on the stator vanes, which can be used as equivalent noise sources for both Ffowcs Williams & Hawkings’ analogy in free space, and the generalized Goldstein’s analogy extended to annular ducts. The sound power level spectra obtained with both analogies present the right trend but over- estimate the noise level at low frequency, most likely because the present tip gap is larger than in the experiments and the tip ﬂow structures are still under-resolved in the present detailed simulation. Moreover, the spurious low-frequency tonal noise may be caused by the limited number of blades in the present model. The correlated addition of the acoustic power for Goldstein’s analogy noticeably reduces the level of this tone. Yet at mid and high frequencies (up to 20 kHz at least), the results from Goldstein’s analogy match the experimental OGV spectra well both in terms of levels and shape, much better than the FWH results that overpredict the radiated sound power, which stresses the importance of accounting for the decay of cut-off modes in this frequency range taking place within the duct. An improved agreement over a larger frequency range is also observed compared with the less resolved hybrid LBM/VLES results with tripping.
Computations were made on the supercomputer Mammoth-MP2 from Université de Sherbrooke, managed by Calcul Québec and Compute Canada. The operation of this supercomputer is funded by the CFI, NanoQuébec, RMGA and FRQNT. The authors would like to thank E. Envia from NASA for fruitful discussions and technical support and for providing the experimental data in the framework of the AIAA benchmark.

This study introduces an analytical model aiming at predicting the tonal acoustic sources generated and radiated by the rotor–stator interaction in a fan stage. This model is able to cope with complex three-dimensional stator geometries and it fully accounts for cascade effects, characteristic of modern fan stages. It is based on a proper description of the rotor wake coupled with an analytic cascade response function and with an acoustic analogy. The proposed model is validated for the ﬁrst time against acoustic sources and sound power measurements, on the Advanced Noise Control Fan from NASA. On this conﬁg-uration representative of an actual fan stage, the model is shown to predict tonal sources and powers accurately, in function of the rotational velocity and of the stator-vane count. Another realistic conﬁguration, namely the low-pressure CME2 research compressor, is then considered in order to demonstrate the suitability of the model to be used as a design tool in an industrial context. A parametric study concerning both the stator vane sweep and lean angles is performed on this rotor–stator stage. The model produces predictions consistent with studies from literature, quantifying the effectiveness of swept and leaned vanes as a tonal noise reduction mechanism. This parametric study allows deﬁning an optimal stator design for minimal noise emission.
In the future of aircraft propulsion, the Ultra-High Bypass Ratio (UHBR) turbofan architecture is promising as it leads to an overall efﬁciency improvement and to a decrease of pollutants emission with respect to the current generation of aeroengines. In particular, the gain in propulsive efﬁciency is partly reached with a larger fan diameter. However the larger nacelle associated with this conﬁguration implies a drag increase that is detrimental to the overall engine efﬁciency. Thus a UHBR nacelle has to be shortened. This in turn induces shorter rotor–stator distances, more pronounced inlet and outlet distorsions and less efﬁcient passive treatment that reduces fan noise propagating outside of the nacelle. Therefore, in the frame of fan noise source reduction, the current work aims at improving fan noise modelling. As stated by several authors,
e.g. Groeneweg et al. [1], Envia et al. [2] and Peake and Parry [3], the interaction of fan wakes with the downstream outlet guide vanes (OGVs) represents the main source of fan noise, a trend growing with the expected decrease of the rotor–stator spacing in UHBR. A blade wake is composed of a mean velocity deﬁcit creating a periodic ﬂuctuation in time and in the azimuthal direction in the stator reference frame. Convected with the mean ﬂow, these wakes interact with the vane leading edges generating pressure ﬂuctuations on the whole vane surfaces. This unsteady loading radiates acoustic waves propagating within the nacelle, at the blade passing frequency (BPF) and its higher harmonics. This mechanism corresponds to the rotor–stator interaction tonal noise. Moreover turbulent structures present in blade wakes, upon interaction with the downstream vanes, generate random wall pressure ﬂuctuations that radiate to form the rotor–stator interaction broadband noise [4]. The current study focuses on fan tonal noise as it becomes crucial in UHBR because of the less efﬁcient passive treatment as well as the decrease of the rotor–stator gap creating a larger interaction of the rotor wakes with the stator.
Several types of Computational Aero-Acoustics (CAA) methods aiming at the prediction of fan tonal noise have been developed [2]. Among them, a ﬁrst category uses Reynolds-Averaged Navier–Stokes (RANS) equations to compute the viscous blade wakes downstream of the rotor. This vortical excitation is then imposed at the inlet of a stator domain on which the linearised Euler equations are numerically solved. This approach has for instance been implemented within the LINFLUX code, that computes the vane row acoustic response in the frequency domain, by Montgomery and Verdon [5], Verdon [6] and Prasad and Verdon [7]. The method of Atassi et al. [8] also uses linearised Euler equations to predict the acoustic response of the vane cascade. With such an approach, actual vane and duct geometries are accounted for in the source and near-ﬁeld acoustic prediction. A second category of methods, based on the Unsteady Reynolds-Averaged Navier– Stokes (URANS) equations, has been successfully surveyed by several authors [9–12]. These approaches allow considering a realistic ﬂow, particularly the vane mean loading and the inﬂuence of the stator on the rotor wakes, in the acoustic pre- diction. However the computational cost associated with these numerical methods still prevents them to be routinely used in an industrial context, especially for pre-design and parametric studies. This is the reason why analytical approaches represent an interesting compromise as they provide fast and exact solutions of simpliﬁed problems. The response function of Amiet [13], extended by Paterson and Amiet [14], Moreau et al. [15] and Roger et al. [16], deals with the interaction of an aerodynamic gust with a single airfoil in free-ﬁeld. In this model, suited for low solidity rotors without external casings such as helicopter rotors, propellers, contra-rotating open rotors and ventilators, the airfoil is modelled as an inﬁnitely thin ﬂat plate immersed in a uniform inviscid ﬂow with zero incidence (neither vane camber nor mean loading). The asymptotic analyses of Myers and Kerschen [17] and Evers and Peake [18] allow considering some mean loading effects in the response. For low solidity ducted fans, as proposed by Glegg [19] for instance, the isolated response can be coupled with Green's function tailored to a duct developed by Goldstein [20]. However cascade effect, i.e. the inﬂuence of the neighbouring vanes on the source generation and radiation of a given vane, has to be considered in a modern fan stage where vanes overlap. For the interaction of a gust with a rectilinear ﬂat plat cascade, the models are based on the resolution of an integral equation, using the acceleration potential method [21] or the lifting surfaces methods [22], leading to the LINSUB code for instance [23–26]. Namba and Schulten's and Zhang et al.'s model [27–29], also based on a lifting surface method, account for an annular cascade and the casing walls, but are limited to vanes with zero stagger angle and no radial geometrical variations. Another group of cascade models is based on a closed-form analytical solution of the integral equation with the Wiener– Hopf technique, successively extended by Mani and Hovray [30], Koch [31], Peake [32] and Glegg [33]. Posson et al. [34] extended Glegg's work to the calculation of the unsteady vane loading and of the pressure ﬁeld in the vane passage. In the cascade model of Hanson [4], also based on the Wiener–Hopf technique and Glegg's blade response, the strip theory approach allows accounting for variable stator geometry along the duct height, but the acoustic power is evaluated in free ﬁeld and decorrelated from one strip to another one. Finally, another approach consists in considering the actual annular distribution of acoustic sources on the stator vanes as well as their radiation within an annular duct, contrary to the free ﬁeld acoustic radiation from a rectilinear cascade. In the model of Ventres et al. [35], the unsteady vane loading is obtained via a numerical resolution of the integral equation then radiated with the annular duct Green's function of Goldstein [20]. This model, developed for fan tonal and broadband noise applications, has been extended by Meyer and Envia [36], Nal- lasamy and Envia [37] and Grace et al. [38]. Finally the model of Posson et al. [39], using a purely analytical solution for the acoustic sources and the in-duct radiation with Goldstein's analogy, has been applied and validated for fan broadband noise predictions.
Tonal fan noise modelling is challenging mainly because the sound emission is sensitive to the stator geometry and because the excitation and the vane response are correlated along the duct height. This is why the present model, based on the analytical cascade response function of Posson et al. [34], uses a radial strip approach in order to consider a complex stator geometry, with variable stagger, sweep and lean angles for the vanes. Moreover this cascade response is coupled with Green's function developed by Goldstein [20] so as to account for the annular duct geometry and an axial mean ﬂow in the acoustic propagation. The aerodynamic excitation is decomposed into skewed gusts that are coupled with the 3D cascade response to ensure a correct representation of the radially correlated vane response. Although this tonal noise model has already been used in preliminary studies [40,41], the ﬁrst objective of the paper consists in performing a complete description of this cascade based acoustic model, in focusing particularly on the complex stator geometry and the aero- dynamic excitation. Then Section 3 aims at validating the model against experimental data collected on the Advanced Noise Control Fan (ANCF) conﬁguration [42,43]. The latter is representative of an actual fan stage of a modern aeroengine and exhaustive available datasets allow performing trends with the vane count and the rotational velocity. Finally the model is evaluated in Section 4 on the CME2 axial compressor stage [44], at higher reduced frequencies than on the ANCF conﬁg- uration. Parametric studies on the vane geometry are performed in order to highlight the model capability of handling complex stator conﬁgurations and of determining an optimal acoustic design.
In this section, the cascade based acoustic model of Posson et al. [34,39], originally developed for fan broadband noise applications, is generalised to fan tonal noise prediction. The conﬁguration treated here, represented in Fig. 1, is a rotor– stator stage mounted in an annular duct. The cylindrical reference frames Rr ðxd; rd; θrÞ and Rdðxd; rd; θdÞ are direct and linked to the rotor and the stator respectively, with xd directed downstream. The rotor is composed of B blades and the stator of V vanes. In order to account for complex blade and vane geometries present in modern turbofan engines, i.e. with stagger, sweep and lean angles varying along the duct height, both the rotor and stator rows are split into annular strips along the span. Each strip at radius rd is then unwrapped into a local rectilinear geometry represented in Fig. 2. The direct Cartesian reference frame R0ðxd; yd; zdÞ is attached to the rectilinear cascade of inﬁnite span ﬂat plates. Following a literature review, it appears necessary to propose a uniﬁed formulation of the equations in order to account for any rotational direction. Indeed,
Fig. 2(a) corresponds to the conﬁguration with the rotational speed Ωo0 used in the model of Posson et al. [34] for instance, whereas Fig. 2(b) represents the conﬁguration treated in the model of Ventres et al. [35] as well as in its further extensions [36,45] with Ω40. The frames Rr and Rd are linked by the following relation, Ω being a relative speed from now on:
In the Rd reference frame, the geometrical sweep φ^ ðrdÞ and lean ψ^ ðrdÞ angles of the vane are deﬁned in Figs. 3 and 4, looking successively from the side and the front of the cascade. It is worth noting that φ^ is independent of the rotational velocity sign, whereas the sign of ψ^ changes with the sign of Ω, as depicted in Fig. 4. Moreover an arbitrary ðyf ; zf Þ surface is introduced, viewed from the side in Fig. 3 (dotted grey line), on which the aerodynamic data (mean ﬂow and rotor wake) are extracted to be input within the model. For an efﬁcient interaction modelling, this surface has to follow the vane leading edge radial evolution. 
The change of coordinates is chosen to only deal with direct reference frames thus implying the direction yd in R0 to be in the opposite direction than θd (in Rd), for any sign of the rotational velocity (see Fig. 2). In R0 the sign of the vane stagger angle is always opposed to the sign of Ω. The cascade reference frame Rc is ﬁxed such as yc is in the yd direction. This imposes yc to be oriented from the pressure side (PS) to the suction side (SS) of the vane for a negative rotational velocity (Fig. 2(a)), and in the opposite direction (from SS to PS) for a positive rotational velocity (Fig. 2(b)). For both conﬁgurations, the vane number ν increases in the yc direction similarly to the convention adopted by Glegg [33].
The rotor wakes formed at the blade trailing edges are convected with the subsonic mean ﬂow and interact with the downstream stator vanes, creating unsteady wall pressure ﬂuctuations that correspond to the tonal acoustic sources. More precisely, following the deﬁnition of Sutliff et al. [42], the aerodynamic excitation corresponds to the velocity component w normal to the direction given by the mass-averaged absolute ﬂow angle α (see Fig. 2). Assuming that this excitation is identical from one rotor blade to another one, it becomes 2π=B-periodic in the azimuthal direction as well as periodic in time with the blade passing pulsation qBΩ. Using the following convention for the temporal Fourier series:
From Eq. (7) only the coefﬁcient W q contributes to the cascade response at the qth harmonic of the blade passing frequency (BPF). Because of the no-slip condition on both the hub and casing walls, the absolute ﬂow velocity is zero in a ducted fan stage at these locations. Thus the upwash velocity can be 
This 3D decomposition aims at a better representation of the incident perturbation than the 2D approach described with Eq. (7). Indeed the latter implies that the radial evolution of the wake properties are only treated by variations of amplitudes with the parameter rd. A 3D decomposition allows accounting for the radial gradients of the perturbation [46] as well as for the radial correlation known to be signiﬁcative for a tonal noise excitation [47]. The principle of the 3D decomposition proposed in Eq. (9) has been validated in [40] but it has never been confronted to an experimental database.
From aerodynamic measurements or simulations, the mean velocity proﬁle U as well as the upwash w decomposed in Section 2.2 are extracted in the ðyf ; zf Þ surface introduced in Section 2.1. As a general rule for the interaction modelling, it is recommended to consider the excitation as close as possible to the vane leading edges, thus implying [48]. A particular feature of the present generalised acoustic model consists in dealing with a non-zero radial velocity component Ur in the duct reference frame Rd in order to improve the realistic ﬂow modelling. From the mean velocity vector UðUxf ; Uyf ; Uzf Þ in Rf, the velocity components in the cascade reference frame Rc are, using Eqs. (2) and (3):
Even if Ur ¼ 0, the swept vanes imply a non-zero value for Uzc in the cascade reference frame, a case already treated by Glegg [33] and Hanson [4]. From the aerodynamic radial wavenumber kr introduced in Section 2.2, it is assumed that the wavenumber along zf veriﬁes the relation:
From the azimuthal periodicity of the conﬁguration, the aerodynamic wavenumber along θd can only be equal to an inﬁnite amount of countable values. In the context of rotor–stator tonal noise, the azimuthal indices of the excitation are related to
the rotational direction such as. From Section 2.1, yd and θd are in the opposite directions thus:
since yf and yd are identical. From the dispersion relation veriﬁed by the wave vector K of the aerodynamic excitation: where. denotes a scalar product, the wavenumber in the xf direction is:
Fig. 2 illustrates that sign keeps the same sign for any value of Ω thus ensuring the independence of kxf relatively to the rotational direction. Finally the wave vector of the excitation in the cascade reference frame Rc can be found with:
crucial parameter for the cascade response is the inter-vane phase angle linking the responses of two consecutive vanes upon an interaction with a gust. Following [4], it is:
with d and h related to the inter-vane distance g ¼ 2πrd=V, represented in Fig. 2, by:
it can be shown that the inter-vane phase angle only depends on the harmonic index as
From the global conﬁguration introduced in Section 2.1, the elementary problem, represented in Fig. 5 in Rc corresponds to an aerodynamic gust of wavenumbers convected with an inviscid ﬂow of uniform velocity Uxc impinging on the cascade. This rectilinear cascade is composed of inﬁnitely thin ﬂat plates with inﬁnite span in the direction. In order to ensure the slip boundary condition on the plate surface, an unsteady pressure loading is created in response to the gust interaction. The cascade response function of Posson et al. [34] based on the previous work by Glegg [33] uses the Wiener–Hopf technique on four sub-problems to resolve the vane loading, with the following boundary conditions: con- tinuity of velocity potential upstream of the leading edge, zero velocity normal to the plate (slip condition) and pressure continuity at the trailing edge (Kutta condition). A closed form expression for the acoustic ﬁeld and for the vane loading is obtained providing that the non-overlapping distance d (see Eq. (17)) is smaller than the plate chord. the vanes partially overlap over their whole span. The analytical expression of the elementary vane loading ΔP 0 is presented in Appendix A.
Moreover the resolution needs the non-overlapped length of the cascade d to be positive, that is the case of the con- ﬁguration with Ωo0 in Fig. 5(a). In other words, s being the cascade gap, the cascade response function is valid for χ 40, with:
However a simple symmetry consideration between both conﬁgurations of Fig. 5 leads to the following relation:
since the pressure and suction sides of a vane are inverted with respect to the yc direction as in Fig. 5(a) and (b). Eq. (19) allows the cascade response function to be used for any sign of the rotational velocity while jdjoC. It must be mentioned that the semi-numerical method employed for the resolution of the cascade response of Ventres et al. [35] makes possible any value of d. The implementation of this 2D model has allowed conﬁrming the validity of Eq. (19) for 2D gusts in [48]. It has also highlighted the limitation of this semi-numerical cascade response as its accuracy depends of the amount of discretisation points, especially at high frequencies, contrary to the closed-form expression for the cascade response pre- sented in Appendix A. The present analytical cascade response has been validated in a rectilinear conﬁguration in [34] by successful comparisons with alternative models, e.g. the LINSUB model [49], the semi-numerical LINC code [22] and the three-dimensional linearised Euler code of Ataasi et al. [8].
The 3D analytical cascade response is used with a correction proposed in [50] in order to account for annular effects in an otherwise rectilinear problem. Indeed this correction has been shown to strongly reduce unphysical effects related to the strip theory approach applied to a varying geometry along the duct height [48]. Previous studies have also evaluated the vane thickness effect as well as the ﬂuid viscosity effect on fan tonal noise generation and propagation [41]. Vane thickness has been found to modify the distribution of the acoustic energy across the radial modes within each azimuthal mode. Viscous effects are of secondary importance, providing neither vortex shedding nor any secondary ﬂow with their own frequency close to tonal frequencies exists. Besides vane camber has been shown to be relevant for fan tonal noise pre- diction, and a simple method for considering vane camber effects has been proposed [51]. It is based on the coupling of two cascade response functions with different stagger angles and is not applied for the present study.
From the pressure jump P x ; z ; σ; k created by the impingement of a gust on the cascade (Eq. (A.2)), the cascade response at the qth harmonic of the BPF is written as:
For the future integration on the vane, the pressure jump needs to be expressed on the chord at constant radius Cd, inclined of φ with the chord C in Rc. Assuming a small value for φ, the pressure jump is [50]:
with Cd ¼ C= cos φ. In Eq. (20), the summation over the indices p of the radial wavenumber is actually truncated. Indeed for a given pulsation of the incoming gust qB Ω, only a limited amount of diffracted cascade acoustic modes is cut-on and directly contributes to the cascade response [4]. In addition, because of the rectilinear to annular transformation in the acoustic analogy, some cut-off modes may also have an inﬂuence on the response, especially at low frequencies [48]. That is why the criterion on kzc deﬁning the truncation of Eq. (20) corresponds to an extension of the expression given in [4]. It reads:
with CΔkzc ¼ 20, and has been shown to be relevant for the present study.
The acoustic analogy is based on the unsteady loading on the whole vane surface, analytically predicted by the cascade response function (Eq. (21)), that is used as dipole sources. Assuming that the stator is placed in an inﬁnite annular duct with a constant section and rigid walls, containing a uniform axial mean ﬂow (Uxd ¼ c0Mxd), Goldstein [20] gives the for- mulation of the acoustic pressure perceived by an observer located at point x ¼ ðxd; rd; θdÞ inside the duct at time t generated by the force f exerted at point x0 ¼ ðx0; r0; θ0Þ and time t0 by the vane surface Sðx0Þ on the ﬂuid:
with T large but ﬁnite and the monopole and quadrupole terms being neglected. G is Green's function tailored to a rigid annular duct with a uniform axial mean ﬂow thus verifying the boundary conditions on the hub and tip walls. G can be
The radial eigenfunction Em;μ, corresponding to the radial component of the solution of the Helmholtz equation, is a linear combination of Bessel functions [20]. This function Em;μ must satisfy the boundary conditions of the problem, written for a rigid duct as:
The duct eigenvalues χm;μ are the constants of the Bessel functions allowing the boundary conditions of Eq. (26) to be satisﬁed. The indices m and μ are associated with the azimuthal and radial mode orders respectively. The axial acoustic wavenumber is γ 7, the superscripts þ and - indicating respectively the upstream and downstream propagations:
For both conﬁgurations in Fig. 5, n represents the unitary vector normal to the vane, directed from the pressure to the suction side, corresponding to n ¼ -sign. Using the matrix Q (Eq. (3)), the coordinates of n in the duct reference frame are:
The ﬂow being inviscid, the force f in Eq. (23) is only composed of its component normal to the vane, i.e. the pressure distribution, thus:
The integrand of Eq. (23) is then:
Using Green's function introduced in Eqs. (24) and (29) becomes:
Using Eq. (23) applied to tonal noise and dealing only with positive frequencies qBjΩj, the acoustic pressure within the duct for the harmonic q is expressed as:
The double inﬁnite sum in Eq. (31) is limited to the acoustic duct modes (m,μ) excited by the rotor–stator interaction that verify the following relation, adapted from Tyler and Sofrin [52], providing a regular equispaced set of rotor blades and stator vanes:
integral along the xc direction of the pressure distribution in Eq. (21):
and the acoustic wavenumber in the xcd direction is:
It can be veriﬁed that the modal pressure coefﬁcient in Eq. (33) keeps the same value for any rotational direction, noting that the sign of the azimuthal index m changes between both conﬁgurations of Fig. 2. Finally the acoustic power of the harmonic q reads after integration in a duct section [36]:
Among the excited duct modes (Eq. (32)), only the cut-on modes, i.e. verifying the relation κ2 40, are considered in the
coustic power calculation of Eq. (36).
Our model dedicated to the prediction of tonal fan noise has been fully described in this section. It has been shown that this model is a generalisation of the model of Posson et al. [34,39] for fan broadband noise prediction. As stated in the introduction, this model uses successive developments from Glegg [33] and Hanson [4]. Several original contributions have been introduced within the present model in order to increase the accuracy of the prediction. First an uniﬁed formulation of the whole set of equations has been proposed in order to account for both rotational directions of the fan. A radial decomposition for the aerodynamic excitation has also been introduced. It aims at accounting for the radial correlation existing between the excitation and the cascade response along the duct height in the tonal noise context. In order to consider a realistic ﬂow, this model is able to consider the radial component of the mean ﬂow velocity. Moreover the aerodynamic input data can be extracted, from experiments or numerical simulations, on a surface matching the radial shape of the vane leading edge. This ensures an accurate gust–cascade interaction modelling, especially for vanes with complex geometries.
The tonal fan noise prediction model detailed in Section 2 is evaluated on the Advanced Noise Control Fan (ANCF) conﬁguration. This test bed has been operated at the NASA Glenn Research Center [53,54] and has supported a large amount of aeroacoustic research directed towards fan noise prediction, control and reduction [55]. The ANCF test case has been chosen as it is representative of an actual fan stage of a modern high bypass ratio turbofan. Indeed the rotor–stator stage represented in Fig. 6 is mounted in a 1.219 m (4 feet) diameter annular duct with a hub to tip ratio of 0.375. At the fre- quencies of interest, this ensures the propagation of azimuthal and radial acoustic duct modes similarly to the acoustic ﬁeld present in a nacelle. As shown in Fig. 6, the large scale structures of the inlet ﬂow are reduced by an inﬂow control device so that no additional tonal noise comes from ingested vortical structures [53]. The particularity of this test rig corresponds to the center body supporting the rotor without any structural element ﬁxed to the outer casing. Thus the rotor alone and rotor–stator conﬁgurations can be investigated without any possible strut interactions in the inlet or exhaust part of the duct. This fan rig is placed in an anechoic test facility equipped with aerodynamic and acoustic measurement devices [56,57]. In particular two rotating rakes [58] allow capturing the modal content of the acoustic ﬁeld generated by the fan stage, in a cylindrical section at the inlet and in an annular section at the exhaust, with a hub to tip ratio of 0.5 (see Fig. 6). This system provides the modal decomposition of the acoustic power used in Section 3.4 for the comparisons with the analytical predictions.
The rotor is composed of 16 blades with a 0.133 m chord. The stator vane chord is C ¼ 0.114 m with an aspect ratio of C=ðRT - RH Þ ¼ 0.3. It is radially stacked with a 12° twist without any sweep nor lean angle. In this highly conﬁgurable fan stage, the stator vane count V can vary between 13 and 30 in keeping the same vane geometry. Thus the solidity changes from CV =2πRT ¼ 0.39–0.84 at the tip. Even with these relatively low solidities, the vane stagger angle close to 0° at the tip ensures that cascade effects are present since the overlapping length is 84 percent of chord with V ¼ 13. The inter-blade row spacing, measured at the hub, can vary between 1/2 and 2 vane chords from the rotor blade trailing edge. One of the stator vanes is instrumented with ﬂush-mounted microphones on each side of the vane [43], as schematically represented in Fig. 7. This allows capturing the unsteady vane loading, partly corresponding to the tonal acoustic sources, that are com- pared with analytical predictions in Section 3.3. At nominal conditions the rotational velocity is 1800 RPM, giving a blade passing frequency (BPF) of 480 Hz, a peripheral tip Mach number around 0.34 and an axial Mach number around 0.15. The mass ﬂow rate is 56 kg s-1 and the Reynolds number based on the blade chord is around 4 x 105.
Among the investigations performed in the ANCF test rig, a particular attention is paid to the work of Sutliff et al. [42,43]. Indeed these references contain the measured vane unsteady loading and modal acoustic powers of the tonal noise gen- erated by the rotor–stator interaction, for several conﬁgurations listed in Table 1. These allow performing trend studies with the stator vane count and the rotational speed. The cut-on and excited duct modes at 1 BPF and 2 BPF in these conﬁgurations are reported in Table 1. For all these cases, the inter-row spacing is ﬁxed at half a vane chord. Moreover Sutliff et al. [42] compare the modal powers with the predictions of V072 code, corresponding to the cascade based model of Ventres et al.
[35] updated by Meyer and Envia [36]. As already mentioned in the introduction and in Section 2.4, this model is close to the present evaluated model, the main differences being the semi-numerical resolution of the 2D cascade response instead of a closed-form expression of the 3D cascade response for the current model. Note that both models use the same acoustic analogy of Goldstein [20] in an annular duct for the acoustic propagation.
In Sections 3.4 and 3.4, the acoustic model introduced in Section 2 is evaluated on the ANCF conﬁgurations listed in Table 1 against experimental results and V072 analytical predictions. The aerodynamic input data needed for these pre- dictions are also provided in [42] and consist of crossed hot-wire measurements in the plane of the stator vane leading edges, the stator being removed. Thus the convection effects on the rotor wakes are correctly taken into account, but the inﬂuence of the upstream potential effect of the stator on these wakes is absent. This potential effect may signiﬁcantly modify the shape of the rotor wake, as studied by de Laborderie et al. [59] for instance. Therefore, the vortical excitation used in the models does not exactly correspond to the actual excitation impinging on the vanes. The X-shape of the probe, placed in a plane perpendicular to the radial direction, provides the axial and tangential velocity components. The experimental uncertainty is around 0.6 m/s on the velocities and 0.2° on the angles [42]. From these two-component hot-wire data, the upwash is determined as the velocity component normal to the mass-averaged absolute ﬂow angle [42]. In order to perform relevant comparisons, the upwash over one blade passage and the mean velocity proﬁles provided in [42] are used as input data for the acoustic predictions with the present cascade model. Although the velocity proﬁles are available for the four rotational speeds of Table 1, the upwash in the stator leading edge plane is only given at 1850 RPM. It is represented in Fig. 8, where the velocity deﬁcit, indicating the rotor wake, is decreasing with the increasing radius. According to Sutliff et al. [42], the upwash for the other rotational speeds are similar to Fig. 8. However it means that the excitations used for the V072 predictions are not identical to the single one used with the present model. It has to be mentioned that the mean ﬂow properties (temperature and density), not available in [42], are coming from a more recent measurement campaign [56]. The equivalent stagger angle χ of the ﬂat plate cascade used for the V072 predictions, cor-responding to a weighted average of the inlet and outlet vane metal angles, is given in the reference paper [42].
As described in Section 3.1, one of the ANCF stator vanes is instrumented with embedded microphones, shown in Fig. 7, in order to locally measure the unsteady vane loading. The latter is known to represent the sources of rotor–stator inter-action tonal noise, as mathematically expressed in Eqs. (31)–(36). The measurements correspond to the amplitudes (in terms of sound pressure levels (SPL)) and phases of the wall pressure on each side of a vane, for all the cases reported in Table 1. These data are provided in [43]. However, for the sake of brevity, only a single case is analysed in this section, corresponding to the stator with 14 vanes and Ω ¼ 1800 RPM. Indeed for this conﬁguration, the amount of microphones is larger than for the other cases since the sensors line at 85 percent of chord in Fig. 7 is only installed in this case. These experimental data allow evaluating the cascade response function of the acoustic model introduced in Section 2.4, in its 2D and 3D versions. To this end, the measured wall pressures are ﬁrst transformed into pressure jumps across the vane, knowing that the microphones are mounted by pair at the same chordwise and spanwise locations on each side of the vane. Unlike the acoustic power comparisons performed in Section 3.4, no analytical results from the V072 cascade response function are available preventing any comparisons between both acoustic models.
Fig. 9 represents the unsteady loading at 1 BPF along the vane chord at the three spanwise positions shown in Fig. 7, measured by the microphones and predicted by the 2D and 3D versions of the cascade response function. From the vane leading edge to around 60 percent of chord, the SPL shapes are in good agreement all over the span, with a higher loading level in the leading edge region followed by a smooth decrease. This is consistent with the vane interaction noise theory (e.g. [13]) predicting that the main acoustic sources are generated in the leading edge region of the proﬁle where the incoming gust is diffracted. In Fig. 9(a), the 2D response gives a prediction closer to the experimental data than the 3D response, whereas both produce equivalent results in Fig. 9(b) and (c). Nevertheless the measured pressure jumps present a sudden SPL rise starting at around 70 percent of chord in the upper part of the vane (at 0.74RT and 0.91RT). This behaviour is not predicted by the model for which the loading monotonically decreases until the trailing edge. Referring to the original data in [43], it is shown that this local SPL increase only occurs on the vane suction side. Indeed as mentioned in [42], the mean ﬂow incomes on the vane with a relatively high incidence thus creating a ﬂow separation on the downstream part of the vane suction side that is responsible for this high level of pressure ﬂuctuations at these locations. As the model assumes a zero incidence mean ﬂow, this ﬂow separation mechanism cannot be predicted analytically. The ﬂow separation is also responsible for the high level of phase ﬂuctuations observed in Figs. 9(e) and (f). In addition, the discrepancy between the measured and predicted loading phases (Figs. 9(d)–(f)) is partly caused by the stator potential effect not present in the upwash used for the predictions (see Section 3.2). This potential effect modiﬁes the phase of the excitation, as shown in [59], thus directly impacts the phase of the vane response.
Figs. 10(a) and (b) compare the loadings in the spanwise direction in the leading edge region (20 percent of chord). The levels match fairly well between the measurements and the predictions in terms of SPL. The phases are also in correct agreement, except for a constant shift existing between 0.5RT and the vane tip, again caused by the stator potential effect on the wake. The global shape of the measured phase, decreasing from hub to tip, indicates that the rotor wake interacts with the vane bottom part before the vane tip. At 85 percent of chord (Figs. 10(c) and (d)), the level of the predicted loading is 
well below the experimental data in the upper part of the vane, conﬁrming the previous observation related to the ﬂow separation on the vane suction side in the trailing edge region.
The vane loadings at 2 BPF are compared in Figs. 11 and 12. Similarly to the case at 1 BPF, the SPL values are higher in the leading edge region than in the rest of the chord, a behaviour correctly predicted by the model. The agreement between the experimental data and the 3D cascade response are very satisfactory at 0.74 and 0.91RT. The ﬂow separation is again visible in the SPL data in Figs. 11(b) and (c) from 70 percent of chord, but the level rises are less pronounced that at 1 BPF. The measured and analytical loading phases (Figs. 11(d)–(f)) are signiﬁcantly different, especially in the upper part of the vane where the large variation of the phases indicates a complex ﬂow behaviour not predicted by the model. Yet some of the phase shifts in the measurements are also questionable and suggest larger experimental uncertainties on the phase at this frequency. At 0.49RT (Fig. 11(a)), the 3D model under-predicts the loading by about 10 dB and the 2D model by a larger value. This is also visible in Fig. 12(a) representing the loading evolution in the spanwise direction. Sharp deﬁcit peaks appear in the analytical responses around 0.49RT and 0.83RT, whereas the measured and predicted vane loading compare fairly well on the rest of the vane height. Sudden phase changes are present at the same radial locations in Figs. 12(b) and (d). These analytical discontinuities originate from the coupling of the strip theory with a cascade response function, as already dis-cussed in [33,60] for instance. Indeed the variation of the rectilinear cascade geometry at each strip makes possible dif-fracted cascade acoustic modes and inter-vane channels modes to become cut-on at speciﬁc spanwise locations. Following a procedure similar to [48], it can be shown that the discontinuity at 0.49RT is caused by a cascade acoustic mode, generated by the interaction of the gust (q ¼ 2, kzc ¼ 0) with the cascade, becoming cut-on at this location. At 0.83RT, the inter-vane channel mode, created by a zero radial wavenumber gust, becomes cut-on at this position for 2 BPF and is responsible for the loading discontinuity. These two discontinuities, present over the whole chord, are also visible at 85 percent of chord in Fig. 12(c). Note that a cascade acoustic mode is also becoming cut-on near the vane tip in Fig. 10 explaining the differences between the 2D and 3D responses at this location. On the rest of the vane span, the predicted levels are lower than the experimental data, a feature again caused by the ﬂow separation taking place on a large part of the vane suction side. As previously noted, the experimental phase data present large uncertainties at this frequency limiting the comparisons in Figs. 12(b) and (d).
The comparisons performed in this section show that the differences between the 2D and the 3D cascade responses increase with the frequency of the excitation. Indeed, a single gust of zero radial wavenumber contributes to the 2D response, whereas the amount of skewed gusts contributing to the 3D response rises with the frequency. This 3D behaviour improves the modelling accuracy relatively to the actual interaction [48]. Moreover, the local discontinuities created by the coupling of the strip theory with a cascade response function are smaller with the 3D model than with the 2D model, leading to a more accurate prediction with the 3D model, e.g. in Figs. 11(a) and (c). Thus the analysis of the predicted unsteady vane loading with respect to wall pressure measurements allows reasonably validating the 3D cascade response function in an actual fan conﬁguration. The main discrepancies encountered in the comparisons have been explained, as they result from the measured upwash not accounting for the stator potential effect as well as a ﬂow separation region on the upper part of the vane suction side.
Using the measured aerodynamic data of Sutliff et al. [42], among which the upwash shown in Fig. 8, the cascade model detailed in Section 2 is applied to the whole set of conﬁgurations listed in Table 1. For a complete evaluation, both the 3D and the 2D version of the model, i.e. with the decomposition of Eq. (7), are considered. In this section, the sound power level (SWL) of Eq. (36) is presented for each harmonic of the BPF and for the upstream and downstream propagations. In Figs. 13– 16, these analytical predictions are compared with experimental power values obtained from the rotating rakes installed in the inlet and exhaust part of the duct (see Fig. 6) as well as with the predictions of the 2D cascade model V072 reported in [42].
In the stator conﬁguration with 13 vanes, only the duct mode (3,0) is excited and cut-on at 1 BPF (Table 1). For this case, Figs. 13(a) and (b) show an excellent agreement between the analytical predictions and the experimental values, for the upstream and downstream propagations respectively. Indeed the results of the model are within 2.4 dB from the measured values upstream. In Fig. 13(a) the 3D model predicts the power with a gap of 0.4 dB at 1800 RPM, the V072 value being at 0.7 dB. Downstream (Fig. 13(b)), the 3D model results vary between 1 and 1.8 dB with respect to the V072 predictions. For both upstream and downstream propagations, the experimental trend shows an increase of sound power with the rota-tional velocity, whereas all analytical predictions appear relatively ﬂat. The difference between upstream and downstream powers is more pronounced experimentally than with the predictions. At 2 BPF, the modes (- 7,0) and (6,0) propagate within the duct. The sum of their SWL is plotted upstream and downstream in Figs. 13(c) and (d) respectively. The present cascade model provides a relatively poor agreement with the V072 prediction, between 7 and 11 dB upstream and around 7 dB downstream. If the 3D model and V072 results present a similar trend with Ω, they both are far from the experimental trends. The latter show large and non-monotonic SWL variations with the rotor speed, that are partly explained in [42] as an acoustic reﬂection in the exhaust section where the rotating rake is mounted. It must also be mentioned that for all the comparisons performed in this section, the analytical results are obtained from an acoustic analogy in a constant section annular duct with RH=RT ¼ 0:375, whereas the measurements at the inlet and exhaust rotating rakes take place in sections with RH =RT ¼ 0 and 0.5 respectively. Thus reﬂections and standing waves may appear in the actual duct geometry thus explaining some differences with the analytical results.
In Figs. 14(a) and (b), corresponding to the stator with 14 vanes at 1 BPF, only the duct mode (2,0) is present. For both the upstream and downstream propagations, the 2D and 3D versions of the model are similar and very close to the V072 predictions (around 0.5 dB). The analytical results are in fair agreement with the experimental data upstream since the gap is about 2 dB, whereas this difference is larger downstream (between 7 and 8 dB). It is interesting to note that the predicted trend with Ω is now similar to the measured one. At 2 BPF, a single azimuthal mode is created, with two radial components (4,0) and (4,1). The upstream and downstream predictions given by the 3D model (triangles in Figs. 14(c) and (d)) are in correct agreement with the measured data since the differences are within 2 dB at 1700 and 1750 RPM and 3 dB at 1800 and 1850 RPM (except at 1850 RPM downstream where the experimental value shows an odd sharp drop). Therefore the 3D model provides better results upstream, and worse downstream, than the V072 code.
With the 26 vane stator, all the duct modes are cut-off at 1 BPF. Figs. 15(a) and (b) show the acoustic power at 2 BPF, corresponding to the single (6,0) mode. Again the evolution of the experimental data with the rotational speed is sig- niﬁcative, with an increase followed by a strong decrease of the levels upstream and by a slight decrease downstream. No acoustic model is able to predict this measured shape, partly inﬂuenced by the actual duct acoustic propagation as stated above. Moreover, according to [42], the main reason for this particular experimental trend was not deﬁnitely found, but could result from mode trapping and scattering. Only for this conﬁguration, it is remarkable to note that the 2D and 3D models clearly present opposite trends. Both provide results below the V072 predictions, between 7.5 and 0.5 dB upstream and around 5 dB downstream for the 3D model.
Again no duct mode is cut-on at 1 BPF in the 28 vane stator conﬁguration. At 2BPF, a single azimuthal mode with two radial components (4,0) and (4,1) propagates. For the upstream propagation (Fig. 16(a)), it is shown that the analytical predictions are similar since they are all comprised within 1 dB. They are very close to the measured power, at least for the available data. Downstream (Fig. 16(b)), the evaluated 3D model provides results closer to the experimental data than the other 2D models and under-predicts the measurements by 0.4–1.6 dB.
In summary the present comparisons of tonal acoustic powers have allowed the evaluation of the cascade model intro- duced in Section 2 relatively to measured data and analytical predictions of an existing code. As explained in Section 3.2, the V072 code and the 2D version of the model should be similar since they both use a 2D cascade response function coupled with an acoustic analogy within an annular duct. Figs. 14(a), (b) and Figs. 16(a), (b) indeed show very close results between these two codes. However in some cases, e.g. in Figs. 13(a), (b) and Figs. 14(c), (d), the 2D results present a similar trend but are separated by a shift. For the other cases, the 2D predictions are relatively different. From this analysis, it may be inferred that the input data used for V072 and the 2D models are not exactly equivalent, especially concerning the mean ﬂow values that are not provided in [42] and come from a more recent study [56]. Therefore, this shows that these acoustic models are particularly sensitive to the input data. As described in Section 2, the 3D cascade model has been designed to improve the fan noise prediction. In half of the studied cases, the 3D model provides better or equivalent (less than 1 dB difference) results than the V072 code relatively to the experimental data. Moreover, the cases where the 3D model gives worse than expected results correspond to the conﬁgurations for which the measured powers are highly inﬂuenced by a complex acoustic duct propa- gation not accounted for in the model (Figs. 13(c), (d) and Figs. 15(a), (b)). This phenomenon is related to the variations of the duct section [42]. Therefore, following the assessment of the 3D cascade response in Section 3.3, a reasonable validation of the 3D model is provided. It is shown to accurately predict tonal noise created by the rotor–stator interaction, in the limits of the assumptions that mainly consist of an acoustic propagation within a constant section annular duct and of the absence of the acoustic transmission and reﬂection through the fan. The inﬂuence of the stator potential effect on the rotor wake, seen in Section 3.3, must also be added to the sources of discrepancies with the measured acoustic powers. Moreover, except for one stator conﬁguration, the trend with the vane count and the rotational speed is predicted by the 3D model. It should also be emphasised that this is the ﬁrst detailed comparison of this analytical model for high-speed rotor–stator interaction noise with detailed source and noise measurements.
Following the validation of the tonal noise prediction model with an actual fan conﬁguration, an axial compressor stage setup is now chosen in order to illustrate the range of applications of this model. The CME2 low pressure research com- pressor (Laboratoire de Mecanique de Lille – Snecma Moteurs, France) is composed of a 30-blade rotor upstream of a 40-vane stator [44]. It is mounted in a convergent annular duct with a hub-to-tip ratio of RH=RT ¼ 0:77 at the leading edge of the stator vanes and an outer casing of 0.55 m constant diameter (Fig. 17). The vanes have a uniform aspect ratio of C=ðRT - RHÞ ¼ 1:25 and are stacked without any sweep nor lean angles. Their stagger angle varies from 17.5° at the hub to 12.5° at the tip. The solidity at midspan of the stator being CV =2πRT ¼ 2:01, cascade effects on the noise sources are expected to be signiﬁcant, thus justifying the use of the present acoustic model. The gap between the rotor and the stator varies from 17 percent at the hub to 29 percent of the vane chord at the tip. The rotational speed is 6300 RPM at nominal conditions, implying a BPF of 3150 Hz. At this operating point, the mass ﬂow rate is 11 kg s-1 and the total pressure ratio of the stage is 1.14. The compressor operates in a fully subsonic regime with a 0.33 axial Mach number at the inlet.
A detailed unsteady compressible ﬂow simulation of this conﬁguration has recently been performed and validated in [59] with available experimental data from [44]. The Navier–Stokes solver Turb'Flow, developed at Ecole Centrale de Lyon in France, was chosen for this simulation as it is speciﬁcally designed for turbomachinery applications. This solver is based on a ﬁnite volume formulation and solves the conservative equations on a multiblock structured grid [61]. This URANS wall- resolved simulation has been performed on one tenth of the compressor, the computational domain being composed of three rotor blades and four stator vanes. The ﬂowﬁeld is transferred from the rotating grid to the ﬁxed grid through an interface using a high-order Fourier decomposition and recomposition with adequate angular phase in the azimuthal direction. The conservative ﬂow variables are spatially discretised with the second-order Jameson centered scheme. Kok's k–ω turbulence model [62] is used along with a limiter for the production of turbulent kinetic energy. The time marching is performed with an explicit single-time-stepping method based on a second order Runge–Kutta scheme using up to ﬁve steps. As an illustration of the ﬂow topology, Fig. 18 presents an instantaneous axial Mach number ﬁeld at midpsan of the compressor. The boundary layers on both sides of the rotor blades merge at the trailing edges to form the wake. The latter are convected with the mean ﬂow and interact with the stator vanes.
This unsteady ﬂow simulation has two main purposes in this aeroacoustic context. On the one hand, it provides the aerodynamic excitation w for the model. As shown in [59], the upwash from a rotor–stator simulation has to be extracted as close as possible to the vane leading edge (around 1 percent of chord upstream of the vane) during a complete cycle, and averaged in the rotor reference frame, in order to account for the convection by the ﬂow and the stator blockage effects on the wake. This excitation is represented in Fig. 19 over a rotor blade passage. On the other hand, the unsteady wall pressure ﬂuctuations, partly corresponding to the tonal acoustic sources, can be recorded on every gridpoint of the vane surfaces. These data have been used in a CAA method developed in [48] that has allowed to perform preliminary assessments of the present acoustic model.
As stated in the introduction, the acoustic model is particularly suited to perform parametric studies on complex geo- metries, contrary to a CAA approach. Thus it perfectly meets the requirements of a pre-design tool in an industrial context. In order to demonstrate these features, a parametric study is performed on the CME2 compressor. From the original con- ﬁguration introduced in Section 4.1, the sweep and lean angles of the stator vanes are progressively modiﬁed. The inﬂuence of these geometrical variations can then be directly surveyed on the aerodynamic excitation, the acoustic sources and the acoustic power prediction.
The effect of swept stator vanes in rotor–stator tonal noise generation has been known from early studies, e.g. by Adamczyk [63] and Hayden et al. [64], since this geometrical modiﬁcation may lead to noise reduction with respect to radially stacked vanes. More recent works [33,65–67] give some physical insights into the mechanism related to this noise reduction. The rotor–stator interaction is highly dependent of the orientation of the rotor wakes with respect to the vanes leading edges. An increase of the rotor–stator spacing enhances the bending of the wakes by the mean ﬂow that is swirling within this gap. In order to obtain a similar wake–vane interactions for UHBR engines with short nacelles, that present a reduced fan-OGV spacing, swept and leaned stator vanes have to be introduced [66]. Sweeping the vanes in the backward direction, i.e. with φ^o0, reduces the mean ﬂow velocity component along the chord Uxc and extends the distance between the upstream rotor and the stator. Consequently the aerodynamic excitation is damped on the additional convection distance up to the vane leading edge, thus creating a lower vane response. Furthermore the main effect of sweeping stator vanes consists of large phase variations of the excitation along the vane leading edge direction. Therefore the vane submitted to this excitation also presents a signiﬁcant phase variation along the direction zc. This phase evolution from strips to strips leads to a lower inte- grated response on the vane compared to a radial vane because of possible cancellation of the source terms, thus to a reduction of the emitted tonal noise. Coupling sweep and lean angles of a vane can enhance the noise reduction efﬁciency.
Envia and Nallasamy [68] propose some guidelines for designing quieter stators: imposing backward sweep angles and lean angles in the rotational direction. These design rules were shown to be efﬁcient on an actual fan stage [69].
The ﬁrst surveyed parameter corresponds to the sweep angle of the CME2 stator vane. According to [68] and Fig. 3, φ^ has to be negative to obtain a tonal noise reduction with respect to the original stator geometry. Although the model is able to handle variable sweep angle along the duct height, it is chosen to keep this angle uniform in the present study for each case. The sweep angle varies between 0° and 30° with a 5° step, i.e. a typical range for modern compressors and fans [70]. The aerodynamic excitation, extracted in the original conﬁguration and represented in Fig. 19, has to be extrapolated until the leading edge plane of the swept stator for each case. This is performed using a linear extrapolation from the simulated ﬂowﬁeld in the inter-row gap, for each value of φ^. For instance, Fig. 20 represents the upwash velocity extrapolated until the leading edge plane of the stator swept with φ^ ¼ - 201. By comparisons with the upwash extracted on a radial plane in Fig. 19, it can be seen that the main sweep effects on the excitation consist of a decrease of the upwash amplitude, a more pronounced bending of the wake and a larger wake width. These effects are more signiﬁcant going from hub to tip since the leading edge plane is located at Δx ¼ ðrd - RHÞ tan jφ^ j in the axial direction from the radial plane. This additional distance is signiﬁcant since it corresponds to Δx 0:3 C at the tip for φ^ 201. It is particularly interesting to note the variation of the rotor wake shape with the orientation of the stator leading edge plane since this directly inﬂuences the wake–stator interaction as shown later. The bending of the wake seen by the swept vane tends to increase the amount of wake inter- sections per vane. This design objective has to be followed to ensure noise reduction according to [68].
For a quantitative analysis of the upwash evolution with the sweep angle, Fig. 21 presents the Fourier coefﬁcients (Eq. (8)) of the excitation for 1 and 2 BPF along the dimensionless coordinate. Concerning the amplitudes of these coefﬁcients, Figs. 21(a) and (c) clearly show the expected decrease of the excitation with the increase of the sweep angle (in absolute value). In this conﬁguration, it is observed that the decrease of the amplitudes is quasi-linear with the variation of φ^. This decrease is signiﬁcant since the amplitudes are globally divided by a factor of two for a 15° variation of the sweep angle. Figs. 21(b) and (d) show the phase of these coefﬁcients along z*c. As expected, the phase variations are increasing when going from φ^ ¼ 01 to φ^ ¼ -301. Again this feature is more signiﬁcant in the upper part of the duct than in the lower part.
Following the sweep effects on the aerodynamic excitation, the cascade response of the model (Section 2.4) is surveyed relatively to variations of φ^. Fig. 22 presents the cascade response along z*c, at 15 percent of chord from the leading edge, in terms of amplitudes. For 1 BPF, the behaviour of the response is similar to the one of the upwash, i.e. a quasi-linear decrease in amplitude with the increase of jφ^ j. The model is thus able to provide quantitative trends of the cascade response with the parametric variation of the sweep angle. For 2 BPF, increasing jφ^ j also leads to a decrease of the cascade response amplitude, nevertheless following a different pattern. Indeed, for the original vane (φ^ ¼ 01), the cascade response exhibits a sharp pressure peak at z*c ¼ 0:87. As already encountered in Section 3.3, this feature is caused by an inter-vane channel mode becoming cut-on at this location, upon the impingement of the gust with jpj ¼ 1 [48]. This phenomenon is directly linked to the local geometry of the equivalent rectilinear cascade and to the value of the incident velocity Uxc. By varying φ^, both the geometry and the mean ﬂow are changed. Therefore the cut-off frequencies of inter-vane channel modes are different for each sweep angle value. In Fig. 22(b), the pressure peak is indeed shifted towards the duct mid-height from φ^ ¼ 01 to, indicating that the corresponding inter-vane channel mode becomes cut-on at lower radii when increasing φ^.
This feature is coupled to a global decrease of the cascade response with the variation of the sweep angle, thus leading to the oscillatory shapes of the model response observed in Fig. 22(b).
Fig. 23 represents the phase of the cascade response over the vane surface for speciﬁc values of the sweep angle at 1 and 2 BPF. For the original vane geometry, the phase of the loading is quite radially uniform at 1 BPF, and already presents some radial variations at 2 BPF. For these two harmonics, the phase variations signiﬁcantly increase in the z* direction for larger values of φ^. This feature is caused by the phase variations of the excitation highlighted in Figs. 21(b) ancd (d). As a result, the cascade response is progressively decorrelated in the radial direction. This is known to be the main effect expected with swept stator vanes that leads to possible cancellations when the response is integrated over the vane [68].
The parametric study on the vane sweep angle is ﬁnally performed in terms of radiated sound power level (SWL) within the duct. Fig. 24 shows the values predicted by the model for each harmonic of the BPF previously investigated in terms of excitation and cascade response. The excited and radiated acoustic duct modes have been summed for each harmonic. As expected from the previous analysis, the predicted acoustic power globally decreases with an increase of jφ^ j. Indeed, the combined effects of reducing the amplitude of the upwash and decorrelating the excitation lead to lower values of the emitted sound power. This study emphasises the efﬁciency of swept vanes to provide a tonal noise reduction, since a value of 5 dB reduction can be expected on the acoustic power at 1 BPF for a 10° increase of the sweep angle. As shown in Fig. 24 (b), the slope is higher at 2 BPF for which a reduction of 10 dB can be found with a 10° increase of jφ^ j. The efﬁciency of acoustic power reduction thus increases with the tone harmonics. Overall the predicted evolution of acoustic power with the vane sweep angle, as well as the amount of sound reduction obtained, are in good agreement with reported results from tests or models [65,68,70]. The sweep angle variation similarly affects the upstream and the downstream radiations, as shown by the slopes in Figs. 24(a) and (b). However a non-monotonic behaviour of the acoustic power is found for high values of jφ^ j, as already encountered in [68] for instance. For 1 BPF, the acoustic power radiated downstream monotonically varies with the sweep angle, whereas this evolution is non-monotonic upstream. Thus the upstream propagation is more sensitive to the axial location of the sources than the downstream propagation, a feature already mentioned in [65]. Note that this parametric study has been performed in using an extrapolation to deﬁne a realistic rotor wake in function of the sweep angle. Within the limits of the error associated with this extrapolation, imposing the constraints of minimising the sound emission for each tone in each direction would lead to the optimal design of a stator composed of vanes with a 20° sweep angle.
Following the above parametric study on vane sweep angle only, the objective now consists of coupling lean and sweep angles. Indeed a proper combination of these design parameters may lead to an even more acoustically efﬁcient rotor–stator stage. According to Envia and Nallasamy [68], the lean angle ψ^ must be oriented in the rotational direction to provide a noise reduction, i.e. with a negative value here (see Fig. 4). Indeed with this conﬁguration, the amount of wake intersections per vane is increased. Due to this negative lean, the vane leading edge will see the rotor wake with a time delay. In other words, a phase shift is introduced in the upwash excitation to account for this delay, such as the upwash w extracted over an inclined plane becomes:
where rdψ^ represents the tangential distance at radius rd caused by the lean angle with respect to a straight vane [68].
Therefore the phase variation of the excitation is enhanced, and so should be the phase of the vane response. It can be thus expected to obtain lower values for the integrated vane loadings due to cancellations of the response caused by this highly varying phase response. In order to evaluate this effect on the CME2 compressor conﬁguration, the lean angle of the vanes varies between 0° and - 20°, for each of the swept stator conﬁguration already surveyed. For the sake of brevity, the upstream and downstream radiated acoustic powers are directly presented in Fig. 25.
Fig. 25 shows that, for each value of the lean angle, the evolution of the sound power with the sweep angle corresponds to a global decrease. For each case, the slopes of the curves are similar between each other and do not seem to depend on a speciﬁc lean value. At 2 BPF, Figs. 25(c) and (d) clearly show the expected inﬂuence of the lean angle. Indeed, for most of the sweep angle values, an increase of jψ^ j leads to a tonal noise reduction. This reduction is not linear, contrary to the sweep effect, and is of the order of 7 dB at 2BPF, for a 0–20° lean variation. Leaned vane alone is thus not a noise reduction mechanism as efﬁcient as swept vane, but the combination of both can lead to drastic reductions, e.g. 25–30 dB at 2 BPF, conﬁrming the results of previous studies [68]. A different trend is observed for the power predictions at 1 BPF in Figs. 25 (a) and (b). At this frequency, increasing the value of the lean angle does not necessarily lead to a power reduction, and may even increase the level of the acoustic emission. It means that the phase shifts brought by the swept and leaned geometry do not lead to the expected cancellations in terms of integrated vane loading, but rather to an increase of the sources. Indeed, similarly to sweep effect, the lean effect efﬁciency increases with the tone harmonics [68]. Therefore Figs. 25(a) and (a) show that the blade passing frequency of 3150 Hz seems too low for a positive power reduction in this conﬁguration. The efﬁcient stator design found in the sweep study, i.e. with φ^ ¼ -201, again appears to represent the optimal value for φ^ with variable lean angles. Indeed the conﬁguration with φ^ ¼ -201 and ψ^ ¼ -201 corresponds to the most efﬁcient design among the tested cases. At 1 BPF, this results in an almost equivalent design with respect to the swept stator only, with a slight increase of 2 dB upstream and an interesting reduction of 4 dB downstream. It is thus shown that, on this compressor, a parametric study focused on vane sweep only is able to provide the optimal value for φ^ of a combined swept and leaned vane design. Moreover the surveyed noise reduction mechanism highly depends on the harmonic since its efﬁciency increases with the frequency. Finally a clear trend in terms of noise reduction is found with the sweep angle whereas this trend appears more complex with the lean angle.
In this paper, a detailed description of an analytical model for tonal fan noise prediction has been performed. The physical mechanism of interest consists of the tonal noise generated by the rotor–stator interaction. Indeed it corresponds to one of the main noise contributions in current turbofan engines and its inﬂuence is expected to grow in future Ultra-High Bypass Ratio turbofan engines. The presented model has been designed to precisely account for complex stator vanes geometries, with variable stagger, sweep and lean angles along the duct height, as tonal noise is sensitive to these para- meters. A procedure has been proposed to use the rotor wake as the input excitation for the model. Namely, it is ﬁrst recommended to extract the upwash, from measurements or ﬂow simulations, in a surface matching the radial evolution of the vane leading edges, and second to decompose this excitation into three-dimensional skewed gusts. The acoustic sources are provided by a cascade response function used within a radial strip theory, so that the radial correlation characteristics of both the excitation and the response, typical for tonal noise, are considered. Moreover cascade effects occurring on noise generation and propagation for blade rows with overlapping elements are fully accounted for. Finally the sources are used as acoustic dipoles in an acoustic analogy tailored to an annular duct containing a uniform mean ﬂow. This ensures the radiated acoustic energy to be distributed on the actual cut-on and excited duct modes. In view of existing cascade models, the present model has been generalised to account for both rotational directions.
The proposed model has then been validated on an actual fan stage conﬁguration. It should be mentioned that this is the ﬁrst detailed comparison of this model with noise sources and far-ﬁeld measurements. The Advanced Noise Control Fan (ANCF), extensively tested at the NASA Glenn research center, is particularly suited for these comparisons. Indeed it is a low hub-to-tip ratio fan-OGV stage, with instrumented vanes (microphones) and duct (rotating rakes), installed in an anechoic environment. The data are available for several stator vane counts and several rotational velocities. Using an experimental dataset for the rotor wake, the cascade response of the model has ﬁrst been compared with unsteady pressure measure- ments on the instrumented vane surface. It has been found that the 3D version of the model provides a more accurate cascade response than the 2D model. These comparisons have led to a reasonable validation of the 3D response function, noting that two sources of discrepancies have been explained. On the one hand, a ﬂow separation in the upper part of the vane suction side enhances wall pressure ﬂuctuations, that cannot be predicted by the inviscid ﬂow model. On the other hand, the measured rotor wake used by the model does not account for the stator potential effect, whereas it inﬂuences the experimental vane response. The measured acoustic duct modes in the inlet and exhaust sections of the duct have then been compared with the results of the model in terms of acoustic power. Parametric studies with rotational velocities and vane counts have been performed, as well as comparisons with results from the V072 cascade model. In half of the studied cases, the proposed model has been shown to predict the radiated acoustic power with equivalent or more accuracy than the V072 model. For some other cases, the measurements have been highly inﬂuenced by a complex acoustic propagation within the duct, not accounted for in any of the models. The capability of the model to predict trends with vane counts and rotational velocities has been demonstrated on this conﬁguration.
This fan noise prediction model has been developed for research and industrial purposes. The model is thus expected to be included in an optimisation procedure aiming at designing quieter fan stages or more efﬁcient acoustic treatments. Thus it has to cope with parametric studies focused on the stator geometry. This feature has been demonstrated on the CME2 compressor stage. The excitation and mean ﬂow properties have been extracted from an existing detailed unsteady simulation of this rotor–stator conﬁguration to be input within the model. The parametric study has ﬁrst focused on the sweep angle of the vanes as results from the literature show this is an efﬁcient tonal noise reduction mean. Indeed swept vanes induce radial phase shifts in the excitation thus in the vane response, leading to lower integrated vane loadings. The model has correctly reproduced this behaviour, showing the expected trend in terms of vane loading evolution with the sweep angle (amplitude and phase). The model has also allowed predicting the acoustic power emitted by the stage in function of the vane sweep angle. An optimal design has been found with a - 20° sweep angle, yielding to a 10 dB reduction at 1 BPF and more than 15 dB reduction at 2 BPF with respect to a radially stacked stator. These values are within the range of what can be expected with this design. A two-parameter study has ﬁnally been performed concerning the vane sweep and lean angles. A proper orientation of the latter is indeed expected to enhance the noise reduction provided by the swept stator alone. This behaviour has been correctly reproduced by the model, showing that this noise reduction mechanism is highly dependent on the tone harmonics. A stator composed of vanes with a -20° sweep angle and a - 20° lean angle has been determined as the optimal acoustic design, bringing approximately an additional reduction of 7 dB with respect to the optimal swept design at 2 BPF.
Finally the proposed fan tonal noise model has been validated against experimental data and its pre-design tool char- acteristics have been demonstrated. Future research aiming at improving the accuracy of the prediction will mainly focus on the acoustic analogy used to radiate the acoustic sources.
The authors wish to acknowledge Dr. Daniel L. Sutliff, from NASA Glenn Research Center, for having provided us with the geometry and the tests data of the Advanced Noise Control Fan (ANCF), and for his fruitful discussions, that have allowed to evaluate the fan tonal noise prediction model on a representative fan test case.
The result of the analytical derivation of the vane loading ΔP 0 xc, corresponding to the source term used in the tonal noise model presented in this paper, is recalled here. From the velocity potential scattered outside of the vane cascade, provided by Glegg [33], in response to the impinging gust:
Posson et al. [34] extended this work to the calculation of the potential inside the inter-vane channels as well as to the pressure jump across the vanes. For a proper use of the Wiener–Hopf resolution method [34], the cascade must be split into the three segments I, II (or II0 ) and III, represented in Fig. A1. Segment II is considered when d o C=2 (Fig. A1(a)) and segment II0 when d 4 C=2 (Fig. A1(b)). Segments I and III represent diffraction problems at the leading edge and at the trailing edge respectively, linking the inter-blade channels mode existing in II (or II0 ) with the radiated cascade modes (these modes are discussed in Section 3.3). The vane loading ΔPb0ðxcÞ reads, according to the considered segment:
All the new terms appearing in the expressions of the vane pressure jumps are deﬁned below. ρ0 is the mean ﬂow density. The variables introduced by Glegg [33] are:
It must be mentioned that the wavenumbers of the excitation and of the cascade response in the zc direction are the same (kzc ¼ k0zc) since the rectilinear ﬂat plate cascade has an inﬁnite span (Section 2.4). The function J þ and J - are deﬁned as:
The Fourier transform of the velocity potential across a vane is decomposed into the four terms:
Air-Blast Cross-Flow Atomization of Cold Fluids
Abstract 
The motivation for 
this work is to gain a better understanding of how particle characteristic can be controlled when metallurgi-cal slags are atomized in cross-flow air atomization. The effect of certain process parameters on the characteristics of liquid droplets produced by air-blast atomization has been investigated using water, castor oil, and glycerin. The gas-to-liquid ratio as well as the distance between the liquid and air stream were manipulated to understand their effect on the particle shape and size. The effect of liquid viscosity on the size and shape of droplets was also investigated. Droplets were observed in-flight with the use of shadow imaging, which were analyzed to provide information regarding the mentioned characteristics. It was found that a single correlation expressed as kinetic energy of the atomizing air per unit mass of the atomized liquid can explain the effects of gas/liquid ratio and distance between the air nozzle and crash point. The average particle size decreased with an increase in the gas-to-liquid flow rate ratio, increased with increasing distance between the liquid and air stream, and increased with increasing viscosity. The aspect ratio was consistent and close to unity for all water atomization experiments. 
Keywords Metallurgical slag.· Dry granulation.· Break-up mechanism.· Atomization 

Introduction 
Slag has traditionally been regarded as a waste byproduct from smelting operations and presents notable environmen-tal, operational, and financial significance for the metallur-gical industry. Typical slag generation rates range from 0.3 tonnes of slag per tonne of metal for blast furnace ironmak-ing to 14 tonnes of slag per tonne of metal for ferronickel production [1]. Global annual slag production is estimated at 680 million tonnes, with ferrous slags accounting for more than 90% of annual slag production at 610 million tonnes. 
Conventional slag handling operations include air cooling, wet, and dry granulation. Air cooling begins with the slag being dumped into a large open area and left to cool, where in some cases it is subsequently sent to downstream crushers for further processing. Operational challenges associated with air cooling include potential emission of toxic fumes and/or possibilities of dust gen-eration [2]. Excavation from the pit can be facilitated by water spraying, which causes the slag to crystallize and crack; this requires specific safety precautions be taken to mitigate the increased risk of explosions. 
Water granulation is carried out when molten slag is broken apart and quenched with a high-pressure water stream. The process has operational and environmental shortfalls, the most significant being the risk of steam explosions, the need for dewatering, and the energy costs associated with pumping and cooling large amounts of water [3]; water consumption can exceed ten times the mass of slag being atomized. The technology is also unat-tractive in certain areas due to water scarcity. 
Dry granulation is the process of producing granules in the absence of water. Common methods of dry granulation include centrifugal granulation and air granulation. Cen-trifugal granulation utilizes a high-speed rotating cup or disk. Molten slag is poured into the center of the container, spreading on the surface forming a thin film that eventu-ally breaks into slag granules. The process has been used and studied extensively; however, it has been shown to have a low production rate, high equipment and processing costs, and produces large particles [4]. The metallurgi-cal industry has therefore been seeking alternative slag handling methods in the past five decades that address the challenges of conventional slag handling operations, namely, the ability to produce valuable slag products and the ability to recover energy from slag [5]. Air granulation has been studied since the 1980s and has demonstrated success in the production of high value slag products [6]. 
Air granulation, also referred to as air-blast atomiza-tion, utilizes high-velocity air to break apart and quench a stream of molten material. The process is a subset of air atomization which has been adapted for high-viscosity melts; an industrial setup installed by Hatch Ltd. is shown in Fig..1. The process is capable of producing particles ranging from 0.5 to 5.mm [3]. The differences between air granulation and air atomization predominantly come from the geometry, relative placement, and positions of the melt and gas streams, as shown in Fig..2. Air-blast granulation is attracting attention for treatment of different types of slag due to the fact that it is more environmentally friendly, and having lower footprint and processing costs. 
It is proposed that production of powders through air atomization occurs in three distinct stages: wave growth, ligament formation, and finally the development of drop-lets [4]. As expected, the melt properties and operating parameters affect the final shape and size of the particles. Material properties include viscosity, density, and surface tension; operating conditions include gas to melt mass flow rate ratio, the degree of superheating, distance between the liquid and air stream, nozzle geometry, thickness of the liquid stream, and angle of impact between liquid and air. 
The motivation for this research is to correlate shape and size of the particles to these parameters for various types of fluids and operating conditions. Cold fluid modeling with different single-phase liquids is a useful primary tool in establishing the basic correlations of air granulation. This paper presents the experimental results of the effect of the air-to-liquid ratio, the distance between the liquid and air stream, and the liquid viscosity on the shape and size of particles produced from the granulation of water, castor oil, and glycerin, which offer a range of material properties that encompass the viscosity and density of industrial slag. This research is part of an ongoing study whose goal is to fill the knowledge gap related to cross-flow air slag granulation and gain a better understanding of how the characteristics of slag granules can be controlled. 
For a fixed nozzle and atomization system configuration, the dominant factor controlling particle size in air atom-ization is the gas-to-liquid (G/L) flow rate ratio [7]. The dynamic viscosity of the liquid being atomized also influ-ences the final size of the particles. A study by Goudar et.al. has shown that viscosity is the dominant material property influencing particle size [7]. The easiest way of altering the melt viscosity is by increasing its superheat, as shown by Small et.al. [8]. 

Experimental Procedure 
Design of.Atomization Chamber 
The experimental setup used in the present study is shown in Fig..3. The air nozzle measured 10.cm × 0.5.cm and was located below the stainless-steel container. The liq-uid stream was produced by allowing liquid to flow at room temperature under gravity through stainless-steel 
1 3 
containers which produced a flowing liquid sheet with a width of either 4 or 8.cm. The air exiting the nozzle impacted the liquid sheet wherein the fluid was broken apart and dispersed as liquid particles into the granulation chamber. Liquid droplets were collected inside the cham-ber and drained. The granulation process was observed and photographed from the side using shadow imaging. A combination of a high-speed camera and a strobe light was used to freeze the motion of the particles for imaging and subsequent droplet shape and size analysis. 

Liquid Stream Properties 
Liquid flow rates were computed using the respective den-sities and measuring the time it took to fill a 2-L beaker. The liquid flow rates and viscosities of the single-phase liquids used in the present study are shown in Table.1. 

Air Stream Properties 
The axial gas velocity at the exit of the nozzle was meas-ured using an anemometer probe. Knowing the dimensions of the nozzle allows for the computation of the gaseous mass flow rate, which are shown in Table.1. 

Distance Between Air Stream and.Liquid Stream 
The length of the air stream represents the distance between the air nozzle and the point where granulation occurs. The setup, resulting in an air stream length of 35 cm, is shown in Fig. 4. The different lengths investi-gated in the present study are shown in Table.1. 

Characterization of.Atomized Liquids 
Air atomization typically produces wide particle size distri-butions. The particle size distribution was characterized by identifying the mean particle diameter (d50, i.e., the diameter at which 50% of a samples mass is comprised of smaller particles) and the Sauter mean diameter (d32, i.e., the diam-eter whose ratio of volume to surface area is the same as 
Fig. 5 Time required to achieve constant mean particle size values during the granulation of cold fluids 
that of the entire droplet sample). For the purpose of these calculations, particles were assumed to be perfect spheres. The actual shape of particles is characterized by identifying the aspect ratio. 
Image analysis was performed using ImageJ software [9], which was used to examine the shape and size of par-ticles produced. The scale was set by using a piece of cop-per wire of known dimension (1.02.mm), giving a scale of 69 pixels/mm. Each experiment was repeated a mini-mum of two times and ran for a minimum duration of 15.s, which was identified as the time required to reach consist-ent d50 values, as shown in Fig..5. The maximum relative difference between the particle sizes in repeated experi-ments was observed to be 7%. An example of images used for further analyses is shown in Fig..6a. When performing the image analysis, special care was taken to remove any particles that were not in focus. An example of the same image post-processing is shown in Fig..6b. 


Results and.Discussion 
Theoretical Treatment 
Atomization occurs as a result of the kinetic energy trans-fer between the gas and liquid stream. The kinetic energy is dissipated to overcome the viscous forces that resist deformation and to overcome the surface energy forces that resist the formation of new surfaces. The ratio of energy required to overcome viscous and surface tension forces, can be expressed in terms of liquid phase Weber and Reynolds numbers, Eq.(1).
 As the gas continues to travel after atomization, at a reduced velocity and pressure, and also the fact that gas jet is wider than the liquid sheet, it is reasonable to assume that only a certain fraction of the gas kinetic energy at the interaction point is used to overcome the surface energy forces. This fraction is denoted with the variable β in the kinetic power equation, Eq.(2). 
This energy is fully consumed for break-up of the stream to a certain number of droplets. The surface energy of this particle cloud per unit time can be presented as Eq.(3). 
For a given mass flow rate of liquid, Eq.(3) can be rewrit-ten as Eq.(4). 
To satisfy the surface energy requirements for the production of new particles, the kinetic power has to be equal to that of the surface energy created in unit time, Eq.(5). 
A schematic of the expansion of the air sheet over the length of the air stream (L) is shown in Fig. 7. The area at the point of atomization is related to the original area by Eq.(6). It should be noted that the thickness of the air sheet is assumed to be much smaller than its width and it is also assumed that there is no pressure change over the short distance. The constant K′ is a relationship between the angle of expansion and the original thickness of the air sheet, as shown in Eq.(7). The average particle size in terms of initial operating conditions and material proper-ties is shown in Eq.(8). 

Experimental Results 
Table.1 shows the details of the experiments conducted during this study. In total, 15 different experiments were conducted at room temperature to investigate to effect of the gas-to-liquid ratio, distance between the air nozzle and crash point and the viscosity on the particle size. The mean and the Sauter mean diameter for each experiment are given in Table 1. 

Effect of.G/L Ratio on.Particle d32 and.Aspect Ratio 
Figure.8 shows the particles size distribution of water. The mass fraction graph indicates that the majority of particles present are between 400.μm and 800.μm. Small sizes < 150 μm as well as large sizes greater than 850.μm show lower mass fractions relative to other ranges. The double-hump distribution was consistent with all the other water modeling experiments conducted in this study. This is believed to be a result of two atomization modes, the pri-mary one with wave propagation and stream break-up and a secondary mechanism in which larger drops are broken into finer drops through an attrition-like mechanism [10, 11]. Only a portion of particles will undergo secondary atomiza-tion, causing the double-hump distribution. 
Typically, the droplet size is presented versus mass flow rate ratio of gas to liquid (G/L), as shown in Fig..9. It can be observed from the results that an increase in G/L flow rate ratio results in a decrease in the particle size. Atomization occurs as a result of kinetic energy transfer from the atom-izing medium to the liquid, larger G/L ratios have a greater kinetic energy available for sheet disintegration, resulting in smaller overall particles [12]. 
According to Eq..(8), when the pressure of the air stream is relatively constant, the particle size should follow a linear 
oo
relationship with m 3 ∕m . Figure.10 tests the correlation, 
gl 
showing that a linear trend is indeed followed. 
Figure.11 shows the influence of the gas-to-liquid flow rate ratio on the particle aspect ratio. The aspect ratio describes the relationship between the width and the height of a droplet. The particle aspect ratio is dependent on the difference between the solidification time and the spheroidi-zation time. In the present study, single-phase liquids do not undergo any solidification when atomized resulting in the droplets having ample time to spheroidize. The expected outcome was for low-viscosity liquids to spheroidize very rapidly, while liquids with a higher viscosity taking longer to spheroidize. Increasing the gas flow rate shortens the time required for droplets to spheroidize by generating smaller droplets, as shown in Fig..11. It is important to note that for high-viscosity liquids the positioning of the camera is very important and will affect the recorded aspect ratio, it is expected that after sufficient flight time the particles will spheroidize completely. 

Effect of.Distance Between Nozzle and.Liquid Stream 
Figure.12 shows the effect of the distance between air nozzle and the disintegration point. As shown, increasing the length of the air stream results in an increase in the particle size. This increase in particle size can be attributed to the loss of disintegration energy as the air has to travel a longer dis-tance to reach the liquid stream, and also the reduction of air velocity due to expansion while traveling towards the liquid stream. This can be confirmed by looking at Eq.(2), where a reduction of velocity would lead to less energy being avail-able for granulation. As the length continues to increase, the mean particle size will increase rapidly until atomization eventually stops as the minimum kinetic energy require-ments would not be met to cause any sheet disintegration. 
In the present study, the particle size was related to the length of the air stream (L), by fitting the geometric parameter K′, this was then used to calculate the particle size. Figure.13 compares the calculated and measured d32 values with change in length of the air stream. The calcu-lated d32 values are obtained by optimizing the thickness From the value of K′, the expansion angle of the air jet was calculated to be 0.2o. This implies very small expansion of the air stream, which is expected as the longest distance examined was 35.cm and the outlet air velocity was very high. It was also evident during the experiments that the disintegration takes place at approximately the same posi-tion of the falling liquid stream, showing little expansion of the air jet. 

Effect of.Fluid Properties 
Figure 14 shows the viscosity versus the particle size for liquids with different viscosities at constant G/L flow rate ratios. Although the number of data points is not large enough to draw a quantitative correlation, it can be seen that as the viscosity increases the average particle size also increases, in what appears to be a linear relationship. This agrees with the works of Goudar et.al. and Mandato et.al. [7, 13]. The Sauter mean diameter for glycerin, castor oil, and water are 1280, 1200, and 510.μm, respectively. An increase in viscosity hinders disintegration leading to larger energy requirements to achieve granulation, hence the reduced particle size. This may be mathematically explained using Eq..(1) where the ratio of viscous to surface tension energies increases linearly with the liquid viscosity. The implication is that the fraction of energy consumed for disintegration which is β in Eq..(8), decreases with viscosity. This is con-sistent with the observed trend in Fig..14, and subject of an ongoing study to include the effects of both viscous and surface energies into a unified correlation that would quan-titatively explain the results of all experimental conditions. 
Figure.15 shows the particle size distribution for glycerin; a similar distribution was observed for castor oil in Fig..16. It can be seen that as the viscosity increases, the mass frac-tion tends towards a single-hump distribution, compared to the double-hump distribution for water (Fig..8). This can be attributed to higher-viscosity liquids resisting secondary drop deformation. It is known that high-viscosity liquids exhibit a greater resistance to secondary atomization [11, 14]. The tendency is quantified using Ohnesorge number which presents the viscous forces to the inertial and surface tension forces and linearly proportional to the viscosity. 
1. Particle size can be controlled by manipulating the G/L flow rate ratio, the length of air streams, and the viscos-ity of liquids. 

2. Larger gas flow rate ratios produce smaller particles due to greater energy available per unit area for sheet disin-tegration. The particle size was related to the mass flow rate ratio of gas and liquid as mg3/ml mass. 

3. An increase in liquid viscosity produces a larger average particle size. 

4. An increase in the distance between the air nozzle and the disintegration point produces a larger average parti-cle size. 

5. Integration of viscous and surface energies involved in liquid atomization appears to be capable of producing a quantitative correlation for prediction of particle size as function of liquid properties and operating conditions. 





















Combined effervescent and airblast atomization of a liquid jet
Effervescent atomization shows great promise towards the production of small droplet sizes, but it can suffer from substantial instabilities. Adding a coaxial shear .ow to a central two-phase bubbly .ow is a simple extension of effervescent atomization, however, the characteristics of a combined air-blasting shear .ow and effervescent mode of fragmentation have not been well described in the literature. By making use of LDA/PDA measurements, high speed microscopic imaging of the atomization zone, and advanced image processing techniques, quantities such as the axial velocity .uctuations, pulsation frequencies, ligament sizes and liquid area fractions are measured and analysed with respect to the relative mass of effervescent air and air-blast air. The work shows that the coaxial air blast .ow does not change the frequency of the effervescent core pulsations but can act to dampen .uctuations whilst simultaneously improving dispersion characteristics. For this hybrid atomization mechanism, the measured axial velocity .uctuations are now a combined result of the instability of the effervescent spray core as well as mixing from the surrounding air .ow. Analysis suggests that frequencies associated with the effervescent atomization process can occur on similar scales as the surrounding mixing frequencies. Furthermore, sinusoidal instabilities from the coaxial air .ow are seen as superimposed onto the effervescent core indicating that a complex coupling can occur between the two modes of atomization.  2016 Elsevier Inc. All rights reserved. 
1. Introduction 
Pressure as well as air assisted atomization techniques are rel-evant to a wide variety of industrial problems, perhaps the most common one being in the combustion of liquid fuels. Airblast atomization, where a coaxially .owing air stream atomizes a cen-tral liquid jet, is widely investigated, with a large volume of litera-ture having examined the in.uence of key dimensionless groups on the atomization process [1–8]. It is now widely accepted that the overall Weber number of the .ow, the mass .ux of liquid with respect to the air .ow as well as the liquid jet Reynolds and Ohne-sorge numbers are relevant to the classi.cation of the fragmenta-tion mechanisms [2]. In general, the atomization regimes can be delineated in a similar way as with air assisted droplet atomization 
[5] 
given that similar phenomena have been observed. More recent work has concentrated on the in.uence of gas phase turbulence on atomization [9] 
re-con.rming the in.uence of the Rayleigh–Taylor instability in these types of .ows, whilst also extending previous work to account for a dimensionless measure of turbulent .uctua-tions. For more details on air-blast atomization the reader is direc-ted to the extensive literature on the subject as reviewed by Lasheras and Hop.ngerg . 
Effervescent atomization, a technique developed by Lefebvre and Wang [10] 
has not yet reached the level of maturity as its air-blast counterpart even though the mechanisms which lead to frag-mentation in this type of atomizer are now broadly understood as reviewed in Sovani et al. [11]. The ratio of effervescent air to liquid mass .ow-rate (the gas to liquid ratio or GLR) is a critical parame-ter that dictates the nature of the bubbly .ow inside the nozzle. The atomization mechanism is largely controlled by the size of bubbles as well as the void fraction in the nozzle (creating bubbly, slug or annular .ow) [11]. The geometry of the atomizer, such as the size and number of aeration holes, ori.ce diameter and GLR all dictate the nature of the two phase .ow and the subsequent droplet size [12–17]. Bubble nucleation is also an important issue in effervescent atomization and this has been studied in a two dimensional con.guration by Lhuissier and Villermaux [18]. Effer-vescent atomization has also been utilized in combustion, where rather than aeration using air, a .ammable gas is injected into the liquid serving both as the atomizing gas as well as part of the combustible mixture [19]. Gadgil and Raghunandan [20] 
examined instability characteristics in greater detail including quantifying the bursting distance which leads to atomization, while other work by Schroder et al. [21] 
has analysed instabilities in very viscous liq-uids. Computations of effervescent atomization, while scarce, are gradually evolving, and Qian et al. [22] 
have reproduced some aspects of the experimental data, but more development is still needed. 
It is clear from the literature that effervescent atomization occurs only at some liquid jet diameters downstream of the nozzle [11], and this has generally been attributed to the expansion of air as the two phase mixture leaves the ori.ce channel encountering a sudden pressure drop. However, more recent work by Shepard [23] 
suggests that longitudinal instabilities in the .ow direction dictate the atomization process. He argued that atomizing packets resem-ble colliding portions of liquid as opposed to bursting bubbles. This conclusion was a result of observing radially expelled as opposed to spherically expelled liquid. The latter is more representative of a bubble explosion while a longitudinal collision dominates the former, as in Meier et al. [24]. 
In many practical applications where effervescent atomization is likely to be used, it is possible that it is employed in tandem with a coaxially .owing air stream. This hybrid mode would be intu-itively expected to affect the mixing characteristics, droplet size as well as general atomization phenomena. To the authors’ knowl-edge, there has been no systematic investigation which considers the combined impact of an effervescent component driving atomization due to aeration in the liquid, and a coaxially .owing air component which drives atomization due to interfacial instabil-ities. The literature has investigated situations where an air .ow exists both within and external to a liquid annulus [25], however this is different from effervescent atomization where the air is injected directly into the liquid. Additionally, effervescent atomiza-tion is known to be unstable, and though the mechanism of the instability formation requires further investigation, it is known that the instabilities are related to the void fraction in the nozzle [15,21,20,26,27]. Given the pulsations that occur in effervescent atomization, which are nevertheless accompanied by greatly reduced droplet sizes, it is possible that with the addition of an atomizing air-blast component, the pulsating characteristics can be modi.ed. 
In this contribution we present a system which allows for a hybrid atomization mechanism to take place near to the liquid injection nozzle. Making use of laser/phase Doppler anemometry, microscopic high speed backlight imaging as well as advanced image processing techniques, we present and analyse mean .ow and mixing characteristics for the case of effervescent and combined effervescent–airblast atomization. Spectral analysis is performed both on the LDA data as well as on the high speed image sequences in order to examine the in.uence of the airblast compo-nent on the effervescently induced instabilities as a function of blasting air, as well as spatial location in the spray. 
The paper will .rst present the atomizer, and subsequently outlines the experimental techniques used with associated errors. Secondly, we will present key results characterizing this hybrid fragmentation process in terms of morphology. Then, we present and discuss the .ow-.eld and mixing characteristics before ana-lyzing the dominant instabilities and other quantitative features of the near-.eld atomization zone. 
2. Experimental methods 
The atomizer used for these experiments is shown in Fig. 1.It has an overall length of approximately 130 mm and consists of an ‘outside-in’ effervescent component as originally presented by these authors in Kourmatzis et al. [28], but now with a high speed air stream which .ows coaxially to the central two phase liquid core, consisting of standard tap water and air. The effervescent design consists of 16 aeration holes each of a diameter of 
0.75 mm, equally spaced apart by 1 mm and perpendicular to the direction of liquid .ow. For these experiments, the inner diameter of the effervescent nozzle steps down from 2 mm to a .nal ori.ce of diameter Dl = 500 lm. The nozzle, shown in zone A of Fig. 
1 is kept .ush with the blasting air which .ows through a tube of diameter, D = 12 mm. The spray is ejected downwards into atmo-spheric conditions where a variety of cases were investigated as speci.ed in Table 
1. The exit Weber number, de.ned for air blast cases only is given by Weexit . qU2Dl=r where U is the air blast mean velocity, r is the surface tension of water and q the density of air. The GLR values presented suggest that all of the conditions tested here are either in a homogeneous bubbly .ow or slug .ow break-up regime with no case exhibiting an annular break-up .ow, which would typically require a GLR > 5% [11]. Observations of multiple small bubbles within the liquid jet would agree with this assessment. Given the very low air-.ow rates necessary for effer-vescent atomization, a narrow range Bronkhorst (EL-FLOW MFC PN64) gas mass .ow controller was used providing a maximum output of 10 ln/min with ‘ln’ denoting normal litres per minute, with a standard accuracy of 0.5%. The liquid was supplied at room temperature through a FP1/8-20-G-5/81 Tri-.at rotameter with an accuracy of at least 5% over the measurement range. The injected pressure of the air was measured just upstream of the ejection noz-zle and ranged from 15 to 180 kPa, as measured using a Wika Swiss movement analogue pressure gauge with a maximum range of 250 kPa. 

2.1. LDA system 
A commercial laser Doppler anemometry system (TSI Model FSA 3500/4000) has been used for characterization of particle velocity where full details on errors and uncertainties have been provided in Gounder et al. [29]. In these experiments, the receiver is positioned in a 45 forward scattering con.guration where the assembly transmits two pairs of beams with wavelengths 
514.5 nm and 488 nm to measure the axial and radial components of velocity respectively, with typically 10,000 samples in each channel being collected per run. A Bragg cell is utilized to shift one beam from each pair by 40 MHz to allow measurement of velocity in the negative direction. Built-in probe volume correction (PVC) has been implemented to correct for lower detectability of small particles towards the measurement volume edge due to the Gaussian light intensity pro.le. Throughout this contribution, we sub-range velocity statistics and present data which has been sampled from droplet sizes in the range 0 < d < 10 lm. As the PDA instrument cannot measure non-spherical droplets of which a substantial number are present in effervescent atomization, we sub-range over a narrow range of spherical droplets in order to remove bias amongst different spray cases. Additionally, this acts as a close representation of the gas phase velocity so that the mean and rms .ow velocity mentioned throughout the paper is obtained from these LDA measurements, sub-ranged from the droplets in the range 0 < d < 10 lm. Furthermore, given the large velocity gradients which can be present in a hybrid effervescent–airblast atomizer, all LDA data is normalized for velocity bias in real-time. 
Where an FFT is conducted on the LDA data, there is a restric-tion on the maximum frequency measurable which is dictated by the frequency of particle detection through the LDA probe volume. Therefore, at outer radial locations, where generally lower particle concentrations are present, maximum frequencies measurable in some instances could not exceed 600 Hz without introducing aliasing. 

2.2. Imaging system 
A high speed diode laser operated at 532 nm and 10 kHz was employed as the high speed light source (Edgewave INNOSLAB model HD3011E) for the microscopic imaging studies and is placed in a backlit arrangement from the camera. The average laser power used in these experiments was 20 W resulting in 2 mJ/pulse. Two opal glass diffusing optical components were used to diffuse the light before crossing the measurement volume. A High Speed Star 6 LaVISION camera, synchronized to the laser at 10 kHz was used with a Questar QM100 long working distance microscope objec-tive. In these experiments, the spatial resolution of the system was approximately 3.7 lm per pixel with a total imaging window size of 768 pixels square. For each downstream location and for each case investigated, 3000 images were collected. Instability res-olution is a key aspect of the work presented here, and as will be shown in later sections of this paper, most atomization modes were in the range of 400 < f < 600 Hz. Hence, with an image sam-pling rate of 10,000 Hz and 3000 image samples in total, 150 images are available to resolve the 500 Hz frequency. The image processing script and methodology employed has been fully docu-mented in Kourmatzis et al. [30] 
along with a detailed account of uncertainties. Brie.y, the image processing technique accounts for defocused objects through a calibration against monodisperse droplet streams, while it also statistically accounts for droplet merging through an appropriate choice of binarization threshold. 
The technique itself has been validated against typical air-blast sprays through comparison with PDA measurements, showing errors in SMD that do not exceed 12% [30] 
with errors in monodis-perse size estimation not exceeding 10% on average. 
For global scale imaging of the spray down to a .eld of view of approximately 50 mm, a Nikon-SLR camera was used with a mea-sured .ash pulse width of 15 ms, providing line integrated Mie Scattering images. A black background was placed behind the mea-surement volume and all images shown have been colour inverted for ease of observation. 
3. Atomization morphology 
Fig. 2 shows some typical instantaneous global scale images of the atomization process over a total distance of 50 mm which encompasses both the initial primary atomization zone as well as regions where entrainment and dispersion take place. This is con-sistent with other work by Kourmatzis and Masri [9] 
which has shown that the primary atomization region is generally completed within a few liquid jet diameters over this Weber number range. From a qualitative point of view it is clear that going from columns 

Fig.2.Aselectionofimagesshowinggeneralfeaturesoftheatomizationprocesswherethewidthofeachimageis50mm.RowsRa–Rdshowanincreaseinthemeanblastcarrierairvelocityfrom10,20(Bl1),30(Bl2)to47m/sandcolumnsCa–Ccanincreaseintheeffervescentair.ow-ratefrom0,0.46(GLR=0.9%,Ef3)to1.39g/min(GLR=2.7%,Ef4).Forallcasestheliquid(water).ow-rateiskept.xedat52.3g/min.Ca to Cc, a .ner mist is observable when compared to going from rows Ra to Rd, and this is indicative of the ef.ciency of effervescent atomization when compared to the airblast mode. For cases with a lower GLR, including ones not shown here, the effervescent com-ponent only acts to destabilize the liquid column slightly, injecting bubbles into the liquid core in such a way that a .ne mist is not observable. In those situations, the addition of an airblast compo-nent has a substantial effect on the overall dispersion of the spray and this is clearly seen through column Cb going from Ra to Rd, where large scale motions in the surrounding gas phase entrain into the central column promoting mixing. The arrows shown on column Ca will be referred to in a later section and indicate the typical lengthscale of large scale instabilities which occur due to entrainment from the surrounding air. 
In the case of more substantial effervescent air, such as that of column Cc with a GLR of 2.7%, the degree of dispersion is substan-tial over all values of the air-blast component showing a lesser dependency of the overall degree of dispersion from the air-blast mode. However, as will be seen through later results, the air-blast component still acts to improve mixing in the primary atomization zone. It also assists in the removal of dominant unde-sired instabilities from the effervescent mode; features that are not observable from Fig. 
2, but considered in the following sections. 



3.1. Microscopic imaging: effervescent atomization 
Fig. 3 shows high speed image sequences for cases with no air-blast component and a GLR of 0.1%, 0.4%, 0.9–2.7% from top to bot-tom for the liquid .ow-rate speci.ed in Table 
1. Concentrating on the top row, large in.ated and isolated liquid bags are observable in the central column which, through observation of hundreds of images for this case, are generally not statistically likely for cases with a GLR greater than approximately 0.5%. For the case of the GLR of 0.1% some of the structures that are observable, shown here in the .rst two frames of the top row, represent a radial bulging of the liquid that is not consistent with a bubble explosion event. This observation has also been made in Shepard [23] 
and may support the existence of axial velocity .uctuations, as in Meier et al. [24] 
which partly drive the effervescent atomization process. For larger GLR ratios, the bag like structures tend to coalesce together which indicates that an upstream portion of liquid moves faster than its downstream counterpart. This coalescence leads to instability which can further atomize the liquid jet. 
Moving on to the third and fourth rows, more substantial atomization is observed. For the highest GLR case shown in the fourth row, the higher void fraction destabilizes the liquid column and leads to atomization which occurs from the core outwards. This type of atomization is markedly different to what would be expected in the case of a laminar liquid jet exposed to a high speed coaxially .owing air stream. In these cases, atomization is driven by interfacial instabilities and therefore begins at the surface of the core. 

3.2. Microscopic imaging: hybrid atomization 
Fig. 4 shows the in.uence of blast air on an effervescent mode of atomization. The top three rows show case Ef3 from Table 1 with an air-blast velocity of 10, 20, and 30 m/s and the bottom row shows case Ef4 with an air-blast velocity of 30 m/s. Comparing row 1 of Fig. 4 with row 3 of Fig. 3 similar features are visible, and portions of liquid are being expelled radially outward. Whilst the time resolution to con.rm that these radial expulsions are due to collision events is not available, the morphology is in qual-itative agreement with the arguments of Shepard [23]. At a velocity of 20 m/s some sinusoidal instability can be discerned although this is not clear until a blast velocity of 30 m/s shown in the third row of Fig. 4. 
Comparing the third row of Fig. 4 to the third row of Fig. 3 the in.uence of the coaxial air.ow is apparent. In the third row of Fig. 4, bubbles are clearly visible in the liquid core and these are superimposed with a sinusoidal instability from the surrounding air with the liquid column having thinned. The reader should note that the liquid jet is not less atomized in this area (as will also be seen in Section 6). This particular image sequence has been chosen to highlight the presence of a primary instability superimposed  onto the effervescent core. A feature not observable in the pure effervescent cases. 
The fourth row of Fig. 4 shows that even with the higher effer-vescent air contribution, superimposed instabilities are still observable. The reader should note again that these images have been chosen to highlight the superimposed instabilities from the air-blast mode. Through observation of hundreds of other images not shown here, it is very clear that the case of the fourth row of Fig. 4 
exhibits substantially more atomization than that seen from these selected images. The reader is directed to Fig. 3 for a better visualization of the in.uence of the effervescent mode on the atomization morphology. 
Fig. 5 shows case Ef3 at three different blast velocities (10, 20– 30 m/s from top to bottom) but at 5.3 mm downstream from the exit plane. The blasting component of 10 m/s has no visible in.uence on the liquid core and therefore the top row also resembles a typical sit-uation with no blasting air. Unlike at 2.5 mm from the exit plane, where substantial differences were observable from the typical snapshots presented, slightly further downstream the differences are not as obvious. In general, for this effervescent case whilst also for case Ef4 not shown here for brevity, the blasting air does not act to qualitatively change the spray morphology which occurs downstream of the primary atomization zone. However, this does not necessarily mean that the rates of atomization are the same amongst the pure effervescent and hybrid modes, given that the atomization will be dictated by the mean .ow .elds, as well as the turbulence characteristics, both of which are markedly different amongst the sprays. This is examined next, while the quality of the atomization will also be examined in Section 6 onwards. 4. Flow-.eld pro.les Fig. 6 shows the mean velocities Umean and the rms of velocity .uctuations urms for case Ef4 with no blast velocity, and with blast level Bl1 and Bl2 as de.ned in Table 1, at a variety of downstream locations. Cases Bl1 and Bl2 of Fig. 6 are the sprays shown in (Rb, Cc) and (Rc,Cc) of Fig. 2. Concentrating .rst on the top row of Fig. 6, showing the mean velocity, it is clear that the addition of an air-blast component acts to increase the mean velocity at the centre of the spray, but it also alters the velocity pro.le of the .ow at the exit plane. This is a result of entrainment from the surrounding air acting to enhance mixing between the air blast stream and the central effervescent air core. The in.uence of the air-blast mode is particularly clear going from Ef4 to Ef4Bl1 where between r=D .0 and r=D .t=:5, the gradient in velocity has diminished signi.-cantly with the addition of the air blast component. 


4.1. Turbulence felds 
Through observation of the second row of Fig. 6, we can see that the addition of the air blast component has acted to reduce the axial rms of the velocity .uctuations. With the effervescent com-ponent, as discussed in Section 3.1, longitudinal .uctuations are largely due to the presence of air in the liquid jet which causes changes in the void fraction and hence in the local velocity mea-sured. The air blast mode acts to decrease the magnitude of these velocity .uctuations whilst also increasing the level of turbulence surrounding the liquid core. The pro.le of the rms shows these two distinct peaks within the .ow, one marks the peak of the axial instability occurring in the central core driven by effervescent air .uctuations, and the other outer peak, occurring in the surround-ing air, is driven by the air blast mode. This argument can be sub-stantiated due to the absence of the double peak feature of the pure effervescent case of Fig. 
6(d), where the axial rms is driven by instability in the effervescent core. In addition to showing a complex .ow-.eld, this also indicates another possible practical advantage of the addition of an air blast component wherein insta-bilities can be dampened in the core whilst improving the mixing .elds in the surroundings. This will be substantiated upon presen-tation of frequency spectra in Section 5. 
Fig. 7 displays the same measurements as in Fig. 6 but now showing the in.uence of going from effervescent case Ef3 to Ef5 for a .xed blast level of Bl2. Unlike case Ef4 shown in Fig. 6, case Ef3 is poorly atomized in the centre hence resulting in a dense core with a velocity closer to the liquid jet as shown in Fig. 7(a). Case Ef5Bl2 of Fig. 7(b) shows a very similar pro.le of the mean axial velocity as with Ef4Bl2 from Fig. 6(c) hence suggesting that at a GLR of 2.7% and above, the effervescent component atomizes the core ef.ciently, given that a statistical increase in velocity would be related to a lower population of larger fragments. The axial rms values of the second row of Fig. 7 show high gradients in the centreline and observations that can generally be made upon observation of Fig. 6. However, case Ef3 shows an rms value which is higher at x/D = 2.5 when compared to x/D = 0. The precise reason for this is unknown though it is likely attributed to the poor atomization at the exit plane which result in a central liquid core with instabilities that only develop much further downstream. 5. Instability characteristics In Section 4 a key result with regards to the role of the air blast component was that it can act to reduce the velocity .uctuations in the centreline. Additionally, the hybrid mode can broaden the velocity pro.le and introduces two shear layers, as shown by the double peak in the rms pro.les. From the literature it is well known that effervescent atomization is unstable [15,21,20,26,27]  and therefore it is of interest to examine the nature of the instabilities in the context of a hybrid atomization system. This is particularly important given that effervescent atomization is driven by intense .uctuations that occur in the liquid core. This is substantially different from air blast atomization which is a combination of inter-facial instabilities and gas phase mixing. 

5.1. Velocity time series FFT 
The instability frequencies are .rst examined through spectral analysis of the velocity time series from the LDA measurements. The FFT amplitudes are all normalized by their respective centreline values. Observing Fig. 8(a) and (b) it is clear that regard-less of the air blast component, the dominant frequency at the core of the spray is similar to within a few percent. This is in agreement with the results of Section 4.1 which showed that the peak in the axial velocity .uctuations did not fully dissipate with the addition of air blast. Fig. 8(a) and (b) also shows that by r/D = 0.09, the normalized FFT amplitude for the hybrid case is similar to that of the pure effervescent case at r/D = 0.13. This clearly indicates the ability for the hybrid mode of atomization to dampen frequency instabilities closer to the central core. The results indicate that in applications where instabilities are to be avoided, the addition of an air blast component can dampen frequency .uctuations closer to the injection point. 
The reader should note that the frequency spectra can also be determined from the blockage area, or the counted ratio of ‘spray pixels’ eNST to total pixels eNT T. A spray pixel here refers to a pixel in the image where a fragment or portion of a fragment is located. This is determined after binarizing the image as described in Kour-matzis et al. [30], such that a spray pixel is a ‘black’ pixel of inten-sity value 1 and any other pixel is part of the background, with intensity value 0. In the context of a spray, this can also be consid-ered as a measure of the area fraction. An FFT analysis of the block-age area for case Ef4 and Ef4Bl2 (not shown) yielded frequencies of approximately 500 Hz. This compares very well with the estimate from the LDA velocity time series. The results con.rm the LDA spectra but also demonstrate that the instability in the axial veloc-ity .uctuations are inextricably linked to the variation in the area fraction, or blockage area of the liquid core. 

5.2. Analysis of frequency response 
To summarize so far, the overall frequencies measured have not shown sensitivity to the addition of an air-blast component. 
Additionally, in the case of a pure air blast component, in these experiments, a dominant frequency was not measurable from the time resolved microscopic images at the exit plane, and in all cases this suggests that the effervescent component dominates in that near-.eld region. 
Generally however, dominant break-up frequencies do exist for pure air-blast atomization. One frequency is associated with the overall ‘break-up’ frequency which has been measured by Raynal 
[31] whilst a general shedding frequency has also been estimated by Engelbert et al. [4]. Such frequencies have also been con.rmed by Kourmatzis and Masri [32] in other work, however here, pure air-blast frequencies cannot be accurately measured as the full break-up of the pure air-blast mode cannot be visualized in a single .eld of view. In the context of the sprays studied here, of particular interest is to compare the dominant effervescent frequency mea-sured, to those associated with large scale mixing. This large scale mixing is a key aspect of air-blast atomization given the coaxial .ow which encourages entrainment from the surrounding air [2]. The arrows on Fig. 2 show the size of typical instabilities acting on the overall spray core which are largely related to entrainment. These are not to be confused with the primary instability which occurs at the exit of the liquid nozzle and is much smaller, gener-ally of the order of the liquid jet diameter as reviewed elsewhere [2]. 
Some typical larger instability length-scales K have been mea-sured for cases of a pure air blast component with a velocity of 10, 20, 30 and 47 m/s and these are of the order of 19, 9, 7.5 and 7 mm. These waves are considered to travel along the main spray core at a velocity given by the cluster velocity, empirically de.ned by Engelbert et al. [4] 
by Eq. (1): . 2 T where UG is the blast air mean velocity and Ul the mean liquid jet velocity. Determining a global frequency as f . VC =K gives val-ues equal to 58 Hz, 333 Hz, 557 Hz and 909 Hz for velocities of 10, 20, 30 and 47 m/s respectively where the cases of a mean of 20 and 30 m/s are representative of cases Bl1 and Bl2 of Table 1. These fre-quencies are of the order of the dominant effervescent frequencies examined here, where cases Ef1–Ef5 of Table 1 ranged from 300 to 600 Hz. This indicates that the dominant effervescent pulsation frequencies can be of the same order of magnitude as dominant mixing frequencies originating from large scale instabilities, due to the air-blast mode. Therefore, there will be instances where the rate at which .uid is ejected due to effervescent atomization can be of the same order of magnitude as the rate at which air is entrained into the core. 
6. Blockage area and atomization 
The frequency range has not been observed to change with radial location or with the addition of an air blast component, though the intensity is dampened. However, microscopic imaging of the near .eld zone revealed that some morphological differences do exist amongst the pure effervescent and hybrid variants and this must be analysed further. 
Fig. 9 shows plots of the blockage area or two dimensional area fraction plotted vs. the image number where each image is sepa-rated by 100 ls. The arrows on the plots for Ef3Bl2 and Ef3 on Fig. 9 show typical ‘pulse widths’ where one pulse is indicative of the liquid jet atomizing. A high magnitude of AS=AT indicates a liq-uid jet which is nearly intact whereas a low magnitude indicates its atomized or nearly atomized state. As an example, if measure-ments were made in a constant area liquid jet then a horizontal line with a positive value of AS=AT would be obtained. 
From observation of Fig. 9, the pure effervescent case shows typically longer pulse widths, which indicates that intact liquid cores remain over longer periods, from cycle to cycle. In contrast, for a case where there is some blasting component, upon atomiza-tion due to the effervescent air, the surrounding coaxial air will penetrate the liquid column hence promoting dispersion and atomization and leaving a gap in time where no liquid is visible. This leads to shorter pulse widths. The in.uence of the blasting mode on the total liquid fragment area at the exit plane can be sta-tistically analysed through calculation of the mean blockage ratio referred to here as hAS=AT i and this is provided in the next section. 



6.1. Mean areas and ligament sizes 
Fig. 10 shows the magnitude of hAS=AT i for a variety of cases as bars. Also plotted on Fig. 10 is the mean size of ligaments in the near-.eld. Ligaments have been de.ned as those objects detected in the high speed images with an aspect ratio AR > 3 [30]. Whilst there is the limitation of a single line integrated .eld of view, the binarization routine chosen has been calibrated as shown in Kourmatzis et al. [30] and has proven to yield physically consistent data. Ligament sizes, particularly in effervescent atomization, are critical in describing the atomization process as also observed by Sutherland et al. [33] and therefore we focus on characterization of these objects here. Additionally, the separate analysis of larger fragments with respect to droplets, as also done in Ghaemi et al. 

Fig.11.StandarddeviationofareafractioneAS=AT T0forcasesEf2(GLR=0.4%),Ef3(GLR=0.9%)andEf4(GLR=2.7%)atthreedifferentairblastmagnitudesatx=D =0.44.[27] for effervescent atomizers, is critical in accurately describing the atomization process. 
First of all, we concentrate on the hAS=AT i measurements. Taking case Ef3 as an example, Fig. 10 shows that at x = 2.5 mm and x = 5.3 mm the area fraction change is minimal when going from no blast to Bl1 whereas an increase to Bl2 leads to a substantial reduction in hAS=AT i.At x = 8.1 mm the in.uence of the air-blast component on the mean area fraction is also signi.cant given that the break-up length of the air-blast mode is shifted slightly further downstream as was also discussed in Section 3.2. Similar conclu-sions are reached by observation of case Ef2 and Ef4 though case Ef4 is less sensitive to the blasting mode. This is attributed to the more substantial in.uence of the effervescent mode in case Ef4 on the atomization characteristics of the near-.eld, which is dis-cussed further now in the context of the ligament sizes Dlig . Unlike the area fraction which provides a global measure of the atomiza-tion and dispersion performance including droplet statistics, the ligament statistics deal only with elongated fragments as de.ned in Kourmatzis et al. [30]. 
For case Ef4 at all downstream locations, it is clear that the mean ligament size is largely unaffected by the addition of an air-blast mode, and this is particularly true at x = 8.1 mm down-stream even though the area fraction has decreased slightly. Though substantial differences in the .ow pro.les between a pure effervescent and hybrid mode were noted earlier in Section 4, the added in.uence of the air-blast mode does not largely alter the near-.eld primary atomization characteristics as de.ned by the ligament size. This indicates a signi.cant advantage of a hybrid fragmentation process such as the one employed here, where liga-ment size can be dictated by the effervescent mode, but dispersion, downstream mixing and instabilities can be partly altered by the blast mode. For the cases of a lower level of effervescent air (Ef2 and Ef3), the blasting mode has a substantial in.uence on the liga-ment size at x = 5.3 mm and x = 8.1 mm. However, this is not the case at the exit plane where changes are minimal andeven show an increase when moving from no blast to Bl1 and .nally to Bl2. This increase in ligament size from no blast to Bl2 at the exit plane is attributed to the shift in the break-up length further downstream. 
Further insight can be gained by observing the .uctuation in the area fraction for the various cases and this is illustrated through the standard deviation of the area fraction eAS=AT T0 in Fig. 11. Once more, the in.uence of the blasting mode on the near-.eld atomiza-tion is clear. With the absence of an air-blast component, the .uc-tuation in the area fraction increases substantially with GLR, indicating pulsations in the core. However, the addition of the blasting mode shows, across all GLR values tested, that the stan-dard deviation remains roughly constant and this is particularly true of case Bl2. At low GLR values, higher standard deviations are noted for the hybrid cases when compared to the pure efferves-cent case due to the presence of the blasting mode. This occurs most likely through a combination of interfacial instabilities driven by the high shear at the exit as well as entrainment from the sur-rounding air. 
7. Conclusions 
Characteristics of combined effervescent and airblast atomiza-tion have been investigated through a number of experiments which offer a comprehensive study on the atomization mecha-nisms of this hybrid form of fragmentation. Both global and near-.eld images have been provided showing that the morphol-ogy of a hybrid atomization system is different to that of a pure effervescent system showing a superposition of sinusoidal instabil-ities onto a bubbly two phase core. The pulsating frequency of the effervescent atomization mode does not change even with the presence of an airblast component, however the addition of the coaxial .ow does signi.cantly dampen the intensity of the .uctu-ations. This has been con.rmed both through measurement of axial velocity .uctuations, and through spectral analysis of the velocity time series and area fraction of the sprays. Analysis of the pulsating frequencies with respect to typical instability time-scales suggests that there can be a close coupling between the effervescent a omization frequency and the frequency at which the liquid phase is mixed with the surroundings. Area fraction results show that from one atomizing pulsation to the next, the coaxial .ow allows for entrainment of air in the central core thereby in.uencing the degree of atomization and dispersion when compared to the pure effervescent mode. Furthermore, for GLR levels below 2.7%, the air-blast mode can have a substantial impact on the ligament sizes at the exit plane. The atomization technique demonstrates that whilst effervescent atomization is a more ef.-cient mode of liquid fragmentation, the addition of the air blast component can act to improve dispersion characteristics whilst simultaneously dampening or restricting undesired pulsations. 














TWO-PHASE FLOW SIMULATION OF HIGH-PRESSURE GAS ATOMIZATION EFFECT OF MOLTEN METAL AND ATOMIZING GAS PROPERTIES ON DROPLET SIZE DISTRIBUTION
This paper deals with the physics of high-pressure gas atomization in metal powder production. To gain understanding of the effect of gas pressure on droplet size distribution, a numerical two-phase .ow study is performed using Eulerian-Eulerian Volume of Fluid (VOF) interface tracking method. Annular-slit, close-coupled gas atomizer is considered to atomize molten aluminum using nitrogen as the atomizing gas. Four cases with different gas pressures are considered, while geometry and other operational param-eters are .xed. Characteristics of several interfacial instabilities have been identi.ed at different stages of the atomization process. Despite the increment in the rate of the atomization with the increasing gas pressure, deformation characteristics and the breakup mechanisms remain unchanged. Droplet size and the cumulative volume distributions indicate that the effectiveness of the atomization process increases with the elevating gas pressure. Cumulative volume obtained from the numerical simulations at low gas pressures display similar trends to the experimental results. 
1. Introduction 
Metal powder production is in high demand for its applications in rapid prototyping, injection molding, and additive manufactur-ing. Metal powder used in additive layer manufacturing is required to have precisely tailored metal powder with a certain size, shape, and morphology [1]. Lawley [2] 
reviewed many existing powder manufacturing technologies and discussed the superiority of gas atomization among other methods considering its controllability over powder size distribution. In gas atomization, a pressurized gas is used to atomize the molten metal. Momentum and thermal energy [3] 
transferred from the expanding gas to the molten metal facilitates disintegration and atomization. The rate of interfacial momentum and thermal energy transfer depends on the driving potential and the interfacial area. Since the cumulative interfacial area increases with the atomization process, it increases the atomization rate to several orders of magnitude [4]. The main advantage of the gas atomization is the controllability [5] 
of the powder size, shape, and the morphology. It is important to study the effect of each operating parameter on the atomization process and the powder distributions. 
The atomization process can be divided into two regimes such as primary and secondary atomization (Fig. 
1a). The primary atomization regime starts when the melt interacts with the gas .ow near the melt tip. Then, the melt stream will deform and cre-ate unstable wave-like structures, when it breaks up into ligaments and large droplets. The secondary atomization regime begins fur-ther downstream when the large droplets and ligaments extend due to Rayleigh-Plataeu instability and break up into smaller dro-plets. This breakup process will continue until the critical Weber number is reached or till the droplet solidi.es [1,6]. 
Several numerical simulations have been performed to study different aspects [7–9] 
of the atomization process. Some of these early studies could not successfully predict the atomization pro-cess as they neglect the strong feedback from the highly dense melt stream [10]. The Eulerian-Lagrangian approach [3,11–13] 
was .rst used to investigate the molten metal atomization and particularly the secondary breakup. The main drawback of these discrete element methods is their inability to predict the primary atomization [10]. These simulations often used random droplet size distribution [14] 
(DSD) to initiate the simulation. Considering these drawbacks, Tong and Browne [10] 
performed the .rst Eulerian-Eulerian based simulations using the front-tracking method. This and other papers [15–17] 
showed that the gas dynamics can be signi.cantly affected by the topological evolution of the melt stream. However, these studies were either conducted in two-dimensional geometries [10,15] 
or considered only the early stages of the atomization process [18]. 
In this study, the gas atomization process of molten aluminum at different gas pressures is studied using the Volume of Fluid (VOF) model [19] 
together with geometrical reconstruction method in OpenFOAM software. The method is Eulerian-Eulerian and does not involve any semi-empirical correlation or any facili-tation of droplet breakup as it occurs naturally without any mod-eling. The effect of atomization gas pressure on the droplet size distribution is investigated. To obtain the effect of inlet gas pres-sure, four inlet gas pressures are considered while keeping other parameters such as melt .ow rates, melt and gas physical proper-ties the same. This paper deals with both primary and secondary breakup processes in conjunction with the calculation of dropsize distribution to understand the trends in gas pressure. Even though a very .ne mesh is required to resolve both types of atomization completely, some characteristics of the secondary breakup process have been identi.ed in the simulation through the calculation of ligament aspect ratio. The objective of this work is to predict the trend of the drop size with pressure and compare this trend with the experimental trend by discriminating dropsize based on aspect ratio. 
2. Computational model 
The computational domain (Fig. 
1) is designed based on a dou-ble induction, discrete nozzle, close-coupled gas atomizer, which consists of 18 circular gas nozzles evenly spaced around the melt tube. The computational domain follows the experimental set up (not shown here) but uses an annular-slit instead of individual nozzles. Since the velocity in the slit is maintained the same as in experiments, the mass .ux is different and only trends in pres-sure can be determined. Once the melt enters the atomization chamber, the interaction with the atomization gas initiates the pri-mary atomization. Larger droplets will atomize into much smaller droplets, while progressing downstream. 
In this study, four gas inlet pressures (1, 1.5, 2 and 2.5 MPa) were used to atomize molten aluminum in the annular-slit, close-coupled atomizer. Fig. 
1b shows a schematic view of the atomizer geometry. Three-dimensional calculations are done in the 90 wedge 
(Fig. 
1c), and the mirror images in the other three quadrants would complete the picture. This is primarily done to reduce the computational power requirement, while imposing two independent symmetry boundaries. Therefore, the results are not fully axi-symmetric as would be the case with a two-dimensional axi-symmetric problem. Axial and the radial lengths of the computational domain are chosen to be 100 mm and 20 mm, respectively to reduce the computational domain which is large enough to determine primary and secondary atomization. A structured mesh with 11.8 million cells was prepared using the blockMesh utility available in OpenFoam. The volume equivalent grid size ranges from 70lm to 700lm. Consider-ing the shape of the cylindrical geometry, grid size increases radially outward. Aspect ratio-based .ltered data corresponding to a .ner mesh with 20.2 M cells (volume equivalent grid size ranges from 50 to 500 lm and 60% of the grid cells are smaller than 100 lm) are compared with the results corresponding to 11.8 M cells. The normalized cumulative volume results show less than 1% change. However, a clear increment in the total number of droplets in the .ner mesh can be observed. All the results shown have been obtained for 11.8 million cells. 
Table 
1 
shows the boundary conditions used in the simulations. k represents the turbulent kinetic energy. The physical properties of Aluminum and Nitrogen are provided in Table 
2. 
A zero-gradient (Neumann boundary) condition for turbulence kinetic energy is imposed on the solid walls for high Reynolds numbers, i.e., turbulence wall function mentioned in Table 
1. The effect of boundary layer developed near the solid wall is assumed to be negligible. Boundary layer developed near the melt-gas inter-face plays a major role in the atomization process. Turbulence intensity is speci.ed at both melt and gas inlets. This boundary condition determines the turbulence kinetic energy based on the speci.ed turbulence intensity and induced mean velocity. To facil-itate reverse .ow, a special outlet condition is imposed for velocity. Nitrogen is used as the atomization gas and constant values are used for density and kinematic viscosity. 
The hydrodynamic Courant number (C . uDt=Dx) is kept at 0.3 and the interfacial Courant number at 0.1. High induced gas veloc-ity ( 1500 m/s) and a suf.ciently .ne mesh restricted the time step required to follow the Courant number criterion for a stable solution to about 10 ns. Considering the number of cells and avail-able computational resources, all the simulations are carried out to the physical time of 2 ms. Computational power required for 1 MPa gas pressure simulation is around 30,000 processor hours (250 pro-cessors for 5 days). 
In this study, the dynamics of the melt-gas interface is simu-lated using the Volume of Fluid (VOF) method, which is discretized using Finite Volume Method (FVM) on OpenFOAM framework. The ‘isoAdvector’ geometrical interface reconstruction method [20,21] 
is used to improve the sharpness of the interface, while ensuring the continuity of each phase. For an incompressible .ow, the mass, momentum and volume fraction conservation equations are as follows. 
where !U is velocity, q density, P pressure, T viscous stress tensor, 
! !
Fb body forces, Fr surface tension force, and a phase volume frac-tion. In VOF method, volume fraction has values of 1 and 0 corre-sponding to liquid and gas phases, while intermediate values represent the interface. Continuous surface force (CSF) method 
[22] 
is used to model the surface tension force as a body force, which only has non-zero values at the interface. The surface tension force is de.ned [22] 
as follows, 
where, r is the surface tension coef.cient, j local interfacial curva-ture and ! n represents the interfacial normal. Density and viscosity are interpolated based on the volume fraction. Large Eddy Simula-tion (LES) is used in these simulations to solve for the large-scale eddies, while modeling small scale eddies. These sub-grid scale (SGS) eddies are modelled following the one equation eddy viscos-ity model [23]. 
A post-processing code is developed to obtain the droplet size distribution using volume fraction. Initially, a threshold in volume fraction is utilized to identify the liquid cell identi.cation numbers. A list is .rst generated including only the neighbor cells that have liquid in them. Then, these sets are compared and appended if they have common elements. The common sets represent the cells cor-responding to each droplet [24]. Special care is given to the calcu-lation of the number and size of droplets that share cells with symmetry boundaries as their sizes and velocities must be updated accordingly. Modi.cations are made to obtain the effect of droplets leaving the computational domain. The code is modi.ed to approx-imate the droplets, which has the potential to leave the computa-tional domain within a given time interval. These approximated droplets are then added to the .nal droplet size distribution. In addition, the aspect ratio (AR) of each droplet was calculated. It is de.ned as the ratio of the longest dimension of the liquid entity to the diameter of the sphere with equivalent liquid volume. Therefore, the aspect ratio represents the degree of deformation of a droplet or a ligament with respect to a spherical droplet with equivalent volume. This enables the comparison of the atomization processes, which progress at different rates i.e. varying gas pres-sures are expected to change the rate of atomization. 
3. Results and discussion 
Since a direct comparison of numerical and experimental data is not possible for reasons mentioned in the previous section (i.e., an annular-slit is used in computations with the gas velocity main-tained at the experimental velocity; the use of incompressible .ow; a smaller computational domain), only the trends obtained for different gas pressures are compared. The melt diameter, funnel-length, and gas nozzle angle are maintained the same as in experiments. 
Fig. 
2 
shows a qualitative comparison of the cumulative droplet volume obtained at .ve gas pressures (1, 1.5, 2, 2.5, and 3 MPa). It is clear that the increasing gas pressure facilitates better atomiza-tion in terms of yield. The experimental data show no increase in atomization beyond 2.5 MPa. The normalized volume curve dis-plays a steep slope with respect to droplet diameter. The numerical simulations in Fig. 
2(b) are made at an early stage of atomization, i.e., at 2 ms. Hence, any comparison with experimental powder data made at the end of the experiment after solidi.cation is expected to have discrepancy. The numerical simulations show that up to 300 lm diameter, there is no signi.cant difference in normalized volume for all pressures. The discrepancy becomes large beyond 300 lm. At large pressures, many droplets are seen to exit the control volume, suggesting a larger computational domain may need to be chosen. 
It is important to note that the larger droplets are not .ltered out in simulations as in experiments. To obtain a complete picture of atomization and capture all droplet deformation and facilitate the breakup of a 100 lm droplet, the mesh resolution should be 
of the order of 1010 cells, which is prohibitively expensive. The cur-rent grid does an adequate job of allowing an early stage analysis of atomization. The current computational geometry is 100 mm in length and 20 mm in diameter. Therefore, some of the droplets can leave the computational geometry before breaking up. The post-processing code can approximate the number of droplets that are leaving the computational domain. 
Since the early stages of atomization are analyzed, an approach would be to understand the simultaneous primary and secondary atomization processes and display the size and cumulative volume of the droplet by discriminating based on the aspect ratio of each droplet. This approach is adopted and discussed in the following 3 .gures. 
In Fig. 
3 
shows the breakup process of a small three-dimensional liquid mass. For clarity, this 3-D ligament circled at the top is divided into three ligaments as marked in red, blue and black colors. The ligament marked in red, which was previ-ously attached to the other two ligaments is separated around 510ls (marked in yellow circle). At 550 ls, this ligament is advected with the gas .ow, displaying a neck that will be eventu-ally pinched off into two segments. The ligaments marked in blue and black are extended along their axial direction due to Rayleigh-Plateau instability [25–27] 
when the surface tension minimizes the ligaments into smaller packets with smaller surface area for the same liquid volume. From Figs. 
3 
and 
4, it can be seen that the liq-uid melt undergoes this instability .rst when the cylindrical vol-ume thins and the gas creates more perturbations on the surface. The liquid then collapses under the action of capillary forces again due to surface tension and can be seen to break into smaller dro-plets. These droplets and ligaments are circled in their respective colors to show the breakdown process in the subsequent time intervals. As previously mentioned in literature, these ligaments are subjected to break into smaller droplets to reduce the surface energy density. This instability plays a major role in the secondary atomization process. 
Fig. 
4 
shows the time evolution of 1 and 2.5 MPa gas pressure sim-ulations. The main difference between these two gas pressures is the rate of progression of the atomization process. For the case of 
2.5 MPa, the gas pressure induces a much higher gas velocity and contributes to enhanced atomization due to higher gas momentum .ux. Consider the time evolution of 2.5 MPa gas pressure (second row). First, the gas stream impinges and penetrates the melt stream. At 0.4 ms, the gas penetrates the melt stream and divides it into two portions at the point of impact, while atomization is slower for 1 MPa. The top melt portion moves upwards due to strong gas recirculation for 2.5 MPa to accumulate near the melt tip. Such accumulation of melt near the nozzle does not seem to be a problem for 1 MPa. 
In Fig. 
5, four images at incremental times, images of atomiza-tion, vorticity and turbulence intensity are displayed. The black contours indicate the melt-gas interface. Note that the plane nor-mal vorticity is shown. The positive vorticity values are pointed into the .gure and negative vorticity values are pointed in the opposite direction. The opposite directions in vorticity around the ligaments suggest that the .ow structures induce torque, which leads to rupture. Thus, different types of instabilities con-tribute to secondary atomization. In the turbulence intensity plots 
(i.e – the log scale is used to properly visualize the variations), the turbulence intensity increases near the melt-gas interface. This is due to the chaotic interaction near the melt-gas interface, which creates perturbations and facilitates interfacial instabilities. The boundary layer developed near the melt-gas interface plays a major role in the breakup process, since the shear forces exert on either side determine the deformation of the melt stream and the eventual breakup. The turbulence intensity plots indicate that the turbulence model can resolve these forces successfully. 
The image shown in Fig. 
2 
consists of ligaments, droplets, and liquid blobs during different stages of atomization. Droplet aspect ratio allows one to understand the number of droplets that have possibly undergone both primary and secondary atomization. Fig. 
6 
shows the droplet size histograms for 1 MPa gas pressure. 
Blue represents droplets with AR > 2, and yellow AR < 2. The com-parisons are made at 2 ms. The few droplets that have left the com-putational domain are also included. In general, there is no signi.cant improvement in the droplet distribution as the pressure is increased, although more droplets tend to leave the computa-tional domain. 
In Fig. 
6, most of the droplets are in the range of 100 lmto 300 lm, with the peak around 150 lm. Although 2 ms is in the early stages of atomization, about 50% of the droplets appear to have an aspect ratio of less than 2. It is also clear that it is the 
Fig. 6. Droplet size distributions at 2 ms for 1.0 MPa; AR > 2 (blue), AR < 2 (yellow). (For interpretation of the references to color in this .gure legend, the reader is referred to the web version of this article.) 
secondary atomization that allows the droplet size (and the even-tual powder size) to be small. At early stages of 2 ms, when the atomization process is not completed, it is still possible to .lter the dropsize information based on aspect ratio and dropsize. In fact, this is what is done in the experiments when the larger drops are .ltered out with the sieve. Fig. 
6 
shows that when the condi-
Fig. 7. Comparison of normalized cumulative droplet volume for 1 MPa gas pressure at 2 ms, black line – the original cumulative volume with all the droplets; red line – cumulative volume for droplets, with AR < 2.0 and droplet diame-ter < 300 lm; blue line – cumulative volume for droplets with AR < 2.0 and droplet diameter < 200 lm. (For interpretation of the references to color in this .gure legend, the reader is referred to the web version of this article.) 
tions are imposed to account for the droplets that have AR < 2 and dropsize less than 200 lm and 300 lm, about half the number of droplets .t in this category, and the cumulative volume trends (see Fig. 
7) to the left where the cumulative volume is steeper and higher for smaller size droplets. This is seen to be the case in the experiments as well as shown in Fig. 
2(a). A direct comparison is not possible as an annular slit is used instead of multiple nozzles to reduce the total number of cells, nevertheless, the computations predict the trend well. 
4. Conclusions 
In this numerical study, four gas pressures have been consid-ered for gas atomization of aluminum melt. The atomization pro-cess is simulated using Eulerian-Eulerian two-phase numerical framework. The rate of atomization is seen to increase with increasing gas pressure. Droplet size distributions indicate a clear improvement in atomization. Higher gas pressures tend to advect droplets at a higher rate than they are atomized, which may neces-sitate a larger computational domain. Cumulative volume plot indicates a clear increment in the effectiveness of the atomization with increasing gas pressure. The cumulative volume does not change with pressure for up to dropsize of 300 lm. When the dro-plets are further .ltered based on a smaller aspect ratio (<2) and less than 300 lm, the simulations show a better trend in cumula-tive volume and tend to follow the experimental data. Due to the complex nature of computations, direct comparison with experi-ments for Eulerian-Eulerian simulations is prohibitively expensive. However, these simulations have shown their ability to serve as a good guidance for further controlled experiments. Thus, the main advantage of the gas atomization process is its controllability to obtain powder properties, which can be guided by numerical anal-ysis by changing geometrical or operational parameters. 
Acknowledgements 
This research was sponsored by the U.S. Army Research Laboratory through cooperative agreement #W911NF-17-2-1072 between the University of Central Florida, United States and the 
U.S. Army Research Laboratory. The views, opinions, and conclusions made in this document are those of the authors and should not be interpreted as representing the of.cial policies, either expressed or implied, of the U.S. Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. 
The authors also acknowledge the Advanced Research Comput-ing Center at University of Central Florida for providing computa-tional resources and support in obtaining the results reported in this paper. 







2D mid-infrared laser-absorption imaging for tomographic reconstruction of temperature and carbon monoxide in laminar flames
Abstract
This manuscript presents the design and initial application of a mid-infrared laser-absorption-imaging (LAI) technique for two-dimensional (2D) measurements and tomographic
reconstruction of gas temperature and CO in laminar flames.  In this technique, the output beam from a quantum-cascade laser (QCL) is expanded, passed through the test gas, and imaged in 2D
using a high-speed mid-infrared camera.  The wavelength of the QCL is scanned across the P(0,20) and P(1,14) transitions of CO near 4.8 µm at 50 Hz to provide 2D measurements of path-integrated
gas temperature and CO column density across over 3,300 lines-of-sight simultaneously.  This enabled the first sub-second (0.1 s), high-resolution (140 µm), 2D laser-absorption measurements
and tomographic reconstruction of flame temperature and CO mole fraction using mid-infrared wavelengths.  Prior to entering the test gas, the beam was reflected off two diffusers spinning at
90,000 RPM (≈9400 rad/s) to break the laser coherence and prevent diffraction-induced image artifacts.  This technique was validated with measurements of CO in an isothermal jet and then
demonstrated in laminar, partially premixed, oxygen-ethylene flames despite large background emission from soot and combustion products.
Introduction
Laser-absorption spectroscopy (LAS) is widely used to provide quantitative, calibration-free measurements of chemical species and thermodynamic conditions (e.g., temperature, pressure,
velocity) in a wide range of combustion applications [1].  Typically in LAS, wavelength-tunable laser light with intensity Io is directed through an absorbing gas and the transmitted light intensity
(It) is measured by a photodetector.  Beer’s Law is used to calculate the spectral absorbance (α = −ln(It/Io)) which is then compared with that predicted by spectroscopic models to provide
path-integrated measurements of gas properties along a line-of-sight (LOS). While useful,  many combustion applications demand multi-dimensional measurements of
chemical species and thermodynamic conditions, for example, to resolve the thermochemical structure  of  flames  and/or,  more  generally,  the  spatiotemporal  evolution  of  combustion.   To
meet this need, LAS diagnostics employing multiple LOS [2–8] or mechanical scanning of the LOS [9–12] have been used with and without tomographic reconstruction algorithms to quantify
how gas properties vary in space and time.  In the simplest form, this approach is performed to  provide  spatially  resolved,  path-integrated  measurements  of  species  and  thermodynamic
conditions.  If absorption transitions with appropriate lower-state energy are used, path-averaged properties (e.g., absorbing-species column density, absorbing-species-weighted path-averaged
temperature) can be obtained despite highly non-uniform conditions along the LOS [13].  For
example, Goldenstein et al. [9] and Spearrin et al. [10] developed mid-infrared LAS sensors for
absorbing-species-weighted path-averaged temperature and absorbing-species column density of
H2O, CO, and CO2  in a model-scramjet combustor.  The authors’ mechanically scanned the LOS
in two dimensions (x-y) to map out path-averaged gas properties.  While effective and convenient
to execute in environments with somewhat limited optical access,  this approach can be time
consuming (often requiring minutes per 2D dataset), provides modest spatial resolution (order
1-10 mm) and is not spatially resolved along the LOS.
Alternatively,  numerous  researchers  have  developed  laser-absorption  tomography  (LAT)
techniques to provide tomographic reconstruction of 2D temperature and species fields within
the  measurement  plane  [5, 7, 11, 12, 14–19].    This  approach  provides  the  added  benefit  of
resolving the gas conditions along the line-of-sight; however, it comes at the expense of more
complicated  data  processing  routines  and  steep  demands  for  many  lines-of-sight  (typically
10s  to  1000s  of  LOS  depending  on  the  level  of  spatial  resolution  desired  [14]).   For  these
reasons, LAT techniques developed to date have primarily used near-infrared wavelengths to
exploit relatively inexpensive telecommunication-grade fiber optics (e.g., fibers, multiplexers,
splitters)  and  detectors  [5, 7, 11, 12, 14, 15].   Unfortunately,  confinement  to  the  near  infrared
significantly limits the number of chemical species that can be monitored at combustion-relevant
concentrations,  conditions,  and  spatial  scales.   As  such,  the  majority  of  prior  LAT  work  in
combustion applications has focused on detecting H2O via its combination and overtone bands
near 1.4  µm [5, 8, 11, 12].  Additionally, some work has been done to tomographically image
hydrocarbons via their overtone C-H stretch absorption band near 1.7  µm [7] and NH3  via its
overtone and combination bands near 1.5 µm [20].  To overcome this limitation several researchers
have developed LAT techniques employing mid-infrared wavelengths for tomographic imaging
of temperature, CO, and CO2  [16–19], however these works have relied on mechanical scanning
of the line-of-sight, thereby significantly limiting the temporal and spatial resolutions to the order
of seconds to minutes and mm, respectively.
That being said, there remains a critical need to develop mid-infrared LAS and LAT techniques
with  higher  temporal  resolution  and  spatial  resolution  (e.g.,  less  than  1  second  and  1  mm,
respectively).  Recent work by Wei et al. [21] demonstrated significant progress towards achieving
this goal through the use of laser-absorption imaging (LAI) with a high-speed mid-IR camera.
The authors demonstrated the ability to acquire LAS measurements of temperature, CO, CO2, and
C2H6  along ≈400 lines-of-sight (in a single plane) simultaneously to provide 1D measurements
and reconstructions of gas temperature and species concentrations.  Diffraction-induced noise
and image artifacts prevented use of the entire focal plane array and, thus, mechanical scanning
of the measurement plane (in the axial direction) was required to achieve 2D measurements and
reconstructions of flame structure.  Using this technique the authors’ were able to acquire LAS
measurements with a spatial resolution of near 50 and 125  µm in the horizontal and vertical
directions respectively, and a 2D image of flame properties was acquired in < 10 seconds [21].
As a result, this approach provided ≈ 10× smaller spatial resolution and ≈ 100× better temporal
resolution compared to more conventional LAT techniques that rely on translating the LOS in
both the x- and y-direction.
Here we present the development and application of the first mid-infrared laser-absorption
imaging technique capable of providing 2D-measurements and -tomographic reconstruction of
flame temperature and CO concentration without mechanical scanning of the measurement plane.
This was achieved by performing high-speed, 2D imaging of scanned-wavelength, mid-infrared
laser light that was reflected off two ground-glass diffusers spinning at 90,000 RPM prior to
passing through the flame.  Spinning the diffusers was required to break the optical coherence
and,  thus,  prevent  diffraction-induced  image  artifacts.   This  approach  enabled  the  following
advancements in mid-IR LAI: 1) a 8x increase in the number of simultaneous measurement paths
(≈3,300 LOS here compared to 400 LOS in [21]) and 2) a ≈20x improvement in 2D-measurement
time (0.1 seconds here compared to ≈2 seconds in [21]) by avoiding the need to mechanically                                                                          
scan the measurement plane.  We demonstrate that this approach enables 2D measurements of
thermochemical flame structure on near-cm scales with a projected pixel size of ≈ 140  µm and
a  time  resolution  as  small  as  0.1  seconds  (i.e.,  10  Hz).   The  remainder  of  this  manuscript  is
primarily devoted to presenting the design and evaluation of the optical setup and describing how
this LAI technique was applied to provide 2D-measurements and -tomographic reconstruction of
flame temperature and carbon monoxide concentration in flames.
2.    Technical approach and equipment
Figure  1  illustrates  the  experimental  setup  used  for  LAI  of  temperature  and  CO  in  partially
premixed,  oxygen-ethylene  flames.   A  continuous-wave  (CW)  quantum-cascade  laser  (QCL)
providing  30  mW  of  optical  power  at  wavelengths  near  4.8  µm  was  used  to  provide  2D
measurements of absorbance spectra.  The wavelength of the laser was scanned across the P(0,20)
and P(1,14) absorption transitions of CO near 2059.9 and 2060.3 cm−1, respectively, at 50 Hz
via  injection-current  tuning  with  a  700  mV  peak-to-peak  triangle  wave.   The  QCL’s  current
was  scanned  below  its  threshold  current  to  enable  the  background  and  flame  emission  to  be
measured in each pixel immediately before each scan.  The wavelength scanning of the QCL was
characterized using a solid germanium etalon with a free-spectral range near 0.0163 cm−1.  It is
worth noting that the absorption transitions employed here have recently been used to measure
gas temperature and CO in several other combustion environments (e.g., scramjet combustor [10],
pulse-detonation engine [3], and propellant flames [22]).  We refer the reader to [10] for additional
details regarding why these absorption transitions are well suited for studying combustion gases.
A custom built optical assembly (see Fig.  1(a)) was used to expand the laser beam and break
its coherence to prevent diffraction-induced image artifacts (see Section 3) that formed by passing
coherent laser light through apertures in the beam-shaping optics and camera.  The laser beam
was immediately expanded by a 12.7 mm diameter, AR-coated, CaF2, concave lens with a focal
length of -18 mm.  The expanded beam was then directed to a gold-coated, ground-glass diffuser
(ThorLabs DG10-120-M01) with 120 grits/inch (≈ 50 grit/cm).  The gold coating provides > 97%
reflectance  at  an  angle-of-incidence  (AOI)  of  12◦  for  wavelengths  near  4.8  µm.   The  diffuse
reflection was then collimated by a 25.4 mm diameter, AR-coated, silicon lens with a focal length
of 25.4 mm and the beam was then focused onto a second diffuser using a 25.4 mm diameter,
AR-coated, CaF2  lens with a focal length of 40 mm.  The AOI for both of these diffusers was
approximately 20◦.  When aligning the diffusers, the beam was directed to the outer edge of each
diffuser to concentrate the beam onto regions of each diffuser with the highest velocity (while
spinning),  thereby providing more rapid scrambling of the laser light’s coherence.  After the
second diffuser, the laser light was partially collimated by a 50.8 mm diameter, concave mirror
with a focal length of 100 mm.  Close to the viewing plane, the beam was passed through a final
25.4 mm diameter lens (CaF2, focal length of 200 mm) to collimate a portion of the beam which
was directed through the flame and into the high-speed IR camera.
Both of the diffusers were mounted in 25.4 mm diameter lens tubes which were mounted to
a custom aluminum adapter (using two set screws) in order to attach each diffuser to a motor
(Castle Creations 1406 Sensored 4-Pole Brushless Motors, 7700kV). The motors were powered
by Mamba X sensored motor controllers.  The motors for the first and second diffusers were
supplied with 26 V DC and 5 and 7 A of current, respectively.  With the diffusers mounted to the
motors, both motors were capable of spinning at 90,000 RPM (≈9400 rad/s).  Due to the high
angular velocity, care was taken to balance the diffuser assembly during machining and shim
stock was used to properly seat the diffuser in the diffuser holder.  Failure to properly balance the
diffusers can cause extreme mechanical vibration which can misalign the optical setup or, at best,
introduce unwanted noise and oscillations in the light intensity imaged by a given pixel.
The laser light was imaged by a Telops Fast-IR 2K high-speed infrared camera which employs
a  cryogenically  cooled  InSb  focal-plane  array  with  320  x  256  pixels.   At  full  resolution,  the
camera is capable of operating at nearly 2000 frames-per-second (FPS). However, during all
LAI experiments the field of view was reduced to 64 x 52 pixels to enable frame rates of 24
kFPS to be used.  This frame rate was required to sufficiently resolve the absorbance spectrum
measured in each pixel when employing the scan rates and amplitudes used here.  A bandpass
filter centered near 2060 cm−1  with a full-width at half-maximum of 40 cm−1  was used to prevent
pixel saturation from background flame emission which was pronounced in the high-temperature
and sooting flame studied here.
Flames were produced by a custom-made coflow burner (see Fig.  1(b)).  The outer body of
the burner is made from a stainless-steel pipe (330 mm long, 19 mm outer diameter) with weld
neck flanges on each end.  A smaller tube (3.2 mm (1/8") outer diameter and 2.16 mm (0.085")
inner diameter) concentric with the outer body of the burner carried a mixture of oxygen and
ethylene to the flame.  Streams of ethylene (0.045 L/min) and oxygen (0.072 L/in) were mixed
≈150 diameters upstream of the burner exit via a mixing T-junction that was connected to 1/4"
diameter gas lines made of 316 stainless steel.  A coflow of air entered the burner orthogonally
through the main outer body 51 mm above the bottom flange.  The air flowed through the annulus
between the body wall and central tube and passed through a 6-cm deep bed of 3-mm diameter
glass beads located 72 mm downstream of the coflow inlet.  The coflow then passed through a
12.7 mm thick honeycomb flow straightener (0.88 mm cell size) at the exit plane to reduce the
turbulence level and lateral velocity components.  The exit of the fuel supply line is flush with the
exit of the honeycomb flow straightener.  To mitigate risk of flashback, the burner was first lit
without coflow and without premixing to establish a pure-ethylene-air diffusion flame.  Once the
diffusion flame was established the flow rate of oxygen was gradually increased until the desired
equivalence ratio was reached.  The C2H4-O2  jet exit velocity and jet Reynolds number were 0.8
m/s and 61.0, respectively.  The coflow velocity was adjusted until the flame appeared steady,
resulting in an approximate exit velocity and Reynolds number of 0.5 m/s and 331.6, respectively.
Figure 1(c) shows the partially premixed (φ=6.43) oxygen-ethylene flame studied here using
LAI. The image shown in Fig.  1(c) was acquired using high dynamic range (HDR) imaging to
highlight flame structure and avoid camera saturation.  This was required due to the extreme
luminosity (too bright to view without eye protection) of this flame.  The HDR images were
acquired using a Nikon D3200 camera with an AF-S Nikkor 18-55 mm lens.  For each HDR
image, 4 photos were taken with varying shutter speeds and then combined with Luminance                                                                        
HDR software to create the final image.  The shutter speed for each image was chosen to be
optimal for a unique region of the flame.
3.    Motivation for optical design
Experiments  were  conducted  without  diffusers  to  illustrate  the  challenges  associated  with
performing  mid-IR  LAI  with  coherent  laser  light.   Without  diffusers,  pronounced  Airy-disk
patterns  were  observed  in  images  of  the  QCL’s  laser  beam  (see  Fig.   2(a)).   These  and  other
diffraction-induced patterns were altered by the presence of the flame and varied in time during
scanned-wavelength  experiments.   For  example,  Fig.   2(b)  illustrates  the  signal  time  history
measured in a single pixel while scanning the wavelength and intensity of the QCL. Pronounced
oscillations  in  signal  intensity  (analogous  to  optical  fringes  formed  by  conventional  etalons)
were found in pixels located in the vicinity of diffraction patterns.  These oscillations severely
complicate in situ determination of the non-absorbing light intensity (Io).  This leads to spatially
dependent errors in the best-fit spectroscopic parameters (e.g., integrated absorbance) obtained
from the spectral-fitting routine, which leads to spatially correlated errors in the measured gas
conditions.  For example, Fig.  2(c) shows a single image of the path-integrated temperature field
(calculated from the two-color ratio of integrated absorbances) that is severely compromised by a
clear Airy-disk pattern in the middle of the flame.
One approach to avoiding these diffraction-induced image artifacts is to destroy the coherence
in the laser light by reflecting the beam off spinning diffusers.  This technique has been used in a
variety of low-speed visible-imaging applications employing laser backlighting [23–25], most
similarly, to perform low-speed (<30 frames per second) imaging of soot-volume-fraction in
flames via 2D imaging of visible (632.8 nm) laser extinction [23].  However, implementing this
strategy into a high-speed scanned-wavelength laser-absorption-spectroscopy technique has, to
our knowledge, never been done previously and this presented several challenges.  Most notably,
high angular velocities (approaching 100,000 RPM) were needed to scramble the speckle pattern
created by the diffusers (see Figs.   3(a) and 3(b)) and sufficiently homogenize the local light
field on the timescale of the short camera integration times required here (order of 10  µs).  For
example, Fig.  3(c) compares single-pixel time histories measured during a scanned-wavelength
experiment with stationary and spinning diffusers.  When the diffusers are stationary the light
field  consists of a  speckle  pattern  (see Fig.   3(a))  and large amplitude,  temporally structured
noise (see Fig.  3(c)) persists in each pixel (albeit with altered characteristics).  The latter likely
originates from wavelength-dependent optical interference.  However, by spinning the diffusers                                                                              
the speckle pattern can be sufficiently homogenized on the timescale of the camera integration
time (see Fig.  3(b)).  This leads to the signal in each pixel consisting of relatively random (i.e.,
unstructured) and lower amplitude noise (see Fig.  3(c)) which is much less problematic for the
spectroscopic fitting routine.  It is worth noting that spinning both diffusers provides only a small
gain in SNR compared to spinning a single diffuser.  However, it was critical to reflect the light
off two diffusers (e.g., 1 stationary, 1 spinning) to achieve an SNR sufficient for 2D LAI in the
flames studied here.   Figure 3(d) illustrates that using a longer camera integration time (tint)
reduces the noise level.  This is because it enables the time-varying speckle pattern to be averaged
over a longer duration.  The measurement of SNR vs integration time was least-squares fit to a
power-law function of the form SNR(tint) = atibnt.  The best-fit values of a and b were 4.82 and
0.51, respectively, and the coefficient of determination (i.e., R2) was 0.999 indicating that the
model captures the effect of integration time on SNR well.  From this analysis, it is clear that
using the highest motor speed and longest integration time possible for a given frame rate is
desirable.
It is important to note that the results shown in Fig.  3 are dependent on the laser wavelength
and diffuser grit size.  Gold-coated ground-glass diffusers with 120, 220, and 1500 grits/inch
were  all  tested.   The  diffusers  with  120  grits/inch  performed  best  (provided  a  diffuse  beam
and dense speckle pattern) when using laser light near 4.8  µm.  Conversely, the diffusers with
1500  grits/inch  performed  best  for  laser  light  near  1.4  µm,  but  performed  poorly  (providing
near-specular reflections with little speckling) for laser light near 4.8  µm.
4.    2D Laser-absorption measurements
4.1.    Validation of LAI diagnostic
The accuracy of the LAI diagnostic was validated by studying a non-reacting, isothermal, laminar
jet  of  49%  CO, 49%  Ar  and  2%  H2  flowing  into  ambient  air.   The  initial  jet diameter  was  2
mm and the exit velocity of the jet was 3.32 m/s, yielding a jet Reynolds number of 217.6.  The
spatial resolution of the LAI diagnostic was estimated by imaging a wire mesh backlit with the
incoherent mid-infrared laser light (see Fig.  4).  The known spacing between wires was used to
determine that the projected pixel size was 140  µm.  In all experiments the camera was focused
onto the central axis of the jet (determined by imaging a wire mesh).  The QCL was scanned
across the P(0,20) transition at 50 Hz and the laser light was imaged at 24 kFPS with a resolution
of 64 x 52 pixels.  The QCL’s current was scanned below the threshold current to enable the
background emission in each pixel to be measured.  The measured background emission was                                                                               
subtracted  from  each  pixel  and  the  baseline  incident  light  intensity  was  then  determined  for
each pixel by least-squares fitting a 3rd-order polynomial to the non-absorbing regions of the
intensity scan.  The absorbance spectrum measured by each pixel (e.g., see Fig.  5(a)) was then
calculated using Beer’s law.  A Voigt profile was least-squares fit to the measured absorbance
spectrum in each pixel with the transition linecenter, integrated absorbance (i.e., integrated area)
and collisional width as free parameters and the Doppler width fixed at the value corresponding
to the known temperature (296 K). With the temperature and pressure known, the integrated
absorbance corresponding to the best-fit Voigt profile was then used to calculate the mole fraction
or column density of CO using Eq.  (1).
Aproj, j  = Sj(T)P χCO L                                               (1)
Here  Aproj, j  [cm−1]  is  the  integrated  absorbance  of  transition   j,  Sj(T)  [cm−2/atm]  is  the
linestrength of transition  j at temperature T, P [atm] is the pressure of the gas,  χCO is the mole
fraction of CO, and L [cm] is the path length through the absorbing gas.  Figure 5(b) shows that
the measured CO mole fraction in the jet core (calculated assuming a constant optical path length
equal to the initial jet diameter) agrees within 3% of the known value (i.e.,  χCO = 0.49) and the
spatial standard deviation in the jet core is 3.1% of the known value.  The image also reveals the
expected structure of a laminar jet, with a near constant mole fraction in the core of the jet and
increasing (in the axial direction) transport of CO into the boundary layer.  Figure 5(c) illustrates
how the measured column density of CO varies across the jet and how it compares to the value
predicted assuming a constant  χCO = 0.49 (i.e., no mixing) and the known initial jet geometry.  At
the jet exit, the measured profile of  χCO L agrees exceptionally well with that predicted assuming
constant  χCO  and  similar  agreement  exists  further  downstream  (y =  4  mm  and  8.5  mm)  in
the middle of the jet (i.e., -0.75 mm < r < 0.75 mm), thereby supporting the accuracy of this
diagnostic.  The 95% confidence interval (obtained from the spectral-fitting routine) of  χCO L
varied between 1% and 3% across the jet.  The radial profiles indicate increasing transport of CO
into the boundary layer and this likely results from a combination of diffusion and laboratory
air  currents,  the  latter  of  which  introduced  an  obvious  asymmetry  in  the  jet  profile  on  other
occasions.
4.2.    Path-integrated measurements of 2D flame structure
2D LAI measurements of path-integrated temperature and CO column density were acquired in
laminar, partially premixed, oxygen-ethylene flames.  Figures 1(b) and 1(c) illustrate the burner
and flame studied, respectively.  The LAI measurements were acquired using a 64 x 52 pixel
window with a frame rate of 24 kFPS and an integration time of 16.4  µs.  Using this exposure
time and a bandpass filter near 2060 cm−1, the flame produced ≈12,000 counts of background
emission, which was measured with the laser off before each scan and then subtracted from each
pixel’s time history prior to calculating the absorbance corresponding to each scan.  Some slight
instabilities in the flame were observed which introduced additional noise in the signal measured
by each pixel.  As a result, multiple scans were averaged together (after background subtraction)
to further improve SNR. Figure 6(a) illustrates how averaging multiple scans together improves
the SNR. Specifically, the scan-averaged SNR improves from 24.7 to 50.4 for 15- and 50-scan
averages,  respectively.   Figure  6(b)  shows  the  corresponding  absorbance  spectrum  (15-scan
average) measured by a single pixel imaging the middle of the flame.  The absorbance spectrum
exhibits a 1-σ absorbance noise level of 0.008.  For each spectrum, the transition lineshapes were
modeled by a Voigt profile and the best-fit spectrum was determined using a least-squares fitting
routine with the transition linecenters, integrated absorbances, and collisional widths as free
parameters.  The Doppler width of each transition was fixed to the value given by the temperature
determined from the two-color ratio of integrated absorbance,  R (given by Eq.  (2)), obtained
from the previous iteration.
Here,  Aproj,P(0,20)  and  Aproj,P(1,14)  are the integrated absorbance of the P(0,20) and P(1,14)
absorption  transitions  determined  by  the  spectral-fitting  routine.   With  R and  spectroscopic
constants known, the path-integrated temperature was determined using Eq.  (3) [26]:
where h [J · s] is Planck’s constant, c [cm/s] is the speed of light, kB [J/K] is the Boltzmann
constant, E′′  [cm−1] is the lower-state energy of a given transition (taken from the HITRAN2012
Database [27]), and To is the reference temperature (296 K). Here subscripts 1 and 2 refer to the
P(1,14) and P(0,20) transitions, respectively.
With the pressure and path-integrated temperature known, the integrated absorbance of the
P(0,20) transition was used to calculate the column density of CO ( χCO L) using Eq.  (1) where the
linestrength of the P(0,20) transition was evaluated at the measured path-integrated temperature
using Eq.  (4).
Here, νo is the transition linecenter frequency [cm−1] and Q is the partition function.  It should
be noted that this approach is strictly valid in the limit that the linestrength of each transition
exhibits a linear dependence on temperature across the range of temperatures that exist along the
line-of-sight [13].
Figures 7(a) and 7(b) show images of path-integrated flame temperature for a 15- and 50-scan
average, respectively, and Figures 7(c) and 7(d) show images of CO column density for a 15-
and 50-scan average, respectively.  The spectral-fitting routine was performed in all pixels where
the peak absorbance on the P(0,20) transition was greater than 0.05,  thereby ignoring pixels
with  an  absorbance  SNR  lower  than  6.25.   The  images  illustrate  that  2D  LAI  is  capable  of
resolving the thermochemical structure of laboratory scale laminar flames.  The images reveal
that the temperature is lowest in the fuel-rich core near the burner exit and increases in both the
radial and axial directions, and peaks near the boundary with the air coflow.  These observations
are  consistent  with  the  expected  structure  of  a  laminar  diffusion  flame.    The  image  of  CO
column density is more difficult to interpret given that the flame diameter increases in the flow
direction.  Regardless, these images illustrate that, in general, the amount of CO present along a
given line-of-sight increases in the flow direction.  All of the images indicate that the flame is
approximately axisymmetric, thereby enabling a tomographic reconstruction to be performed to
provide greater detail regarding the 2D flame structure.
Tomographic reconstruction of 2D flame structure
Figure 8 shows tomographic reconstructions of gas temperature and CO mole fraction in the
flame’s radial plane for a 5-scan (Figs.  8(a) and 8(c)) and 50-scan (Figs.  8(b) and 8(d)) average.
Radial  profiles  of  temperature  and  CO  mole  fraction  are  shown  in  Fig.   8(e)  at  select  axial
locations to further illustrate the thermochemical flame structure.  These results were obtained
by applying the  tomographic reconstruction algorithms  developed by Daun et al.  [28] to  the
path-integrated measurements of integrated absorbance obtained via LAI. These alogrithms have
previously been applied to numerous flame studies [14, 19, 21].  The results shown in Fig.  8 more
clearly reveal the flame structure (compared to path-integrated results) and illustrate temperature
and species profiles that are consistent with that expected in a fuel rich, partially premixed flame.
The temperature is lowest in the core of the flame and, in general, increases in both the axial and
radial directions.  In comparison to the images of path-integrated temperature, the tomographic
reconstruction reveals that the temperature rise in the axial direction within the flame core is
more modest, increasing from 740 K to 1320 K between 1 mm and 9 mm above the burner exit,
respectively.  The temperature is highest (reaching a maximum near 2200 K) near the boundary
between the fuel-rich C2H4/O2  stream and air coflow where oxygen supplied by the coflow can
reach the unburnt fuel and intermediate species.  The reconstructions illustrate that very little CO
is present near the burner exit and that the mole fraction of CO increases significantly in both the
axial and radial directions before falling off in the boundary layer.
The remainder of this section is devoted to describing the pertinent details of the tomographic
reconstructions.  Tomographic reconstructions were performed assuming each pixel images a
unique line-of-sight.  Flame-induced beam steering could cause this assumption to breakdown
and ultimately reduce the spatial resolution of the diagnostic.  Since tomographic reconstruction
is sensitive to noise in the input image, a 2×2-pixel average was applied to all images prior to                                                                                 
averaging 5, or 50 measured spectra together (i.e., prior to performing time averaging).  This
significantly  reduced  the  noise  induced  by  beam  steering  and  the  rotating  diffusers.    Next,
the absorbance spectra were processed using a two-line Voigt fit,  as described previously,  to
determine the integrated absorbance (Aproj) of the P(0,20) and P(1,14) absorption transitions
corresponding to each pixel.  The tomographic reconstruction algorithm employed here relies on
assuming that the flame is axisymmetric.  As a result,  Aproj  only needs to be resolved along the
flame radius.  Since  Aproj  was measured here along both halves of the flame, the results for each
half were simply averaged together, however it should be noted that this has a small effect on
image quality (see Fig.  9).  The image (i.e., matrix) of each transition’s integrated absorbance
was then passed to the tomographic reconstruction algorithm as described next.
The  tomographic  reconstruction  algorithm  requires  the  assumption  that  the  flame  can  be
divided into N concentric rings of thickness Δr with a homogeneously distributed field variable,
k, which is given by Eq.  (5) for a specific absorption transition.
k = Sj(T)P χCO                                                                 (5)
k effectively represents the local integrated absorbance per unit path length and the integral of k
along a line-of-sight represents the back-projected data (i.e., the measured integrated absorbance:
Aproj).  N is given by the number of pixels that resolve the flame radius and Δr is given by the
projected pixel size.                                                                                    
First, a process called onion-peeling deconvolution is used to generate a set of equations of the
form  Ax = b.  Here,  A is the coefficient matrix which contains the path length of light through
each annular section (i.e., the absorbing path length of the line-of-sight imaged by a given pixel)
and is given by Eq.  (6),  x contains the to-be-solved-for field variable (k), and  b contains the
back-projected data (i.e., the integrated absorbance,  Aproj, measured by each pixel).
Solving  this  system  with  back  substitution  may  return  a  solution  that  is  sensitive  to  noise  in
the back-projected data which manifests as errors or noise in the reconstruction of  k.  This is
because onion-peeling deconvolution yields an ill-conditioned coefficient matrix which makes it
difficult to find a well defined global minimum [28].  To overcome this, Tikhonov regularization
is employed to increase the condition of the matrix [28].  In doing so, the equations shown below
are used to provide a more robust solution.
Here, k˜∗  is the field variable that yields the global minimum residual after Tikhonov regularization
(i.e., the solution that is ultimately used to calculate images of gas properties).  For the zeroth
order regularization used for the data presented in this manuscript,  L0  is given by the  N × N
matrix shown in Eq.  (10).
The  parameter  α0  is  called  the  regularization  parameter  and  it  controls  the  balance  between
solution accuracy and smoothness/stability [28].  This works by approximating solutions to the
ill-conditioned set of equations with those of a similar well-conditioned set.  A high value of
α0  will return a solution which is smooth, but less faithful to the original problem, and lower
values return high-fidelity but often oscillatory solutions.  As α0  → 0, k˜∗  →  k∗  where k∗  is the
field variable that yields the true minimum residual.  Although methods exist for determining
an  optimum  value  for  α0  [29],  the  value  is  often  chosen  at  the  user’s  discretion  to  obtain  an
acceptable balance of accuracy and smoothness [28].  Figure 9 illustrates how increasing α0  from
0 to 1 improves image quality but reduces spatial resolution.  In addition, Fig.  9 also illustrates                                                                                    
how averaging both halves of the absorbance data together (prior to reconstruction) and using
a 2x2 pixel average both slightly improve image quality without significantly altering the final
images of  χCO.  The images presented in Fig.  8 were obtained using α0  = 0.1 since this produced
solutions with acceptable stability while preserving the vast majority of flame structure that was
revealed using α0  = 0.
The  solution  for  each  individual  transverse  plane  (i.e.,  row  of  pixels)  was  stable,  however
differences between adjacent planes gave the final 2D images a somewhat noisy appearance.  This
noise was attenuated by applying a 6-point moving average filter along the axial direction of the
2D images of k˜∗  for each absorption transition.  Figure 10 illustrates the row-to-row oscillations
in k˜∗  for both absorption transitions near the flame’s central axis and how the 6-point moving
average is effective at removing the oscillations while preserving flame structure.
After reconstructing the 2D image of k˜∗  for each absorption transition, the two-color ratio of
k˜∗, Rk˜∗ , was calculated using Eq.  (11).
The final 2D image of temperature was obtained by replacing R with Rk˜∗   in Eq.  (3).  Next, the
2D image of  χCO was obtained using Eq.  (5) with k replaced by k˜∗P(0,20)  and using the 2D image
of temperature to calculate the corresponding image of the P(0,20) transition’s linestrength.
The SNR of the measured absorbance for the P(1,14) transition is low in the center of the flame
near the exit of the burner since the gas is relatively cool and little CO is present there.  This
prevented a reliable reconstruction of the 2D temperature field in a 1.7 mm wide region located
0 to 3 mm above the burner exit.  As a result, 224 pixels of the temperature data in this region
were replaced with values extrapolated from a 2nd-order polynomial which was obtained via
least-squares fitting to the remainder of the data set.  A total of 224, or 6.7% of the temperature
image presents a value that was obtained via extrapolation.
Conclusion
This work has demonstrated, for the first time, that mid-infrared LAI is capable of providing 2D
measurements and tomographic reconstructions of flame temperature and carbon monoxide in
axisymmetric flames without mechanical translation of the line-of-sight.  This was achieved by
reflecting scanned-wavelength laser light off two ground-glass diffusers spinning at 90,000 RPM
prior to the light traversing the flame.  This was done to break the coherence in the laser light
and, therefore, prevent the formation of diffraction-induced image artifacts which have limited
previously developed LAI techniques to 1D (simultaneously) imaging.  Ultimately this approach
enabled  measurements  of  CO’s  absorbance  spectra  to  be  acquired  at  50  Hz  across  ≈3,300
lines-of-sight simultaneously.  While additional time averaging of spectra was done to improve
the  final  image  quality,  this  work  demonstrated  that  this  technique  can  provide  high-fidelity
2D measurements and tomographic reconstructions of temperature and CO mole fraction on
near-cm scales with a projected pixel size of 140  µm and a time resolution of 0.1 to 1 seconds.
In comparison to conventional mid-IR LAT techniques employing mechanical translation of the
line-of-sight, this technique offers a reduction in data collection time of ≈1,000× and potential
for a ≈10× gain in spatial resolution compared to LAT methods that are limited by the laser beam
diameter.  In short, the capabilities of this 2D mid-IR LAI technique represent a significant step
towards achieving high-speed, high-resolution measurements of thermochemical flame structure
via LAT.

CFD simulation of gas每solid bubbling fluidized bed: an extensive assessment of drag models 
Abstract 
In the computational fluid dynamics modeling of gas每solid two phase flow, drag force is one of the dominant mechanisms for interphase momentum transfer. Despite the profusion of drag models, an extensive comparison is missing from the literature. In this work the drag correlations of Syamlal-O＊Brien, Gidaspow, Wen-Yu, Arastoopour, Gibilaro, Di Felice, Zhang-Reese and Koch et al. are reviewed using a multifluid model of FLUENT software with the resulting hydrodynamics parameters being compared with experimental data. Also adjustment of drag models based on minimum fluidization was studied. A new method adopted to adjust the drag function of Di Felice showed a quantitative improvement compared to the adjusted drag model of Syamlal-O＊Brien. Prediction of bed expansion and pressure drop showed excellent agreement with results of experiments conducted in a Plexiglas fluidized bed. A mesh size sensitivity analysis with varied interval spacing showed that mesh interval spacing with 18 times the particle diameter and using higher order discretization methods produces acceptable results. 
Keywords:  multiphase flow, fluidization, computation, modeling, CFD, drag models, two-dimensional. 
1 Introduction 
Studies conducted on the dynamics of a single particle in a fluid have proven several mechanisms of momentum transfer between phases: drag force, caused by velocity differences between the phases; buoyancy, caused by the fluid pressure gradient; virtual mass effect, caused by relative acceleration between phases; Saffman lift force, caused by fluid-velocity gradients; Magnus force, caused by particle spin; Basset force, which depends upon the history of the particle＊s motion through the fluid; Faxen force, which is a correction applied to the virtual mass effect; Basset force to account for fluid velocity gradients; and forces caused by temperature and density gradients [1]. Several factors should be considered in extension of single particle model to describe interaction forces in multi-particle systems, including the effect of the proximity of other particles, which implies that the drag force is a function of solid volume fraction, in addition to the particle Reynolds number. Also the single-particle interaction force must be corrected to account for the effect of mass transfer between the phases, and the momentum transfer accompanying such mass transfer must be included in the interaction force. Buoyancy, drag, and momentum transfer due to mass transfer have been considered as controlling mechanisms of gas每solid momentum transfer, since they are the dominant forces as a result of the large density difference between the particles and the fluidizing gas and also due to lack of satisfactory formulations of the other forces. Whilst the inherent instabilities due to inclusion of buoyancy are still not resolved, prediction of a drag model that covers the whole range of Reynolds number and phasic volume fraction has been looked at as the main challenge of numerous of the studies in multiphase flow modeling [2]. These attempts have resulted in the appearance of a substantial number of drag correlations in the literature.  
The copiousness of drag models available in the literature and the selective attitudes of some researchers have resulted in some inconsistencies regarding the appropriate comparison of available drag models. Almost all the available studies have included efforts to compare two, or at most three, drag correlations, and occasionally the discrepancies between the reported results in modeling fluidization hydrodynamics are easily observed. In this respect, the underlying objective of this study is to accomplish an extensive assessment of frequently used drag correlations in a large selection of published literature and provide a comprehensive comparison between simulation and experimental results using the variety of the drag models. Also, a new approach to adjust the drag model, based on minimum fluidization velocity, is proposed and compared with experimental values. CFD simulation was carried out using the commercial CFD code, FLUENT.  
2 Model equations 
The drag force depends on the local relative velocity between phases and void fraction and some other factors, such as particle size distribution, particle shape, etc. However, void fraction dependency is very difficult to be determined for any conditions other than a packed bed or infinite dilution (single particle). Also, some factors, like particle size distribution, particle shape, and particle clustering have not been considered in deriving drag correlations.  In an ideal case, it could only be determined how the drag for specific material varies with local ※slip§ velocity and packing, although, totally unrealistic. On the other hand, most researchers have information on the minimum fluidization velocity of their own material.  In this respect, Syamlal and O＊Brien [3] introduced a method to adjust drag law using minimum fluidization velocity as a calibration point. This adjustment has been introduced in order to make the drag law more accurate for a specific system under study. However, this method requires measurement of the minimum fluidization velocity and void fraction of the bed at minimum fluidization velocity by means of experimentation. As another alternative based on the same concept used by Syamlal and O＊Brien [3], we developed the following method to adjust the Di Felice drag model. Di Felice [4] expressed the drag coefficient model as the product of drag force on an unhindered particle subjected to the same volumetric flux of fluid and a voidage function: 
where f (汐)  is defined as 
and the empirical coefficient (x) as a function of Res is expressed as 
In the absence of gas-wall friction and solid stress transmitted by the particles, the momentum balance at minimum fluidization can be written as follows [5]: 
Buoyancy = Drag 
At the minimum fluidization velocity, considering that
Plugging the drag model into Equation (7) and utilizing a nonlinear optimization algorithm the drag model parameters P & Q in Equation (3) can be adjusted for the system under study using experimental data at minimum fluidization velocity. However, when adjusting the drag models it should be kept in mind that the adjustment should not alter the behavior of the drag correlation when voidage approaches 1. Most drag correlations are formulated such that in that limit, the single sphere CD can be recovered. 
3 Experimental set-up 
The experimental set-up used in this study has been shown in fig. 1. Experiments were carried out in the Department of Chemical and Biological Engineering at the University of British Columbia. The Column is a 2D Plexiglas of 1.2 m height, 0.28 m width, and 0.025 m thickness. Spherical glass beads of 250每300 米m diameter and density 2500 kg/m3 were fluidized with air at ambient conditions. Pressure drops were measured using three differential pressure transducers located at elevation 0.03, 0.3, and 0.6 m above the gas distributor, respectively. The static bed height of 0.4 m with a solid volume fraction of 0.6 was used in all the experiments. Pressure drop and bed expansion were monitored at different superficial gas velocities ranging from 0 to 0.8 (m/s).  
Figure 1: Geometry of 2D Plexiglas fluidized bed. 
4 Simulation set-up 
The two-dimensional (2D) geometry was discretized using 13440 structured rectangular cells. Performing a grid size sensitivity analysis using different mesh sizes, 5 mm mesh interval spacing was chosen for all the simulation runs. The discussion on the effect of the different mesh sizes has been brought up in a later section.  A preliminary case study proved that using fixed time step; in order of magnitude 10-3, which has been reported in literature, is not sufficient to avoid the instability in convergence for 2D multiphase simulations. Therefore, an adaptive time-stepping algorithm with 100 iterations per each time step was used to ensure a stable convergence. The automatic determination of the time step size is based on the estimation of the truncation error associated with the time integration scheme (i.e., first-order implicit or second-order implicit). If the truncation error is smaller than a specified tolerance, the size of the time step is increased; if the truncation error is greater, the time step size is decreased.  A minimum value of order 10-5 was used for the lower domain of time step. A convergence tolerance of 10-4 for each scaled residual component was specified for the relative error between two successive iterations. The governing equations were solved using the finite volume method. The Phase-Coupled SIMPLE algorithm (PC-SIMPLE) [6], which is an extension of the SIMPLE algorithm to multiphase flow, was applied for the pressure-velocity coupling. In this algorithm, the velocities are solved, coupled by phases, in a segregated fashion. Subsequently, the block algebraic multigrid scheme used by the couple solver was used to solve the equation formed by the velocity components of phases at the same time. Also, a pressure correction equation is built based on total volume continuity. Pressure and velocities are then corrected so as to satisfy the continuity constraint. Second-order upwind discretization schemes were used for all the simulation runs. Including the adjusted drag model cases, 9 drag correlations in total, were studied in this work (i.e., Arastoopour, Di Felice, Gibilaro, Gidaspow, Syamlal-O＊Brien, Wen-Yu, Zhang-Reese, Koch et al.). FLUENT employed an approximate CPU time of 32 hours for 30 s of real-time simulation at a mean time step of 0.0005 s on a double core Sun Microsystems workstation W2100Z with 2 AMD/Opteron 64-bit processors and 4 GB RAM. 
5 Results and discussion 
Experimental runs were conducted to measure the pressure drop and bed expansion ratio, H/H0, at different superficial gas velocities. The gas-phase volume fraction from pressure drop measurement across the bed was obtained [7]. At experimentally determined minimum fluidization velocity, Umf = 0.065 m/s, the overall pressure drop, bed expansion ratio, and voidage found to be 4.4 KPa, 1.1, and 0.5, respectively. A wide range of gas superficial velocity (0.011-0.75 m/sec) was considered to measure these parameters. The CFD simulations were carried out using the transient Eulerian-granular model in FLUENT v6.3. Several superficial gas velocities, 0.11, 0.21, 0.38, and 0.46 m/s, which correspond to 1.6, 3.2, 5.8, and 7Umf, respectively studied.  
 The drag coefficient values as a function of solid volume fraction for different drag models are plotted in Fig. 2. All the drag functions show a rising trend of drag coefficient value with increasing the solid volume fraction. The values of drag coefficients were calculated at a typical Reynolds number, Res =10. At low volume fraction of solids (<0.18), excluding the Syamlal-O＊Brien adjusted model, which overestimated and the Arastoopour function, which slightly underestimated the drag coefficient values, all the drag models represent almost the same value of drag function. Also, it should be noted that at the limit of extremely dilute suspension all the drag models approach the single particle drag value. For the values of solid volume fraction above 0.2, the Di Felice adjusted model precipitously separates from the other model toward the higher values of drag coefficient. This trend continues until it crosses the Syamlal-O＊Brien adjusted drag model at the value of solid void fraction equal to 0.46.  From this point on, Di Felice adjusted model gives the highest values of the drag coefficient. With the exception of the Syamlal-O＊Brien adjusted drag model which shows an decreasing trend regarding the slope of the curve for values of solid volume fraction greater than 0.14, all other models show an approximately constant slope (i.e. linear growth in drag coefficient value). It is also noted that the differences among the drag models mainly occur when the solid volume fraction is higher than 0.2. The graph also reveals that adjustment of drag models based on minimum fluidization velocity results in the prediction of higher values of drag coefficient through the whole range of solid volume fraction. 
Figure 2: Variation of drag coefficient vs. solid volume fraction in different drag laws. 
5.1 Bed expansion 
Fluctuation and vigorous motion of the bed surface in fluidized beds have made the determination of bed expansion by visual observation a challenging task. The general method employed to determine the bed expansion is normally based on the bed voidage measurement, which in turn is deduced from the mean pressure drop [8]. Also, bed height has been measured experimentally by means of the overhead observation of a probe tip. Such measurement has been believed to be highly biased, and, more importantly, no standard errors or deviations of data have been reported [8]. Fryer and Potter [9] reported that the experimental technique might well underestimate the bed expansion due to the diffusing characteristic of the bed surface. The other frequently employed method is to plot the time-mean gauge pressure (single-point pressure) against the height of the pressure transducer taps, where the intersection of the two slopes corresponds to the height of the expanded bed. However, this method requires an adequate number of pressure transducers at different elevations along the bed and freeboard [10]. In case of a limited number of pressure transducers, Ellis [10], using the time-mean differential pressure drop data across a certain interval inside the dense bed and across another section extending from the lowest pressure tap to a tap in the freeboard, adopted the following correlation to estimate the expanded bed height: 
However, due to the fact that this technique relies on a single pressure drop measurement inside the bed, one should be more cautious about the accuracy of data rather than the method based on the gauge pressure measurement profile.  To determine the bed expansion, from modeling perspective, we considered the height of the bed that contains 95% of the bed weight as the bed height. The results of this method, as reported by Syamlal and O＊Brien [8], are not sensitive to the percent bed-weight value chosen within a small range, due to the fact that most of the time, experimental values are reported as bed expansion percent rather than as actual bed height. For this series of simulations, a static bed height of H0=0.4 m over a range of superficial velocities 11.7, 21, 38, and 46 cm/s was used. All the simulations show the correct qualitative behavior of bed expansion. Ascending trend of bed expansion with increasing superficial gas velocity can be observed from the graph (fig. 3). Also, all the available drag correlations with the exception of two adjusted drag models (i.e. Di Felice and Syamlal-O＊Brien) and the original Di Felice drag model at high superficial gas velocity (0.46 m/s), underestimate the bed expansion. 
Figure 3: Comparison of simulated bed expansion ratio with experiment data. 
5.2 Pressure drop 
In order to eliminate the large temporal fluctuation of pressure drop in the early seconds of the simulation, the time-average of pressure drop for comparing simulation and experimental results were taken after statistical steady state was established. Numerical results for all the models showed that 3 s of simulation is adequate to reach statistical steady state behavior. Time-averaging was carried out over a range of 4-20 s of real time simulation.  As indicated in Fig. 4, the pressure drop inside the bed between two specific elevations (i.e. 0.03 m and 0.3 m as demonstrated in Fig. 1) for all the models showed a declining trend with increase of the superficial gas velocity, which is in good qualitative agreement with the experimental data.Here again, two adjusted drag models (i.e. Di Felice and Syamlal-O＊Brien) showed their superiority in predicting the pressure drop inside the bed. At higher gas velocities, 7Umf, deviation from reported experimental data was observed, which may be explained by the underestimation of the effect of particle clustering at high superficial gas velocities and the influence of the gas distributor at higher velocities. 
Figure 4: Pressure drop inside the bed (.P = P . P ).
5.3 Grid size sensitivity analysis 
To study the effect of mesh size resolution on numerical results, a grid size sensitivity analysis was carried out using three distinctive mesh intervals spacing of 5 mm, 4 mm, and 2 mm for 20 seconds of real-time simulation. The results indicate that the grid size spacing selected for simulation in this work (i.e. 5 mm) was adequate for satisfactory prediction of the hydrodynamics in computational geometry. On the other hand, the results did not support the previously proposed criteria (i.e. adequacy of mesh size less than or equal to 10 times the particle diameter) in the literature, [8,11], for CFD simulation of fluidized beds. Table 1 compares the time required for 20 s of real-time simulation. Required time for simulating 20 seconds of 2D fluidized bed drastically increases from 32 hr to almost one week for a decrease in grid interval spacing from 5 mm to 2 mm, respectively. 
Table 1: Grid size sensitivity results. 
Mesh spacing (mm)  .P1 (kPa)  .P2 (kPa)  Voidage  Simulation time (hr)  
2mm  3.36  5.50  0.55  168  
4mm  3.38  5.37  0.54  80  
5mm  3.39  5.39  0.54  32  
6 Conclusion 
The influence of most widely used drag functions, including the Wen-Yu, Gidaspow, Di Felice, Syamlal-O＊Brien, Zhang-Reese, Arastoopour, Gibilaro, Koch et al. models, on CFD simulation of a 2D fluidized bed using FLUENT software was studied. All the models showed an acceptable qualitative agreement with the experimental data. Also, adjustment of drag models based on minimum fluidization velocity showed a quantitative improvement in prediction of hydrodynamics parameters. In this respect, the new method of adjustment based on minimum fluidization velocity in absence of gas-wall friction and solid stress transmission applied on the Di Felice drag model showed excellent agreement with experimental results regarding the prediction of bed expansion and pressure drop inside the bed. The mesh size sensitivity analysis carried out in this study demonstrated that even grid interval spacing of 18 times of the particle size (i.e. 275 米m) was able to give acceptable results which is contradictory with some other mesh size sensitivity analyses reported in the literature. However, further modeling efforts are required to study the influence of other parameters such as gas distributors, which have not been studied; comparison of 2D and 3D modeling of fluidized bed reactors and also, effect of particle size distribution which has been underestimated using the mean particle diameter. Moreover, new experimental studies should be carried out using recent advancements in instrumentation engineering in order to resolve the available experimental discrepancies reported in the literature such as void fraction measurements, bed expansion ratio etc. 
Nomenclature 
References 
[1] Johnson, G., Massoudi, M. & Rajagopal, K.R., A review of interaction mechanisms of fluid-solid flows, U.S. DOE, 1990. 
[2] Syamlal, M., Rogers, W. & O＊Brien, T. J. MFIX Documentation Theory Guide, 1993. 
[3] Syamlal, M. & O＊Brien, T. J. Derivation of a drag coefficient from velocity- voidage correlation; U.S. Department of Energy, Office of Fossil Energy, National Energy Technology Laboratory, Morgantown, WV; April, 1987. 
[4] Di Felice, R., The voidage functions for fluid-particle interaction system. International Journal of Multiphase Flow, 20 (1), pp. 153每159, 1994. 
[5] Gidaspow, D., Multiphase Flow and Fluidization: Continuum and Kinetic Theory Descriptions. San Diego: Academic Press, 1994. 
[6] Vasquez, S. A., Ivanov, V. A. A phase coupled method for solving multiphase problems on unstructured meshes. In Proceedings of ASME FEDSM＊00: ASME 2000 Fluids Engineering Division Summer Meeting; ASME Press: New York, 2000. 
[7] Yang W.C., Handbook of fluidization and fluid-particle systems, New York: Marcel Dekker Inc., 2003. 
[8] Syamlal M., O＊Brien T., Fluid dynamic simulation of O3 decomposition in a bubbling fluidized bed. AIChE J, particle tech. and Fluidization, 49(11), pp. 2793-2801, 2003. 
[9] Fryer, C., and O. E. Potter, Experimental investigation of models for fluidized bed catalytic reactors. AIChE J., 22(1), pp. 38-47, 1976. 
[10] Ellis N., Hydrodynamics of gas每solid turbulent fluidized beds, PhD thesis, The University of British Columbia, February 2003. 
[11] Zimmermann, S. & Taghipour, F., CFD modeling of the hydrodynamics and reaction kinetics of FCC fluidized bed reactors. Ind. Eng. Chem. Res., 44(26), pp. 9818-9827, 2005. 




Engineering Applications of Computational Fluid Mechanics 


Characteristics of Secondary Flow Induced by 90-Degree Elbow in Turbulent Pipe Flow 
Jongtae Kim, Mohan Yadav & Seungjin Kim 
To cite this article: Jongtae Kim, Mohan Yadav & Seungjin Kim (2014) Characteristics of Secondary Flow Induced by 90-Degree Elbow in Turbulent Pipe Flow, Engineering Applications of Computational Fluid Mechanics, 8:2, 229-239, DOI: 10.1080/19942060.2014.11015509 

To link to this article: https://doi.org/10.1080/19942060.2014.11015509 


Copyright 2014 Taylor and Francis Group LLC 

Published online: 19 Nov 2014. 
Submit your article to this journal 


Article views: 4477 
View related articles 

View Crossmark data 


Citing articles: 24 View citing articles 


Full Terms & Conditions of access and use can be found at https://www.tandfonline.com/action/journalInformation?journalCode=tcfm20 


CHARACTERISTICS OF SECONDARY FLOW INDUCED BY 90-DEGREE ELBOW IN TURBULENT PIPE FLOW 
Jongtae Kim^*, Mohan Yadav#, and Seungjin Kim# 
^Severe Accident and PHWR Safety Research Division, Korea Atomic Energy Research Institute, Republic of Korea # Mechanical and Nuclear Engineering Department, The Pennsylvania State University, USA *E-Mail: ex-kjt@kaeri.re.kr (Corresponding Author) 
ABSTRACT: The purpose of this study is to characterize the swirling secondary flow in the downstream of a pipe bend using a numerical simulation of the flow. The CFD (Computational Fluid Dynamics) software OpenFOAM is used to simulate the turbulent flow in pipes with elbow. Various turbulence models are benchmarked with the existing experimental data and a comparative study is performed to select an appropriate turbulence model for the analysis. Predictions made by the selected turbulence model are compared with the LDA (Laser Doppler anemometer) measurements from the experiments currently conducted to find the dependency of the flows on the Reynolds number. It is found that the swirl intensity of the secondary flow is a strong function of the radius of curvature of the bend and a weak function of the Reynolds number. Additionally, it is found that the dissipation of the swirl intensity is exponential in nature. 
Keywords: secondary flow, 90-degree bent elbow, swirl intensity, CFD 
1. INTRODUCTION 
A 90-degree-bent elbow is commonly used in many piping systems to connect different components. It is well known that during the flow of fluid in a pipe with elbow, a secondary motion of the flow is developed and it is superimposed on its primary streamwise flow. The secondary flow is generated due to an imbalance between centrifugal force and pressure gradient near the bent-pipe wall in the radial direction of the bend curvature. In general, the structure of the secondary flow is dependent on the radius of the bend curvature (Rc) and Reynolds number (Re) based on the pipe diameter (D) and bulk velocity (Ub). When the non-dimensional radius of curvature of the bend is greater than 1.5 (i.e. Rc/D > 1.5), the secondary flow consisting of a pair of counter-rotating vortices are generated. At the same time, the velocity profile of the primary streamwise flow is distorted and shifted away from the center of the curvature of the elbow. If Rc/D is smaller than 1.5, the flow becomes unsteady because of a flow separation occurring immediately downstream of the bend (Weske, 1948; Hellstrom et al., 2011). The flow in the downstream region of a 90-degree bend is important for the primary and secondary cooling systems of nuclear power plants, where many bends are used to interconnect the components. The distortion in the streamwise velocity profile of the coolant by an elbow can affect the distribution of heat transfer rates. In the case of a bubbly two-phase flow which may occur during an accident in a nuclear power plant, the distribution of the bubbles in the pipe with an elbow is very different compared to that in a pipe without an elbow. Due to the inherent difference in density of the two phases, the gas bubbles are driven toward the centers of the two counter-rotating swirls generated by the elbow (Yadav, 2013). This accumulation of bubbles can increase the possibility of a bubble collision and subsequently alter the interfacial heat and mass transfers. Furthermore, when the curvature radius of an elbow is small enough to create a fluctuating flow after the elbow, the downstream pipe can undergo oscillating mechanical stress, which can lead to structural fatigue in the piping system (Yamano et al., 2009). In the past, several researchers have investigated turbulent flows in pipes with elbows by means of theoretical, experimental, and numerical methods. Weske (1948) experimentally investigated the velocity distribution at the outlet of elbows with various shapes of cross-sections including round, elliptical, square, and rectangular cross sections at 

Received: 22 Jul. 2013; Revised: 12 Oct. 2013; Accepted: 9 Dec. 2013 
Reynolds numbers ranging from 0.2 to 0.6 ¡Á 106 for the design of an aircraft duct. Sudo et al. (1998) performed the experiments for turbulent flow in a 90-degree-bent pipe with Rc/D = 2. The study used a hot-wire anemometer to measure streamwise velocity (Us) and circumferential velocity (Uc) for a turbulent flow with Reynolds number of 60,000. Al-Rafai et al. (1990) performed an experimental and numerical study of a turbulent air flow in circular pipe bends to investigate the influence of Rc/D on the flow. During the experiments, two bends with Rc/D = 
3.49 and Rc/D = 6.975 were used, and the flow had the Reynolds number of 34,132. Homicz (2004) performed numerical simulations of the flow through a 90-degree elbow with a Rc/D =1.4 at a Reynolds number of 5.4¡Á105 to validate the Flow-Accelerated Corrosion (FAC) test conducted by Korea Atomic Energy Research Institute (KAERI). These simulations and FAC tests are used to better characterize the flow-accelerated corrosion/erosion in the pipe elbows. More recently, Tanaka et al. (2009) applied a large eddy simulation (LES) approach using a standard Smagorinsky model with a wall function law for flows in elbows with various Rc/D at several Reynolds number conditions. Most of the above-mentioned studies focused only on the flow structure in the immediate vicinity of the elbow and only a limited number of studies investigated the flow in the downstream region of the elbows (Sudo et al., 1998; Al-Rafai et Al., 1990). The knowledge of the flow structure in the downstream region of the elbow is of significant importance to understand the dissipation characteristics of the secondary flow and to accurately assess the development of the flow structure. Additionally, an understanding of the dissipation characteristics is important in evaluating the length required for the elbow effects to be dissipated and in modeling the effects associated with flow restrictions. In view of the above, the objectives of the current study are to perform experimental and numerical studies on turbulent pipe flows after 90-degree elbows and to characterize the secondary flows induced by the 90-degree elbows including the dissipation characteristics. In order for that, it is necessary to choose an appropriate turbulence model specifically for the turbulent flow in a pipe with an elbow. Unexpectedly, there are few papers which are devoted to a numerical study of the turbulent pipe flow through an elbow. In this study, a quantitative assessment method based on hit rate using the experimental data of Sudo et al. (1998) is applied to choose an appropriate turbulence model among the available turbulence models in OpenFOAM CFD code. 

2. EXPERIMENTAL FACILITY AND MEASUREMENT 
As mentioned above, the flow structure after an elbow depends on the Reynolds number and bend curvature. An experiment is conducted to evaluate its dependency on the Reynolds number. 

Fig. 1 Experimental facility and locations of LDA measurement. 
A schematic diagram of the experimental setup, as well as the co-ordinate system used for the current study, is shown in Fig. 1. The detailed description of the test facility is presented in previous studies (Yadav, 2013) and only characteristic features of the test facility are presented here. The experimental facility consists of both upstream and downstream sections constructed from 50.8mm inner diameter (D) acrylic pipes and interconnected by a 90¡ã glass elbow. The elbow has a radius of curvature of 152.4mm (3D), and the lengths of the upstream and downstream sections are 3.35m (L/D = 66) and 9.1m (L/D = 180), respectively. A laser leveling system and a digital level are employed to ensure the alignment of both the upstream and downstream sections within ¡À0.1¡ã. The flow rate for the experiments is measured by an electromagnetic flow meter, with an accuracy of ¡À0.5% of the full-scale reading. 
Table 1 Specifications of LDA employed for experiments. 
Measurement Length  300 mm  
Beam width  2.0 mm  
Measurement Volume  0.1 (mm)¡Á0.1 (mm)¡Á1.0 (mm)  
Maximum Velocity  27 m/s  
Measurable Velocity Fluctuation  From 0.7 ¦Ìm/s to 4.6 mm/s.  


An integrated Laser Doppler anemometer (LDA), which is capable of one-dimensional velocity measurements, is used to measure the axial water velocity. The light beam in the LDA is generated by 35 mW laser with a wavelength of 600 nm. The principal characteristics of the LDA are listed in Table 1. It should be noted that due to the difference in the refractive index of air, acrylic, and water, the LDA measurement volume inside the pipe is shifted by a finite amount. This shift is 
accounted for by employing Snell¡®s law. The 
tracking/seeding particles consist of Polyamide particles with a size distribution of around 20¦Ìm and specific gravity of 1.04. To ensure the statistical reliability of the measurements, a minimum of 5000 samples are measured at each local point and the validation of the burst signals was above 98%. As indicated in Fig. 1, the measurements are performed at 13D upstream of the elbow (53D from inlet) to figure out a fully-developed velocity profile and 3.5D, 10D, and 50D downstream of the elbow in the downstream section. The LDA is mounted on a two-dimensional traversing system, which has a resolution of 1¦Ìm in both traversing directions. This traversing system is further attached to a plate, which is designed with mounting holes at every 22.5¡ã in the azimuthal direction. As such, the LDA along with the traversing system can be attached to the mounting plate to measure the velocity across entire pipe cross-section. Along each azimuthal direction, the LDA is traversed to obtain measurements at 23 radial positions. These radial positions are selected such that a finer measurement mesh is obtained in the near wall region, where a higher velocity gradient is expected. 
Table 2 List of flow conditions investigated in current study. 
Test No.  Ub [m/s]  Re  
Run1  1.00  50800  
Run2  2.00  101600  
Run3  4.00  203200  

A total of three flow conditions, as shown in Table 2, are investigated in the current study. These flow conditions are selected to study the effect of increasing Reynolds number on the velocity profiles. It is expected that the swirl intensity and its decay characteristic in a pipe with a 90-degree bend are dependent on Reynolds number of flow and curvature radius of the bend. To evaluate the dependencies, the experiment of Sudo et al. (1998) was referenced. In their experiment, a bend with a curvature radius of 2D was used, and the flow velocity was measured at a Reynolds number of 60000 based on the pipe diameter and bulk velocity. The experimental facility used in this study is equipped with an elbow of 3D curvature radius. The test condition of Run1 is chosen with a bulk velocity of 1 m/s to obtain a similar Reynolds number. The experiment of Sudo et al. (1998) and Run1 in the present experiments were conducted at a similar Reynolds number with different curvature radii of 2D and 3D, respectively. The conditions of other tests, Run 2 and Run 3, are chosen by doubling the bulk velocity to scrutinize the effect of the Reynolds number. 
3. NUMERICAL  SIMULATION  OF  
TURBULENT FLOWS  IN  90-DEGREE- 
BENT PIPE  

The turbulent flow in a 90-degree-bent pipe is complicated by the secondary flow, which is generated by an imbalance of the centrifugal force and pressure gradient in the radial direction of the bend curvature. Additionally, the streamwise flow is distorted by the existence of adverse pressure gradient along the convex wall of the pipe bend. These make an accurate numerical simulation of the flow difficult, especially for turbulent flows with high Reynolds number. To select an appropriate turbulence model for the numerical simulation of a turbulent flow in a pipe with an elbow, validation of the numerical results from turbulence models is very important. In the current study, the experiments conducted by Sudo et al. (1998) on the turbulent flows in a bent pipe are used to validate the turbulence models. In addition, the selection criteria which involve a quantitative assessment are used to perform a comparative study among the tested turbulence models. The selected turbulence model is then used for evaluation of swirl characteristics in the downstream of an elbow. 

3.1 Numerical simulation of Sudo et al.¡¯s experiment 

3.1.1 Test conditions and numerical simulation 
Sudo et al. (1998) performed experiments for turbulent flow in a 90-degree-bent pipe with Rc/D = 2. A straight pipe-section with 100D of development length was installed in the upstream of the bend to provide a fully developed turbulent flow to the bend. The experiments were performed with air as the working fluid and flow Reynolds number Re = 60000. The study used a hot-wire anemometer to measure the streamwise velocity (US) and circumferential velocity (UC) at several locations along the pipe.   The open source CFD code OpenFOAM (OpenCFD, 2012) is used to simulate Sudo et al.¡®s experiments. OpenFOAM solves the incompressible Navier-Stokes equations discretized in the context of the cell-centered finite volume method (FVM). Numerical solutions are iteratively sought by pressure correction method named SIMPLE algorithm. The standard k-¦Å turbulence model is applied to choose a suitable convective scheme and under-relaxation parameters of the discretized equations. The computational domain is composed of an upstream pipe with 20D length and a downstream pipe with 50D length connected to an elbow as shown in Fig. 2. The boundary conditions at pipe inlet are set as follows: Velocity is fixed with a fully developed power-law profile which is from the experimental data of Zagarola and Smits (1999) for fully developed turbulence pipe flow. The values of the turbulent kinetic energy and its dissipation rate are calculated from the inlet turbulence intensity (5% of inlet bulk velocity in this study) and pipe diameter. 


Fig. 2 Computational geometry for simulation of 90o-bent pipe flow. 

Fig. 3 Computational mesh near pipe exit. 
The mesh for the bent pipe is generated by stacking a two-dimensional unstructured mesh of pipe cross-section along the streamwise direction to minimize the non-orthogonality of the mesh. The two-dimensional unstructured mesh of the pipe cross-section is constructed with quadrilateral cells. The constructed mesh of the bent pipe is shown in Fig. 3 where the surface mesh near the pipe exit is magnified to visualize the orthogonality of the mesh. The value of the non-dimensional distance from the pipe wall based on the friction velocity , Y+, for a near-wall cell is strictly controlled to be in the range of 20 ~ 50, when a wall-function is used. However, the Y+ value for a near-wall cell is set below 1 in the case without a wall-function. The distribution of mesh cells along the streamwise direction is selected based on iterative grid-dependency tests. So, the mesh size used in this study is depending on the use of a wall function. The mesh used for a high-Re turbulence model is composed of 152,150 hexahedral cells. But for a low-Re turbulence model, the number of cells is 562,080. Fig. 4 shows the numerical results of the Sudo et al.¡®s experiment by using the standard k-¦Å turbulence model. Fig. 4a is streamwise velocity profiles on cut-planes near the elbow, and Fig. 4b is streamlines to show the swirling flow. From the figures, it can be expected that the primary streamwise flow and secondary swirling flow are correlated in the elbow pipe flow. 

Fig. 4 Numerical results for experiment of Sudo et al. (1998): (a) streamwise velocity profiles, (b) streamlines. 


A comparison between the calculated and measured streamwise velocity profiles along the 
symmetric lines (i.e. ¦È=90¡ã) in the downstream 
region of the elbow are shown in Fig. 5. In the figure, the positive and negative r/D indicates the outer and inner part from the center of the circular pipe section, respectively. It should be noted that 
the horizontal length of the legend .+¡® used for 
the measured data in Fig. 5 indicates a ¡À10% error range normalized by the bulk velocity. It is found that the streamwise velocity components from the standard k-¦Å turbulence model agree very well with the measured data in the outer region with positive radii. On the contrary, some discrepancies between the numerical and measured data in the inner region with negative radii are observed. These discrepancies may arise due to the adverse pressure gradient near the inner wall of the pipe bend.  


Fig. 6 Comparisons of streamwise velocity components along cross lines at x/D=1, x/D=3, x/D=5, x/D=7, and x/D=10. Solid line: standard k-¦Å turbulence model, +: Experimental data of Sudo et al. (1998). 


Fig. 7 Comparisons of circumferential velocity components along cross lines at x/D=0, x/D=0.5, x/D=1, x/D=2, and x/D=5. Solid line: standard k-¦Å turbulence model, +: Experimental data of Sudo et al. (1998). 
Figs. 6 and 7 show the streamwise and circumferential velocity components, respectively, 
along the cross lines (i.e. ¦È=0¡ã) in the downstream 
region of the bend pipe. The circumferential velocity component means secondary or swirling velocity component. The velocity profiles along the cross lines are predicted well at different locations in the downstream region of the elbow. 


3.1.2 Assessment of turbulence models 
The main objective of the present study is to characterize the secondary flow in the downstream of the elbow through numerical simulations. As such, after selection of mesh size, suitable convective scheme and under-relaxation parameters, it is of great importance to select appropriate turbulence model that can simulate the flow across the elbow accurately.  In view of this, OpenFOAM contains various turbulence models, which include one-equation and two-equation models based on Boussinesq assumption, Reynolds stress transport model (RSTM), and large eddy simulation (LES). It is still challenging to apply LES for a high Reynolds number turbulent flow in a very long pipe. In this study, the RSTM turbulence models are excluded from the assessment because of numerical and turbulence model problems in obtaining an acceptable solution, and remain as future work. For the assessment, only the linear and non-linear eddy-viscosity turbulence models are considered. In Table 3, the turbulence models to be used for the assessment are listed. 
Table 3 Turbulence models used for simulation of Sudo et al.¡®s experiment. 
No  Abbr.  Turbulence model  
1  SKE  Standard k-¦Å with wall function  
2  EKE  Extended k-¦Å with wall function (Chen and Kim, 1987)  
3  RNGKE  RNG k-¦Å with wall function (Yakhot and Orszag, 1986)  
4  RKE  Realizable k-¦Å with wall function (Shih et al., 1995)  
5  KMSST  k-¦Ø SST with wall function (Menter, 1994))  
6  SNKE  Shih¡®s nonlinear k-¦Å with wall function (Shih and Zhu, 1993)  
7  LCKE  Lien cubic k-¦Å with wall function (Lien et al., 1996)  
8  LSLOW  Launder-sharma low-Re k-¦Å (Launder and Sharma, 1974)  
9  LBLOW  Lam-Bremhorst low-Re k-¦Å (Lam and Bremhorst, 1981)  
10  LCLOW  Lien cubic low-Re k-¦Å (Lien et al., 1996)  
11  LLLOW  Lien-Leschziner low-Re k-¦Å (Lien and Leschziner, 1993)  


Turbulence models can be grouped based on modeling of near wall flow and turbulent eddy viscosity. In Table 3, turbulence models numbered 1 through 7 utilize a wall-function to model the near wall region and the remaining models simulate the near-wall region without a wall-function. Shih¡®s model (No. 6 in Table 3) (Shih et al., 1993) and Lein¡®s models (Nos. 7 & 10 in Table 3) (Lien et al., 1996; Lien and Leschziner, 1993) are the non-linear models, where eddy viscosity is related by more than second moment of velocity gradient. 

Fig. 8 Comparison of streamwise velocity profiles from different turbulence models along symmetric line at x/D = 0. 

Fig. 9 Hit rates of turbulence models for streamwise velocity components on symmetric lines of Sudo et al.¡®s bent pipe, ---: average hit rate (74.3%). 
Fig. 8 shows the numerical results from different turbulence models for streamwise velocity profiles along the symmetric line at x/D = 0. The numerical results from some of the turbulence models listed in Table 3 provide reasonable 
agreement compared with Sudo et al.¡®s experimental data and Tanaka et al.¡®s LES results. 
However, it is difficult to choose an appropriate turbulence model from the qualitative comparison. As such, a quantitative assessment method is applied to select an appropriate turbulence model. Each turbulence model is implemented to simulate Sudo et al.¡®s experiments and a comparative study is performed between numerical simulations and experimental measurements. The number of points within the error bounds of the experimental data is counted. 
A comparison is called a ¨Dhit¡¬ if the numerical 
result lies within ¡À10% of the bulk velocity from the measured value, otherwise it is called a 
¨Dmiss¡¬. 


The probing locations and measured data used for the quantitative assessment of the turbulence models are plotted in Figs. 5, 6, and 7 which denote the three assessment objects, i.e., streamwise velocity component on the symmetric lines and streamwise and circumferential velocity components on the cross lines. The number of points used in each assessment is 135. Figs. 9, 10, and 11 show the hit rates for each turbulence model, where the turbulence models are sorted by the number of hits. In the assessment of streamwise velocity along the symmetric lines, SNKE has the highest number of hits followed by LCKE and RKE. It is found that along the cross 




Fig. 14 Dependency of streamwise velocity components on Reynolds number along cross lines (¦È=0¡ã) at x/D=3.5, 10, and 50, ¡ö: Exp. Run1 (Ub=1m/s, Re=50,800), ¡Á: Exp. Run 2 (Ub=2m/s, Re=101,600), +: Exp. Run 3 (Ub=4m/s, Re=203,200). 
without wall functions, such as LSLOW, LLLOW, LBLOW, and LCLOW, have a higher than average number of hits. However, in the assessment of circumferential velocity along the cross lines, the low-Re number turbulence models perform poorly and KMSST & RNGKE record the highest number of hits. The above assessment shows that some of the turbulence models consistently perform below the average. These turbulence models are eliminated from the future assessments. Additionally, it is not likely to find a single turbulence model with maximum number of hits for all three assessments. As such, two additional criteria are applied for the selection of an appropriate turbulence model. The first step utilizes the average number of hits for each assessment and the second criterion considers the deviation from the maximum hit rate. The deviation of each assessment, 
, from the maximum hit rate is given by: 

(1) 
where xi, xmax, and N represent the hit rate for the ith turbulence model, maximum hit rate and the number of turbulence models used in the assessment, respectively It is found that RNGKE has consistently greater than average hit rates for the assessment of all the three velocity profiles. Although SNKE has highest number of hits for the streamwise velocity along the symmetric lines, it does not predict other velocity profiles satisfactorily. Therefore, SNKE is not considered for the final calculations. Four turbulence models namely, EKE, RNGKE, RKE, and KMSST satisfy the criterion based on the deviation from the maximum hit rate. In order to select a single turbulence model out of these four, the selection criterion is changed to reduce the range of deviation by 2/3 of ¦Ò. Based on this quantitative assessment RNGKE turbulence model is selected for investigating the flow in the elbow. This is consistent with the previous study (Homicz, 2004), which has shown that RNGKE turbulence model performs better than SKE for flow in bends. Also in the simulation of Rahimzadeh et al. (2012) of a turbulent flow over circular spillways, it is seen that RNGKE turbulence model gives very similar results to experiment and RSTM model. 





3.2 Analysis of current experiment 

lines, the streamwise velocity is predicted well by most of the turbulence models except SNKE and The experiment on a turbulent flow in a pipe with LCKE. The low-Re number turbulence models a 90-degree-bent elbow conducted in this study is numerically simulated with the RNGKE turbulence model, which was chosen through a quantitative evaluation of the simulation results of 
Sudo et al.¡®s bend pipe. Numerical and 
experimental results are shown in Figs. 12 and 13 for the case of Run1 where the bulk velocity is 1 m/s and its Reynolds number is 50,800. Streamwise velocity distributions along vertical symmetric lines at 3.5D, 10D, and 50D in the horizontal pipe after the elbow are plotted in Fig. 
12. It is seen in the figure that the streamwise velocity is shifted upward just after the elbow, and is redistributed downstream. Fig. 13 shows the streamwise velocity distributions along the horizontal cross lines at the same locations as in Fig. 12. The symmetry of the flow is easily observed from the figure. The streamwise velocity near the pipe wall is a little augmented because of its reduction in the central region of the pipe section by swirls at 3.5D. However, the peaks near the pipe wall are diffused by turbulent dissipation at 10D. It looks like the calculated results by the currently used numerical schemes and turbulence model match well with the measured data. From a comparison of the velocity profiles, it can be thought that the turbulence model used in this study gives mostly acceptable results for the flow in a bend pipe. To evaluate the dependency of a flow after the elbow on the Reynolds number, the streamwise velocities normalized by the bulk velocity of each test are compared in Fig. 14. It is shown that the velocity profiles from the three tests are coincident with each other, which means that there exists a similarity in the structure of flows after a bend with the same curvature radius if the Reynolds number is in the range of 50000 and 200000. Furthermore, it is found that the normalized velocity at the center of the pipe at 50D downstream of the elbow decreases with increasing Reynolds number. This happens because in a fully developed turbulent pipe flow the flattening of the velocity profile increases with Re. This observation is also consistent with the recent experiments in super-pipe flow facility (Zagarola and Smits, 1997). The streamwise velocities from a numerical simulation of the three test cases Run1, Run2, and Run3 are compared in Fig. 15. The velocity profiles along the cross lines at three different locations downstream of the elbow coincide well with each other, as do the measured data. It is also shown in the figure that the velocity near the center of the pipe at 50D downstream is flattened more as the Reynolds number increases. 

Fig. 15 Dependency of streamwise velocity components on Reynolds number along cross lines (¦È=0¡ã) at x/D=3.5, 10, and 50, ©¥©¥: Calc. Run1 (Ub=1m/s, Re=50,800), ..: Calc. Run 2 (Ub=2m/s, Re=101,600), ¡¤¡¤-¡¤¡¤: Calc. Run 3 (Ub=4m/s, Re=203,200). 


Fig. 17 Decay of swirl intensity along straight pipe after elbow with Rc = 2D. 



4. CHARACTERIZATION OF SECONDARY FLOW INDUCED BY 90-DEGREE BEND 
In this section, the dissipation characteristics of secondary flow in the downstream region of the 90-degree elbow are discussed. The secondary flow generated during the flow across the elbows consists of two counter-rotating vortices or swirls. In general, the strength of a swirl on a cut plane normal to the axial flow direction is represented by the swirl number (Chigier and Beer, 1964). The swirl number is the area-averaged flux of angular momentum and it is used to quantify the strength of a swirl around the axis of the main flow. However, a direct application of the swirl number to counter-rotating vortices such as those created by the secondary flow in an elbow is not appropriate. To define the intensity of counter-rotating swirls, Sudo et al. (1998) used the magnitude of velocity tangential to a pipe section area. Eq. (2) shows the definition of the swirl intensity used for the elbow flows in this study. 

(2) where 
is the swirl intensity, 
is the vector of 
the flow velocity and 
is a unit vector normal to the pipe section area. To obtain the area-averaged swirl intensity at each pipe section along the downstream pipe after the elbow, the flow solution is mapped to a two-dimensional mesh with the same area as the pipe section because of the use of unstructured meshes in this study, and the tangential velocities from the mapped solution are integrated on the 2-D mesh. Fig. 16 shows planar 2-D meshes inserted in the computational domain for a pipe flow. In the figure, the dark-colored areas mean the 2-D meshes used for area-averaged swirl intensities. The intensity of secondary flow calculated from Eq. (2) at different axial locations downstream of the elbow is normalized by the value of intensity at the exit of the elbow. The normalized swirl intensity for elbow with Rc/D = 2 along the downstream pipe after the elbow is depicted in Fig. 17. It can be seen that the swirl intensity generated by the elbow is exponentially diminished along the downstream pipe. The numerical result by the RNG k-¦Å turbulence model reproduces well the decay characteristic of 
the swirl obtained by Sudo et al.¡®s experiment.  
In the same way, the decay characteristic of the secondary flow downstream of the elbow with curvature radius of 3D, which represents the experimental facility used in this study, is evaluated only based on the numerical solutions because the swirl components of the flow velocities are not measured in the experiment. The swirl intensities for the three test conditions (Run1, Run2, and Run3) were calculated along the downstream horizontal pipe. Transitions of the swirl intensities along the pipe for the three cases (50800 ¡Ü Re ¡Ü 203200) are shown in Fig. 18. In the comparison of the swirl intensities among the cases, as shown in the figure, they show similar trends of swirl intensity decay to the case of 2D curvature radius of elbow with a minor dependency on the Reynolds numbers of the flows. It is found that the dissipation of Is for all flow conditions can be expressed as an exponentially decreasing function as 

(3) 
where the coefficient 
represents the rate at which the secondary flow intensity decreases 
downstream of the elbow. The numerical value of 
is found to be 0.21 to obtain a best fit between the data and correlation presented in Eq. (3). Based on Eq. (3), 
dissipates to 10% of its initial value at approximately 11D downstream of the elbow. To investigate the effect of other geometric parameters, such as the radius of curvature, the results are compared with the previous velocity measurement of Al-Rafai et al. (1990) with LDA in bent pipe flow with two different radii of curvature, RC/D = 3.49 and 6.98, at Re = 34132.  Additionally, numerical simulations of the flow were performed by using the commercial CFD code Phoenics. Turbulent flows in the 90-degree bent pipes with the same geometric and physical conditions as in Al-Rafai et al.¡®s experiment are simulated using the RNG k-¦Å turbulence model and it is found that the current numerical method and the turbulence model give very acceptable results. Here, only the swirl intensities from the computed results are introduced and compared with other results. Table 4 shows the list of experiments used to characterize the swirl intensity of secondary flow. In the list, the radius of the elbow curvature is in the range of 2D to 7D, and the Reynolds number is from 30000 to 200000. In Fig. 19, the swirl intensities along the downstream pipes of the elbows are shown for the cases listed in Table 4. In the cases of curvature radii lower than or equal to Al-Rafai et al.¡®s smaller bend (3.49D), the swirl intensities show a very similar trend of decay even with different Reynolds numbers. However, in the case of Al-Rafai et al.¡®s larger bend (6.975D), it decays very quickly compared to the other cases. 




Table 4 Tests and their conditions used for study of swirl characterization. 
Test  Radius of elbow curvature (Rc)  Reynolds number (Re)  
Current  3D  50000 ¡«200000  
Sudo et al.  2D  60000  
Al-Rafai et al.  3.49D, 6.975D  34132  

The swirl intensities at the exit of the elbows for 
the five cases (one for Sudo et al.¡®s study, three 
for the current experiments, and two for Al-Rafai et al.¡®s study) are compared in Figs. 20 and 21. It is observed that the swirl intensity at the exit of the elbows has a weak dependence on the flow Reynolds number and a higher dependence on the radius of curvature of the bend. However, additional studies are necessary to provide a correlation between swirl intensity and the elbow radius of curvature. 


5. CONCLUSIONS 
In this study, experimental and numerical work on a turbulent flow in a pipe with a 90-degree elbow was performed. In order to select an appropriate turbulence model for the turbulent flow in the bent pipe, a quantitative assessment method based on hit rate was applied by using Sudo et al.¡®s experimental data. The assessment showed that the RNG k-¦Å turbulence model gives good results for primary streamwise velocity and secondary swirling velocity profiles compared to other turbulence models. The non-linear turbulence models showed a good predictability for the streamwise velocity components but they gave poor results in secondary swirling velocity components. As a future work, it is required to study the applicability of the non-linear turbulence models for complicated swirling flows. The experiments on turbulent flows in the pipe with an elbow with a curvature radius of 3D were conducted by varying the Reynolds number, and numerically simulated using the RNG k-¦Å turbulence model to evaluate the dependency of a flow after the elbow on the Reynolds number. It was found from the experimental and numerical study that there is a strong similarity in streamwise velocity profiles in the range of Reynolds number from 50,000 to 200,000. Using the numerical method and turbulence model selected from this study, the characteristics of swirl intensity after an elbow were examined. It was found that the swirl intensity defined in terms of area averaged tangential velocity decreases exponentially along the flow after the elbow, dissipating quicker as the radius of the elbow curvature is larger. It was also found that this dissipation characteristic depends weakly on the Reynolds number in the range of Reynolds numbers investigated in this study and strongly on the bend curvature. The swirl intensities at the exit of the elbows also display similar characteristics. 


REFERENCE 
1. 
Al-Rafai WN, Tridimas Y D, Woolley NH (1990). A study of turbulent flows in pipe bends. Journal of Mechanical Engineering Science 204:399-408. 

2. 
Chen YS, Kim SW (1987). Computation of Turbulent Flows Using an Extended k-. Turbulence Closure Model. NASA CR-179204 USA. 

3. 
Chigier NA, Beer J M (1964). Velocity and static pressure distribution in swirling air jets issuing from annular and divergent nozzles. Journal of Basic Engineering 4:788-796. 

4. 
Hellstrom LHO, Sinha A, Smits AJ (2011). Visualizing the very-large-scale motions in turbulent pipe flow. Physics of Fluids 23:011703. 

5. 
Homicz GF (2004), Computational Fluid Dynamic Simulations of Pipe Elbow Flow. SAND Report, SAND2004-3467, USA. 

6. 
Lam CKG, Bremhorst KA (1981). Modified form of k-¦Å model for predicting wall turbulence. ASME Journal of Fluids and Engineering 103:456-460.  

7. 
Launder BE, Sharma BI (1974). Application of the energy dissipation model of turbulence to the calculation of flow near a spinning disk. Letters in Heat and Mass Transfer 1(2):131-138. 


8. Lien FS, Leschziner MA (1993). Computational modeling of 3D turbulent flow in S-diffuser and transition ducts. Engineering Turbulence Modeling and Experiments 2 (Edited by W. Rodi and F. Martelli), p. 217, Elsevier Science Publishers. 
9. 
Lien FS, Chen WL, Leschziner MA (1996). Low-Reynolds-number eddy-viscosity modeling based on non-linear stress-strain/vorticity relations. Proc. 3rd Symposium on Engineering Turbulence Modeling and Measurements, 27-29 May, Crete, Greece, 91-100. 

10. 
Menter FR (1994). Two-equation eddy-viscosity turbulence models for engineering applications. AIAA Journal 32(8):1598¨C1605. 

11. 
OpenCFD (2012). OpenFOAM: The Open-source CFD toobox, programmer¡®s guide 2.1.  

12. 
Rahimzadeh H, Maghsoodi R, Sakardeh H, Tavakkol S (2012). Simulating flow over spillways by using different turbulence models. Engineering Applications of Computational Fluid Mechanics 6(1):100-109. 


13. 
Shih TH, Zhu J (1993). A Realizable Reynolds Stress Algebraic Equation Model. NASA TM-105993, ICOMP-92-27. 

14. 
Shih TH, Liou WW, Shabbir A, Zhu J (1995). ¨DA new k-¦Å eddy-viscosity model for high Reynolds number turbulent flows¡ªModel development and validation. Computers and Fluids 24(3):227-238. 

15. 
Sudo K, Sumida M, Hibara H (1998). Experimental investigation on turbulent flow in a circular- sectioned 90-degree bend. Experiments in Fluids 25:42-49. 

16. 
Tanaka M, Ohshima H, Monji H (2009). Numerical investigation of flow structure in pipe elbow with large eddy simulation approach. Proceedings of the ASME 2009 Pressure Vessels and Piping Division Conference PVP2009, July 26-30, Prague, Czech Republic, PVP-77598. 

17. 
Weske JR, (1948). Experimental Investigation of Velocity Distributions Downstream of Single Duct Bends. NACA TN-1471, USA. 

18. 
Yadav MS (2013). Interfacial Area Transport across Vertical Elbows in Air-water Two-phase Flow. Ph.D thesis, PSU, USA. 

19. 
Yakhot V, Orszag SA (1986). 

20. 
Yamano H, Tanaka M, Ono A, Murakami T, Iwamoto Y, Yuki K, Sago H, Hayakawa S (2009). Unsteady elbow pipe flow to develop a flow-induced vibration evaluation methodology. Int. Conf. on Fast Reactors and Related Fuel Cycles, Kyoto, Japan, 291-292. 

21. 
Zagarola MV, Smits AJ (1997). Scaling of the mean velocity profile for turbulent pipe flow. Physics Review Letters 78:239-242. 


Renormalization  group  analysis  of  
turbulence:  I. Basic  Theory.  Journal  of  
Scientific Computing 1(1):1-51.  





CO-COMBUSTION OF COAL AND OIL SHALE BLENDS IN CIRCULATING FLUIDIZED BED BOILERS 
Abstract.
Coal co-firing experiments were conducted in a 250 MW oil shale fired circulating fluidized bed combustion (CFBC) boiler. The objective of the experiments was to test whether adding coal to oil shale would allow the use of the latter with lower heating value. Bituminous coal was mixed with oil shale and fed into the boiler via existing fuel feeding ports. Two test series were accomplished: 11每29% thermal input of coal mixed with 8.4 MJ/kg oil shale (standard fuel), and 12每32% thermal input of coal mixed with 7.5 MJ/kg oil shale. During the experiments, which lasted in total for 15 days, ash samples were collected and flue gas analysis was performed. The boilers were able to continue work with all the fuel mixtures, but a significant increase of nitrogen oxides (NOx) emissions and heat losses due to unburnt carbon in the bottom and fly ashes were observed. The heat losses can be reduced by upgrading the fuel preparation system, but NOx emissions limit can be reached only with installation of an additional DeNOx system. The ash chemical composition remained similar. Sulphur emissions stayed minimal, but a slight increase of carbon monoxide concentration was noticed. Coal co-firing is possible in oil shale CFBC boilers, but the coal must have low fuel itrogen content and extra attention to the fuel preparation system has to be paid. 
Keywords: circulating fluidized bed combustion, oil shale and coal blend, co-combustion, emissions. 
1. Introduction 
Estonian oil shale (OS) industry has long-term experience in oil shale utilization. Today＊s industry has implemented circulating fluidized bed com-bustion (CFBC) technology, older pulverized combustion (PC) power units have been equipped with modern DeSOx and DeNOx equipment. Transition to the CFBC technology resolved problems with fouling of heating sur-faces, and NOx and SO2 emission. CFBC boilers have demonstrated fuel flexibility 每 biomass and peat co-combustion decreased gaseous pollutants, particulate matter and ash emissions [1, 2]. The oil shale CFBC boiler has also been used for utilization of pyrolytic wastewater. Successful com-bustion tests were conducted, the NOx and SO2 emissions remained low [3]. The power industry is trying to decrease its environmental impact while still keeping in use old PC power units although it has been found that PC boilers have even a higher environmental impact during load cycling [4, 5]. Considering this, co-combustion of biomass in a CFBC unit offers a good alternative to impact reduction. A number of biomass co-firing examples are available [6每9], the biomass co-combustion experiments showed positive impact also at oil shale power plants [1]. The motivation for coal co-firing in oil shale boilers enables the use of oil shale with lower heating value in the existing boilers without major retrofits. Calorific value of oil shale depends on oil shale layers and mining locations. Therefore, oil shale beneficiation plants are needed [10]. The use of oil shale in Estonia is limited and is directed towards oil production. One reason for the use of coal in OS CFBC energy units is that power companies try to expand the portfolio of fuels and leave more OS for shale oil production. Prior to industrial testing, the Department of Energy Technology, Tallinn University of Technology conducted a laboratory test on its own laboratory scale 60 kW CFB combustion test facility. The laboratory test gave the input to industrial tests and enabled choosing a proper methodology and paying attention to possible problems. Laboratory tests revealed there was a problem with nitrogen emissions. To explore the possibilities of coal and oil shale co-combustion in a CFBC boiler oil shale was mixed with coal. At the second stage, oil shale with a slightly lower heating value was used. The results and conclusions of the experiments are summarised in this paper. 
2. Experimental 
2.1. The boiler 
The firing tests of oil shale and coal blends were carried out in the CFBC double boiler energy unit. The electrical capacity of the energy unit was 215 MWel. The main parameters of the power unit are given in Table 1. Table 1. Main parameters of the power unit Parameter  Value  Steam mass flow (primary/secondary), kg/s Primary/reheat steam pressure, MPa Primary/reheat steam temperature, During the tests, the analyses of fuel, ash and flue gas were carried out. The location of ports (1每8) for collecting ash samples is shown in Figure 1. Fuel samples were collected on a daily average basis. The ash was sampled from several extraction ports located in the furnace chamber, super-/reheater (SH, RH), economizer (ECO), air preheater (APH) and from all four fields of the electrostatic precipitator (ESP). Samples of fly ash for determining the mass division (total suspended particulates PM 10/2.5) were collected after ESP. The samples were used for determining the detailed chemical composi-tion of ash. 
Fig. 1. CFBC boiler sampling points. 
The results of analyses were averaged to reach a representative estimate. During the tests, the major process parameters of the boiler and energy unit were recorded with the plant data acquisition system. The temperature and composition of flue gas were measured before ESP. The composition of flue gas and the flue gas moisture content were determined applying a Fourier transform-infrared spectroscopy (FTIR)-type analyser for wet gas, at a tem-perature of 180 ∼C. 
2.2. Fuels 每 oil shale and coal 
Table 2 and Table 3 present the results of proximate and ultimate analysis of the fuels. 
Carbon was present in both the organic and mineral matter of oil shale, the same applied to coal. During oil shale combustion the mineral (CO2)m is released as a result of decomposition of carbonate minerals. The average extent of carbonate minerals decomposition was around 70%, varying strongly in different ash flows [11]. The main element in the oil shale ash was calcium. Calcium oxide content in the coal ash was considerably lower (Table 4). 
Table 2. Proximate analysis of fuels 
Parameter  Symbol  Unit  Coal 19.7 MJ/kg  Oil shale 7.5 MJ/kg  Oil shale 8.4 MJ/kg  
Lower heating value, as received fuel Moisture content Ash content Volatiles Fixed carbon 
Table 3. Ultimate analysis of fuels, wt% 
Element  Coal  Oil shale  Oil shale  
19.7 MJ/kg  7.5 MJ/kg  8.4 MJ/kg  
Carbon  62.1  27.3  29.6  
Hydrogen  4.04  2.74  2.79  
Nitrogen  1.75  0.08  0.06  
Sulphur  0.53  1.41  1.45  
Total inorganic  0.29  5.16  5.62  
carbon (TIC)  
Table 4. Chemical composition of fuels ashes, wt% 
Component as  Oil shale  Oil shale  Coal   
oxide  7.5 MJ/kg  8.4 MJ/kg  19.7 MJ/kg  
Na2O  0.2  0.1  1.2  
MgO  5.4  2.4  3.0  
Al2O3  10.3  1.8  27.5  
SiO2  31.8  8.3  45.9  
P2O5  0.2  1.1  
SO3  3.9  3.6  
K2O  3.8  0.6  2.3  
CaO  39.7  54.5  8.0  
TiO2  0.5  1.4  
MnO  0.1  0.1  
Fe2O3  4.1  3.5  5.6  
The results of thermogravimetric analysis (TGA) of coal and oil shale are shown in Figure 2. The figure reveals that the coal ignition occurred at a rather low temperature (< 200 ∼C) and the combustion was finished before the temperature reached 650 ∼C. No later char burning could be noticed, there was only a small mass loss due to decomposition of carbonate minerals. Based on the data no problems with fuel burnout were predicted. The fuel mixtures for full-scale experiments (Table 5) were prepared on the fuel conveyor belt. The experiments lasted for 15 days. Each fuel mixture was fired for at least 24 h. During the experimental period, a large number of samples were collected. 
Fig. 2. Thermogravimetric analysis of oil shale (dashed line) and coal (black solid line) combustion in the air at a heating rate of 10 K/min. 
Table 5. Fuel test matrix of co-combustion tests 
Fuel blend  Symbol  Wt%  % thermal input  Lower heating value, Qir  
OS 8.4  A  100  100  8.4   
OS 8.4/coal OS 8.4/coal OS 8.4/coal  B C D  95/5 90/10 85/15  89/11 79/21 71/29  9.0* 9.5* 10.1*  
OS 7.5/coal OS 7.5/coal OS 7.5/coal  E F G  95/5 90/10 85/15  88/12 77/23 68/32  8.1* 8.7* 9.3*  
3. Results and discussion 
3.1. Temperatures of bed, furnace, separators and air preheater 
Coal co-firing experiments were conducted in the oil shale fired industrial CFBC boiler. Overall the experiments were successful, no emergency shut downs occurred. The plant data acquisition system provided temperature readings from various locations. To illustrate changes in the CFBC boiler some chosen temperatures are shown in Figure 3. The temperature above the dense bed increased with the increased share of coal. In contrast, the flue gas temperature in the entrance of the solid separator and after the air preheater was stable. The average temperature values support this notice (Fig. 4). The increased temperatures may lead to the fouling and sintering of ash on heating  surfaces,  which  were  typical  problems in oil shale PC  boilers [9]. 
Fig. 3. The temperatures: a) in the bed nearby the front wall, b) in the furnace dilute 
zone, c) in the entrance of the solid separator, d) after the air preheater. The darker 
colour represents more coal in the fuel blend. 
Fig. 4. The average temperature trends depending on the heat share of coal: a) in the bed nearby the front wall, b) in the furnace dilute zone, c) in the entrance of the solid separator, d) after the air preheater. 
Experiments carried out by Parve et al. [12] showed that sintering of Estonian oil shale ashes became an issue when the temperature exceeded 900 ∼C. The temperatures measured in the boiler were well below the dangerous zone, but there were no measuring points in the centre of the boiler where the temperatures may be higher. Investigations carried out by Al-Otoom et al. [13] have shown that firing of Jordanian El-Lajjun oil shale would pose minimal operational problems related to sintering and agglomeration if the operating temperature is between 800 ∼C and 900 ∼C. 
3.2. The chemical composition of ash samples 
Typically, there is no problem with oil shale burnout, the organic carbon content in the boiler ashes is below 0.1% [1]. The results of chemical analysis of ash show relatively high levels of organic carbon, Corg (Table 6). When obtaining the ash samples, an unusual picture was taken of the bottom ash conveyor (Fig. 5). Orange glowing particles and black particles were seen. Unburnt particles were visible to the naked eye. This encouraged in depth fuel preparations control.  As seen in Figure 6, oversized coal particles 
Table 6. Chemical composition of ashes (85/15 oil shale/coal blend), wt% 
Ash  Com-pound/ element  CO2  CCO2 TC *  ** Corg  SO3 SiO2  R2O3 CaO  CaOfr MgO K2O  Na2O  Cl  
Bottom ash  14.93  4.07  4.70  0.63  9.41  14.03  5.65  49.88  20.68  5.06  1.8  0.33  0.16  
ESP1  3.33  0.91  1.15  0.24  5.33  34.00  14.16  29.6  13.49  6.1  2.44  0.24  0.21  
ESP2  3.15  0.86  1.11  0.25  5.25  36.16  13.93  31.38  12.71  6.03  3.55  0.48  0.37  
were found in the fuel. Particle size analysis showed that a higher share of coal in the fuel mixture resulted in larger particles (Fig. 7). The maximum particle diameter allowed for the boiler by the design was 15 mm. The fuel preparation system was not able to crush coal as efficiently as required because Oil Shale Hard Groove Index (HGI) is considerably lower when compared with coal. The mineral composition of ash is presented in Table 7. The composition of ash flows, i.e. bottom ash (BA) and ashes of electrostatic precipitator field 1 (ESP1) and field 2 (ESP2), is similar to that of ash from pure oil shale combustion. The calcite content is typically higher in BA, dolomite exists only in BA and is absent in fine ash fractions (ESP1 and ESP2). The anhydrite (CaSO4) content is the highest in the ESP2 ash, which indicates the binding of sulphur to finer ash fractions. 
Chemical analysis of the ash samples showed a measurable organic carbon content (Fig. 8). Based on on-site observations, unburnt organic carbon was expected in the bottom ash, but fly ash contained also some unburnt organic carbon. The carbon content and share of coal in the fuel mixture correlated. Heat losses due to unburnt fuel during experiment D (29% heat from coal) were 0.79%. This value is low compared to the typical values for coal boilers. The heat losses due to unburnt fuel when working with oil shale are as low as 0.02% [1]. 
Table 7. Mineral composition of ashes (85/15 oil shale/coal blend), wt% 
Ash Compound  CaCO3  CaO  CaSO4  MgO CaMg(CO3)2  KAlSi3O8  Fe2O3  Ca2Al[AlSiO7]  SiO2 KCl  
Bottom ash ESP1 ESP2  44.69 8.50 8.01  8.57 16.98 16.65  3.62 6.38 7.22  3.41 3.70 3.88  2.63 0% (< 0.5) 0% (< 0.5)  1.34 6.25 5.11  0.2% (< 0.5) 1.01 0.78  0.00 2.18 2.03  3.06 8.30 6.49  0.03 0.43 0.3  
Fig. 7. Share of particles with a 
Fig. 8. Total organic carbon content in the diameter over 14 mm, %. 
bottom ash and ESP1 depending on the heat share of coal in the fuel mixture. 
Ash samples were collected and analysed, but no major changes in its chemical composition were noticed. The ashes were similar to typical oil shale CFBC ashes described by Plamus et al. [11]. The insignificant influence of coal additive on ash chemical composition could be explained by the relatively low coal ash content (15%) compared to oil shale＊s (55%). Nevertheless, some trends were still observed in the bottom ash: manganese had a positive correlation and chloride a negative correlation with coal mass share. The higher share of coal in the fuel mixture resulted in an increased content of sulphur, sodium, phosphorus, manganese, magnesium, aluminium and titanium in the fly ash. The content of potassium in the fly ash decreased. Although a number of trends were noticed, the changes were minor and should not alter the physical or chemical properties of the ashes. 
3.3. Flue gas and emissions 
The values of average concentrations of major gas emissions are presented in Table 8. The flue gas did not meet environmental requirements (NOx > 200mg/Nm3 at 6% O2) due to the high nitrogen oxides concentration (Fig. 9). The data analysis showed that nitrogen oxides concentration correlated with nitrogen content in the fuel mixture (Fig. 10). So the excess NOx was formed due to additional nitrogen in the fuel. The temperature in the boiler was around 800 ∼C, thus the nitrogen in air did not contribute to the NOx formation. The nitrogen content in oil shale was low (< 0.1%). Adding any fuel with a higher nitrogen content necessitated the use of a DeNOx solution. Sulphur dioxide emissions remained nearby the quantification limit of a standard FTIR gas analyser (5 ppm). The coal sulphur content was rather low and in addition, oil shale contained a lot of calcium that binds sulphur well in the CFBC boiler. It is possible to co-fire oil shale in the CFBC boiler with coals containing more sulphur without extra sulphur removal devices. 
Table 8. Average concentration of main pollutants in flue gas (dry, 6% O2), mg/Nm3 
Fuel  Heat share of coal, %  CO  NOx  SO2  
Fig. 10. Nitrogen oxides concentration Fig. 11. Carbon monoxide content in the in the dry flue gas compared with the dry flue gas. nitrogen content of the fuel mixture. 
tional CFB combustion [16]. The nitrogen conversion ratio to NOx was found to be up to 24% and the emissions were rather low [16]. Investigations by Al-Makhadmeh et al. [17] have shown that in case of Jordanian El-Lajjun oil shale the NOx emissions are lower under oxy-fuel conditions than in air-firing mode. 
Estonian oil shale has a molar Ca/S ratio of 8每10 and no further addition of lime is necessary for the complete binding of sulphur. The sulphur content of El-Lajjun oil shale is nearly four times higher than that of Estonian oil shale. CaO and ash contents are comparable in both fuels. Al-Makhadmeh et al. [18] have found that a Ca/S molar ratio of 3 is sufficient to obtain the desulphurisation efficiencies up to 93每100%. To bind the sulphur it is necessary to add limestone. 
The content of total suspended particles (TSP) remained within the same limits, 30每40 mg/Nm3, independently of fuel blends. The content of the finest particles of fly ash together with its mass division (PM10/2.5) after ESP is an important indicator of flue gas composition. The mass division of the finest fly ash after ESP was determined for three coal and oil shale blends: 5/95, 10/90 and 15/85 (Fig. 12). It can be concluded that the dis-tribution of fine fly ash does not depend on the fuel blend. 
3.4. Issues and solutions 
The most important problem was the high nitrogen oxides concentration in the flue gas. There are no cheap ways to solve it. If it is necessary to use more nitrogen containing fuel in the power plant, then the DeNOx equipment must be installed. In the CFB boiler, the temperatures are considerably lower than in the PC boiler, but the technology requires temperature between 760 ∼C and 1090 ∼C. From Figure 3 it can be seen that the temperatures in the boiler stayed below the required values. It might be possible to increase the temperature of the gases, but it would be a technical challenge. There are no easy ways for reduction of nitrogen oxides emission for this type of technology. 
When co-firing coal and oil shale, two further problems were related to the presence of unburnt combustibles in the ashes and increased carbon monoxide concentration in the flue gas. Both increased the boiler heat losses, but were not as crucial as excess nitrogen monoxides in the flue gas. The TGA (Fig. 2) and proximate analysis (Table 2) suggested that there should be no problem with fuel burnout since coal had a high volatile content and its ignition and burnout temperatures were low. Coal was basically similar to oil shale; the only major difference was its considerably lower contents of ash and carbonate. However, the fuel-handling system for OS was not able to prepare the coal fuel. It resulted in oversized particles in the fuel feed and increased unburnt carbon content in the bottom ash. Upgrading the fuel handling system can reduce unburnt carbon in the bottom ash. Increasing the temperature in the furnace and balanced fuel feeding may decrease the losses due to unburnt fuel in the fly ash and carbon monoxide in the flue gas. 
4. Conclusions 
Coal co-firing with oil shale in an industrial CFBC boiler was performed. The maximum heat share of coal in the fuel blend was 31% and maximum mass share 15%. During the experiments the boiler maintained a stable load of 250 MWth (83每89 kgsteam/s). The flue gas emission did not meet the environmental requirements due to increased NOx concentration. The NOx concentration in the flue gas correlated strongly with the nitrogen content in the fuel mix. This indicates that the formation mechanism of NOx did not change. The average SO2 concentration in the flue gas remained close to the detection limit. The fuel preparation system could not provide the fuel of required quality, it contained oversized coal particles. This resulted in unburnt carbon in the ash and increased heat losses. The average CO concentration correlated with the coal share in the fuel mix, but remained in all cases below 70 mg/Nm3. The ash produced was similar to the typical calcium rich oil shale ash. 
The chemical analysis of the ash samples showed a measurable organic carbon content especially in BA, but also in the ESP ash. The highest TOC content was 0.6%. Typically, there was no problem with oil shale burnout, the organic carbon content in the boiler ashes was below 0.1%. Coal and oil shale co-firing fuel preparation should be continuously improved and the coal used should contain minimal organic nitrogen. Acknowledgements This research was supported by the European Regional Fund, Project RITA1/01-60 ＆＆Innovative Gasification, Pyrolysis, and Combustion Technologies for Oil Shale§. The authors are grateful to the staff of the Enefit Energiatootmine AS and especially to Mr. Rain Veinj.rv, Head of Development Projects Department, for his support and help. 
REFERENCES 
1. Konist, A., Pihu, T., Neshumayev, D., Klaots, I. Low grade fuel - oil shale and biomass co-combustion in CFB boiler. Oil Shale, 2013, 30(2S), 294每304. 
2. Pihu, T., Konist, A., Neshumayev, D., Loo, L., Molodtsov, A., Valtsev, A. Full-scale tests on the co-firing of peat and oil shale in an oil shale fired circulating fluidized bed boiler. Oil Shale, 2017, 34(3), 250每262. 
3. Konist, A., J.rvik, O., Pihu, T., Neshumayev, D. Combustion as a possible solution to pyrolytic wastewater utilization. Chem. Eng. Trans., 2018, 70, 859.864. 
4. Neshumayev, D., Rummel, L., Konist, A., Ots, A., Parve, T. Power plant fuel consumption rate during load cycling. Appl. Energ., 2018, 224, 124.135. 
5. Rummel, L., Neshumayev, D., Konist, A. Power plant ash composition transformations during load cycling. Chem. Eng. Trans., 2018, 70, 655.660. 
6. Kokko, A., Nylund, M. Biomass and coal co-combustion in utility scale: operat-ing experience of Alholmens Kraft. In: Proceedings of the 18th International Conference on Fluidized Bed Combustion, Toronto, Ontario, Canada, May 22每 25, 2005. New York, 2005. 
7. Sahu, S. G., Chakraborty, N., Sarkar, P. Coal每biomass co-combustion: An overview. Renew. Sust.. Energ. Rev., 2014, 39, 575每586. 
8. Krzywanski, J., Rajczyk, R., Bednarek, M., Wesolowska, M., Nowak, W. Gas emissions from a large scale circulating fluidized bed boilers burning lignite and biomass. Fuel Process. Technol., 2013, 116, 27每34. 
9. Coda Zabetta, E., Barisic, V., Peltola, K., Sarkki, J., Jantti, T. Advanced technology to co-fire large shares of agricultural residues with biomass in utility CFBs. Fuel Process. Technol., 2013, 105, 2每10. 
10. Valgma, I., Reinsalu, E., Sabanov, S., Karu, V. Quality control of oil shale production in Estonian mines. Oil Shale, 2010, 27(3), 239每249. 
11. Plamus, K., Ots, A., Pihu, T., Neshumayev, D. Firing Estonian oil shale in CFB boilers 每 Ash balance and behaviour of carbonate minerals. Oil Shale, 2011, 28(1), 58每67. 
12. Parve, T., Ots, A., Skrifars, B.-J., Hupa, M. The sintering of Estonian oil shale ashes. Oil Shale, 1995, 12(4), 341每356. 
13. Al-Otoom, A., Al-Harahsheh, M., Batiha, M. Sintering of Jordanian oil shale under similar conditions of fluidized bed combustion systems. Oil Shale, 2014, 31(1), 54每65. 
14. Plamus, K., Soosaar, S., Ots, A., Neshumayev, D. Firing Estonian oil shale of higher quality in CFB boilers - environmental and economic impact. Oil Shale, 2011, 28(1S), 113每126. 
15. Konist, A., Pihu, T., Neshumayev, D., Siirde, A. Oil shale pulverized firing: boiler efficiency, ash balance and flue gas composition. Oil Shale, 2013, 30(1), 6每18. 
16. Loo, L., Konist, A., Neshumayev, D., Pihu, T., Maaten, B., Siirde. A. Ash and flue gas from oil shale oxy-fuel circulating fluidized bed combustion. Energies, 2018, 11(5), 1218. 
17. Al-Makhadmeh, L., Maier, J., Al-Harahsheh, M., Scheffknecht, G. Oxy-fuel technology: An experimental investigation into oil shale combustion under oxy-fuel conditions. Fuel, 2013, 103, 421每429. 
18. Al-Makhadmeh, L., Maier, J., Al-Harahsheh, M., Scheffknecht, G. Oxyfuel technology: Oil shale desulphurisation behaviour during unstaged combustion. Fuel, 2015, 158, 460每470. 
received March 6, 2019 




Combustion characteristics of solid waste biomass, oil shale, and coal 
ABSTRACT 
In this study, a developed two-dimensional mathematical model was used to represent the physical model of the combustion process of olive cake and date seed, and solve the governing equations using finite-volume method. The simulation was performed using ANSYS/Fluent software in order to estimate maximum temperature, heating values and pollutants concentrations. The obtained results were compared with experimental results, and corresponding values of oil shale and coal. The experimental work of direct burning of olive cake and date seeds was performed using an existing circulated fluidized bed (CFB) unit. 
It was found that the adiabatic flame temperatures were 1380 K and 839 K for olive cake and date seed, and 2260 K and 1080 K for coal and oil shale, respectively. The experimental results showed that the maximum tempera-tures were 1126 K and 723 K for olive cake and date seed, respectively. The lower heating values were 19,500 kJ/kg and 16,400 kJ/kg for olive cake and date seed, and 29,000 kJ/kg and 7000 kJ/kg for coal and oil shale, respectively. Thus, biomass such as date seed and olive cake may be used as an alter-native fuel in electrical power plants in olive-or date-producing countries, which may save 40% of fuel cost. 
Introduction 
Power generation from biomass has increased its share of total power generation in several countries. Despite the fact that prices of fossil fuels have dropped in recent years, benefits of biomass power generation such as carbon neutrality and energy security have prevented governments of many countries from abandoning the policy of increasing dependence on renewable energy and biomass. Biomass may be used to meet a wide variety of energy needs, including electricity generation, process heat for industry, and households heating. Partial replacing of conventional fossil fuels with biomass for energy production will result in a net reduction of greenhouse gases emissions and reduce the landfill (Gavaskar et al. 2012). The most common types of biomass energy applications reduce carbon dioxide emissions by 55每98% as compared to fossil fuels, even when transported long distances, as long as the biomass production does not cause any land-use change. Biomass may be used in a number of ways such as direct burning, gasification, fermentation, anaerobic digestion, and pyrolysis (Caputo et al. 2005; Gavaskar et al. 2012). Direct burning is the oldest and most common way of converting biomass energy into heat, mechanical power, or electricity. Circulated fluidized bed (FB) technique is one of the most competitive direct burning technologies for the utilization of biomass that enhance the efficiency of the combustion process while reducing the emission of harm gases (Caputo et al. 2005; Goswami 1986; Kunii and Levenspiel 1991). Biomass undergoes ignition, devolatilization, and solid char particles combustion stages during its combustion. Ignition is the process of initiating the combustion phenomenon, which is very important due to its influence on flame stability and extinction, pollutant formation, and emission. Devolatilization process involves the emission of the volatile gaseous matters, which are ignited and undergoes a homogenous combustion after heated up to a certain temperature. Solid char particles combustion, which is ignited as a result of volatile combustion, followed by a heterogeneous reaction, involves the direct attack of oxygen on the solid particles (Kunii and Levenspiel 1991). 
Olive cake, which is a waste of olive oil-mill, is an important biomass generated in Mediterranean countries where it is available in large amounts and low cost. Olive cake is considered as an alternative fuel source with very low sulfur contents, <0.1 wt%, which makes it an environmental-friendly fuel. Moreover, olive cake is a waste that needs a suitable and acceptable disposal, or it may cause environmental problems, which makes the utilization of olive waste in energy production not only a clean energy source but also an acceptable disposal of this waste. Jordan is one of the rich countries with olives and dates. Olive tree farming in Jordan started 1600 years ago. In the past few decades, the country witnessed a dramatic expansion in the olive tree agriculture. It is estimated that 15 million olive trees exist in Jordan. These trees produce around 230,000 tons of olives per year. The production process of olive oil involves the pressing of olives to produce 37,000 ton of olive oil and about 80,000 tons of solid waste, named olive cake (Ministry of Agriculture of Jordan, 2015). Topal, Atimtay, and Durmaz (2003) studied the combustion characteristics of olive cake produced in Turkey using a circulating FB. The concentrations of O2,SO2,CO2, CO, NOx, and total hydrocarbons were measured in the flue gas. It was found that the combustion efficiency for olive cake changes between 82.25% and 98.66% for various excess air ratios used in the study. 
The main problem of utilizing olive cake is the continuous supply since it is a seasonal product. This problem could be solved by co-firing olive cake with other fuels such as coal, oil shale, and other biomass like date seed in industrial and utility boilers. Atimtay and Topal (2004) studied the co-combustion of Turkish olive cake with coal in a circulating FB of 125-mm diameter and 1800-mm height. The results suggest that olive cake is good fuel that can be mixed with coal for cleaner energy production in small-scale industries using CFB. 
Co-combustion of coal and olive oil waste has been investigated by Armesto et al. (2003). They used two different Spanish coals in their study. The results of their tests have shown that the combustion of olive cake/coal mixtures decreases the emissions of SO2 and NOx. Regarding date seed, Jordan has considerable amounts of date seed that may be utilized as renewable energy source. Saudi Arabia and Iraq, which are Jordan neighbor countries, produce about two million tons of date per year. The date seed waste represents around 15% of those quantities by weight. 
Hussain, Farooq, and Bassyouni (2014) studied the utilization of Saudi Arabian Date Palm waste through pyrolysis process. They did Thermogravimetric analysis to understand the pyrolysis behavior of palm date wastes and designed an FB to study hydrodynamics and develop optimum conditions for the pyrolysis of palm wastes. The pyrolysis is used to produce activated carbon and the waste can also be readily converted to liquid phenolic products. Experimental results have indicated potential opportunities of using the date biomass waste as a potential fuel in the Kingdom of Saudi Arabia. Joardder, Shazib Uddin, and Nurul Islam (2011) studied the conversion of date seed waste into activated carbon and bio-fuel by fixed bed pyrolysis reactor. The date seeds in particle form were pyrolyzed in an externally heated fixed bed reactor with nitrogen as the carrier gas. A maximum liquid yield of 50 wt% and char of 30 wt% were obtained at a reactor bed temperature of 500∼C with a running time of 120 min. The oil is found to possess favorable flash point and reasonable density and viscosity. Al Asfar et al. (2016) developed a two-dimensional (2D) mathematical model to analyze the combus-tion process of Jordanian oil shale in a circulating FB burner. They found that the adiabatic flame temperature was 1080 K, while the obtained experimental results of maximum temperature at various locations of the vertical burner in an actual, non-adiabatic, non-stoichiometric combustion reached 950 K. Hammad and Al Asfar (2017) performed a comparative theoretical and experimental study on the direct burning processes of oil shale and coal in a circulating fluidized bed (CFB) combustion unit. 
They solved using Ansys/Fluent the governing equations of continuity, momentum, energy, and mass diffusion using finite-volume method. They validated their model by comparing their theoretical results with experimental ones. They found that the adiabatic flame temperature was 1080 K for oil shale compared with 2260 K for coal. 
In this work, the adiabatic temperature, lower heating value, air fuel ratio, and effect of burner＊s length on minimizing combustion pollutant gases are investigated theoretically. An existing direct burning unit of 3-kW power was used to conduct experimental work to evaluate the values of temperature at certain locations during combustion process of the biomass and other selected solid fuels. The temperature values were also recorded during actual burning of oil shale and solid waste bio mass, and compared with theoretical values. A developed 2D mathematical model was used to represent the physical model of the combustion process and solve the governing equations of continuity, momentum, energy, mass diffusion, and chemical combustion reactions kinetics numeri-cally using finite-volume method. The simulation was performed using ANSYS/Fluent software in order to estimate maximum temperature, heating values, and pollutants concentrations. The obtained results are important to investigate the possibility of using biomass as an alternative energy source in olive-and date-producing countries. 
Composition and thermophysical properties 
The elemental composition of date seed and olive cake in terms of the molar percentages of C, H, O, N, and S components was determined experimentally. The proximate composition in terms of moisture, volatile, fixed carbon, and ash fractions was also determined. Thermal conductivity test equipment and bomb calorimeter were used to determine the thermal conductivity and heating value. Table 1 shows the results of proximate and ultimate analyses of date seed and olive cake in addition to experimentally estimated thermophysical properties(Al Asfar et al. 2016; Hammad and Al Asfar 2017). 
Physical model 
The physical model consists of an experimental combustion unit composed of a 0.5-m diameter, 3-m-height vertical combustion chamber, two 0.3-m-diameter cyclones, and temperature measurement system. Figure 1 represents a 3D sketch of the burner used in this study. For more details on the combustion unit, see (Al Asfar et al. 2016). 
Table 1. Ultimate (dry, ash free basis) and proximate (as received) analyses. 
Date seed  Oil shale  Olive cake  Coal  
Analysis  wt%  DAF (mol%)  wt%  DAF (mol%)  wt%  DAF (mol%)  wt%  DAF (mol%)  
Ultimate:  
C  50.84  0.31  49.4  0.356  53.9  0.3432  89.3  0.581  
H  6.83  0.50  4.9  0.422  6.1  0.4660  5.0  0.390  
O  37.88  0.17  36.5  0.194  38.5  0.1832  3.4  0.016  
N  4.45  0.02  1.0  0.006  1.4  0.0074  2.3  0.013  
S  0.00  0.00  8.2  0.022  0.1  0.0002  0.0  0.000  
Proximate  
Volatile matter  77.70  82.6  21  60  71.5  80.7  28  30.4  
Fixed carbon  16.30  17.4  14  40  17.1  19.3  64  69.6  
Moisture  5.02  每  2  每  6.9  每  8  每  
Ash  0.98  每  63  每  4.5  每  8  每  
Density (kg/m3)  1680  1570  600  950  
LHV (kJ/kg)  16,400  7000  19,500  29,000  
HHV (kJ/kg)  17,700  8100  21,200  30,300  
(A/F)stoic  6.54  2.67  6.50  10.20  
1. Water Tank  2. Water Pump  3. Motor  4. Feed Hopper 5.Feeding Pipe 6. Distribution Plate   
7. Bed Section  8. Torch (LPG auxiliary burner)  9. Heat Exchanger  10. Cyclone Section              
              11. Exhaust Section  12. Ash box  13. Main Line of Compressed Air  
Figure 1. Three-dimensional sketch of the burner used in this study. 
Experimental work 
The combustion of olive cake and date seed was investigated experimentally under excess air condition. Temperature measurement was achieved using 11 k-type thermocouples distributed along burner length. Thermocouples were connected to an LAB view unit with interference designed for this project. Gas Analyzer Nova Plus with serial number (012293) was used for exhaust gases analysis. Continues combustion was noted for both biomass fuels. 
To validate the simulation model, the maximum temperature resulted from biomass model is compared to the maximum temperature obtained experimentally in the experimental combustion burner unit. The maximum temperature of 962 K was measured experimentally using the burner unit, which is close to the result of the simulation (1080 K). The difference from theoretical results for temperature is due to the fact that in the actual burning process the combustion efficiency was not 100%, and the process was not adiabatic due to heat transfer from the burner to surroundings. In addition to presence of certain percentage of excess air which cause reducing of combustion temperature. 
Mathematical model 
The mathematical model represents the governing equations of continuity, momentum, energy, mass diffusion, and chemical combustion reaction kinetics. Those equations were solved numerically using a high-resolution mesh accounting for the solid and gaseous phases, k 每 汍 turbulence, non-premixed combustion model, and reacting computational fluid dynamics (CFD) model with the same dimension and material of the experimental combustion burner of this study (Al Asfar et al. 2016). 
The burner mesh was created with two velocity inlet surfaces: one for the air at 2 m/s and the other for the fuel with a flow rate of 0.01 kg/s. In addition to one pressure outlet surface at the exhaust, and three wall-surfaces with a temperature of 1000 K for the combustion chamber walls. Non-premixed combustion model was selected since the fuel and the oxidizer enter the chamber in distinct streams. Non-adiabatic system was also assumed, which requires the solution for the modeled transport equation for time average enthalpy (Kunii and Levenspiel 1991): 
where Sh accounts for source terms due to radiation, heat transfer to wall boundary, and heat exchange with the second phase, and the total enthalpy is defined as follows: 
where Yj is the mass fraction of species j, and is the formation enthalpy of species j at the reference temperature Tref,j. The turbulent viscosity is solved using the k 汍 model that 
where k and 汍 are obtained from the transport equations and C米 is constant. 
The mole fractions of oil shale species during the combustion process were estimated using probability density function (PDF) module integrated in ANSYS/Fluent software. Then, a discrete particle model (DPM) in 3D steady-state space is used to solve numerically the governing equations of continuity, momentum, energy, and mass diffusion to estimate species fraction before and throughout combustion, temperature, radiation, convection heat transfer, pressure, density, etc. The chemical reversible reactions involving those species during combustion were taken into account (Al Asfar et al. 2016). Those species include C, H, N, O, S, C(s),S(s),CH4,H2, CO, CO2,N2,O2, OH, H2O, HS, H2S, SO, SO2,CS2, NO, NO2, and C2H6. The chemical reversible reactions involving these species during combustion were taken into account, since they are already built in fluent PDF module. Thermal, prompt, and fuel NOx formation were taken into account, in addition to thermal SOx which was computed through the simulation by enabling NOx and SOx modules. 
Results 
The following set of figures presents obtained results of direct burning of olive cake, date seed, oil shale and coal for flame temperature, and discrete particle module of solid combustion. In Figures 2 and 3, the devolatilization rate contours for date seed and olive cake show that volatiles are released with a maximum rate of 1.8 g/s, leading to slightly downstream of solid fuel feeding position. Volatiles were released slower in date seed with a rate of 0.5 g/s. Both biomass fuels have faster devolatilization rate as compared with oil shale, 0.125 g/s (Al Asfar et al. 2016). After completing devolatilization, the burnout started with a maximum rate of 0.0746, 0.0119, and 0.0123 g/s for olive cake, date seed, and oil shale, respectively. Burnout rate for date seed was the slowest, whereas burnout rate for coal was the fastest. 
Figures 4 and 5 show the temperature distribution in the FB burner for olive cake and date seed, respectively. The maximum temperature of olive cake combustion was 1380 K, while the average bed temperature was about 950 K, which represents a good agreement with experimental results. On the other hand, the maximum temperature of date seed combustion was 839 K in flame front, then decreased to 761 K. The experimental results showed that the maximum temperatures were 1126 K and 723 K for olive cake and date seed, respectively. 
Figure 2. The DPM devolatilization rate for fluidized bed combustion of date seed. 
Figure 3. The DPM burnout rate for fluidized bed combustion of olive cake. 
Figure 4. Temperature contours for fluidized bed olive cake combustion chamber. 
Figure 5. Temperature contours for fluidized bed date seed combustion chamber. 
The maximum temperatures for all three alternative fuels were much lower than that for coal, which is 2260 K (Hammad and Al Asfar 2017). 
The maximum carbon dioxide CO2 mass fractions were 5.9%, 3.74%, and 3.8% for olive cake, date seed, and oil shale, respectively. The contour lines for CO2 mass fraction differ according to the burnout shape, while the concentration of CO was reduced from 8.01 PPM at the flame to <0.801 PPM at the exit. The results also show that the maximum concentration of produced SOx and NOx may not be taken into account when environmental effects are considered. 
Based on obtained results, it may be suggested that it requires about 1.5 tons of biomass (olive cake and date seed) to obtain the same energy produced from 1 ton of coal. This increase in mass will not be in coal favour; since the price of waste biomass in Jordan is about one tenth of coal price. Thus, the use of biomass will save around 40% of fuel cost. In terms of pollution, biomass produces very little sulfur and carbon oxides: SOx and NOx, which will have no serious effects on environment or health. On the other hand, the lower heating value was 16,400 kJ/kg and 19,500 kJ/kg for date seed and olive cake, and 29,000 kJ/kg and 7000 kJ/kg for coal and oil shale, respectively. 
References 
Al Asfar, J. J., A. Hammad, A. Sakhrieh, and M. A. Hamdan. 2016. Two-dimensional numerical modeling of combustion of Jordanian oil shale. Energy sources PartA: Recovery. Utilization, and Environmental Effects 38 (9):1189每96. 
Annual Report. 2015. Ministry of agriculture. Jordan. Armesto, L., A. Bahillo, A. Cabanillas, K. Veijonen, J. Otero, A. Plumed, and L. Salvador. 2003. Co-combustion of coal and olive oil industry residues in fluidised bed. Fuel 82:993每1000. Atimtay, A. T., and H. Topal. 2004. Co-combustion of olive cake with lignite coal in a circulating fluidized bed. Fuel 83:859每67. Caputo, A. C., M. Palumbo, P. M. Pelagagge, and F. Scacchia. 2005. Economics of biomass energy utilization in combustion and gasification plants: Effects of logistic variables. Biomass and Bioenergy 28:35每51. Gavaskar, J., V. Pillai, N. Sanker, M. Ayyapan, M. Saravanan, and A. Pasupathy. 2012. Energy economics study on 
biomass energy conversion techniques. Ijrmet 2 (1):49每53. Goswami, Y. 1986. Alternative energy in agriculture. Boca Raton, FL-USA: CRC Press Inc. Hammad, A., and J. Al Asfar 2017. Comparative study on direct burning of oil shale and coal. The 11th International 
Symposium on Numerical Analysis of Fluid Flow. Heat and Mass Transfer Numerical Fluids. AIP Conference Proceedings 1863. Hussain, A., A. Farooq, and M. Bassyouni. 2014. Pyrolysis of Saudi Arabian date palm waste: A viable option for converting waste into wealth. Life Science Journal 11 (12):667每71. Joardder, M., M. Shazib Uddin, and A. Nurul Islam. 2011. The utilization of waste date seeds as bio-oil and activated 
carbon by pyrolysis process. Rajashi, Bangladesh: Rajashi Univ. Kunii, D., and O. Levenspiel. 1991. Fluidization engineering, 2nd edn ed. Boston: Butterworth Heinemann. Topal, H., A. T. Atimtay, and A. Durmaz. 2003. Olive cake combustion in a circulating fluidized bed. Fuel 82:1049每56. 


Deep neural network inversion for 3D laser absorption imaging of methane in reacting flows
Abstract
Mid-infrared laser absorption imaging of methane in flames
is performed with a learning-based approach to the limited
view-angle  inversion  problem.  A  deep  neural  network  is
trained with superimposed Gaussian field distributions of
spectral absorption coefficients, and the prediction capabil-
ity is compared to linear tomography methods at a varying
number of view angles for simulated fields representative
of a flame pair. Experimental 3D imaging is demonstrated
on a methane–oxygen laminar flame doublet (<cm) backlit
with  tunable  radiation  from  an  interband  cascade  laser
near 3.16 µm. Spectrally resolved data at each pixel provide
for species-specific projected absorbance. 2D images were
collected at six projection angles on a high-speed infrared
camera,  yielding  an  aggregate  of  27,648  unique  lines  of
sight capturing the scene with a pixel resolution of ∼70 µm.
Mole fraction measurements are inferred from the predicted
absorption coefficient images using an estimated tempera-
ture field, showing consistency with expected values from
reactant flow rates. To the authors’ knowledge, this work
represents the first 3D imaging of methane in a reacting
flow.   
INTRODUCTION
Methane  (CH4)  is  an  important  molecule  in  combustion
chemistry, representing the primary fuel component of natural
gas  and  a  key  intermediate  in  larger  hydrocarbon  oxidation.
Accordingly, quantitative imaging of methane in flame envi-
ronments  is  desired  to  discern  the  competitive  physics  of
chemical kinetics with mass and heat transport. Some optical
methods have shown potential for imaging methane in high-
temperature reacting flows, albeit with somewhat constrained
applicability. Point and line measurements of CH4  in flames
have  been  performed  with  Raman  scattering  techniques  [1],
and 2D tomographic measurements have been performed using
near-infrared  diode  laser  absorption  in  oscillating  CH4–air
flames [2] and marine engine cylinders [3]. Here, we present a
novel mid-infrared optical method for 3D imaging of CH4  in
high-temperature flame environments.
Advances  in  midwave  infrared  photonics  have  enabled
tomographic  absorption  spectroscopy  [4]  at  the  fundamen-
tal  vibrational  frequencies  of  important  combustion  species,
providing for sensitive thermochemical measurements in small-
scale reacting flows using compact low-power light sources [5].
Laser  absorption  imaging  (LAI)  is  a  complimentary  method
developed to capture scenes backlit with tunable laser radiation
at very high spatial resolution using high-speed infrared cameras
[6–8]. A representative optical arrangement, used in this work,
is shown in Fig. 1. Recently, LAI has been successfully coupled
with 1D tomography for quantitative imaging of axisymmetric
flow-field thermochemistry at sub-100-µm resolution [6,7].
Tomographic  imaging  of  non-axisymmetric  flows  is  more
challenging—multiple   projection   angles   are   required   to
estimate  the  flow-field  scalars,  and  the  inversion  problem  is
underdetermined  [4,9].  Limited  view  angles  often  result  in
a blurring effect and artifacts in the reconstructed flow-field,
complicating  applications  to  flames,  which  have  very  thin
(∼1 mm) reaction zones. Laser absorption tomography efforts
in combustion flows have demonstrated different approaches to
the inversion problem, including both linear [10–13] and non-
linear [14,15] methods utilizing various optical arrangements.
Increasingly  complex  and  dense  lines  of  sight  configurations
require  computationally  efficient  image  processing  methods.
Recently,  predictive  models  utilizing  neural  networks  have
shown  promise  to  more  efficiently  solve  the  inversion  prob-
lem with limited information, and have been applied to both
simulated flows [16] and emission measurements of real flows
[17–19]. Here, we combine a deep neural network inversion
with LAI to achieve high-resolution 3D imaging of CH4  in a
non-axisymmetric  high-temperature  flow-field.  We  compare
the deep neural network inversion to linear tomography, assess-
ing performance with regards to accuracy and computational
cost, via both simulation and experiment.
With  any  tomographic  absorption  spectroscopy  method,
the   analytical   problem   reduces   to   inverting   line-of-sight
integrated  (or  projected)  absorption  data  [4,9].  For  a  non-
uniform gas medium, the Beer–Lambert law integrated over
wavenumber   ν[cm−1]—or   the   projected   absorbance   area
A j ,proj[cm−1]—can be expressed for each line-of-sight
where α(ν) is spectral absorbance, I0  is incident light intensity,
and It  is transmitted light intensity. L [cm] is the aggregate path
length along the line of sight. Thermochemical properties of
the nonuniform medium are embedded in the spatially resolved
absorption coefficient K j [cm−2], where total pressure P  [atm]
is assumed 1 atm, S j (T)[cm−2/atm] is the line strength of rovi-
brational transition j at temperature T[K], and X abs is the mole
fraction. For tomographic LAI, Eq. (1) applies to each camera
pixel, wherein spectrally resolved absorbance can be integrated
to yield 2D images of A j ,proj, representative examples of which
are depicted in the insets of Fig. 1 for different viewing angles.
With multiple projection angles, each horizontal row of pixels
in each 2D image can be treated independently for subsequent
reconstruction of the flow-field scalar K j . For the experimental
setup depicted in Fig. 1, we target a collection of rovibrational
transitions comprising the R(15) manifold of the v3  asymmet-
ric stretch band of CH4  near 3.16 µm. The line strengths for
these transitions do not vary more than 10% for temperatures
between 450 and 750 K [20], enabling quantitative inference of
mole fraction from K j  in the preheat zone.
METHOD
In this study, we utilize a deep neural network to perform the
inversion of line-of-sight integrated absorption measurements
A j ,proj to solve for K j , and compare its reconstruction perform-
ance with linear 2D tomography methods. The training dataset
consists of 2D fields of the methane absorption coefficient K j ,
represented as superimposed Gaussian distributions as shown
in the top of Fig. 2. The simulations included 5000 variations in
intensity, size, and relative locations of the Gaussians. The cor-
responding projected absorbance areas A j ,proj  are subsequently
calculated utilizing Radon transforms to represent the measure-
ment data at each angle with appropriate pixel density reflecting
the LAI experimental setup. Gaussian noise (3%) was applied
to the input, enabling the network to learn robust inversion
from potentially corrupted measurements. The neural network
architecture is adapted from a unified deep learning framework
shown to efficiently reconstruct a variety of imaging fields [21].
As illustrated in Fig. 2, the input layer FC1 takes a measured
sinogram of  A j ,proj  (6 × 64) of the CH4  rovibrational transi-
tion, reshapes it into a 1 × 384 vector, and then fully connects it
to a 1 × 4096 dimensional hidden layer FC2 with a hyperbolic
tangent activation. This hidden layer FC2 is fully connected to
another hidden layer FC3 with hyperbolic tangent activation,
and is reshaped to a 64 × 64 matrix for convolutional process-
ing. The convolutional layers C1 and C2 convolve 64 filters
of 5 × 5 with stride 1 followed by rectifier nonlinearities. The
final output layer deconvolves the C2 layer with 64 filters of
7 × 7 with stride 1, representing the reconstructed K j  field. The
loss function minimized during training is an L2  mean squared
loss  between  the  network  output  and  the  labeled  K j    fields.
The RMSprop algorithm is used with minibatches of size 50,
learning rate 0.001, momentum 0.0, and decay 0.9. The net-
work was trained for 100 epochs (typically 20–30 mins) on the
Tensorflow deep learning framework using an 8 GB NVIDIA
RTX 2080 graphics card. Via the training process, an effective
inversion operator is learned to predict spatially resolved 2D K j .
This bypasses the numerical inversion of A j ,proj to K j  obtained
through conventional tomography methods.
To  assess  the  predictive  accuracy  of  the  deep  learning
inversion method on a flame pair, the approach is applied to
reconstruct reference simulations of representative 2D K j  fields
using various numbers of projection angles, as shown in Fig. 3.
These  2D  K j    fields  and  their  A j ,proj   projections  are  similar
to but not part of the original training set, and so serve as an
independent reconstruction assessment. Tikhonov-regularized
linear 2D tomography is also applied to the reference fields.
The regularization parameter λ and the smoothing matrix L are
chosen to yield a value of λ that pads the small singular values
but  does  not  overwhelm  the  large  nontrivial  singular  values
[9]. An aggregate root mean square error (RSME) was used as
a comparative metric, calculated as the square root of the aver-
aged squared differences between a reconstructed image and its
corresponding “ground truth” image.
As expected, utilizing a greater number of projection angles
was shown to improve the resolution of peaks and valleys in the
reconstructed  K j   fields for both methods. The deep learning
approach is shown to predict the steep spatial gradients with
equal or greater accuracy (as measured by RMS error) than linear
tomographic methods while concurrently reducing the required
number of projections, even in the cases where the  K j   fields
overlap. Nonphysical absorption artifacts are also observed in
the linear tomographic reconstructions outside of the simulated
reaction zones, a common consequence of the underdetermined
inversion [9]. Notably, the linear tomographic methods show
increasing reconstruction error when the pixel density per flame
structure is reduced, as in the smaller flames in the bottom row
of Fig. 3. The neural network inversion appears less sensitive
to the pixel density, showing a typical reduction in RMSE by a
factor of ∼4−6 relative to corresponding linear tomographic
solutions. We can observe for the experimentally relevant test
case (bottom row), the learning-based approach utilizing three
projection angles has a RSME only 10% greater than the linear
tomographic approach utilizing 12 projection angles.
RESULT
With the deep learning approach tested via simulation, the
neural network inversion was utilized for measurements of the
flame doublets shown in Fig. 1. A distributed feedback (DFB)
interband cascade laser (ICL) near 3.16 µm is used to spectrally
scan across the CH4  R(15) manifold. The beam is expanded
with a concave lens, and expanded again horizontally with a
cylindrical  lens.  The  expanding  beam  is  recollimated  with  a
concave mirror, then pitched through the flow-field comprising
two  Bunsen-style  flames,  each  with  flame  brushes  approxi-
mately 3 mm in diameter. The beam is spectrally isolated with
a bandpass  filter  (Spectrogon, 3160 ± 60 nm),  and  a plano–
convex lens focuses the expanded beam onto the detector array
of the camera. The beam is captured in the camera detector
subwindow of size 64 × 48 with a frame rate of 40 kHz and
integration time of 5 µs. The laser is injection-current tuned
using a sawtooth waveform at 400 Hz, resulting in 100 points
per scan for both I0 and It  for each pixel. The spectrally resolved
absorbance α(ν) is determined using Eq. (1) and is subsequently
fitted using the Voigt line shape model to obtain the projected
absorbance area A j ,proj. For each projection angle measurement,
data are collected at two heights using a vertical translation stage
and later combined after deleting overlapping pixels, yielding
64 × 72 lines of sight to capture the whole flame brush. Each
projection image is collected and averaged over 1 s (400 scan
average), an interval over which the flames are assumed steady.
By imaging a wire mesh with known dimensions backlit with
laser radiation [7], the pixel resolution was determined to be
∼ 70 µm per pixel in the horizontal direction.
The flame pair is mounted on a rotational stage to image
the flow-field from a variety of angles, as shown in Fig. 1. The
partially premixed flames were controlled via thermal mass flow
controllers (MKS MFC GE50A) with combined reactant flow
rates of 150 sccm CH4  and 100 sccm O2. This corresponds
to a fuel-rich mixture with a molar fuel–air equivalence ratio
of φ ≈ 3. After the flows are split to the two burners, one flow
is  measured  with  a  rotameter  to  ensure  equal  flow  through
each burner. The exit velocity of each flow is 0.33 m/s, and the
jet exits of the stainless steel burners are 1.6 mm in diameter,
providing a laminar jet Reynolds number of ∼36.
For each horizontal row of pixels in the measured 2D A j ,proj
images,  the  learning-based  inversion  operator  is  applied  to
predict  the  K j    field  distribution  for  the  R(15)  manifold  of
CH4.  Assembling  the  predictions  for  each  horizontal  row,  a
3D field of K j   is obtained, as shown in Fig. 4. Learning-based
predictions utilizing six projection angles of 2D A j ,proj  images
are shown alongside corresponding results obtained using 2D
linear  tomography.  The  images  are  self-consistent  in  recon-
struction of the twin flames, exhibiting similar heights, widths,
and  magnitudes.  However,  the  flames  diverge  in  magnitude
and  diameter  when  comparing  the  deep  learning  and  linear
tomography results. Steeper spatial gradients of K j  are resolved
within the reacting flow utilizing the learning-based approach,
and this is seen more clearly in the 2D cross sections in the top of
Fig. 4. As in the simulations, the learning-based approach also
mitigates  image  artifacts  and  angular  asymmetry  around  the
flame observed in the results of the linear tomographic method.
Reconstruction performance of both the linear tomography
and neural network inversion is examined in further detail with
1D profiles shown in Fig. 5. The left of Fig. 5 compares 1D
cross-sectional  profiles  of  K j    reconstructions  obtained  from
both linear tomography and deep learning in relation to the
“ground truth” for the simulated K j   field indicated in Fig. 3.
The  uncertainty  shown  associated  with  each  reconstruction
is calculated by performing the reconstructions with different
flow-field orientations [22]. Though a “ground truth” for the
reconstructed K j  fields determined from the experimental mea-
surements is unknown, an analogous comparison can be made
for the experimental K j   fields at the base of the flames nearest
the burner surface, indicated by Fig. 4. Given an independently
measured or estimated temperature field, Eq. (1) can be used
to obtain a mole fraction field for CH4. The right of Fig. 5
shows the resulting 1D profiles of CH4 mole fraction assuming
a  uniform  temperature  of  450  K  (based  on  a  thermocouple
measurement  at  burner  exit).  While  the  temperature  in  this
plane is certainly not uniform, the CH4 absorption line strength
exhibits relative insensitivity to temperature—Error bars cap-
ture  a  possible  temperature  range  between  400  and  750  K.
This simplified analysis provides clear indication that the mole
fraction at the core of the flame (X CH4  ≈ 0.59) is more closely
estimated by the neural network inversion, even with only three
imaging angles.
Additionally, the learning-based approach reduces compu-
tational load compared to linear tomography with Tikhonov
regularization. The reconstructions are more efficient once the
networks are established (20–30 min of training): on an Intel(R)
Core(TM)  i7-9700  K  3.60  GHz  CPU,  the  neural  network
completed reconstructions in 0.35 s while linear tomography
took ∼3 s.
CONCLUSION
A  deep  learning  approach  to  the  inversion  problem  for
absorption  tomography  was  coupled  with  a  high-resolution
LAI for 3D species imaging, and evaluated by simulation and
experiment. To the authors’ knowledge, the experimental effort
represents the first 3D imaging of methane in a flame, attaining
a pixel resolution ∼ 70 µm. Initial results suggest that the neural
network inversion has potential to more accurately predict flame
structures with fewer projection angles than linear tomography,
and reduces computational load. Extension of this method to
more complex flame structures may require more sophisticated
simulations to generate appropriate training data. While the fuel
structures examined here were well-approximated by Gaussian
distributions  (validated  independently  by  single-flame  Abel
inversion), turbulent flow structures and intermediate or prod-
uct fields likely require more physical (and difficult to simulate)
representations.
Experimental and numerical study on the influence of equivalence ratio on key intermediates and silica nanoparticles in flame synthesis
ABSTRACT
Tetramethyl silane is a precursor often used for the production of flame synthesized silica nanoparticles or coatings. This study investigates the chemical reaction mechanism of tetramethyl silane in a series of H2/O2/Ar low pressure (p 30 mbar) flames from fuel lean to slightly fuel rich flame conditions (? 0.8, 1.0 and 1.2). Mole fraction profiles are obtained by molecular beam mass spectrometry. The experimental data are compared to simulations using a recently published reaction mechanism. The present study reveals the influence of the flame composition on the depletion of the precursor TMS, the formation of its main carbon containing products (e.g. CO2 and CO) and the main silicon containing intermediates (e.g. Si(CH3)3(CH2)OO), Si(OH)4, SiO2, Si4O10H4) appearing along the routes of particle formation. TEM images of synthesized particles reveal that the nanoparticles obtained from the gas phase synthesis are spheres with a low degree of agglomeration. The particle size distribution appears to be dependent on the equivalence ratio of the synthesis flames and the changes can tentatively be traced to different particle formation pathways. The data set provided in this work can serve a basis for improvements to the reaction mechanisms of the Si/C/H/O system that are urgently needed to improve particle synthesis processes.
INTRODUCTION
Silica nanoparticles are useful materials for advanced medical and pharmaceutical applications e.g. biosensing, or drug delivery. To enable the synthesis of nanoparticles with properties, e.g. size distributions, adequate for the respective application, the fundamentals of the synthesis process must be understood. They include the precursor decomposition in the flame, interaction of precursor and intermediates with flame species and the influence of the flame conditions on the particle properties. Although, many studies were carried out with different silicon containing precursors, the exact reaction mechanisms leading to the formation of silica nanoparticles from the intermediates are still unclear. Britten et al. developed one of the first reaction mechanisms for silane combustion consisting of 25 species and 70 reactions. Miller et al. extended the reaction mechanism to 58 Si containing species and 201 reactions. These include a reaction path for silane combustion starting from SiH3 O2 and the first steps towards silica cluster formation from SiO.
Feroughi et al. studied hexamethyl disiloxane (HMDSO) doped flames by measuring SiO profiles via laser induced fluorescence and postulated SiO as an important species for the silica cluster formation in flames. Chrystie et al. investigated the impact of precursor load on the SiO concentrations. Shekar et al. emphasized the relevance of Si(OH)4 monomers in the gas phase for the formation of nanoparticles and based a detailed population balance model for the aerosol synthesis of silica nanoparticles from tetraethyl orthosilicate (TEOS) on this hypothesis.
Nurkowski et al. proposed a detailed chemical reaction mechanism for the pyrolysis of TEOS, reduced it to a skeletal chemical mechanism and validated it for a hydrogen oxygen flame. Various computational studies on high temperature oxidation of TEOS postulate Si(OH)4 as one of the most relevant silicon species over a wide range of the temperatures.
Recently, a detailed reaction mechanism for the destruction of tetramethyl silane (TMS) combustion was developed by Janbazi et al.. Based on numerous experimental data and in particularly on molecular beam mass spectrometry data of TMS oxidation in hydrogen flames, a TMS sub-mechanism consisting of 28 reactions and 24 Si containing species was incorporated into the C1 mechanism proposed by Li et al.. The transport data and thermodynamic properties of SiCH and Si COH containing species are taken from literature. Rate coefficients were estimated either by analogy to the hydro carbon chemistry, by Rice每Ramsperger每Kassel每 Marcus calculations or by an optimization engine based on genetic algorithms with the flame data as optimization targets. The performance of the mechanism was further evaluated with flames of the same equivalence ratio doped with 400 and 800 ppm of TMS, which show that different doping leads to different mole fractions of intermediates in the experiments without changing the overall reaction pathways in the flames. The next step to improve the reaction mechanism further, is the evaluation if it can reproduce the interaction between precursor and flame chemistry correctly. One way to test the influence of different oxygen, water, and radical mole fractions on the reaction pathways of TMS oxidation, is to change the equivalence ratio of the flames. The work reported here provides the validation data necessary for this development step and compares new experimental data with simulations of the different flame conditions using the mechanism of Janbazi et al.
METHOD
The formation of main flame species (H2, O2, H2O, Ar, O) and the decomposition of the precursor to intermediate species e.g. TMS, Si(CH3)3(CH2)OO, SiO2, Si(OH)4, Si4O10H4, CO2, CO are monitored in laminar H2/O2/Ar low pressure flames by means of molecular beam mass spectrometry (MBMS). Details on the experiment, the data analysis and uncertainties of the mole fractions can be found in Karakaya et al. and only a brief description is given here. All low pressure flames are stabilized on a matrix burner with a diameter of 36 mm installed in a reactor with a pressure of 30 0.03 mbar. Flame conditions are listed in Table 1. The burner position can be changed in upwards flow direction relative to the quartz sampling probe between HAB 0每 100 mm (HAB, height above burner) with an accuracy of 0.01 mm. A throttle valve maintains the pressure in the reactor chamber at 30 mbar with an accuracy of 0.1%. A molecular beam is formed by extraction of a gas sample from the reactor chamber through a 90 米m orifice in the tip of a conical quartz sampling probe into a second vacuum stage with a pressure of 10?3 mbar. The molecular beam passes through a skimmer with an orifice diameter of 1.2 mm into the ionization chamber of an orthogonal time of flight mass spectrometer (TOF). The nominal kinetic energy of the electrons used for electron ionization is 17 eV in order to detect the major species products (e.g. H2, O2, H2O, TMS, CO2, CO, O, Si species and 70 eV to see silicon containing clusters (e.g. Si4O10H4) with low concentrations in the flames. 107 mass spectra are summed to obtain sufficient sensitivity for the detection of mole fractions in the x 10?6 range. Each integrated mass peak is proportional to the mole fraction of the respective species in the gas sample weighted by the ionization efficiency of the substance. The TOF has a resolution of m/ilm 3000 at a mass to charge ratio of m/z 28, which is useful for peaks with near identical m/z. The settings were optimized to see the decomposition of the precursor and the subsequent formation of the most relevant small gaseous clusters below m/z 400. TMS was metered with a syringe pump with an accuracy of 2 vol% and evaporated at 333 K. Thermal mass flow controllers regulate the gas flows with an accuracy of 5%. The inside and the outside of the burner matrix were not coated significantly with SiO2, so it can be concluded that TMS starts to decompose in the flame and a significant change in burner porosity is not expected.
The errors of the presented mole fractions depend on the calibration procedure and range between 15% for major species (O2, H2, H2O), 30每 60% for minor intermediate species and TMS, and a factor of two for flame radicals (e.g. O) and silicon containing intermediates. The temperature profiles of flames AH were measured with a thermocouple and are radiation corrected according to the method of Bahlawane et al. The uncertainty of the temperature measurement is evaluated to be 100 K. Additionally, the relative perturbed temperature profile is evaluated from the temperature dependence of the argon signal as described by Struckmeier et al. These temperature profiles include the influence of the nozzle and will be used for the simulations of flames A每H. Synthesized particles are collected from a filter in the exhaust line of the flame reactor under ambient conditions after a sequence of experiments with different equivalence ratio in order to characterize the particle morphology. Nanoparticles are mechanically deposited onto copper TEM grids with lacey carbon films without any solvents. Investigations are performed using a JEOL JEM2200FS TEM instrument and ImageJ software to obtain the nanoparticle size distribution by analysis of measured diameters of more than 500 particles from the sample. The x ray diffraction pattern is obtained using a P analytical XP? ert Pro diffractometer.
For the simulations of flames AH, the detailed reaction mechanism together with transport and thermodynamic properties published by Janbazi et al. is used for one dimensional simulations of the low pressure laminar flames with the measured perturbed temperature profiles (see Fig. 1a) using Cantera and ChemKin Pro 19.2. ChemKin Pro 19.2 is used for the detailed re action flow analyses.
RESULT and DISCUSSION
Figure 1a compares the temperature profiles of the doped and the undoped flames with different equivalence ratios. The maximum temperature is almost identical in the neat and doped flames of identical equivalence ratio, but the temperatures in the reaction zone are about 100 K higher. The temperature decrease in the exhaust gas is less pronounced in the doped flames. Both observations, can be explained by exothermic reactions of TMS oxidation close to the burner and exothermic hydrolysis re actions of silicon containing intermediates in the exhaust gas. Fig. 1(b每f) shows measured and simulated main species profiles for the systematic change of equivalence ratios (?) between is included. The simulated and measured mole fraction profiles of the main species H2, O2, H2O are in a very good agreement. Small deviations occur close to the burner surface. These can be traced for all species to the influence of the burner sur face and the quartz probe on the flame structure as discussed before, e.g., in a study of Deng et al. The hydrogen and oxygen are consumed in the reaction zone of the flame and reach equilibrium values around HAB 15 mm in all flames. Water also reaches an equilibrium value at the same HAB. TMS is consumed completely at HAB 10 mm (see Fig. 1f). As a consequence, the TMS consumption occurs at very different flame conditions for the flames of different equivalence ratios. The oxygen mole fraction is approximately 50% higher, the hydrogen mole fraction is 30% lower, and the water mole fraction differs by 40% between the flames with ? 0.6 and ? 1.2 in the TMS destruction region. The profiles of the other equivalence ratios fall between the profiles of the flames with the largest and smallest equivalence ratio. The flame temperatures rise at HAB 10 mm to 1000 K for all doped flames. It can be observed that the addition of TMS shifts all main species profiles closer to the burner surface as a result of the temperature rise in the reaction zone. Deviations between simulation and experiment concerning TMS are extensively discussed in the work of Janbazi et al]. In summary, the decomposition steps of TMS are estimated by analogy to the hydrocarbon chemistry of neopentane and were not optimized against the experimental profiles during model development. Also, probing effects can affect the flow field and temperature field. This results in widening the flame front and cooling the flame close to the sampling probe. The effect of the sampling probe on the flame temperature and the resulting changes to the flame structure are tentatively shown in the Supplementary Material Fig. S3. Figure 2 shows the measured and simulated mole fraction profiles of the oxygen radicals in the reported flames. The measured O radical profiles have comparatively large uncertainties and exhibit more noise than the profiles of other intermediates because the small radicals OH and O are often lost in the sampling process. According to the simulation the oxygen radical is involved in many re actions with intermediate species of TMS. As expected, the mole fraction of the O radical is higher at lean than at rich flame conditions. Compared to flame B it decreases by 20%, 38%, 55% in the exhaust gas of flames D, F, and H, respectively.  For all equivalence ratios, the maximum O radical mole fraction is reached downstream of the point of complete decomposition of TMS and is followed by a slow consumption of O radicals in the exhaust gas. The deviations in the O consumption rates between doped and undoped flames can be traced by reaction path analyses and sensitivity analyses to the reaction between CO and O forming CO2. The model overpredicts this difference for the lean conditions
Most likely, either the dependence of this re action on the equivalence ratio is not accurately captured in the model or reactions with silicon containing species, which are not included in the model, contribute markedly to the consumption of O radicals under lean conditions.
The comparison of measured and simulated TMS profiles in Fig. 1f reveals significant deviations. The simulated mole fraction profiles show a convex down behavior and the experiments a convex up behavior, for all equivalence ratios. The difference in profile shape can be explained by the effect of the sampling probe on the flow and temperature field of the flame. In addition, the simulations predict a much faster TMS destruction than observed in the experiments, that was also observed in the original reaction mechanism development work. TMS destruction is dominated by hydrogen abstraction reactions from TMS by O, H and OH radicals as can be seen from the rate of production analysis (ROP) in Fig. 3. ROP analyses were performed at HAB 1 mm, 2.5 mm and 6 mm. The rate coefficient for the hydrogen abstraction from TMS by H radicals is among the few rate coefficients in the Si/C/H/O system that has been accurately measured by Peukert et al. The rate coefficients for the hydrogen abstraction by other radicals have been estimated with reasonable ac curacy based on analogies of H abstraction from neopentane. These reaction rates were not changed in the reaction mechanism optimization. The reactions with OH and H radicals contribute strongly to the decomposition of TMS for equivalence ratios investigated here, while the reaction with O radicals plays only a minor role. The importance of the H abstraction by H radicals increases with increasing equivalence ratios in agreement with the larger H radical mole fractions, while importance of the reaction with OH radicals is unaffected. The fact that the overestimation of the reaction rate of TMS in the experiments is also independent of the flame conditions, e.g., independent of mole fractions of O, H and OH radicals, suggests that the discrepancies in the TMS consumption rate are most likely linked to the H abstraction reaction by OH radicals. Figure 1e presents measured and simulated mole fraction profiles of the CO2 and CO formed during oxidation of the TMS. The carbon mono x ide profiles peak around HAB 10 mm, so at the same HAB where all TMS is consumed. In all flames, the CO is further oxidized to CO2 reach ing equilibrium values close to 2400 ppm. Carbon is only introduced into the gas mixture by the TMS addition (2400 ppm). From the carbon balance, it is evident that all carbon is converted to CO and subsequently CO2 at high HAB, which is desirable in the synthesis of ultrapure silicon oxide particles. The simulation reproduces the measured CO2 and CO mole fraction profiles quite well, but deviations appear close to the burner due probe perturbations. Deviations between measured and simulated CO2 mole fractions at higher distances are slightly larger than the expected error limit of 15%. They can be explained by a stream stall of the one dimensional flow field leading to a source for further perturbations.
The comparison of the experimental and simulated mole fraction values demonstrates that the reaction mechanism reproduces the overall flame chemistry with and without addition of TMS well. This is a prerequisite for further analysis and interpretation of observations using the simulations.
Figure 4 shows two reaction path analyses at HAB 10 mm and HAB 20 mm for ? 1 at temperatures of 1013 K and 1250 K. They show that only the relative importance of the reaction path ways changes with HAB, while the active pathways are identical. Similar reaction pathways were found for all equivalence ratios.
Since Si(CH3)3CH2 could not be detected,  the first experimentally observed silicon containing intermediate at all equivalence ratios is Si(CH3)3CH2OO. This species may be a peroxide or one of its more stable isomers. In the reaction mechanism the peroxide Si(CH3)3CH2OO is produced by O2 addition to the initially formed TMS radical. The maxima of the mole fraction profiles of Si(CH3)3CH2OO move to slightly larger distances from the burner with decreasing equivalence ratio. This trend and the absolute mole fraction profiles shown in Fig. 5 are in good agreement between experiment and simulation, and in accord with the shifts i the temperature profiles (Fig. 1a). Also, the mole fraction of Si(CH3)3CH2OO increases with increasing amount of O2. For all equivalence ratios, Si(CH3)3CH2OO is completely consumed at around 700 K. Other silicon intermediate species, e.g., OSi(CH3)3, Si(CH3)3OH, Si(OH)3 appear in the reaction zone as it is postulated in the reaction pathway in Fig. 4.
Figure 6a reports measured and simulated mole fraction profiles of SiO2, Si(OH)4, Si4O10H4 that are important intermediates of particle formation in the reaction pathway in Fig. 4. These species are formed from the initial intermediates and peak at HAB 10 mm. According to the reaction mechanism, two distinct pathways lead to the formation of silica nanoparticles. The first pathway forms SiO, which condenses to SiO2 nanoparticles. Condensation of Si(OH)4 to larger clusters such as Si4O10H4 which in turn form silica nanoparticles constitutes the second pathway.
The measured and simulated mole fraction pro files of SiO2 in Fig. 6a have their maxima near HAB 10 mm at 1000 K. SiO2 is consumed slowly in the exhaust gas. The maximum mole fractions of SiO2 decrease with increasing equivalence ratio and reach the detection limit in the richest flame H. The simulations predict the measured profiles within the accuracy of the data. According to the reaction path analysis at ? 1 and HAB 10 mm SiO2 is mainly formed by the reaction SiO O2 SiO2 O, so consequently less SiO2 is formed with increasing equivalence ratio because less oxygen is available in the exhaust. It is conceivable that as a consequence more SiO could react to nanoparticles.
Figure 6b presents measured and simulated mole fraction profiles of Si(OH)4. The simulation captures the measurements within the absolute un certainties of the data. Species like Si(OH)4 are quantified using estimated ionization efficiencies and have large absolute uncertainties of a factor of two. In addition, the model was optimized in the work of Janbazi et al., to fit the absolute mole fractions in flame B, so that a good match of experiment and simulation is expected. When comparing profiles from different equivalence ratios, the uncertainties in the ionization efficiency can be neglected because they will shift the mole fraction values in the same direction.
At the maximum of the Si(OH)4 profiles at ap proximately HAB 10 mm, the experimental mole fractions of the cluster Si4O10H4 are about 40% lower in flame B (?  0.6) compared to flame H (? 1.2), reflecting the decrease in the respective Si(OH)4 profiles quantitatively. This observation confirms, that independent of the equivalence ratio of the flame the formation of Si4O10H4 is linked directly to Si(OH)4.
In conclusion, the model describes the effect of equivalence ratio on important species in the particle formation path correctly and can be used to estimate the relative importance of both pathways as a function of equivalence ratio.
The nanoparticles can be formed either via the SiO or via the Si4H10O4cluster route shown in Fig. 4. According to a rate of production analysis at ? 1 at HAB 10 mm, the ROP of nanoparticles by the SiO pathway is 105 time higher than the ROP by the cluster pathway. At HAB 20 mm the cluster pathway is more important with a 104 times higher ROP. This observation indicates that the pathway for the formation of nanoparticles changes within the flame with increasing HAB. The relative importance of both pathways in the complete synthesis process is estimated by comparison of the integrated ROP for both routes over all HAB. This analysis shows that 97% of the particles are formed via the SiO route and 3% are formed via the Si4O10H4 cluster route.
Compared to the study of hydrocarbon systems the investigation of inorganic nanoparticle formation in flames is considerably more difficult, e.g., due to the lack of any prior knowledge of the species occurring in theses flames, unknown species properties and the more rapid particle formation which presents a technical challenge. Consequently, the presented data are state of theart but do not reach the accuracy that can be achieved in hydrocarbon systems. The study presented here is the only systematic speciation study of the Si/H/O system reaching this level of chemical detail in the literature. As a result, the chemical reaction mechanism trained on these data is currently an ※engineering model§ with a majority of optimized rate constants [11]. However, the change in nanoparticle formation pathways observed is significant and outside the error margin.
If a change in particle formation pathways occurs as function of equivalence ratio, this effect should be evident in the synthesized nanoparticles. The particle sizes could not be determined online in the experiment described here, but the particle properties were investigated with offline techniques. Silica powders are collected after a series of measurements with the three flames D, F and H of different equivalence ratio. Consequently, the powders are a mixture of the nanoparticles produced with different oxygen flows. 8 TEM images are used and 511 particles are counted to obtain a representative and statistically significant nanoparticle size distribution. Figure 7 shows a particle size distribution of silica nanoparticles. A lognormal fit of the data clearly shows a bimodal size distribution with two fractions of particles with mean diameters of
46.7 nm and 158 nm.
Changes to the size distribution have previously been observed in flame synthesis by Akurati et al.
who measured the BET based nanoparticle diameter of silicon oxides depending on the oxygen flow rate. The inset of Fig. 7 shows a representative TEM micrograph of the spherical particles. The amorphous lattice of the particles is supported by a Xray diffraction investigation with a broad feature at 25∼ 2牟 typically for amorphous SiO2 (see Supplemental Figs. S1 and S2).
The data support the observation that the equivalence ratio seems to affect the final particle size but not the particle structure. The flame data of ferseveral possible explanations for the observed effect. On the one hand, the different particle sizes may be related to the different particle formation pathways. On the other hand, the changes in the mole fractions of the radicals in the exhaust gas may lead to different growth patterns of the initially formed nanoparticles. These open questions will be addressed in future research.
CONCLUSION
This work investigates the influence of the equivalence ratio of H2/O2/Ar synthesis flames on the reaction pathways of tetramethyl silane, inter mediates and nanoparticles. Mole fraction profiles of the most important species are compared to simulations with a reaction mechanism for TMS combustion from literature. The results demonstrate that the reaction mechanism can be improved by more accurate rate coefficients for the hydrogen abstraction by OH radicals from TMS, and by inclusion of reactions of silicon containing species with the flame radicals in the exhaust gas. Nevertheless, the reaction mechanism reproduces the experimental data with satisfactory accuracy for all equivalence ratios. In particular, it is shown that two nanoparticle formation pathways are active at all equivalence ratios and that they may lead to different particle size distributions. The data set can be further used to improve reaction kinetics in the Si/C/H/O system.
High-pressure,  high-temperature  optical  cell  for  mid-infrared spectroscopy 
Abstract 
The design, characterization, and operation of a new high-temperature, high-pressure optical cell for in- 
frared spectroscopy is presented. The optical cell uses 16 cm CaF 2 window rods to penetrate the tem- 
perature gradient between the heated optical path length and the ambient conditions surrounding the 
system. Along the 21.3 cm optical path length, good temperature uniformity was measured with devia- 
tions being less than 2.4% of the mean absolute temperature. Further characterization of the optical cell 
revealed usable transmission up to ∼8 μm and pressure stability from vacuum to over 30 atm. More- 
over, stable operation of the optical cell during simultaneous heating and pressurization to 800 K and 
over 30 atm was demonstrated. To illustrate the utility of the new facility, high-resolution measurements 
of the R-branch in the fundamental absorption band of nitric oxide near 5.2 μm were collected at several 
pressures (1-34 atm) and temperatures (294–802 K) using an external cavity quantum cascade laser. The 
measured spectra were compared with Voigt line shape simulations using the HITRAN/HITEMP databases. 
At high gas densities, deviations from the Voigt line shape model are observed, emphasizing the need to 
further study infrared molecular spectra at high temperatures and pressures. 
1. Introduction 
Laboratory  measurements  of  high-pressure,  high-temperature 
molecular  spectra  are  desirable  for  their  utility  in  combustion 
sensors  [1] ,  planetary  atmospheric  study  [2–4] ,  predicting  ra- 
diative  heat  transfer,  chemical  sensing  in  industrial  processes 
[5,6] , and sensing for other gas phase systems [7–9] . To that end, 
high-pressure (5–100 atm)  and  high-temperature (650–2500 K) 
spectroscopy has been performed in short-duration ( ∼ms) shock 
tube experiments [8,10–12] . In these experiments, typically only a 
few wavelength regions are studied at a time as traditional wide 
spectrum devices (e.g. FTIR) cannot complete a measurement in 
this time frame. Resolving full absorption bands or branches in 
conditions generated behind reflected shock waves in shock tube 
experiments  has  proven  difficult  until  a  recent  demonstration 
by  Strand  et  al.  using  a  new  rapid-tuning,  broad-scan  ECQCL 
measured  the  absorption  cross  sections  of  ethylene  and  other 
molecules in the 8.5–11.7 μm region [13] . 
The  spectroscopy  literature  contains  numerous  descriptions 
of spectroscopic cells whose achievable temperature or pressure 
conditions  overlap  with  the  range  of  conditions  achieved  in 
shock  tube  experiments.  However,  simultaneously  heated  and 
pressurized spectroscopic cells are relatively rare in the literature, 
particularly in the mid-infrared (IR) region beyond 5 μm. Adding 
this capability yields a more convenient, repeatable, and econom- 
ical  means  to  measure  new  spectra,  test  current  spectroscopic 
models and databases, and to calibrate sophisticated tunable diode 
laser absorption spectroscopy (TDLAS) sensors. 
In  this  paper,  we  present  the  design  and  construction  of  a 
high-pressure, high-temperature optical cell (HPHT) designed for 
mid-infrared spectroscopy via unique calcium fluoride (CaF 2 ) win- 
dow rods. The maximum operating conditions successfully demon- 
strated by the HPHT were simultaneous heating and pressurization 
to 800 K and over 30 atmospheres. Furthermore, the temperature 
non-uniformity of the cell is shown to be no greater than 2.4% 
of the mean absolute temperature across the entire path length, 
and the leak rate is insignificant in comparison to the data col- 
lection time of a typical TDLAS system. To demonstrate and vali- 
date the reported conditions, high-pressure and high-temperature 
laser absorption spectra of nitric oxide (NO) measured in the HPHT 
optical cell are presented and compared to simulations calculated 
from the HITRAN [14] and HITEMP [15] databases. Evidence of col- 
lisional effects beyond Lorentzian-type broadening are observed at 
sufficiently high gas densities. These effects further emphasize the 
need for facilities to perform high-temperature and high-pressure 
spectroscopic experiments. 
2. Absorption spectroscopy fundamentals 
Laser absorption spectroscopy (LAS) is a well-known technique 
used to study molecular structure, and it is also utilized in sensors 
to  measure  the  concentration  of  particular  gaseous  species  as 
well as their temperature, pressure, and even bulk velocity for 
applications in combustion, propulsion, and hypersonics [1,16] . The 
main advantages of LAS are its relatively simple operation in its 
most basic form and its versatility especially in advanced variants 
such as wavelength modulation spectroscopy (WMS) [10–12,17] . 
The IR wavelength regions are of particular interest because un- 
known molecules can be identified from absorption at different 
wavelengths correlating to the bonds and functional groups of the 
molecules. Furthermore, the strong fundamental absorption bands 
of common combustion species (H 2 O, CO 2 , and CO) are found in 
the mid-IR and are becoming increasingly utilized in sensor de- 
signs as mid-IR light sources and optics continue to improve [16] . 
The absorption of monochromatic light is defined by the Beer–
Lambert Law [18] 
which relates the ratio of transmitted to incident light,  II 0 , the 
spectral absorbance, αν, which is the product of the spectral ab- 
sorption coefficient at frequency ν, k ν, and the path length of the 
absorbing medium, L . For an isolated absorption transition, j , the 
spectral absorption coefficient can be further defined                                   
where S is the line strength of the isolated transition, φ is the line 
shape function of the isolated transition, T is the absolute temper- 
ature, P is the absolute pressure, and X i is the mole fraction of the 
absorbing species i . The line shape is often described by the Voigt 
line shape function, which is the convolution of Gaussian ( φD ) and 
Lorentzian ( φL ) line shapes. 
The Gaussian line shape is a manifestation of the velocity distribu- 
tion of the absorbing molecules and their associated distribution 
of Doppler shifts, also known as Doppler broadening. Lorentzian 
line shapes are manifestations of lifetime broadening of energy 
states of the absorption transition; examples are natural broaden- 
ing which is the result of spontaneous emission from the upper 
and lower states of the transition and collision broadening which 
alters the lifetime of a molecule in an energy state via energy ex- 
changes during collisions with other molecules. In many situations, 
Doppler and collision broadening dominate the line shape, neces- 
sitating proper modeling of these two broadening mechanisms for 
use in LAS-based sensors. 
The Doppler broadening line shape can be determined ana- 
lytically from the vacuum frequency of the absorbing transition, 
the molecular weight of the absorbing molecule, and the absolute 
temperature. The collision broadening line shape depends on the 
collision partners present in the gas mixture, the pressure, and the 
temperature. These dependencies are summed up in Eqs. (4) and 
where νC is the collision broadening full width at half maximum 
(FWHM), γB −A is the pressure broadening coefficient of absorbing 
molecule B with collision partner A, T 0 is the reference tempera- 
ture (typically 296 K or 10 0 0 K for high-temperature applications), 
and n is the temperature dependence exponent. While analytical 
collision models exist, 2 γ  and n are usually determined empir- 
ically for each absorbing molecule and collision partner. In fact, 
determination of these collision parameters for different absorbing 
molecules over a range of conditions is a primary motivator for 
the development of the HPHT. 
The  previous  discussion  on  fundamental  absorption  spec- 
troscopy  of  isolated  transitions  is  necessary  to  understand 
collisional  effects  on  molecular  spectra.  As  pressure  increases, 
typically νC follows. At moderate pressures, isolated transitions 
begin overlapping, and typically a superposition of transition line 
shapes sufficiently describes the observed spectrum. As a result, 
a summation over all transitions must be added to the definition 
presented in Eq. (2) . 
At yet higher pressures, the spectrum loses its discrete nature in 
favor of a continuum. Thus, TDLAS sensors designed to operate 
at  high  pressures  require  accurate  knowledge  of  the  spectral 
parameters for all transitions with significant contributions to the 
absorption coefficient at the sensor’s optical frequency. However, 
additional collision effects such as line mixing and the breakdown 
of the impact approximation bring complications beyond the Voigt 
profile and can introduce significant errors between measured and 
simulated absorbance in both the peaks and wings of individual 
transitions and the entire band [17,19,20] . Facilities such as the 
high-pressure,  high-temperature  optical  cell  described  here  are 
essential to better understand these phenomena as a function of 
various thermodynamic conditions. 
3. Previous experimental facilities and studies 
A  number  of  experimental  facilities  have  been  reported  for 
the study of high-pressure and/or high-temperature spectroscopy 
of  various  gaseous  species.  For  the  purposes  of  improving  the 
knowledge of NO radiation transport, Green and Tien performed 
measurements of low-resolution NO spectra in a static cell capable 
of reaching temperatures up to 1200 K and pressures up to 4 atm 
[21] . The cell body was constructed of a zirconia ceramic tube 
placed in a high-temperature furnace, and the windows protruded 
to the more uniform central zone of the furnace via nitrogen- 
cooled  stainless  steel  tubes  mounted  with  sapphire  windows. 
Temperature uniformity was found to be acceptable as the tem- 
peratures measured at the nitrogen-cooled windows and 1.5 cm 
from the windows were 20% and 4% lower, respectively, than the 
center temperature of the furnace [22] . 
Motivated  by  TDLAS  sensor  development  for  high-pressure 
combustion, Reiker et al. studied high-pressure, high-temperature 
water vapor spectroscopy near 7204 and 7435 cm −1 in a static 
cell that demonstrated operation up to 30 atm at 700 K [17] . The 
window design used a tapered window housing mounted with a 
matching tapered sapphire window that was compressed between 
two copper seals by a compression nut. The entire cell was placed 
in a uniform single-zone furnace, requiring the window seal to 
survive all operating pressures and temperatures. The Inconel body 
of the optical cell presented in this paper is similar to Reiker’s cell 
body except with modifications made for the new window design. 
More recently, Stefani et al. studied the spectrum of CO 2 from 
60 0 0 to 10,0 0 0 cm −1 (1–1.67 μm) at pressures and temperatures 
up to 20 atm and 560 K, respectively [23] . The reported con- 
ditions were obtained in a commercial stainless steel (316) op- 
tical cell capable of withstanding pressures up to 200 bar and 
temperatures  up  to  650  K.  The  2  cm  optical  path  length  cell 
used zinc sulfide (ZnS) windows which have good transmission up 
to 12 μm. 
Another example of sapphire’s utilization for high-temperature, 
high-pressure  spectroscopy  was  reported  by  Christiansen  et  al. 
who built and demonstrated an optical cell whose operation range 
reached  simultaneous  heating  to  10 0 0  K  and  pressurization  to 
100 atm [24] . The cell body was made of concentric, high-grade 
aluminum oxide ceramic tubes, and the 3 mm thin sapphire win- 
dows were glass-bonded to the internal ceramic tubes to form a 
seal. A central heating coil heated the 3 cm optical path length 
and was supplemented by two more heating coils on either side 
to form a highly uniform path length. 
A high-temperature, low pressure static cell with excellent tem- 
perature uniformity at 1723 K was reported by Melin and Sanders 
with sample high-temperature spectra of water vapor presented 
[25] . This particular architecture also used concentric alumina ce- 
ramic tubes to form the cell body. The windows were made from 
sapphire tubes  with optically contacted sapphire windows that 
formed a seal on the process side. The optical path length of this 
cell was reported to be changeable, but for the demonstration pre- 
sented a path length of 16 cm was used. 
In many of the designs, penetration across the temperature gra- 
dient from ambient temperatures to the uniform, high-temperature 
zone was achieved, in one way or another, by mounting optical 
windows to the end of a window mount that protrudes into the 
center zone of a high-temperature furnace. For the applications 
discussed, sapphire’s high strength and resistance to thermal stress 
makes it the best solution. However, a limitation of sapphire is 
that its usable transmission range is limited to ∼ 6 μm (depending 
on  the  thickness),  and  to  complicate  matters  the  transmission 
cut-off wavelength  of  sapphire  shortens  as  its  temperature  in- 
creases  [26] .  This limits the number of species and conditions 
available  for  spectroscopic  investigation,  particularly  for  hydro- 
carbons and even NO if the window is too thick. Other window 
materials with longer transmission like ZnS, ZnSe, and the fluoride 
crystals (CaF 2 , BaF 2 , MgF 2 ) have their own design challenges (e.g. 
brittleness, thermal stress, lower maximum temperature stability), 
but  are  necessary  to  extend  the  transmission  range  of  static 
cells. 
The pressures reached by the optical cells described in this 
section  are  representative  of  combustion  engine  pressures,  yet 
there is a drive to reach higher temperatures and longer wave- 
lengths. The need for spectroscopic studies at higher temperatures 
is clear in that combustion temperatures can easily exceed 20 0 0 K. 
The  need  for  measurements  of  high-temperature  and  pressure 
spectra  at  longer  wavelengths  is  evident  in  pyrolysis  kinetics. 
Recent combustion kinetics studies indicate that fuel fragments 
from  pyrolysis  have  a  pronounced  effect  on  oxidation  kinetics 
[27] . Beyond the C –H stretch near 3 μm, a number of hydrocarbon 
(i.e. fuel fragment) identifiers are present in the mid to far-IR. 
Accurate spectroscopic databases of pyrolysis fuel fragments can 
improve time-resolved species measurements during pyrolysis and 
combustion processes. Such measurements significantly improve 
combustion models. In addition to combustion applications, mea- 
surements of the mid-IR spectra of many molecules over a variety 
of conditions up to 40 0 0 K and 10 0 bar are of interest for the study 
of exoplanetary atmospheres [2] . The present optical cell provides 
a  stable  environment  to  study  the  spectra  of  many  molecules 
up  to  ∼8  μm  in  a  wide  range  of  relevant  thermodynamic 
conditions. 
4. Design of the high-pressure, high-temperature optical cell 
The high-pressure, high-temperature (HPHT) optical cell was 
initially designed for infrared spectroscopy of gases at high pres- 
sures  and  temperatures  up  to  900  K.  In  previous  works,  such 
conditions were reached using shock tube or static cell facilities 
with windows made with robust optical materials such as sapphire 
or  fused  quartz.  The  diminishing  transmission  of  longer  wave- 
length light through these materials is problematic for infrared 
spectroscopy  as  sapphire’s  transmission  begins  to  cut  off near 
4.5  μm  and  fused  quartz’s  near  3.5  μm.  Infrared  spectroscopy 
at  longer  wavelengths  requires  use  of  window  materials  with 
superior  transmission  properties.  At  room  temperature,  use  of 
materials such as zinc selenide, calcium fluoride, barium fluoride, 
or germanium for infrared optics is common. Unfortunately, many 
of  these  infrared  optical  materials  are  much  less  robust  than 
sapphire or fused quartz and are more susceptible to a variety 
of failure modes at elevated temperatures such as thermal shock, 
oxidation, and opacity [28–30] . Although a compromise in trans- 
mission wavelength range (up to  ∼8 μm) compared to other IR 
materials, calcium fluoride (CaF 2 ) presents a stable alternative at 
high-temperatures when care is used to prevent damage due to 
thermal stress or exposure to moisture [29,30] . Additionally, CaF 2 ’s 
low index of refraction mitigates back-reflection problems other 
high index materials may cause. Thus, CaF 2 was selected as the 
window material for the HPHT. 
The body of the HPHT is made from Inconel 625 high-pressure 
fittings (High Pressure Equipment Co.); its geometry and dimen- 
sions are shown in Figs. 1 and 2 . SAE ports were machined into 
the boss fittings at either end of the HPHT and are where the 
window  assembly  and  o-ring  sit  to  form  a  vacuum  tight  seal. 
Inlet and outlet tubes at opposite end of the optical cell provide 
flexibility in setting up a flow configuration. The HPHT body sits 
in a 40 cm wide single-zone tube furnace (Barnstead|Thermolyne 
21100 Tube Furnace) that provides the primary heating load and 
insulation.  Additional  high-temperature  insulation  is  added  to 
cover the exposed ends of the metal body. Before being placed in 
the tube furnace, the HPHT was fitted with a stainless steel sheath 
that is used to translate a long K-type thermocouple along the 
HPHT to measure the temperature at different points along the op- 
tical axis. In this configuration, the HPHT temperature distribution 
was found to be unacceptably non-uniform for spectroscopy mea- 
surements (see Fig. 3 ). To overcome the non-uniform path length, 
high-temperature  band  heaters  (Watlow)  were  secured  around 
the center tube of the cell body (see Fig. 1 ). Independent control 
was provided by temperature controllers (Omega CN7533) with 
feedback from type-K thermocouples placed at the band heater 
locations. Fig. 3 shows the improved temperature uniformity of 
the HPHT with the band heaters. 
The  window  design  consists  of  three  components;  namely, 
the window housing, the window, and the sealing adhesive. To 
minimize coefficient of thermal expansion (CTE) mismatch with 
CaF 2 , aluminum SAE plugs were selected for the threaded window 
housings that mated with the SAE ports on the cell body. The 
plugs  were  given  a  clear  aperture  of  1  cm  for  optical  access. 
Although the band heaters dramatically improved the uniformity 
of a 22 cm region in the center of the furnace, a large temperature 
gradient exists between the band heater and the window housing 
locations.  To  penetrate  the  temperature  gradient,  16  cm  long, 
1.168 cm diameter CaF 2 rods are used as the window material, 
creating a gas path length of 21.3 cm between the internal faces 
of the CaF 2 rods. The internal faces of the CaF 2 rods are wedged 
to 1 ◦  to mitigate etalons due to constructive and deconstructive 
interference of the laser beam intensity. Two adhesive materials 
were examined to bond the CaF 2 rods to the aluminum window 
housings.  A  high-temperature  epoxy  selected  for  its  close  CTE 
match with CaF 2  and aluminum proved to be unsuitable as it 
cured  significantly  harder  and  stronger  than  the  CaF 2  crystal. 
After curing and upon installation into the HPHT cell, the CaF 2 
rod  fractured  near  the  bonding  location  when  applying  torque 
to  the  SAE  window  fitting.  A  successful  alternative  was  found 
in  a  low-outgassing  silicone  adhesive  (ACC  Adhesives  AS1724) 
with a maximum working temperature of 200 ◦C. Despite a larger 
difference  in  CTE  compared  to  CaF 2 ,  the  softer,  more  elastic 
silicone adhesive has continued to seal under many heating and 
pressurization cycles. To further combat thermal stress at the bond 
location, the boss ends of the HPHT were cooled by water-chilled 
copper coil collars (see Fig. 1 ). With the HPHT temperature set to 
900 K, the temperature at the bond location was measured to be 
less than 80 ◦C, well within the adhesive’s limits. 
5. Characteristics of the HPHT cell 
5.1. Transmission 
Depending on the sample thickness and data source, the re- 
ported optical transmission of CaF 2 crosses 10% between roughly 
8–13 μm (900–1250 cm −1 ) [28,31] . Since the CaF 2 rods used for 
the HPHT are much longer than the specimens typically measured 
in the literature, the authors felt it was worthwhile to measure 
the transmission spectrum to better understand the limits of the 
facility and to plan future experiments. The transmission spec- 
trum of a 12 cm long CaF 2 rod from the same manufacturer of 
the 16 cm long rods used in the HPHT was measured at room- 
temperature with a Nicolet 6700 FTIR spectrometer. The measured 
spectrum is shown in Fig. 4 from 2.5 to 8.3 μm with transmission 
above 90% over much of the measured range until the transmission 
transitions to 10% between 6 and 8 μm. Theoretical transmission 
through the rod was calculated from the Fresnel equations and 
the wavelength-dependent index of refraction and absorption co- 
efficient of CaF 2 [32] . The theoretical simulation, plotted in Fig. 4 , 
agrees with measurements at all measured wavelengths. 
With increasing temperature, the transmission edge of many 
infrared  optical  materials  becomes  shorter  [26,28] .  CaF 2  is  no 
exception  and  its  infrared  transmission  edge  has  been  studied 
from cryogenic temperatures to the elevated temperatures rele- 
vant here [31,33,34] . Namjoshi et al. and Lipson et al. measured 
the absorption coefficient of CaF 2 and other fluoride crystals at 
temperatures between 295 and 800 K and found the temperature 
dependence of the absorption coefficient in the infrared transmis- 
sion edge to be described by a multiphonon absorption process. 
During elevated temperature operation, the temperature gradient 
across the window rods would limit the useful transmission range 
of  the  HPHT  cell  to  ∼6.5–7  μm.  Additional  transmission  data 
found in studies of the contamination of fluoride crystals after 
heat treatment in oxidative and wet environments suggest great 
care must be taken to limit the exposure to moisture and ox- 
idative environments when performing experiments above 600 ◦C 
[29,30,32] . Furthermore, published thermal dispersion coefficients 
for  CaF 2  indicate  the  index  of  refraction  exhibits  little  change 
with temperature ( d n/d T ∼ −10 −5 C1  ) [35] . Thus, the temperature 
gradient  across  the  window  rods  during  elevated  temperature 
operation of the HPHT is not expected to significantly alter the 
refractive properties observed at room temperature. 
5.2. Path length 
The  path  length  of  the  HPHT  cell  was  calculated  to  be 
21.3 cm based on the precisely measured geometry of the various 
components used in the cell’s construction. Additionally, room- 
temperature absorption measurements of NO at known pressure 
and concentration confirmed the path length to within 2% (See 
Fig. 7 (a)). Since the cell is a metal body, changes in path length 
due to thermal expansion must be considered. At the maximum 
operating condition, the change in path length due to thermal 
expansion  was  estimated  to  be  less  than  1%  of  the  nominal 
length due to most of the expansion of the Inconel metal body 
(CTE = ∼ 14 × 10 −6 1/m) being countered by the expansion of the 
CaF 2 rods (CTE = ∼ 20 × 10 −6 1/m). 
5.3. Temperature uniformity 
To measure the HPHT cell’s temperature distribution along the 
optical path length, a type-K thermocouple with 1 cm graduations 
was translated through the stainless steel thermocouple sheath. 
The temperature measurements of the outer wall is equal to the 
inner gas temperature as shown in the analysis of a similar gas cell 
by Schwarm et al. [36] . The measured temperature distributions 
at different conditions are shown in Fig. 3 . Without supplemen- 
tary heating from the band heaters, the heat load supplied by 
the single-zone furnace provided only a short region of uniform 
temperature ( ∼5 cm). Furthermore, the inherent distribution of 
the furnace was found to be biased to one side. To improve and 
optimize  the  temperature  uniformity,  the  band  heater  settings 
were  iteratively  adjusted  until  the  results  were  satisfactory.  A 
22  cm  region  bounding  the  21.3  cm  optical  path  length  was 
measured  at  1  cm  increments  for  each  of  the  profiles  shown. 
At 802 K, the temperature profile saw a maximum deviation of 
19 K or 2.37% from the mean. However, this maximum deviation 
represents only a small fraction of the total path length. Perhaps 
a more indicative metric of the temperature uncertainty along the 
path length would be a standard deviation which is less than 1% 
for each condition shown. 
5.4. Pressure stability 
Pressure  transducers  (Setra)  rated  to  67  and  1.7  atm  were 
used to measure pressures above and below 1.7 atm, respectively. 
Pressure loss was evaluated at different temperature and pressure 
conditions. However, no significant change in leak rate was ob- 
served at elevated temperatures. Under vacuum conditions, the 
pressure was found to be very stable at all temperatures with a 
pressure change of 1 mTorr/s which is negligible for the duration 
of  a  TDLAS  measurement.  At  pressures  above  1  atm,  the  rate 
of pressure change per minute was on the order of 0.01% of the 
nominal pressure. For instance, the drop in pressure at 30 atm was 
observed to be < 0.02 atm after two minutes at any temperature 
condition. During the measurements discussed in the following 
sections, the resolution of the 67 atm pressure transducer could 
not resolve any pressure changes throughout the duration of the 
external cavity quantum cascade laser measurement that lasted 
roughly 6 seconds. Thus, the pressure stability was found to be 
adequate for high-pressure spectroscopy.  
6. Experimental setup and procedure 
6.1. Gas system 
The HPHT was connected to a high-pressure valve manifold that 
controls the flow of gases to the HPHT facility, room-temperature 
reference  cell,  gas  cylinders,  mixing  tank,  vacuum  pump,  and 
exhaust ventilation. At several strategic locations in the manifold 
matrix, pressure transducers monitor the system pressure. Before 
making a measurement, the desired gas mixture of dilute nitric 
oxide in nitrogen was allowed to slowly flow through both optical 
cells for several minutes to allow saturation of NO wall adsorption. 
Next,  the  room-temperature  reference  cell  was  filled  with  the 
gas mixture to a low pressure ( < 0.1 atm), providing an absolute 
wavelength marker during experiments. After isolating the refer- 
ence cell, the HPHT was slowly filled to the desired pressure with 
the NO mixture. 
6.2. Optical system 
An external cavity quantum cascade laser (ECQCL) (Daylight 
Solutions), with a maximum continuous wave output power of 
50 mW, was used to probe the R-branch of the NO fundamental 
band. The laser’s output frequency range (1880–1943 cm −1 ) spans 
most of the R-branch and can be tuned over that range via the 
rotation of its diffraction grating. However, its mode-hop-free re- 
gion spans only about one third of the total frequency range from 
1924 to 1943 cm −1 . Out of the laser package, the beam displays 
excellent beam quality and good collimation with little divergence 
observed through the many laser paths. Its line width is specified 
to be less than 10 MHz which qualifies as monochromatic light for 
the purposes of this work. 
Fig.  5  shows  the  optical  components  and  laser  beam  paths 
through  the  HPHT  and  reference  cells.  After  exiting  the  laser 
cavity,  the  laser  beam  was  split  with  a  zinc  selenide  (ZnSe) 
window  wedge.  One  of  the  split  beams  was  directed  through 
the HPHT and focused onto a thermoelectrically cooled mercury–
cadmium–telluride (MCT) detector (Vigo Systems) with a parabolic 
mirror. Before reaching the parabolic mirror, the remaining optical 
power ( ∼70% after transmission through ZnSe) was attenuated by 
collecting the reflection off a CaF 2 flat to ensure a maximum laser 
power to thermal emission ratio. The second beam path from the 
ZnSe beam splitter was split a second time to a reference detector 
(IR Associates) and a third beam splitter. The third beam splitter 
directs one beam through a solid germanium Fabry–Perot etalon 
(FSR = 0.0163 cm −1 ) before it is focused onto a thermoelectrically 
cooled MCT detector (Vigo Systems) with a CaF 2 lens. Lastly, the 
final beam is directed through the reference cell after which the 
beam is focused onto a thermoelectrically cooled MCT detector 
(Vigo Systems) with another CaF 2 lens. 
The grating tuning was controlled by the ECQCL controller with 
no external inputs to the controller or laser head. Timing signals 
from the ECQCL controller and signals from the four detectors 
were  recorded  at  100  kHz  by  the  multi-channel  DAQ  system 
(National Instruments PXI-6115 boards in a PXIe-1062Q chassis). 
This  sampling  rate  provided  more  than  adequate  resolution  as 
full scans from 1880 to 1945 cm −1 were complete in roughly 6 s. 
For each measurement, a dark signal, a transmission background 
signal,  and  a  transmission  signal  were  needed  for  proper  data 
processing. The dark signals capture any systematic detector signal 
offsets from the system without the laser passing through. The 
transmission background signals represent the I 0 in Eq. (1) and are 
recorded when no absorbing gas species is present in the HPHT 
cell. Finally, the transmission signals are recorded with an absorb- 
ing gas species present in the HPHT cell. The data processing steps 
will be discussed in the next section. 
7. Measurements and results 
7.1. Data reduction process 
Proper data reduction required the 4 detector signals and 3 
measurement  types  described  in  the  previous  section.  The  EC- 
QCL exhibits slight scan-to-scan intensity variations, and this is 
mitigated through use of a common-mode rejection strategy. In 
common-mode rejection, a ratio between the reference detector 
and gas cell detectors is measured as a function of optical fre- 
quency during the transmission background measurement. Thus, 
any non-absorption intensity variations measured between I 0 and 
I t are removed. The etalon signal was used as a high-resolution 
wavelength calibration to ensure alignment between the I 0 and I t 
measurement sets. To calibrate absolute wavelength, all measure- 
ments sets were collected with a low pressure mixture of dilute 
NO in N 2 in the reference cell. Fig. 6 displays transmission spectra 
after implementing common mode rejection and an example of 
the wavelength calibration procedure. 
After implementing common-mode rejection and properly cali- 
brating wavelengths, the absorbance baseline was measured from 
two background transmission measurement sets. Many optical ma- 
terials under mechanical stress exhibit birefringence. Although the 
extent of the stress-induced birefringence in the long CaF 2 rods 
was expected to be low, this effect was investigated at several 
pressures. The top panel of Fig. 6 shows the measured absorbance 
baseline when the HPHT cell was filled with 20 atm of oxygen. 
The oxygen ensured any remaining NO in the gas system was ox- 
idized, resulting in a true absorbance baseline measurement. No 
significant changes in transmitted light intensity were observed, 
and the measured baseline noise was found to be  ≈ ±0.01. Ob- 
served temperature effects were negligible since all measurements 
are recorded with the same heating load applied to the CaF 2 rods. 
7.2. Room-temperature validation 
Operation of the cell and performance of the ECQCL were first 
evaluated in room-temperature (294 K here) experiments at sev- 
eral  pressures  and  compared  to  simulations  using  the  HITRAN 
database. Following the experimental procedure outlined in the 
previous section, a mixture of 2.03% (Praxair certified grade with 
X NO uncertainty of ±2%) NO in nitrogen was added to the HPHT 
and reference cells. Fig. 7 (a) displays the validation measurement 
at 1 atm superimposed by a HITRAN simulation at the same con- 
ditions. Transition peak absorbances from the HITRAN simulation 
are about 4% higher than the measured absorbance because the 
collision-broadening coefficients ( γ) reported by HITRAN are ad- 
justed for air whereas the current measurements are in nitrogen 
[37] . Measurements of the oxygen broadening coefficient [38] in- 
dicate that γNO –O 2  is about 20% smaller than γNO –N 2  leading to 
γNO –air reported by HITRAN to be about 4% less than γNO –N 2 . With 
this adjustment made, the measured and simulated spectra agree 
more favorably with peak absorbances differing by less than 0.5%. 
Room-temperature  measurements  at  higher  pressures  up  to 
34 atm are shown in  Fig. 7 (b).  To  remain below the  optically 
thick limit, a mixture of 3500 ppm NO in nitrogen was made 
for these measurements. Since the uncertainty in this mixture is 
much higher than that of the 2.03% NO mixture, Voigt line shapes 
were fit to a 1 atm measurement to more accurately determine the 
concentration of NO. The resulting best-fit concentration was then 
used in high-pressure simulations for a better comparison with the 
measurements. 
Observed deviations from the HITRAN simulations grow with 
pressure. Super-Lorentzian absorbance is observed at the lower 
frequencies until a crossover to sub-Lorentzian absorbance occurs 
near 1936 cm −1 . These observations are consistent with trends 
found in line mixing studies [20,39,40] and in particular with ex- 
periments and calculations by Abels and DeBall [41] and Hirono 
and Ichikawa [19,42] , who reported a band-correction function for 
the absorption coefficient in the troughs between adjacent transi- 
tions at 296 K and below 1 atm in NO–N 2 mixtures. The band- 
correction function, K ( ν), is defined in Eq. (7) as the ratio of the 
observed absorption coefficient with line mixing and the absorp- 
tion coefficient due to Lorentz-type broadening. 
Measured and calculated band correction functions in [19,42] re- 
ported peak values of about 1.1 near the R-branch center which 
are  consistent  with  the  deviations  observed  here  between  1.1 
and 1.2 in the troughs between absorption peaks. Additionally, 
the reported super-to-sub Lorentzian transition frequency is near 
1936 cm −1 as observed in this work. 
7.3. High-temperature, high-pressure NO spectra 
Using the methods outlined above, the spectrum of NO in N 2 
was measured at 618 and 802 K at pressures up to 33 atm. These 
measurements  are  shown  in  Fig.  8 .  For  comparable  pressures, 
deviations  from  the  Voigt  model  using  the  HITEMP  database 
are less pronounced in high-temperature measurements than in 
room-temperature measurements. Studies have demonstrated that 
line-mixing and the breakdown of the impact approximation are 
primarily effects of gas density rather than simply gas pressure 
[17,40] .  Observed  deviations  trend  similarly  at  comparable  gas 
densities  with  the  room-temperature  measurements,  yet  the 
transition  from  super-to-sub  Lorentzian  is  no  longer  observed 
near  1936  cm −1  as  in  the  room-temperature  case.  To  the  au- 
thors’ knowledge band-correction functions for NO as a function 
of  temperature,  pressure,  and  collision  partner  have  not  been 
reported. Line mixing studies of other small molecules reporting 
band-correction  functions  at  several  temperatures  indicate  that 
as  temperature  increases  the  super-to-sub  Lorentzian  transition 
frequency  shifts,  like  the  rovibrational  intensity  distribution, 
to  frequencies  near  higher  J ′′  transitions  [39,40] .  Therefore,  a 
super-to-sub  Lorentzian  transition  is  expected  to  exist  outside 
the  tuning  range  of  the  ECQCL  used  in  this  study.  Additional 
observed deviations between the measured and simulated spec-  
tra reveal that improvements to the HITEMP database collision 
broadening  parameters,  particularly  for  high  J ′′  transitions  can 
be  made.  Furthermore,  the  inconsistencies  between  simulated 
and measured line shape broadening makes determination of the 
super-to-sub  Lorentzian  transition  frequency  difficult.  The  need 
to improve existing databases to properly account for collisional 
effects  at  high-temperatures  is  evident  from  the  high-pressure, 
high-temperature spectra presented here. 
8. Summary and conclusions 
The design, build, and operation of an optical cell for high- 
temperature,  high-pressure  mid-IR  spectroscopy  was  presented. 
For  optical  access  from  ∼ 0.15  to  8  μm,  the  optical  cell  uses 
16 cm CaF 2 rods to penetrate the temperature gradient imposed 
by  the  single-zone  furnace  used  as  the  primary  heating  load. 
Penetration by the CaF 2 rods allows the optical cell’s gas-tight seal 
to be maintained at much lower temperature during operation at 
high-temperature set points. Temperature uniformity across the 
optical path length of 21.3 cm is maintained by the temperature- 
controlled  single-zone  furnace  and  temperature-controlled  band 
heaters attached to the body of the optical cell. Good temperature 
uniformity up to 800 K and pressure stability above 30 atm were 
demonstrated. 
Spectra of the R-branch in the fundamental rovibrational band 
of nitric oxide at several temperatures and pressures up to 800 K 
and 34 atm, respectively, were measured by an ECQCL for the op- 
tical cell’s characterization and demonstration of utility. Deviations 
between the measurements and a Voigt-based model using the 
HITRAN/HITEMP databases were observed with increasing gas den- 
sity. These deviations are attributed primarily to line-mixing, but 
at elevated temperatures, inconsistencies between the observed 
pressure broadening and that of the HITEMP/HITRAN databases 
is evident. These findings emphasize the need for high-pressure 
and  high-temperature  spectroscopy  experiments  to  characterize 
high gas density phenomena, and this facility provides a practical 
means to study the spectra of molecules into the mid-IR. 
High-temperature gas-phase kinetics of the thermal decomposition of tetramethoxysilane
ABSTRACT
The decomposition of tetramethoxysilane (Si(OCH3)4, TMOS) was studied in shock-tube experiments in the 1131每1610 K temperature range at pressures ranging from 1.9 to 2.3 bar behind reflected shock waves combining gas chromatography/mass spectrometry (GC/MS) and high-repetition-rate time-of-flight mass spectrometry (HRR-TOF-MS). The initial reaction is a four-center elimination to form methanol. At elevated temperatures, TMOS also decomposes via a O每C bond scission forming a methyl (CH3) and the corresponding OSi(OCH3)3 radical. The main observed products were methane (CH4), methanol (CH3OH), ethylene (C2H4), and ethane (C2H6). The yields of these products increase with temperature. mechanism contains 13 silicon species and 24 reactions with silicon-containing species. It was combined with the methanol mechanism of Burke et al. The measured global rate constant for TMOS decomposition was found to be koverall[TMOS products].
INTRODUCTION
Silicon dioxide (SiO2) is widely used as a coating material and additive because of its chemical stability, hardness, transparency in the visible spectrum, and high resistivity. It is the basis for production of ultra-pure optical glass and a major application in the semiconductor industry is the use as an electrically insulating layer. Tetram-ethoxysilane (TMOS) is the simplest homolog in the group of the alkoxylsilanes Si(OR)4 and is used as a precursor for SiO2 coatings deposited through chemical vapor deposition (CVD) and is used for flame-based nanoparticle synthesis. For process development, the kinetics of chemical composition and primary reactions must be understood. There is a variety of potential precursors for high-temperature synthesis of SiO2, such as tetraethoxysilane (TEOS), hexamethyldisiloxane (HMDSO), and octamethylcyclotetrasiloxane (D4). Therefore, the dependence of the reactivity on the molecular structure is of interest when selecting precursors and process conditions for specific applications. This paper aims at providing experimental kinetics data and developing and validating a kinetics mechanism for the decomposition of TMOS and the following products formation in a wide range of temperatures.
Chu et al. studied the decomposition kinetics of TMOS by Fourier-transform infrared spectrometry (FTIR) in a static cell at temperatures between 858 and 968 K and a pressure of 930 mbar. They obtained the following overall first-order rate coefficient for the consumption of TMOS
reported that TMOS was consumed to a significant extent by bimolecular reactions and indicated that H atoms have the largest impact on the TMOS consumption. They developed a mechanism based on quantum-chemical calculations and assumed two rate-controlling reactions: TMOS CH3OH (CH3O)2SiOCH2 and CH2OSi(OCH3)3 CH2O Si(OCH3)3. Their model accounts semi-quantitatively for the formation of the secondary products. Similar experiments have recently also addressed oxygenfree silicon precursors such as tetramethylsilane (Si(CH3)4, TMS).
In the present work, experiments were carried out in a single-pulse shock tube coupled to gas chromatography/mass spectrometry (GC/MS) that enables concentration measurements of stable products in a conventional shock tube. The shock tube is also coupled to high-repetition-rate time-of-flight mass spectrometry (HRR-TOF-MS) for time-resolved concentration measurements of multiple species including intermediates from gases continuously sampled from the shock tube. This time-resolved information provides an important additional constraint for the development of reaction mechanisms. The sensitive GC/MS technique enables experiments with low reactant concentrations (250每1200 ppm) and permits the use of chemical inhibitors in large excess to suppress bi molecular reactions. In contrast, the measurements with HRR-TOF-MS require reactant concentrations in the 0.25每0.5% range. 
METHOD
The experiments were performed in a stainlesssteel shock-tube with an inner diameter of 80 mm and a total length of 8.8 m. The shock tube consists of a driver section with a length of 2.5 m and the driven section of 6.3 m, both sections are separated from each other by an aluminum diaphragm with thicknesses of 30每70 米m depending on the target temperature. To operate the shock tube in singlepulse mode, the driven section is equipped with a 0.35 m3 dump tank, which can be separated from the shock tube by a ball valve (60 mm inner diameter). In single-pulse mode, the dump tank is filled with nitrogen (N2) at the same filling pressures (50每150 mbar) as the driven section and the valve is opened just before an experiment is performed. The driver and driven sections were connected to a four-stage rotary pump (Edwards dry star, Model QDP 80) combined with a mechanical booster pump (Edwards, EH 500A) to achieve a final pressure of 3 10?3 mbar. The initial pressure p1 of the test gas in the driven section was measured using a capacitance manometer (Edwards, Trans 600 AB). The velocity of the shock wave was determined using a set of four pressure transducers (Kistler, 603B) placed at the end of the driven section in equidistant intervals of 150 mm and the velocity was extrapolated to the end plate of the driven section with an observed attenuation of less than 1%. The conditions of the gas behind the reflected shock wave (temperature T5 and pressure p5) were calculated from the shock-wave velocities using the standard ideal gas shock-wave relations with uncertainties in the calculated temperature around 15 K. A fifth pressure transducer (PCB, 112A05) was positioned 20 mm away from the end plate. For further details of the experimental facility, see Ref.
All mixtures were prepared in a 50l stainlesssteel vessel and stirred for at least four hours. A turbomolecular pump (Pfeiffer, TMH 071 P) evacuated the vessel down to a pressure of 7 10?7 mbar. Depending on the detection technique, Ar or Ne was used as bath gas (s. Table 1). In one set of experiments, toluene was used as a chemical inhibitor, which suppresses secondary reactions of reactive hydrogen atoms and methyl radicals formed during TMOS pyrolysis. In this case, toluene (C6H5每CH3) scavenges the formed radicals and permits to study the decomposition of TMOS without interference of secondary reactions.
For the identification and quantification, the shock tube is operated in single-pulse mode. The shock tube is coupled to a heated (363 K) gas sampling system that automatically transfers gas samples to a GC/MS (Agilent 7890A and MSD 5975C) after each experiment. The gas chromatograph (GC) was equipped with a PLOT-Q column for the separation of nonpolar and polar compounds up to C12 and a HP5 column for the separation of larger species. Both columns are connected to a quadrupole mass spectrometer (QMS). Kr was used as internal standard. Further details are given in Ref.
To simultaneously measure multiple species with a time resolution of 10 米s, the shock tube is operated in conventional mode and coupled to a high-repetition-rate time-of-flight mass spectrometer (HRR-TOF-MS, Kaesdorf). Details are reported in Ref. [11,15], therefore only a brief description is given here. The HRR-TOF-MS is connected by an ISO flange to the end plate of driven section of the shock tube. The end plate contains a nozzle with a diameter of 60 米m protruding 1 mm into the shock tube. An electron-impact ion source with an energy of 45 eV was used for the ionization and the acceleration of the ionizing electrons. The TOF-MS is operated in reflection mode and the ions are detected with a two stage micro-channel plate detector (MCP). The repetition rate was set to 100 kHz, which provides full mass spectra with masses up to 175 u in intervals of 10 米s. The data was recorded by a 8 bit A/D card (Agilent acqiris, DP214) with a sampling rate of 2 GS/s. In all HRRTOF-MS measurements, Ne was used as bath gas because of its low electron-impact ionization cross section and the integrated mass filter was used to further suppress the Ne signal to prevent detector overload. To maximize the signal-to-noise ratio (SNR), the MCP voltage was adjusted to maximize the reactant peak intensity within the linear range of the instrument while partially saturating the Ne peak. The raw data were converted into intensity每 time profiles by summation of the peak area over a certain time profile by a Matlab routine. 
The reaction mechanism was adopted from Chu et al. and updated in this work. It includes possible initial dissociation processes of TMOS and molecular elimination reactions as well as bond fissions. The mechanism is divided into two sub-mechanisms: The silicon/silicon-oxide sub-mechanism was taken from Chu et al. and was complemented with the results of this study; and the sub-mechanism describing the CH3OH chemistry was taken from Burke et al. Thermodynamic data of most Si-containing species were calculated by Ho and Meliu. However, for five species, n thermodynamic data were available (CH2OSi (OCH3)2, C2H5OSiO (OCH3), HSi(OCH3)3, CH2OSi (OCH3)3,and CH2OSi (OCH3)2) and thus obtained here from quantum chemical calculations at the G4 level of theory using the GAUSSIAN 09 package. The respective data is shown in the supplemental material.
Based on molecular structures, frequencies and moments of inertia obtained from the quantum chemical calculations, thermodynamic data Cp(T), Hf (T) and S(T) were derived by using the GPOP program. Cp, H, and S are also functions of the temperature and TMOS is considered as a perfect gas. At 1每2 bar and diluted conditions real gas effects are not expected. The vibrational frequencies calculated at the B3LYP/6-31G(2df,p) level of theory during the G4 procedure were scaled by 0.99 for the zero-point energy calculation and by 0.97 for the vibrational partition-function calculations. B3LYP functionals are known to underestimate imaginary frequencies in transition-state theory calculations, but apart from this problem, they usually yield very reasonable frequency values and we expect that this also applies for silicon-organic compounds. To support this assumption, we compared simulated IR spectra of TMOS and experimental frequencies (Fig. S1 in the supplemental material). The measured data were obtained by IR absorption measurements of thin amorphous films of condensed TMOS reported by Ignatyev et al. At wavenumbers below 1000 cm每1, the B3LYP calculation is able to reproduce experimental frequencies within deviations of less than 20 cm每1. Between 1000 and 1600 cm每1, the calculated frequencies deviate from the measured values by not more than 40 cm每1. Therefore, we assume that even for larger silicon-organic compounds like TMOS, the groups attached to oxygen atoms were treated as hindered rotations using the Pitzer-Gwinn approximation as implemented in GPOP. Rotational barrier heights for CH3 rotors were obtained from relaxed potential energy surface (PES) scans at the B3LYP/6-31G(2df,p) level of theory with dihedral angles used as coordinates for the scans. CH3 groups represent symmetric rotors, an asymmetric rotor. Since the Pitzer-Gwinn approximation for the hindered rotor partition function, qPG, depends on the rotational barrier height, V0, the partition function of the C2H5 rotor needs to be evaluated to derive suitable values for V0. For this purpose, the potential of the C2H5 rotor was fitted to a regression function. Here, 汐 represents the dihedral angle, a1每a12 are the fitting parameters, and c1每c6 are constants (Figure S1 (a)). With the program BEx1D, which is a Schr?dinger-equation eigen-problem solver for an intramolecular onedimensional hindered rotation, the exact values, qHR, for the hindered rotor partition function can be obtained based on the fitting function V(汐). The Pitzer-Gwinn approximation qPG is evaluated for the lowest vibrational mode corresponding to an internal rotation. The value of V0 is adjusted until qPG is within less than	5% of qHR (Figure S2 (b)). To limit the computational effort, the PG approximation is preferred over the exact partition function, which is only needed in special cases when strongly asymmetric rotor potentials are involved.
C2H5OSiO(OCH3) can also decompose by eliminations. There are two possible elimination channels: C2H5OSiO(OCH3) CH4 CH3CHO SiO2 R14) which represents a six-center elimination, and C2H5OSiO(OCH3) C2H4 OHSiO(OCH3) R15), which is a four-center elimination. Only by adjusting the rate constants k14 and k15 it was possible to correctly reproduce CH4 and C2H4 concentration profiles. Future quantum-chemical calculations should validate the possibility of CH4 elimination and the choice of rate constants for k14 and k15. Furthermore, the simulation of the experimental data of Chu et al. with our mechanism considering the initial TMOS decomposition steps in the fall-off range is in good agreement for CH4, CO, and TMOS except for CH2O at 915 and 953 K.
RESULT and DISCUSSION
In this study, we measured the rate constants k[TMOS products] of the decomposition of TMOS behind reflected shock waves in a singlepulse shock tube with GC/MS, and with HRRTOF-MS. The experiments were carried out at 1131每1610 K and 1.9每2.3 bar and concentrations between 330 and 5000 ppm TMOS in Ar or Ne (Table 1).
The pyrolysis of TMOS is endothermic and the heat consumption rises with increasing temperature. All measurements were modeled with Chemical Workbench based on non-reactive pressure profiles of pure Ar or Ne which account temperature variations during the reaction and cooling phases gure 1 shows the consumption of TMOS and the product distribution during the thermal decomposition of TMOS for varying temperatures and initial concentrations. The decomposition of TMOS starts noticeably around 1200 K and at 1400 K TMOS is completely consumed. The main products during the decomposition were CH3OH, CH4, C2H4, and C2H6. Acetylene (C2H2) was found in much smaller concentrations while no silicon-containing products were detected. The primary stable product CH3OH is formed via Si(OCH3)4 H3OH CH2OSi(OCH3)2 (R2). ocal sensitivity analysis for CH3OH demonstrates that R2 has the largest impact on the measured CH3OH concentration and at T > 1400 K, reaction CH3OH + H CH2OH + H2 strongly influences the CH3OH concentration (Fig. 2). This is the main reaction consuming CH3OH that was missing in the mechanism of Chu et al. 
Besides the elimination of CH3OH, TMOS can also decompose by the dissociation R1. This is only a minor channel and at 1000 K the branching ratio BRR1 k1/(k1 k2) of Chu et al. suggests that only 1.3% decomposes by R1. The impact of R1 rises with increasing temperature and for instance at 1525 K 30% of TMOS were decomposed by reaction R1 postulated by Chu et al.(Fig. 3).
Figure 1 shows that the formation of CH4, C2H4, and C2H6 starts at 1300 K. The CH4 concentration reaches its maximum at 1400 K. In comparison to CH4, C2H4, and C2H6 are formed in much smaller concentrations. CH3 is formed mainly by the bond fission R1 and slightly contributes to CH4 formation by bimolecular reactions with TMOS and TMOS decomposition products. Chu et al. identified carbon monoxide (CO) by FTIR as another reaction product during the decomposition. We also found CO, but due to its low sensitivity with the GC/MS it was not possible to quantify the CO concentration. The use of C6H5每CH3 as chemical inhibitor shifts the decomposition to significantly higher temperatures because C6H5每CH3 suppresses bimolecular reactions between TMOS and radicals. In addition, C6H5每CH3 inhibits the molecular elimination reaction to CH3OH probably because C6H5每CH3 reacts with the dipolar intermediate OC?H2每(OCH3)Si每O+H每CH3 postulated by Chu
et al. to TMOS and benzyl so that the molecular rearrangement of the intermediate to methanol and CH2OSi(OCH3)2 does not occur. In the presence of C6H5每CH3, almost no CH3OH was formed and only CH4 was measured. The formation of CH4 indicates that TMOS decomposes mainly by an O每C bond scission and is in accordance with the calculated energy barriers of C每O and Si-O bonds with 410 kJ/mol and 548 kJ/mol, respectively [9]. Figure 1 also shows the simulation of the measured TMOS and its products with the proposed mechanism (Table 2). The decomposition of TMOS for initial concentrations of 300每1200 ppm as well as the formation of products simulated based on the mechanism of this work agree with the experimental data. The carbon yield represents the amount of C atoms of formed products divided by the amount of C atoms of the decomposed TMOS and was calculated for different initial concentrations of TMOS (Figure S4). At the lowest extents of reaction, only 25% of the consumed carbon was measured. With increased temperature, more products are formed and the carbon yield increases up to 50%. We attribute this carbon-yield deficit to carbon bonded to undetectable intermediate silicon species after the initial decomposition step. Another factor that lowers the carbon yield, is the missing CO and CH2O in the carbon balance.
Figure 4 compares HRR-TOF-MS mass spectra for pre-shock conditions (T1 295 K, p1 42.5 mbar, averaged for 0.5 ms) and for post-reflected-shock conditions (T5 1516 K, p5 1.41 bar, averaged fo 1.5 ms). TMOS shows a base peak at m/z 121 and the parent peak at m/z 152 and many smaller fragment peaks spread over the complete spectrum For thee valuation of the concentration每time profiles of TMOS we used the base peak at m/z 121 that corresponds to Si(OCH3)3+. The peaks at m/z 40 and 80每86 are related to the internal standards Ar and Kr, respectively. The peak at m/z 28 is the base peak of CO and overlaps with a smaller fragment peak of TMOS and the base peak of C2H4, nevertheless it is possible to separate the signals of TMOS, C2H4 and CO at m/z 28, though every separation step deteriorate the SNR. For the evaluation of C2H4, the fragment signal at m/z 27 (C2H3+) was used. It is speculative but it is possible that the peak at m/z 30 is the parent peak of CH2O, the base peak at m/z 29 overlaps with a fragment peak of TMOS. The GC/MS measurements show that CH3OH is formed during the decomposition of TMOS, therefore the signal at m/z 32 is attributed to the parent peak of CH3OH and the base peak of CH3OH at m/z = 31 overlaps with fragment peak of TMOS. It was not possible to quantify CH3OH and CH2O concentrations because their signal intensities were too weak. However, the appearance of the peaks at m/z 30 and 32 shows qualitatively the formation of these products and reflects the decomposition of TMOS. With the HRR-TOF-MS, no silicon-containing product species were detected.
Figure 5 shows measured and calculated concentration每time profiles for TMOS, and CO for T5 1533 K, p5 1.35 bar for 0.5% TMOS in Ne. The simulation accurately predicts [TMOS]t and underpredicts [CO]t by 30%. The scatter of the CO data is caused by the evaluation using separation of three overlapping peaks at m/z 28. It was not possible to evaluate the base peak of C2H4 at m/z 28 because this peak overlaps with the base peak of CO and a fragment peak of TMOS. The SNR of the smaller fragment peak at m/z 27 was too low to quantify the C2H4 concentration.
Figure 6 contains the data of the simulated values for the TMOS decomposition using the mechanism discussed in Table 2 based on the TMOS consumption measured with GC/MS and HRR-TOF-MS. The Arrhenius expression koverall[TMOS products] 2.9 1011exp( 225 kJ mol?1/RT)s?1 was obtained by ascertaining the disappearance of TMOS to the sum of the dissociation channels (R1, R2). For the k1/k2 ratio, we used the temperature-dependent branching ratio BRR2 k2/(k1 k2) given by Chu et al. [9]. Figure 6 shows that the GC/MS data for temperatures between 1131 and 1438 K ties seamlessly to the HRR-TOFMS data for 1440每1610 K and the results of both techniques are consistent (Fig. 6), the uncertainty of the final rate expression is about 30%. The extrapolated rate coefficient of Chu et al. agrees well with our measurements at lower temperatures. At the higher temperatures of our study, the rate coefficients and the observed activation energy are lower due to fall-off effects at these conditions. The rate constant of the measurements with C5H6每CH3 compared to the measurements without inhibitor is roughly a factor of four slower and coincides with the rate constant of the dissociation of CH3 (R1). The result confirms the exclusive formation of CH4 when the chemical inhibitor was used.
CONCLUSION
The pyrolysis of TMOS was studied behind reflected shock waves at 1131每1610 K and 1.9 to 2.3 bar. The results indicate that TMOS decom- poses mainly via four-center isomerization with subsequent three-center cyclization and elimination forming CH3OH and (CH3O)SiOCH2. At higher temperatures, the C每O bond fission to CH3OSi(OCH3)3 gains importance. The main detected products are CH3OH, CH4, C2H4, and C2H6. Measurements with C6H5每CH3 as radical inhibitor reveal that the TMOS decomposition is influenced by secondary reactions. When C6H5每 CH3 was used, mainly CH4 and no CH3OH was formed. It is assumed that C6H5每CH3 interacts with a dipolar intermediate during the elimination of CH3OH and inhibits the formation towards CH3OH. A kinetics mechanism based on the mechanism of Chu et al. and the methanol mechanism from Burke et al. was developed. The silicon species sub-mechanism consists of 24 reactions containing 13 silicon species. The mechanism was updated with the rate coefficients of this study and additional reactions were included to account for the product concentrations. A good agreement was achieved with all detected species. This study substantially expanded the temperature range of previous TMOS pyrolysis studies and the mechanism can be applied for modelling high-temperature processes in gas-phase systems using TMOS as a precursor for the synthesis of SiO2.
Revisiting diacetyl and acetic acid ?ames: The role of the ketene + OH reaction
ABSTRACT
The mechanism of the reaction of ketene with hydroxyl radical has been studied by ab initio CCSD(T) F12/cc-pVQZ-F12//B3LYP/6-311G(d,p) calculations of the potential energy surface. Temperature and pressure-dependent reaction rate constants have been computed using the RRKM-Master Equation and transition state theory methods in the temperature range of 300每3000 K and in the pressure range of 0.01每100 atm. Three main channels have been analyzed: through direct abstraction of H atoms or starting with OH addition to the terminal carbon and to the central carbon atoms. Major products identi?ed agree with the recent theoretical studies, however, signi?cant difference was found with the rate constants derived by Xu et al. and Cavallotti et al. To investigate the impact of the choice of reactions between CH2CO and OH radicals on the predicted burning velocities of the ?ames sensitive to ketene chemistry, namely diacetyl and acetic acid ?ames, a detailed kinetic mechanism was updated with pertinent reactions suggested in the literature. Then the rate constants of four most important product channels of reaction CH2CO + OH forming HCCO + H2O, CH2OH + CO, CH3 + CO2 and CH2COOH from the present and from the recent theoretical studies were tested. Good agreement with the burning velocities of diacetyl + air ?ames was found for the present model, while the expressions from the literature underestimate them. On the contrary, any combination of the rate constants of reactions between ketene and hydroxyl radical overpredicts burning velocities of acetic acid + air ?ames, which strongly indicates that the kinetic model of acetic acid is most probably incomplete and requires consideration of additional reactions.
INTRODUCTION
A common feature of combustion chemistry of diacetyl, (butane-2,3-dione, (CH3CO)2), and acetic acid, CH3CO2H, is the formation of signi?cant concentrations of ketene that allows for pinpointing important aspects of CH2CO conversion through experimental and theoretical analysis of pertinent reactions. Therefore, several studies focusing on reactivity of diacetyl and acetic acid have been performed quite recently and kinetic mechanisms including updated ketene chemistry have been developed. Earlier analyses of high-temperature diacetyl reactions focused on its pyrolysis until the ?rst ?ame study by Christensen and Konnov. Yang et al. investigated dissociation of (CH3CO)2 in a shock tube and determined pressure-dependent rate constant of the initial decomposition step. Moreover, a kinetic model for diacetyl pyrolysis has been proposed aiming at prediction of formation of methyl radicals in shock tube kinetic studies. This mechanism was extended by reactions of diacetyl oxidation to calculate burning velocity of diacetyl + air ?ames measured using the heat ?ux method. Particular attention in that study was paid to updates of ketene sub-mechanism and its validation by comparison with available experimental data on ketene pyrolysis and oxidation. The model closely reproduced burning velocity in lean and rich mixtures while underpredicted it in stoichiometric and slightly rich ?ames. The focus of the recent work of Zhang et al. was on methyl-related carbon chain growth reactions during pyrolysis of diacetyl from low to high pressures. The sub-mechanism for (CH3CO)2 decomposition was very similar to that suggested by Yang et al. One important mistake, which originates from the misprint in was spotted by Zhang et al. in the low-pressure rate constant of reaction implemented in the model of Christensen and Konnov. Most recently Sun et al. investigated diacetyl ?ame structure at low pressure using probe sampling and time-of-?ight mass spectrometer. They performed theoretical calculations of several important abstraction rate constants and introduced pressure-dependent reaction of decomposition which in other models was assumed to be instantaneous.
Sun et al. also improved the model proposed earlier implementing several rate constants relevant to diacetyl and acetic acid decomposition calculated in their earlier study. The chemistry of acetic acid is of importance in combustion of some oxygenated fuels, namely ethyl esters and acetates. These esters may decompose directly forming CH3 CO2 H, and therefore pertinent sub-mechanism suggested by Christensen and Konnov was incorporated into recent kinetic models for esters and further developed using theoretical calculations. Speci?cally, Cavallotti et al. calculated rate constants of H abstraction reactions from acetic acid by radicals and found them somewhat different from previous theoretical study by Mendes et al. Moreover, pressure-dependent acetic acid decomposition reactions and addition reactions of the key radical CH2 CO2 H + H have been obtained. Finally, decomposition of CH2 CO2 H and reactions between ketene and OH on the same potential energy surface were explored and pressure-dependent rate constants have been derived. All these updates will be explored in the present work.
Simultaneously, Xu et al. studied the same system of CH2 CO + OH and calculated rate constants of different reactions channels. Detailed comparison of the theoretical studies by Cavallotti et al. and Xu et al. reveals that predicted rate constants are not very close as will be shown in the following. On the other hand, in the study of CH3 CO2 H chemistry it was concluded that the mechanism performance in prediction of the burning velocities of acetic acid ?ames could be improved by revision of reactions between CH2 CO and OH radicals, while keeping its agreement with other ?ames studied. The goal of the present study was therefore threefold: (a) to calculate the rate constants of ketene + OH, (b) to update detailed kinetic mechanism of the authors by reactions of acetic acid and diacetyl suggested in the literature and mentioned above, and (c) to investigate the impact of the choice of reactions between CH2 CO and OH radicals on the predicted burning velocities of the ?ames most sensitive to ketene chemistry.
METHOD
Geometries of the reactants, products, transition states, and reaction intermediates in the reactions of ketene with radical OH have been optimized using the hybrid density functional theory (DFT) B3LYP and doubly hybrid DFT B2PLYPD3 methods with the 6-311G(d,p) and Dunning＊s cc-pVTZ basis sets, respectively. Vibrational frequencies have been computed at the same levels of theory to characterize stationary points as local minima or transition states, to obtain zero-point vibrational energy corrections (ZPE), and to be utilized in partition function calculations. However, the B2PLYPD3 method gave unrealistic imaginary frequencies for some transition states and hence, B3LYP geometries and frequencies were chosen for the evaluation of rate constants including calculations of partition functions and tunneling corrections. The connections between transition states and local minima were veri?ed and con?rmed by intrinsic reaction coordinate (IRC) calculations at the B3LYP/6-311G(d,p) level. All DFT calculations were performed employing the Gaussian 09 program package. Further, the B3LYP optimized geometries, which appeared to be very similar to those obtained at the B2PLYPD3 level, were used to re?ne single-point energies using the explicitly-correlated coupled clusters CCSD(T)-F12 method with the cc-pVQZ-F12 basis set; the MOLPRO package was utilized for these calculations. The CCSD(T)-F12/cc-pVQZ F12 approach closely approximates CCSD(T)/CBS energies and it is expected that the accuracy of the CCSD(T)-F12/cc-pVQZ F12//B3LYP/6-311G(d,p) + ZPE(B3LYP/6-311G(d,p)) relative energies should be within 0.7 kcal/mol or better. Two versions of CCSD(T) were tested, restricted open-shell ROCCSD(T)-F12 and unrestricted UCCSD(T)-F12, both based on the ROHF initial wave function. While for most structures the differences between ROCCSD(T)-F12 and UCCSD(T)-F12 relative energies were very small (within 0.1每0.3 kcal/mol), these differences appeared to be signi?cant for the critical entrance transition states TS(i1-i2) and TS(i1-i4) connecting the reactant van der Waals complex H2 CCO＃OH (i1) with covalently bound intermediates i2 and i4.
We selected to use UCCSD(T)-F12 energies for these transition states following the recommendation by Harding, Klippenstein and coworkers who carefully analyzed the performance of different ab initio methods for reactive potential surfaces. They found in particular that for radicals UCCSD(T) normally performs somewhat better than ROCCSD(T) providing a closer agreement with the benchmark full CI results or with the most accurate multireference CASPT2 values. In our calculations, the best agreement with the available experimental data (see Section 3.4) for reaction rate constants was found with UCCSD(T)-F12/cc-pVQZ-F12 single point energies for TS(i1-i2) and TS(i1-i4). T1 diagnostics was used to monitor whether the wavefunctions of the calculated structures possess a signi?cant multireference character. Only one structure, TS(i1-P1) for direct H abstraction by OH to form H2 O, was found with a large T1 diagnostics value exceeding 0.03. Therefore, for this structure we performed muti-reference perturbation theory CASPT2 calculations with the active space including 17 electrons distributed on 13 orbitals (17,13); this active space contained all valence electrons and all valence orbitals (23,19) excluding those corresponding to C每H and O每H bonds and respective antibonding orbitals (6,6). The multistate CASPT2 calculations included 3 electronic states of the species considered, i.e., the CH2 CO + OH super molecule, i1, and TS(i1-P1). The use of the IPEA shift of 0.25 and the standard MOLPRO level shift of 0.2 in the CASPT2 calculations did not affect the relative energies signi?cantly. The CASPT2(17,13) calculations were carried out with the cc-pVTZ and cc-pVQZ basis sets and the energy was then extrapolated to the complete basis set (CBS) limit using the two-point extrapolation:
In order to determine the relative energy of TS(i1-P1), the CASPT2(17,13)/CBS calculations were also carried out for the re actants, ketene + OH, considering them as a super molecule, i.e., as two non-interacting fragments (with geometries optimized for isolated species) separated by a large distance. Noteworthy, if the i1 complex instead of the super molecule is used as a reference in CASPT2 calculations of the relative energy of TS (i1-P1), the result is not affected. The CASPT2(17,13)/CBS barrier height at TS(i1-P1), 4.8 kcal/mol relative to the reactants computed using multistate calculations with 3 electronic states included, appeared to be lower than the values reported in the previous works, 8.7 kcal/mol at G3(MP2) and 6.5 kcal/mol at the composite UCCSD(T)-F12/cc pVTZ-F12 level with an MP2-F12 correction to the cc-pVQZ-F12 basis set. It should be noted however that the CASPT2 barrier height varies between 2.8 and 6.4 kcal/mol depending on the number of states included in the CASPT2 calculations and therefore the uncertainty of the this barrier height is likely ㊣ 2 kcal/mol. Temperature and pressure-dependent rate constants for the re actions considered were evaluated within the framework of RRKM theory in combination with the Master Equation approach (RRKM ME). The MESS code was used for the rate constant calculations in the temperature range of 30 0每30 0 0 K and in the pressure range of 0.01每100 atm. The rigid rotor-harmonic oscillator approximation (RRHO) was employed in partition function calculations. ※Soft§ vibrational modes, which can be characterized as intramolecular rotations and have low vibrational frequencies, were treated as hindered rotors. The internal rotational potentials for the hindered rotors were evaluated using B3LYP/6 311G(d,p) calculations. For the submerged transition states TS(i1 i2) and TS(i1-i4), variational transition state theory (VTST) calculations were performed, i.e., the IRC minimal energy reaction path (MEP) was mapped at the B3LYP/6-311G(d,p) level and single-point energies of the IRC structures were re?ned using the UCCSD(T) F12/cc-pVQZ-F12 method based on the ROHF initial wavefunction. The IRC structures were included as transition state candidates in variational calculations by the MESS package. Tunneling corrections using asymmetric Eckart potentials were included in rate constant calculations. As will be seen below, tunneling may play a signi?cant only at low temperatures for the H abstraction channel leading to the HCCO + H 2 O products since the other important reaction channels are controlled by the submerged transition states TS(i1-i2) and TS(i1-i4). The H abstraction channel does not con tribute to the overall reaction outcome substantially at low temperatures and hence more sophisticated treatment of tunneling was not pursued. Collision parameters, such as Lennard-Jones parameters needed for the calculation of the pressure dependence of rate constants were taken from Jasper＊s database for C 2 H 3 O 2 ( 汍/cm ?1, 考 / ?A) = (1762.424, 4.104) and N 2 ( 汍/cm ?1, 考 / ?A) = (68.0, 3.610). Since the entrance channel in the reaction of ketene with hydroxyl radical to form the van der Waals complex i1 is barrierless, we used phase space theory to generate its high pressure limit rate constant. The phase space theory rate constants were then combined with RRKM rate constants for other reactions on the potential energy surface (PES) in RRKM-ME calculations. The potential prefactor and power exponent were selected in such a way that the association rate constant for ketene + OH ↙ i1 at the high pressure limit was close to the gas-kinetics limit of ~2 ℅ 10 ?10 cm 3 molecule ?1 s ?1. However, the actual values of the 
association rate constant and the rate constant for the reverse re action of dissociation of i1 back to the reactants are not relevant at considered temperatures above 300 K when the equilibrium between the reactants and the rather weakly bound complex i1 is rapidly established. Under such conditions, the high-pressure limit rate constant e.g., for ketene + OH ↙ i2, can be expressed as Thus, under the conditions where the pre-equilibrium between the reactants and i1 has been established, the rate constant k (R-i2) depends only on the partition functions of the reactants Q R and the most important calculated rate constants were ?tted to modi?ed three-parameter Arrhenius equations. The RRKM-ME the oretical approach employed in the present study is capable of computing temperature and pressure-dependent rate coe?cients with ＆kinetic accuracy＊, i.e., with accuracies comparable to that of experiment, typically within a factor of 2 or better. The exception is the H abstraction channel, where the relatively high uncertainty in the entrance barrier height increases the uncertainty of the calculated rate constant, especially at low temperatures.
RESULT and DISCUSSION
At the ?rst stage of the interaction of the OH radical with ketene, an intermolecular complex i1 is formed, which further decomposes due to chemical processes between ketene and OH. Noteworthy, we found two possible reactant complexes, a planar structure i1 where a hydrogen bond is formed between H of OH and O of ketene and non-planar i1＊ with a hydrogen bond between O of OH and H of ketene. While i1＊ was found in, the present calculations show i1 to be very slightly (by 0.05 kcal/mol) more stable than i1＊. The reaction of ketene with OH can then proceed by the following 3 channels, where channels 2 and 3 have several different products: Channel 1 is carried out by the abstraction of H atom from ketene by the OH radical and leads to the formation of a water molecule and ketenyl radical. Channel 2 proceeds by the OH addition to the terminal carbon atom followed by the formation of ?ve different products: CO + CH 2 OH, H + OHCHCO, H + (HCO) 2, HCO + H 2 CO, and CH(HCO)OH. Channel 3 involves the OH addition to the central carbon atom eventually leading to the formation of two different products: CO + H 3 CO and CO 2 + CH 3. Figures S1 and S2 in the Supplemental Material show optimized geometries of intermediates and transition states along with their notations, which will be used from here on. The calculated total energies of all species at the B3LYP and CCSD(T)-F12 levels, ZPE, symmetry point groups, electronic terms, rotational symmetry numbers, the number of optical isomers, and imaginary frequencies (for transition states, TS) are assembled in Table S1 in the Supplemental Material. Note that we did not consider product complexes here, which were studied and presented in, since they are not expected to play a signi?cant role in the reaction kinetics under combustion conditions. Figure 1 depicts the pro?le of the PES. PES was studied earlier by Hou et al., Xu et al., and in part by Cavallotti et al., and the relative energies from these works are also shown in Fig. 1 for comparison. Our results are in a close agreement with 
the values from and except for channel 1 (due to a multireference character of the transition state noted above), and the present calculation method, CCSD(T)-F12/cc-pVQZ-F12 is generally more accurate than the G3(MP2) approach used by Hou et al. According to the present results, channel 1 proceeds via a bar rier of 4.8 ㊣ 2 kcal/mol. Channels 2 and 3 exhibit submerged entrance barriers following the formation of the van der Waals com plex i1. Channel 2 proceeds via an intermediate i2, after which it is divided into several subchannels. However, only subchannel 2a leading to the formation of CO and CH 2 OH passes through only one low barrier. Alternatively, subchannels 2b-2f exhibit high barriers in the 36.5每53.0 kcal/mol range and therefore are unlikely to provide meaningful contributions to the total product yield. As shown by rate constant calculations below, the only product that is formed in signi?cant quantities through channel 2 is CO and CH 2 OH. Channel 3 also has a submerged entrance barrier leading to an intermediate i4 residing in a deep potential well. Therefore, one can expect that i4 can be stabilized and thermalized by collisions or dissociate back to the reactants. This radical intermediate, if stabilized, can participate in bimolecular reactions with other molecules or radicals available or undergo unimolecular dis sociation, also preferably back to ketene + OH. After the formation of the intermediate i5 channel 3 diverges to several subchannels. However, subchannel 3a has a lower barrier than 3b and 3c, so it can be expected that the products CH 3 and CO 2 are likely to prevail along this reaction channel. Interestingly, while the relative energy of TS(i5-P7) computed here is very close to the values reported i and ( Fig. 1 ), the present energy of i5 is ~3 kcal/mol lower than the result in but identical to the value in  13 . The i5 ↙ TS(i5-P7) ↙ CH 3 + CO 2 dissociation channel is peculiar because both i5 and the transition state have planar structures, but different electronic states, 2 A§ and 2 A＊, respectively. IRC calculations without symmetry constraints con?rmed that TS(i5-P7) connects i5 and P7 via a pathway along which the planar symmetry is broken. In the meantime, an excited 2 A＊ electronic state of i5 is computed to lie 15.7 kcal/mol above the ground 2 A§ state. The overall reaction rate constants computed as a sum of rate constants of all reaction channels and branching ratios of individual products calculated at the atmospheric pressure are collected in Table S2, whereas Fig. S3 in the Supplemental Material illustrates the products branching ratios at different pressures. One can see that the main product in the entire considered range of temperatures and pressures is CO + CH 2 OH. Even at high temperatures, its branching ratio exceeds or is close to 50% up to 2400 K decreasing to 41% at 30 0 0 K. The hydrogen abstraction channel 1 producing a water molecule and a ketenyl radical gives a second largest contribution at high temperatures, with the branching ratio of about 21% at 1500 K and 56% at 30 0 0 K. The dependence of the branching ratio of CO 2 + CH 3 on temperature shows a maximum 
of ~11% at 10 0 0每130 0 K and 1 atm. The branching ratio of the intermediate i4 (CH 2 COOH) increases with pressure and at 1 atm has its maximal values at temperatures 50 0每60 0 K and then decreases. The remaining products have negligible branching ratios. With this overall picture of the product distribution in mind, below we discuss calculated rate constants for individual product channels and compare them with rate expressions used in current kinetic models. It should be noted that in the recent studies some rate constants related to ketene + OH were not calculated theoretically, but were estimated through thermochemical methods, as will be speci?ed in the following.
Modi?ed Arrhenius expressions for the formation of the ketenyl + H 2 O products are presented in Table 1 and Fig. 2 illustrates temperature dependence of the rate constant of reaction CH 2 CO + OH = HCCO + H 2 O and compares the calculated ex pression with the literature data. As can be seen from the ?gure, the rate constants increase with increasing temperature. The calculated rate constant here is independent of pressure. The low-temperature values are particularly sensitive to the barrier height which, as discussed above, have a signi?cant uncertainty of ㊣ 2 kcal/mol. When the ＆central＊ value of 4.8 kcal/mol is used obtained by three-state CASPT2 calculations, the rate constant at 300 K computed here is a factor of 20.6 higher that the value obtained in. The difference between the two sets of values remain signi?cant at higher temperatures but decreases to fac tors of 4.9, 3.6, and 3.1 at 10 0 0, 20 0 0, and 30 0 0 K, respectively. On the other hand, the present rate constant computed with the 4.8 kcal/mol barrier agrees within a factor of 2 with the values obtained from the rate expression in above 700 K. Similar agreement within a factor of 2 is also seen with rate expression proposed in in the 90 0每220 0 K temperature range. The uncertainty in the calculated rate constant caused by the uncertainty in the barrier height is large at low temperatures up to 10 0 0 K. However, at these temperatures, the contribution of the H abstraction channel to the total reaction rate constant is small and does not exceed 0.1% at 300 K and is ~7% at 10 0 0 K. The uncertainty in the rate constant decreases at higher temperatures. For instance, at 10 0 0 K and above the values computed with the barrier heights of 2.8 and 4.8 kcal/mol agree within 30%. Alternatively, the rate constant calculated with the barrier height of 6.4 kcal/mol is factors 4.5每2.3 lower at 10 0 0每30 0 0 K but is close to the values computed Channel 2 can produce ?ve different product pairs including CO + CH 2 OH, H + OHCHCO, H + (HCO) 2, HCO + H 2 CO, and a stabilized CH(HCO)OH intermediate. However, only the formation of CO + CH 2 OH has a high branching ratio, the branching ratios of the remaining products are insigni?cant. Our calculations show that the rate constants for the formation of CO + CH 2 OH, H + OHCHCO, H + (HCO) 2, and HCO + H 2 CO are nearly independent of pressure, whereas the dependence of the rate constant for the stabilization of CH(HCO)OH on temperature at different pressures is shown in Fig. S4d in the Supplemental Material. Modi?ed Arrhenius expressions for the formation of CO + CH 2 OH, H + OHCHCO, H + (HCO) 2, HCO + H 2 CO and CH(HCO)OH are presented in Tables 1 and 2. Figure 3 illustrates temperature dependence of the rate constant of reaction 
CH 2 CO + OH = CO + CH 2 OH and compares the calculated expression with the literature data. Experimentally, only Grussdorf et al. determined branching ratios in the reaction between ketene and OH, which was later substantiated by ab initio calculations of Hou et al. Other measurements yielded overall rate constants at temperatures close to the room one. Since the branching ratio for CO + CH 2 OH at these conditions is above 80%(see Fig. S3) these values are also shown in Fig. 3.
One can see that our results for the rate constant of CO + CH 2 OH formation are in a good agreement with experimental data. As the temperature rises, the rate constant at ?rst slightly decreases up to 500 K, but then noticeably increases at higher temperatures. A similar trend at low temperatures was ob served earlier experimentally. This behavior can be reproduced only if the barrier at TS(i1-i2) is submerged, i.e., if this transition states lies lower in energy than the reactants. Restricted open-shell ROCCSD(T)-F12 calculations gave a slightly positive relative energy for TS(i1-i2), whereas the UCCSD(T)-F12 approach gave a negative energy. The inclusion of variational effects for this tran sition state does not affect the low-temperature values substantially but decreases the high-temperature values by 25%, 58%, and 75% at T = 10 0 0, 20 0 0, and 30 0 0 K, respectively. Although the energetics for channel 1 calculated in the present work is within 0.3 kcal/mol from the values obtained recently in, the calculated rate constants disagree by up to a factor of 6.4 at 300 K. The present values are higher and closer to the available experimental data. Both calculations here and in show a similar trend with temperature. The calculated rate constant of the minor HCO + H 2 CO channel is rather small and therefore it was not considered in other modeling studies. However, using input ?le for the MESS code provided in the Supplemental material to the work of Xu et al.one may retrieve the values of this rate constant. These values agree within a factor of 2 with our expression listed in Table 1. Channel 3 leads to the formation of two bimolecular products, CO 2 + CH 3 and CO + CH 3 O; in addition, the collisionally stabilized intermediate i4 can be also formed through this channel. The formation of CO + CH 3 O is insigni?cant, but CO 2 + CH 3 represents an important product. Modi?ed Arrhenius expressions for the formation of the products of channel 3 are presented in Tables 1 and 2. Note that variational effects for the submerged entrance channel TS(i1-i4) do not in?uence the calculated rate constants for channel 3 signi?cantly. The dependence of the reaction rate constants for CO 2 + CH 3 and intermediate i4 on temperature at different pressures is shown in Fig. S4 in the Supplemental Material. The reaction rate constant for CO 2 + CH 3 increases with temperature and decreases with pressure. The dependence of the rate constants for the formation of these products on temperature at the pressure of 1 atm is given in comparison with the literature data in Fig. 4. Again, the calculated behavior differs from the previous literature estimation but shows a reasonably close agreement with the results of theoretical studies. Note that Cavallotti et al calculated this rate constant at temperatures above 700 K. The different trend is observed for the stabilized intermediate i4 (CH 2 COOH); the reaction rate constant does not change or slightly increases with temperature at low temperatures (below 600 K) and decreases with increasing temperature at the temperatures relevant to combustion (10 0 0 K and above) as shown in Fig. 5. Also, the rate constant for the formation of i4 always increases with pressure. Estimated rate constant and theoretical calculations  11, 13  are also shown in Fig. 5. Cavallotti et al. calculated this rate constant in the reverse direction. For comparison, the rate of recombination was calculated using their reverse rate and thermo data for CH 2 COOH (also accepted in the present model) taken from. The present kinetic model consists of 1797 reactions and 162 species and is based on the most recent Konnov mechanism provided by Capriolo et al. with some important modi?cation introduced by Konnov. Speci?cally, the termolecular reactions associated to hydrogen oxidation, namely, H + O 2 + R suggested by Klippenstein and Burke were included in the mechanism; furthermore, transport properties for a large number of molecules and radicals were updated using calculations of Jasper et al. Reactions introduced and/or revisited in the present tudy are listed in Table 3 and described below. Note that several pressure-dependent rate constants are incorporated in the mechanism using PLOG function and without ( + M) notation, while Table 3 shows only expressions at 1 atm. 4.1. Reactions of diacetyl Zhang et al. recently investigated pyrolysis of diacetyl and largely adopted pertinent mechanism from Yang et al. They noted important mistake in the low-pressure rate constant of re action implemented in the model of Christensen and Konnov due to numerous misprints in the original pape; now this rate constant is correctly listed in Table 3. Using probe sampling and time-of-?ight mass spectrometer Sun et al. investigated the structure of a low pressure diacetyl ?ame. In the previous models for diacetyl it was assumed that CH 2 COCOCH 3 radicals formed by abstracting H atoms decompose immediately forming ketene and acetyl. Sun et al. introduced pressure-dependent reaction of decomposition. Thermodynamic data for CH 3 COCOCH 2 have been also adopted from Sun et al. Moreover, they performed theoretical calculations of important abstraction rate constants by H atoms ( R13) and CH 3 radicals (R14), as well as displacement reactions. It was demonstrated that introducing the latter reaction al lows for much better prediction of the acetaldehyde pro?le in di acetyl ?ame as compared to the model of Christensen and Konnov. For other abstraction reactions by C 2 H 5, O, OH, HO 2, and O 2 the same rate constants as suggested earlier were kept, however ketene and acetyl radicals in the products were replaced by CH 2 COCOCH 3. Finally, to evaluate possible role of displacement re action with OH, (CH 3 CO) 2 + OH = CH 3 CO + CH 3 CO 2 H (R17) 
it was included in the present model with estimated rate constant. 4.2. Reactions of acetic acid. Although the work of Sun et al. was focused on diacetyl ?ame chemistry they also proposed to implement several rate constants relevant to acetic acid decomposition calculated in their earlier study. However, Cavallotti et al revisited this system also theoretically and found signi?cant departure from the results of Sun et al. It was argued that the level theory used in is higher than that in, and therefore most of the rate constants calculated by Cavallotti et al. are adopted here. Only isomerisation and decomposition of 1,1-ethenediol not covered in yet included in our model with the rate constants previously taken from Clark et al. H abstraction reactions ( R24 )每(R32) were also presented in as forward reactions only, moreover, CH 3 CO 2 was assumed to decompose immediately, forming CH 3 + CO 2. These reactions were all made reversible in the present work and radical CH 3 CO 2 was considered as a product, where appropriate. Cavallotti et al. compared their calculated rate constants with the previous theoretical study by Mendes et al. and in most cases found differences within a factor of 2. However, considerable divergence from Mendes et al. was observed for H abstraction by H carboxyl group and for H abstraction by OH attributing these differences to the higher level of theory used i. For both channels of reaction with OH Cavallotti et al. showed in Supporting Information of that their rate constants are higher by a factor of 2.5每4. Regrettably, this Supporting Information contains two mistakes. First, the rate constant of reaction ( R24) derived by Mendes et al. is actually higher (by about 10%) compared to that of as shown in Fig. 6. The rate expression of reaction ( R29) listed in Table S6 of is also lower than that from  12  that contradicts to the good agreement between experimental data of Khamaganov et al. and the total rate shown in Fig. 4 of  11 . This contradiction was re solved by digitizing the rate constant of reaction ( R29) from Fig. S5f of  11 , which yields expression listed in Table 3 and shown in Fig. 6. The total rate constant of reaction CH 3 CO 2 H + OH derived in 11 is then in good agreement with available experiments at least up to 700 K.
High and low-pressure limiting expressions for reaction (R-38) obtained by Wagner and Zabel was adopted in the mechanism of Christensen and Konnov and in other models. Friedrichs and Wagner implemented frequency modulation spectroscopy to detect singlet methylene during decomposition of ketene in a shock tube. They summarised previous works, and suggested to use pressure-dependent rate constant of recombination based on RRKM calculations, which is adopted in the present mechanism. Reactions of ketene with CH 3 and CH 2 radicals have been investigated in our recent theoretical studies and all pertinent channels with calculated rate constants have been implemented in the present model. hey are included in the present mechanism for completeness that requires additional reactions of their consumption to make the model consistent. The last two species were incorporated in the mechanism of Battin-Leclerc et al. mostly with estimated rate constants. In the present model reactions of HOCHCO were added and sub-mechanism of glyoxal was up dated largely relying on the studies of Fa?heber et al. and of Friedrichs et al. The sub-mechanism of formic acid, HCOOH, which is formed in some reactions of these three species, was also updated mainly following the work of Marshall and Glarborg. None of these reactions affects calculated burning velocities, therefore the choice of their rate constant is presented in the Supple mental material. Modeling was performed using ANSYS Chemkin 17.0 software. The laminar burning velocity was simulated using the premixed laminar ?ame speed calculation module with multicomponent transport and thermal diffusion options taken into account. Radiative heat losses were also considered in the simulations since they may affect calculated burning velocities of slowly burning ?ames. The radiation model considers the radiation from CO 2 and H 2 O through an optically thin medium with absorption coe?cients taken from 62. The impact of the radiative heat losses on the calculated burning velocity was typically smaller than 0.2 cm/s. A grid independent solution was ensured by setting the parameters GRAD and CURV to values of 0.03 and 0.02, respectively, resulting in a typical number of grid points above 600 over the domain of 3 cm. 
In the following three models will be tested and compared with experimental data for burning velocities of diacetyl and acetic acid. Present Model I includes all reactions listed in Tables 1 每3. In Model II the rate constants of reactions calculated by Cavallotti et al. were implemented; all other re actions, thermodynamic and transport parameters were not modi?ed. Similarly, in Model III the rate constants of reactions (R1), ( R2 ), ( R4) and ( R5) calculated by Xu et al. were implemented. 
The laminar burning velocities of diacetyl + air ?ames at 1 atm were determined using the heat ?ux method in non-stretched ?ames at initial gas temperatures of 298 K, 318 K, and 338 . The measurements at 298 K with associated experimental uncertainties are presented in Fig. 7, while those for 318 and 338 K are illustrated in Figs. S11 and S12, respectively. The burning velocities were obtained over 3 non-consecutive days using two different installations as described by Christensen and Konnov and therefore presented by different symbols on these plots. Calculated burning velocities of diacetyl + air ?ames using the previous and present mechanisms are also shown in Figs. 7, S11 and S12. The model of Christensen and Konnov closely reproduced experimental results in lean and rich mixtures while underpredicted them in stoichiometric and slightly rich ?ames with maximum deviation by 2每3 cm/s. Models II and III predict burning velocities very close to the result of the previous model slightly di verging from it only in rich ?ames. Model I is in a slightly better agreement with experimental data over the entire range of equivalence ratios. It should be noted that the difference between Models I, II and III is only in the rate constants of reactions ( R1 ), ( R2 ), ( R4) and ( R5) between ketene and OH, which were not present among 
sensitive for diacetyl + air ?ames as was shown earlier by Christensen and Konnov or by Sun et al. To clarify which changes affect the burning velocity of di acetyl + air ?ames the sensitivity analysis was performed using Model I as depicted in Fig. 8. Since the models I每III are only differ ent by the rate constants of 4 reactions, the sensitivity spectra for them are very similar. Comparing previous and the present sensitivity spectra, one may note that among fuel-speci?c reactions 
displacement keeps its positive impact on the burning velocity of diacetyl. The rate constants of these reactions have been updated in the current mechanism based on the study of Sun et al. For ketene, only reaction CH 2 CO + H = CH 3 + CO appeared in the earlier sensitivity spectra with negative normalized coe?cients because it converts H atoms into less reactive methyl radicals. However, reaction (R2) forming CH 2 OH + CO emerges in the present analysis ( Fig. 8) because in Model I it has signi?cantly higher rate constant (see Fig. 3) comparing to the other models. It effectively competes with the reaction between ketene and H atoms that leads to a slight increase in the predicted burning velocity. Hence, one may conclude that thanks to these and other modi?cations implemented in the present work, Model I is suitable for calculations of the burning velocities of diacetyl + air ?ames. 
The laminar burning velocities of acetic acid + air ?ames at atmospheric pressure and initial gas temperatures of 338 K, 348 K, and 358 K have been determined for the ?rst time by Christensen 
and Konnov. These experiments were accompanied by di?culties caused by the corrosiveness of acetic acid towards the burner material. However, the procedure of normalization of the apparent burning velocity values by the well-known burning velocities of methane ?ames allows to circumvent these problems. On the other hand, this approach leads to additional signi?cant uncertainty that was evaluated to be about ㊣ 2 cm/s and includes day to-day variations observed over 3 non-consecutive days using two different installations. Therefore, the measurements at 338 K presented in Fig. 9, possess two types of experimental uncertainties: smaller ones based on standard procedures described elsewhere and additional error bars due to problems just mentioned and discussed indetails in 8. Similar plots showing burning velocities at 348 and 358 K are presented in Figs. S13 and S14, respectively. Different symbols on these plots and in Fig. 9 illustrate day-to-day variations of the measurements. 
The model developed by Christensen and Konnov overpredicted measured burning velocities of acetic acid + air ?ames by about 3 cm/s that exceeded evaluated experimental uncertainty of ㊣ 2 cm/s. Modi?cations implemented in the present work and ap plied in Models I每III lead to divergent behavior as illustrated in Figs. 9, S13 and S14. Models II and III are closer to the experimental data in rich ?ames as compared to the previous mode 8 . Present Model I, however, still predicts higher burning velocities than the measurements by about 3每3.5 cm/s, which is certainly associated with different rate constants of reactions ( R1 ), ( R2 ), ( R4) and ( R5 ). To elucidate reactions responsible for the differences observed, the sensitivity analysis was performed using Model I as shown in Fig. 10. Similar to the previous analysis 8 it was found that the calculated burning velocities are mostly governed by C1 chemistry typical for all hydrocarbons and by reactions of ketene. Thus, implementation of different C0-C2 reaction subsets may lead to no table changes in the predicted burning velocities as it was demonstrated by Cavallotti et al. using various versions of the POLIMI model. Furthermore, reactions ( R1) and ( R2 ), which in the present Model I have largely different rate constants than those in Mod els II and III (see Figs. 2 and 3) directly contribute to the overprediction of the burning velocities. The differences between our calculated rate constants and previous works have been dis cussed above and can be attributed to the higher level of theory implemented in the present study for reaction ( R1 ). Reaction ( R5) not visible in Fig. 8 appears in the sensitivity spectra of acetic acid ?ames ( Fig. 10) because in the reverse direction it constitutes important channel of CH 2 COOH decomposition leading to formation of signi?cant concentrations of ketene in these ?ames. 
The calculated burning velocities can be decreased by increasing the rate constants of reactions having small negative sensitivity coe?cients. Cavallotti et al. estimated the uncertainty of their rate constants of a factor of 2 or better. However, multiplication of them by this factor is not su?cient to match experimental data and it was not implemented in the present model.
Summarizing these different options and taking into account that majority of reactions of ketene and of acetic acid have been re?ned using theoretical calculations and can hardly be changed signi?cantly, one may support the opinion of Cavallotti et al. that kinetic model of acetic acid is most probably incomplete and requires consideration of de?cient reactions of the CH 2 COOH radical. Indeed, its reactions with other radicals could provide additional chain termination reducing calculated burning velocity and modifying the balance of the intermediate products observed in several attempts to model acetic acid ?ame structure investigated by Leplat and Vandooren.
CONCLUSION
The present work was motivated by the need of updating and improving kinetic sub-mechanism of ketene, which is important for combustion modeling of hydrocarbons and oxygenated species. Direct comparison of the recent theoretical studies by Cavallotti et al. and by Xu et al. reveals that predicted rate constants of the reaction of ketene with hydroxyl radical are notably different. To resolve this ambiguity ab initio CCSD(T)-F12/cc-pVQZ- F12//B3LYP/6-311G(d,p) calculations of the potential energy surface for CH2 CO + OH have been performed. Temperature- and pressure- dependent reaction rate constants have been computed using the RRKM-Master Equation and transition state theory methods in the temperature range of 300每3000 K and in the pressure range of 0.01每100 atm.
Then the rate constants of four most importan product channels of reaction CH2 CO+OH forming HCCO + H2O (R1), CH2 OH + CO (R2), CH3 + CO2 (R4) and CH2 COOH (R5) from the present and from the recent theoretical studies were assessed. The differences between our calculated rate constants and previous works can be attributed to the higher level of theory implemented in the present study for reaction (R1).
Ketene is known to be important intermediate in high- temperature chemistry of diacetyl and acetic acid ?ames. To investigate the impact of the choice of reactions between CH2 CO and OH radicals on the predicted burning velocities of these ?ames, detailed kinetic mechanism was updated with pertinent reactions suggested in the literature. Furthermore, the rate constants of four most important product channels of reaction CH2 CO+OH from the present work and from the theoretical studies by Cavallotti et al. and by Xu et al. were tested and compared with available data for the burning velocities of diacetyl and acetic acid ?ames. Good agreement with the burning velocities of diacetyl + air ?ames was found for the present model, while the expressions from the literature underestimate them. On the contrary, any com- bination of the rate constants of reactions between ketene and hydroxyl radical overpredicts burning velocities of acetic acid + air ?ames, which strongly indicates that kinetic model of acetic acid is most probably incomplete and requires consideration of additional reactions.

The three-dimensional structure of swirl-switching in bent pipe fow 

Swirl-switching is a low-frequency oscillatory phenomenon which a ects the Dean vor-tices in bent pipes and may cause fatigue in piping systems. Despite thirty years worth of research, the mechanism that causes these oscillations and the frequencies that characterise them remain unclear. Here we show that a three-dimensional wave-like structure is responsible for the low-frequency switching of the dominant Dean vortex. The present study, performed via direct numerical simulation, focuses on the turbulent fow through a 90. pipe bend preceded and followed by straight pipe segments. A pipe with curvature 0.3 (defned as ratio between pipe radius and bend radius) is studied for a bulk Reynolds number Re = 11 700, corresponding to a friction Reynolds number Re. ¡Ö 360. Synthetic turbulence is generated at the infow section and used instead of the classical recycling method in order to avoid the interference between recycling and swirl-switching frequencies. The fow feld is analysed by three-dimensional proper orthogonal decomposition (POD) which for the frst time allows the identifcation of the source of swirl-switching: a wave-like structure that originates in the pipe bend. Contrary to some previous studies, the fow in the upstream pipe does not show any direct infuence on the swirl-switching modes. Our analysis further shows that a three-dimensional characterisation of the modes is crucial to understand the mechanism, and that reconstructions based on 2D POD modes are incomplete. 
Key words: Pipe fow boundary layer -Turbulence simulation 
1. Introduction 
Bent pipes are an essential component of a large number of industrial machines and processes. They are ideal for increasing mass and momentum transfer, passively mixing di erent fuids, which makes them e ective as heat exchangers, inverters, and other appliances. For a review of the applications of bent pipes in industry see Vashisth et al. (2008); the most recent advances in experiments and simulations can be found in the review by Kalpakli Vester et al. (2016). The high mass and momentum transfer is generated by the secondary motion caused by the centrifugal force acting on the fuid in the curved sections. This secondary motion, which is of Prandtl¡¯s frst kind, takes the shape of two counter-rotating vortices, illustrated in fgure 1, which move the fuid 
. Email address for correspondence: jcanton@mech.kth.se 
towards the outside of the bend, along the centreline, and back towards the inside along the wall, therefore increasing the mass and momentum transfer across the pipe section. 
These vortices were frst observed by Boussinesq (1868) and Eustice (1910), and later described analytically by Dean (1928) from whom they received the name of Dean vortices. The intensity of these vortices increases with Reynolds number, here based on pipe diameter and bulk velocity (i.e., Re = DUb/¦Í, where ¦Í is the kinematic viscosity of the fuid), as well as with pipe curvature, defned as the ratio between pipe radius and bend radius, ¦Ä = R/Rc (see Canton et al. 2017 for laminar fow; Noorani et al. 2013, the review by Kalpakli Vester et al. 2016, and references therein, for turbulent fows). 
For laminar, steady fow the Dean vortices are symmetric with respect to the bend symmetry plane (the I-O plane in fgure 1); but when the fow becomes unstable the vortices start oscillating periodically (K¡§uhnen et al. 2014, 2015; Canton et al. 2016). These large-scale oscillations are caused by the appearance of periodic travelling waves which, as also observed in other fows (see, e.g. Hof et al. 2004), are at the base of transition to turbulence for toroidal and helical pipes. 
A di erent kind of large-scale oscillations is observed for high Reynolds numbers: here the turbulent fow is modulated by a low-frequency alternation of the dominant Dean vortex. This vortex alternation excites the pipe structure and is presumed to be the cause of structural, low-frequency oscillations observed in heat exchangers (e.g. in microgravity conditions such as in a test for the international space station; Br¡§ucker 1998), as well as the origin of secondary motion in the bends of the cooling system of nuclear reactors (Kalpakli Vester et al. 2016). The Dean vortex alternation was initially, and unexpectedly, observed by Tunstall & Harvey (1968), who experimentally studied the turbulent fow through a sharp, L-shaped bend (¦Ä = 1). These authors measured ¡°low random-frequency¡± switches between two distinct states and, by means of fow visualisations, were able to identify an either clockwise or anti-clockwise predominance of the swirling fow following the bent section. The switching was found to have a Strouhal number St = f D/Ub highly dependent on Re and comprised between 2 ¡Á 10.4 and 
4.5 ¡Á 10.3 . Tunstall & Harvey attributed the origin of the switching to the presence of a separation bubble in the bend and to the ¡°occasional existence of turbulent circulation entering the bend¡±. 
To the best of our knowledge, the frst author to continue the work by Tunstall & Harvey (1968) was Br¡§ucker (1998), who analysed the phenomenon via particle image velocimetry (PIV) and coined the term ¡°swirl-switching¡±. Br¡§ucker studied a smoothly curved pipe with ¦Ä =0.5 and identifed the oscillations as a continuous transition between two mirror-symmetric states with one Dean cell larger than the other. He confrmed that the switching takes place only when the fow is turbulent, and he reported two distinct peaks, St =0.03 and 0.12, at frequencies considerably higher than those measured by Tunstall and Harvey, despite the lower Reynolds numbers considered. 
R¡§utten et al. (2001, 2005) were the frst to numerically study swirl-switching by performing large-eddy simulations (LES) for ¦Ä =0.167 and 0.5. The main result of this analysis is that the switching takes place even without fow separation; moreover, R¡§utten and co-workers found that the structure of the switching is more complex than just the alternation between two distinct symmetric states, since the outer stagnation point ¡°can be found at any angular position within ¡À40.¡±. R¡§utten et al. found a high-frequency peak at St ¡Ö 0.2, attributed to a shear-layer instability, and, only for their high Reynolds number case, one low-frequency peak for St ¡Ö 5.5¡Á 10.3 , which was connected to the swirl-switching. However, the simulations for R¡§utten et al.¡¯s work were performed by using a ¡°recycling¡± method, where the results from a straight pipe simulation were used as infow condition for the bent pipe. These periodic straight pipes were of length 

Figure 1. Schematic of the computational domain. (a) A section of the pipe with curvature  = R/Rc =0.3, including the defnition of the geometrical parameters and an instantaneous fow feld coloured by normal velocity, i.e., normal to the bend symmetry plane. (b) Streamlines of the mean cross-fow showing the Dean vortices on a cross-section extracted at so = 0. An animated view of the setup is provided in the supplementary online material, movie1.m4v. 
L =3.5D and 5D and likely infuenced the frequencies measured in the bent pipes since the periodicity of the straight pipes introduced a forcing for St = Ub/L =1/3.5 and 1/5, respectively. 
Sakakibara et al. (2010) were the frst to analyse the fow by means of two-dimensional proper orthogonal decomposition (2D POD) performed on snapshots extracted from stereo PIV. Their results for ¦Ä =0.75 reveal anti-symmetric structures that span the entire pipe cross-section and contain most of the energy of the fow. A spectral analysis of the corresponding time coe.cients shows peaks between St ¡Ö 0.07 at so =2D and St ¡Ö 0.02 for so = 25D, in the range found by Br¡§ucker (1998). In a subsequent work Sakakibara & Machida (2012) conjectured that the swirl-switching is caused by very large-scale motions (VLSM) formed in the straight pipe preceding the bend. 
¡§ 
Hellstr¡§om et al. (2013) and Kalpakli & Orl¡§u (2013) also presented results based on 2D POD. The former performed experiments for ¦Ä =0.5 and found non-symmetric modes resembling a tilted variant of the Dean vortices with St =0.16 and 0.33, corresponding 
¡§ 
to the shear-layer instabilities found by R¡§utten et al. (2005). Kalpakli & Orl¡§u (2013), on the other hand, studied a pipe with ¦Ä =0.31; di erently from previous works, the section of straight pipe following the bend was only 0.67 diameters long. Their results at the exit of this short segment show clearly antisymmetric modes as most dominant structures. The swirl-switching frequency obtained from the POD time coe.cients was St =0.04; peaks of St =0.12 and 0.18 were also measured but were found not to be related to swirl-switching. In a later work Kalpakli Vester et al. (2015) repeated the experiments for ¦Ä =0.39 and found again a dominant frequency corresponding to St =0.04. 
¡§ 
Carlsson et al. (2015) performed LES in a geometry similar to that of Kalpakli & Orl¡§u (2013), namely, with a short straight section following the bend, for four di erent curvatures. The infow boundary condition was generated by means of a recycling method, 
Reference  Re St 
Tunstall & Harvey (1968) 1 50 000 ¨C 230 000 2 ¡Á 10.4 ¨C4.5 ¡Á 10.3 Br¡§ucker (1998) 0.5 5000 0.03, 0.12 R¡§utten et al. (2001, 2005) 0.167, 0.5 27 000 5.5 ¡Á 10.3 Sakakibara et al. (2010) 0.75 120000 0.02 ¨C 0.07 Hellstr¡§om et al. (2013) 0.5 25 000 0.16, 0.33 
¡§ 
Kalpakli & Orl¡§u (2013) 0.31 34000 0.04 Kalpakli Vester et al. (2015) 0.39 24000 0.04 Carlsson et al. (2015) 0.32, 0.5, 0.7, 1 34000 0.003 ¨C0.01, 0.13, 0.5¨C0.6 Noorani & Schlatter (2016) 0.1, 0.3 11 700 0.01, 0.087 
Table 1. Reference Strouhal numbers measured in previous studies and attributed to swirl-switching. The analysis in the present work is performed at Re = 11 700 in a bent pipe with curvature  =0.3. 
as in R¡§utten et al. (2001, 2005), with a straight pipe of length 7D, exciting the fow in the bent pipe at St =1/7. The three lower curvatures were therefore dominated by the spurious frequencies artifcially created in the straight pipe by the recycling method, while the frequencies measured for ¦Ä = 1, corresponding to 0.5 < St < 0.6, were in the same range identifed by Hellstr¡§om et al. (2013) but were found to be mesh dependent. 
Noorani & Schlatter (2016) were the frst to investigate the swirl-switching by means of direct numerical simulations (DNS). By using a toroidal pipe they showed that swirl-switching is not caused by structures coming from the straight pipe preceding the bend, but is a phenomenon inherent to the curved section. Two curvatures were investigated, ¦Ä =0.1 and 0.3, and both presented a pair of antisymmetric Dean vortices as the most energetic POD mode with St =0.01 and 0.087. 
Table 1 summarises the main results of the aforementioned studies. It is clear from this literature review that there is a strong disagreement among previous works not only on what is the mechanism that leads to swirl-switching, but also on what is the frequency that characterises this phenomenon. In the present work an answer to both questions will be given, which will also explain the discrepancies between previous studies. 
The paper continues with a description of the numerical methods employed for the analysis, presented in ¡ì2, devoting special attention to the infow boundary conditions. The results of the simulations and POD analysis are presented in ¡ì3 and are discussed and compared with the literature in ¡ì4. 
2. Analysis methods 
2.1. Numerical discretisation The present analysis is performed via DNS of the incompressible Navier¨CStokes equations. The equations are discretised with the spectral-element code Nek5000 (Fischer et al. 2008) using a PN . PN.2 formulation. After an initial mesh-dependency study, the polynomial order was set to N = 8 for the velocity and, consequently, N =6 for the pressure. We consider a 90. bent pipe with curvature ¦Ä =0.3 for a Reynolds number Re = 11 700, corresponding to a friction Reynolds number Re. ¡Ö 360 (referred to the straight pipe sections). A straight pipe of length Li =7D precedes the bent section (see ¡ì3.1), and a second straight segment of length Lo = 15D follows it. Further 
details about the mesh, including element number and size, are reported in table 2. The supplementary video movie1.m4v shows the setup and a visualization of the fow. 
nel ndof r+ (R)+ z+ 
 =0.3 480 000 595 258 500 (0.56, 4.89) (2.26, 4.40) (0.93, 10.29) 
Table 2. Details of the mesh employed in the present work. nel corresponds to the number of hexahedral elements, while ndof is the total number of degrees of freedom (velocity and pressure). Quantities indicated as (¡¤)+ are reported in viscous scaling, and the numbers between parenthesis correspond to minimum and maximum values. 
2.2. Infow boundary and divergence-free synthetic eddy method Since the aim of the present work is to reproduce and study swirl-switching, a periodic or quasi-periodic phenomenon, the treatment of the infow boundary is of utmost impor-tance. The fow feld prescribed at the infow boundary should not introduce any artifcial frequency, in order to avoid the excitation of unphysical phenomena or a modifcation of the frequencies inherent to the swirl-switching. A recycling method, as the one used by R¡§utten et al. (2001, 2005) and Carlsson et al. (2015), should therefore be avoided, as highlighted in ¡ì1. In the present work the velocity feld at the inlet boundary of the straight pipe preceding the bend is prescribed via a divergence-free synthetic eddy method (DFSEM). This method, introduced by Poletto et al. (2011) and based on the original work by Jarrin et al. (2006), works by prescribing a mean fow modulated in time by fuctuations in the vorticity feld. The superposition of the two reproduces up to second order the mean turbulent fuctuations of a reference fow and requires a short streamwise adjustment length to fully reproduce all quantities. The fuctuations are provided by a large number of randomly distributed ¡°vorticity spheres¡± (or ¡°eddies¡±) which are generated and advected with the bulk velocity in a fctitious cylindrical container located around the infow section. When a sphere exits the container, a new, randomly located sphere is created to substitute it. The cylindrical container is dimensioned such that newly created eddies do not touch the inlet plane upon their creation and they have stopped a ecting it before exiting the container, i.e., the cylinder extends from . max(Deddies) to max(Deddies) (see fgure 5 in Poletto et al. 2013, for an illustration of the container). The random numbers required to create the fuctuations are generated on a single pro-cessor with a pseudo random number generator (Chandler & Northrop 2003) featuring an algorithmic period of 21376 iterations, large enough to exclude any periodicity in the simulations, which feature approximately 10 000 synthetic eddies. The size of the spheres is selected to match the local integral turbulence length scale, and their intensity is scaled to recover the reference turbulent kinetic energy, producing isotropic but heterogeneous second-order moments. The method prescribes isotropic turbulence, instead of the correct anisotropic variant, because it was shown that the former leads to a shorter adjustment length in wall-bounded fows (see fgure 11 in Jarrin et al. 2006). In order to satisfy the continuity equation, no synthetic turbulence is created below (R . r)+ < 10; however, this does not signifcantly a ect the adjustment length since the dynamics of the viscous sublayer are faster than the mean and converge to a fully developed state in a shorter distance. The turbulence statistics necessary for the method, specifcally the mean fow U (r), the turbulent kinetic energy k(r), and the dissipation rate .(r), were extracted from the straight pipe DNS performed by El Khoury et al. (2013). Section 3.1 presents the 
validation of our implementation of the DFSEM; more details can be found in Hufnagel (2016). 
2.3. Proper orthogonal decomposition 
Besides point measures, we use POD (Lumley 1967) to extract coherent structures from the DNS fow felds and identify the mechanism responsible for swirl-switching. More specifcally, we use snapshot POD (Sirovich 1987) where n three-dimensional, full-domain fow felds of dimension d (corresponding to the number of velocity unknowns) are stored as snapshots. POD decomposes the fow in a set of orthogonal spatial modes i(x) and corresponding time coe.cients ai(t) ranked by kinetic energy content, in decreasing order. The most energetic structure extracted by POD corresponds to the mean fow and will be herein named ¡°zeroth mode¡±, while the term ¡°frst mode¡± will be reserved for the frst time-dependent structure. 
A series of instantaneous fow felds (snapshots) is ordered column-wise in a matrix S ¡Ê Rd¡Án and decomposed as: 
d 
X 
S = UV . = iai, (2.1) i=1 
where U ¡Ê Rd¡Ád ,  = diag(¦Ò1,¦Ò2, ¡¤¡¤¡¤ ,¦Òm, 0), with m = min(d, n), and V ¡Ê Rn¡Án . The decomposition in (2.1) is obtained by computing the singular value decomposition (SVD) of M1/2ST 1/2 , where M is the mass matrix and T is the temporal weights matrix, which 
. .. 
results in . VU and . are unitary matrices ( . U. V .= I); the U . , where . VU = I and . V 
M.1/2 . .1/2 .
POD modes are then obtained as U = U and V = TV . To improve the convergence of the decomposition, we exploit the symmetry of the pipe about the I . O plane, which results into a statistical symmetry for the fow, and store an additional mirror image for each snapshot (Berkooz et al. 1993). 
3. Results and analysis 
3.1. Infow validation An auxiliary simulation for Re. = 360 was set up to test the performance of the DFSEM in a 25D long straight pipe, provided with the same mesh characteristics used for the bent pipes. Classical statistical quantities were used for the validation and compared with the reference values by El Khoury et al. (2013). The comparison is presented in fgure 2(a) as a function of distance from the infow boundary, and shows that the DFSEM approaches a fully developed turbulent state (within ¡À1% of error) at approximately 5D from the infow boundary. Figures 2(b-d) present the velocity, stress profles, and the turbulent kinetic energy budget at the chosen streamwise position of si =5D, which confrm the recovery of fully developed turbulence by the divergence-free synthetic eddy method. A length of 7D was therefore chosen for the straight pipe preceding the bent section, in order to allow for some tolerance and to account for the (weak, up to 1D) upstream infuence of the Dean vortices (Anwer et al. 1989; Sudo et al. 1998). For comparison, the more commonly used approach where random noise is prescribed at the infow requires a development length between 50 and 110D (Doherty et al. 2007). POD modes were also computed to further check the correctness of this method. The results, not reported here for conciseness, were in good agreement with those presented by Carlsson et al. 
(2015) for a periodic straight pipe, that is, streamwise invariant modes with azimuthal wavenumbers between 3 and 7. 




20 15 

1 

10 5 0.95 
0 


100 102 

3 2.5 2 1.5 1 0.5 0 
0.06 

0.04 
0.02 
0 -0.02 -0.04 

Figure 2. Recovery of fully developed turbulence statistics for the divergence-free synthetic eddy method at Re. = 360, compared to the reference values by El Khoury et 
al. 
(2013). Panel 
(a) shows the ratio between the DFSEM and the reference data as a function of streamwise distance from the infow plane. The grey shaded area indicates a ¡À1% tolerance with respect to the data by El Khoury et 
al. 
(2013). Panels (b-d) show classical statistical profles as a function of radial position at si =5D. Solid lines indicate the reference data, while symbols represent the current results (note that the number of shown points is reduced and does therefore not represent the grid resolution; see table 2). 
3.2. Two-dimensional POD 
Two-dimensional POD, considering all three velocity components, is employed as a frst step in the analysis of swirl-switching. Instantaneous velocity felds are saved at a distance of 2D from the end of the bent section and are used, with their mirror images, to assemble the snapshot matrices (Berkooz et al. 1993). 1 234 velocity felds were saved at a sampling frequency of St =0.25, and the sampling was started only after the solution had reached a statistically steady state. As a consequence of exploiting the mirror symmetry, all modes are either symmetric or antisymmetric, a condition to which they would converge provided that a su.cient number of snapshots had been saved. 
The frst three modes are shown in fgure 3 by means of pseudocolors of their streamwise velocity component and streamlines of the in-plane velocity components. Two out of three modes are antisymmetric: (i, ii) and are in the form of a single swirl covering the whole 

Figure 3. Pseudocolor of the streamwise velocity component and streamlines of the in-plane velocity components for the frst three POD modes (i¨Ciii). The modes are oriented as in fgure 1(b). The snapshots were extracted at so =2D. 

10-3 
3 
2.5 
2 
1.5 
1 
0.5 
0 
PSD 


Figure 4. Welch¡¯s power spectral density estimate for the time coe.cients ai of the most energetic 2D POD modes. The frequencies are scaled with pipe diameter and bulk velocity. The 2D modes were extracted at so =2D. The markers and corresponding labels report the frequency of the highest peak of each spectrum. 
pipe section, (i), and a double swirl, (ii), formed by two counter-rotating vortices disposed along the inner-outer direction on the symmetry plane. The third mode, (iii), resembles a harmonic of the Dean vortices. 
These fndings are in agreement with previous experimental work, such as that of ¡§ 
Sakakibara et al. (2010) and Kalpakli & Orl¡§u (2013), which attributed the dynamics of swirl-switching to the antisymmetric modes. The frequency content of these modes is presented in fgure 4, in terms of Welch¡¯s power spectral density estimate for the time coe.cients of the frst three modes, corresponding to the structures shown in fgure 3. It can be observed that the spectra have a low peak-to-noise ratio and that each mode is characterised by a di erent spectrum and peak frequency, in agreement with previous 2D POD studies: see, e.g., fgure 8 in Hellstr¡§om et al. (2013), which presents peaks with similar values to the present ones, although their study was for a slightly larger curvature of ¦Ä =0.5. This fact has caused some confusion in the past, with disagreeing authors attributing di erent causes to the various peaks, without being able to come to the same conclusion about the frequency, nor the structure, of swirl-switching. The reason is that swirl-switching is caused by a three-dimensional wave-like structure, as will be shown by 3D POD in ¡ì3.3, and a two-dimensional cross-fow analysis cannot distinguish between the spatial and temporal amplitude modulations created by the passage of the wave. A simple analytical demonstration of this concept is provided in the Appendix, and shows that conclusions drawn from a fow reconstruction based on 2D POD modes (see, e.g., Hellstr¡§om et al. 2013) are incomplete. 

3.3. Three-dimensional POD For the 3D POD, the same snapshots as for 2D POD were used. In order to reduce memory requirements, the snapshots were interpolated on a coarser mesh before com-puting the POD. This is, however, not a problem because the swirl-switching is related to large-scale fuctuations in the fow. The four most energetic modes are depicted in fgure 5 by means of pseudocolors of nor-mal and streamwise velocity components, as well as streamlines of the in-plane velocity. It can be observed that the modes come in pairs: 1-2 and 3-4, as is usual for POD modes and their time coe.cients in a convective fow. The frst coherent structure extracted by the POD is formed by modes 1 and 2 and constitutes a damped wave-like structure that is convected by the mean fow (see fgure 5 for the spatial structure and fgure 6 for the corresponding time coe.cients; the supplementary video movie2 modes0-2.mov shows their behaviour in time). This is not a travelling wave such as those observed in transitional fows, as the ones of the examples mentioned in the introduction, but a coherent structure extracted by POD from a developed turbulent background that persists in fully developed turbulence, and is just a regular component of the fow on which irregular turbulent fuctuations are superimposed (see, e.g., Manhart & Wengle 1993, for a similar case). Nevertheless, the present wave-like structure could be a surviving remnant of pre-existing, purely time-periodic, fow structures formed in the bent section and arising in the process of transition to turbulence past bends (see, e.g., the case of the fow past a circular cylinder by Sipp & Lebedev 2007). It was found that the frst instability of the fow inside of a toroidal pipe is characterised by the appearance of travelling waves (K¡§uhnen et al. 2014; Canton et al. 2016). It is therefore possible that similar waves appear in the transition to turbulence of the present fow case, and continue to modulate the large scales of the fow at high Reynolds numbers while being submerged in small-scale turbulence. To support this hypothesis, the frequencies and wavelengths of the present coherent structures are in the same range as those measured in toroidal pipes (Canton et al. 2016), and Br¡§ucker (1998) observed swirl-switching even for Re as low as 2 000, although the measured oscillations had very low amplitude. The present modes are, obviously, not strictly periodic in space nor in time: as can be seen in fgure 5(b) showing the swirl intensity, the intensity of the modes is essentially zero upstream of the bend (s< 0), reaches a maximum at about 1D downstream of the bend end, and then decreases with the distance from the bend. Furthermore, the respective time coe.cients are only quasiperiodic, as can be observed from their temporal signal, depicted in fgure 6(a), and by their frequency spectra, fgure 7(a). Nevertheless, it can be observed in fgure 5 that the spatial structure of these modes is qualitatively sinusoidal along the streamwise direction so, with a wavelength of about 7 pipe diameters. The fgures in Br¡§ucker (1998) actually already suggest the appearance of a wave-like structure in the presence of swirl-switching. This wave-like structure is formed by two counter-rotating swirls, visible in the 2D cross-sections in fgure 5, which are advected in the streamwise direction while decaying in intensity and, at the same time, move from the inside of the bend towards the outside, as can be seen in the longitudinal cuts in fgure 5 and in the supplementary video movie2 modes0-2.mov. The temporal amplitude of these modes is also qualitatively cyclic, as illustrated by the projection along the time coe.cients in fgure 6(b). The 
wave-like behaviour can be appreciated even better in the aforementioned video showing the fow reconstructed with these two modes, movie2 modes0-2.mov. Modes 1 and 2 are 



20 
(b) 
10 
0 
-10 
-20 

Figure 5. Panel (a) shows the four most energetic three-dimensional POD modes. The four longitudinal cuts show pseudocolours of the normal velocity component un, while the eight cross-sections display the in-plane streamlines and are coloured by streamwise velocity us. The supplementary material includes two videos showing the reconstruction of the fow based on these modes. Panel (b) shows the swirl intensity, measured by circulation . , along the streamwise axis of the two most energetic modes, ¡ã 1 and ¡ã 2, and their envelope. The spatially decaying, wave-like behaviour can be appreciated. 

20 


30 

40 

50
(a) 
50
0.1 
(b) 
40
0.05 

30 
0 






20 



-0.05 
10 
-0.1 
10 
0 



a 1 
Figure 6. Time coe.cients ai(t) of the two most energetic three-dimensional POD modes. Panel 
(a) shows the temporal signal, which allows to observe the qualitative quarter-period phase shift of mode 2 with respect to mode 1; panel (b) shows the (colorcoded) time over coe.cients a1 and a2, illustrating the oscillating character. The time axis is (arbitrarily) cut at t = 50D/Ub for illustration purposes, the total recorded signal is over 300D/Ub . 
0.02 
0.015 

PSD
0.01 0.005 0 

Figure 7. Welch¡¯s power spectral density estimate for the time coe.cients ai of the most energetic 3D POD modes. The frequencies are scaled with pipe diameter and bulk velocity. The markers and corresponding labels report the frequency corresponding to the peak of each spectrum. The range of the Strouhal number is identical to that of fgure 4 to ease comparison. 
phase-shifted by a quarter of their quasi-period: fgure 5 shows that the structure of mode 2 is located approximately a quarter of a wavelength further downstream of the structure of mode 1; while, fgure 6 illustrates the constant delay of the time coe.cient of mode 2 with respect to that of mode 1. 
The second structure, formed by modes 3 and 4, has a spatial layout that closely resembles that of the frst pair, i.e., it is also a wave-like structure, and constitutes the frst ¡°harmonic¡± of the wave formed by modes 1 and 2. The spatial structure of modes 3 and 4 has half of the main wavelength of modes 1 and 2, and the highest peak in the spectrum of the third and fourth time coe.cients is at exactly twice the frequency of the peaks of a1 and a2, as can be seen in fgure 7(a). The video movie3 
modes0-4.mov shows the reconstruction of the fow feld by including modes 3 and 4. It can be observed that these modes introduce oscillations with higher frequency and smaller amplitude when compared to the reconstruction employing only modes 1 and 2. 
As can be observed from fgure 5, the modes do not present any connection to the straight pipe section preceding the bend. This is in direct contrast with the fndings of Carlsson et al. (2015), whose results were likely altered by the interference of an intrinsic frequency and wavelength on the recycling infow boundary with the structure of the swirl-switching. Our results are, instead, in agreement with Noorani & Schlatter (2016) who observed swirl-switching in a toroidal pipe (i.e., in the absence of a straight upstream section) confrming that these large-scale oscillations are not caused by structures formed in the straight pipe, but by an e ect which is intrinsic to the bent section. 
The power spectral density analysis of the time coe.cients of these modes, computed as a Welch¡¯s estimate, is presented in fgure 7. One can see that, unlike the PSD of the two-dimensional POD modes (fgure 4), the three-dimensional modes present two distinct peaks, one per pair of modes. The peak for the frst modal pair is located at St ¡Ö 0.16, which is in the range of Strouhal numbers found by both Br¡§ucker (1998) and Hellstr¡§om et al. (2013). More importantly, this frequency is the lowest for this pair of modes and matches that given by the wavelength and propagation speed of the wave as well as that of the swirl-switching, as observed by reconstructing the fow feld with the most energetic POD modes (see the online movies). 
The present analyses were also performed on a pipe with curvature ¦Ä =0.1 for the same Reynolds number. Swirl-switching was observed in this case as well, with dynamics which is qualitatively identical to the one observed for ¦Ä =0.3, but is characterised by lower frequencies, peaking at St ¡Ö 0.045. The lower frequencies and larger scales (wavelength of about 20D) characterising the wave-like structure at this curvature meant that a quantitative analysis was too expensive with the present setup. We have therefore limited this work to the study of one curvature only, but preliminary, not converged results can be found in Hufnagel (2016). 
4. Summary and conclusions 
This work presents the frst DNS analysis of swirl-switching in a 90. bent pipe. The simulations were performed by using a synthetic eddy method to generate high-quality infow conditions, in an e ort to avoid any interference between the incoming fow and the dynamics of the fow in the bent section, as was observed in previous studies. Three-dimensional POD was used to isolate the dominant structures of the fow. This method allowed the identifcation of a wave-like structure, originating in the bent section, constituted by the frst modal pair. A reconstruction of the fow feld using the most energetic modal pair confrmed that the swirl-switching is caused by this structure. 
The swirl-switching frequency found in the present study is in the range of those deduced by Br¡§ucker (1998) and Hellstr¡§om et al. (2013). The structure of the modes, which presents no connection to the upstream straight pipe, confrms what was conjectured by Noorani & Schlatter (2016), who observed swirl-switching in a toroidal pipe, namely that swirl-switching is a phenomenon intrinsic to the bent pipe section. 
Clearly, the present fndings are in contrast with previous conclusions drawn from fow reconstructions based on 2D POD modes and Taylor¡¯s frozen turbulence hypothesis (see, e.g., Hellstr¡§om et al. 2013): the 2D analysis mixes convection and true temporal variation, and thus cannot reveal the full three-dimensional structure of travelling modes. This does not only apply to the present fow case, but to any streamwise inhomogeneous fow in which 2D POD is utilised in the cross-fow direction. 
The wave-like structure found in the present study is di erent from those observed in transitional fows (see, e.g. Hof et al. 2004), in the sense that it is simply a coherent structure extracted by POD from a turbulent background fow, as opposed to an exact coherent state. Nevertheless, we conjecture that this structure may be a surviving remnant of a global instability caused by the bend (K¡§uhnen et al. 2014; Canton et al. 2016). 
Financial support by the Swedish Research Council (VR) is gratefully acknowledged. 
Computer time was provided by the Swedish National Infrastructure for Computing (SNIC). We acknowledge that part of the results of this research have been achieved using the DECI resource SiSu based in Finland at CSC with support from the PRACE aisbl. This material is also based in part upon work supported by the US Department of Energy, o.ce of Science, under contract DE-AC02-06CH11357. 
Appendix. Considerations on the use of 2D POD 
This section explains, analytically, the reasons why a two-dimensional cross-fow POD analysis is an ine ective tool for understanding swirl-switching. In order to capture the essence of the phenomenon, the example is without spatial dissipation and noise, but these can be added at will without changing the discussion or the results. A Matlab script performing the operations described in this section is provided as part of the supplementary online material. 
Consider a sine wave of period 2¦Ðl, travelling at speed v, and with amplitude modulated at a frequency ¦Ø/(2¦Ð): 
 
x . vt 
g(x, t)=sin cos(¦Øt) . (A 1) 
l When measuring its passage at a given spatial position, say g(xm,t), the recorded time signal will contain two frequencies, f1 =(¦Ø . v/l)/(2¦Ð) and f2 =(¦Ø + v/l)/(2¦Ð), that combine the spatial component, fs = v/(2¦Ðl), and the temporal component, ft = ¦Ø/(2¦Ð). This combination is a result of the fact that g can be rewritten, using one prosthaphaeresis formula, separating the time and space dependencies: 
hi hi
1 xv 1 xv 
g(x, t)=sin + ¦Ø . t + sin . ¦Ø + t. (A 2) 
2 ll 2 ll The two components, fs and ft, would be measured in isolation if the function g were a pure travelling wave (¦Ø = 0) or a pure standing wave (v = 0). However, when both aspects are present (¦Ø =6 0 and v =6 0) a complete knowledge of g is necessary in order to separate fs from ft. This, clearly, is possible in the present example, where the analytical expression of g(x, t) is known. When studying an unknown phenomenon (such as swirl-switching) the knowledge of f1 and f2 is insu.cient: one does not know what is causing the measured frequencies: it could be two travelling waves advected at di erent speeds (or provided with di erent period); two standing waves modulated at di erent frequencies; or, as in this case, one travelling wave with modulated amplitude. This problem can transferred to a POD analysis as well: the 2D POD in the pipe corresponds to a zero-dimensional POD in this example, which employs the measurements g(xm,ti) as snapshots, while the 3D POD of the bent pipe fow corresponds to a one-dimensional POD which uses the function g(x, ti) over the whole x domain as snapshots. The 0D POD returns a single mode which assumes a value of either +1 or .1 and does not provide any information about the spatial structure of g. The spectrum of the time coe.cient corresponding to this single mode contains both frequencies f1 and f2. When using 0D POD one does not have any information about the spatial nature of g, and is lead to believe that the oscillations measured in xm are caused by two periodic phenomena with frequencies f1 and f2. This likely is what has caused so much disagreement in the literature about the value of the Strouhal number related to the swirl-switching and on the 2D POD mode responsible for this phenomenon. The answer is that none of the 2D POD modes reported in the literature is actually the swirl-switching mode, and the Strouhal numbers extracted from time coe.cients do not provide a correct description. A 1D POD analysis of the function g, which is the analogue of the 3D POD in the 
bent pipe, provides the correct answers. It results in two sinusoidal modes which, with the corresponding time coe.cients, reproduce the complete travelling and oscillatory behaviour of g. The spectra of the time coe.cients still contain only f1 and f2, but have a much higher peak to noise ratio compared to the 0D POD, as observed in the bent pipe by comparing fgures 4 and 7. Moreover, by analysing the reconstruction of g, they allow the separation of fs from ft. 
It is now clear why in the case of a streamwise-dependent spatial structure, such as the one creating swirl-switching (as shown in ¡ì3.3), only a fully three-dimensional analysis can correctly identify the actual spatial and temporal components. 
REFERENCES 
Anwer, M., So, R. M. C. & Lai, Y. G. 1989 Perturbation by and recovery from bend curvature of a fully developed turbulent pipe fow. Phys. Fluids 1, 1387¨C1397. Berkooz, G., Holmes, P. & Lumley, J. L. 1993 The proper orthogonal decomposition in the analysis of turbulent fows. Annu. Rev. Fluid Mech. 25, 539¨C575. Boussinesq, M. J. 1868 M¡äemoire sur l¡¯infuence des frottements dans les mouvements r¡äeguliers des fuides. J. Math. Pure Appl. 13, 377¨C424. Brucker,¡§ C. 1998 A time-recording DPIV-study of the swirl-switching e ect in a 90. bend fow. In Proc. 8th Int. Symp. Flow Vis., pp. 171.1¨C171.6. Sorrento (NA), Italy. 
¡§ 
Canton, J., Orlu,¡§ R. & Schlatter, P. 2017 Characterisation of the steady, laminar incompressible fow in toroidal pipes covering the entire curvature range. Int. J. Heat Fluid Flow 66, 95¨C107. 
¡§ 
Canton, J., Schlatter, P. & Orlu,¡§ R. 2016 Modal instability of the fow in a toroidal pipe. J. Fluid Mech. 792, 894¨C909. Carlsson, C., Alenius, E. & Fuchs, L. 2015 Swirl switching in turbulent fow through 90. pipe bends. Phys. Fluids 27, 085112. Chandler, R. & Northrop, P. 2003 Fortran random number generation. http://www.ucl.ac.uk/~ 
ucakarc/work/randgen.html 
. Dean, W. R. 1928 The streamline motion of fuid in a curved pipe. Phil. Mag. 5, 673¨C693. Doherty, J., Monty, J. & Chong, M. 2007 The development of turbulent pipe fow. In 16th Australas. Fluid Mech. Conf., pp. 266¨C270. 
El Khoury, G. K., Schlatter, P., Noorani, A., Fischer, P. F., Brethouwer, G. & Johansson, A. V. 2013 Direct numerical simulation of turbulent pipe fow at moderately high Reynolds numbers. Flow, Turbul. Combust. 91, 475¨C495. 
Eustice, J. 1910 Flow of water in curved pipes. Proc. R. Soc. London, Ser. A 84, 107¨C118. Fischer, P. F., Lottes, J. W. & Kerkemeier, S. G. 2008 Nek5000 Web page, http://nek5000.mcs.anl.gov. 
Hellstr¡§ 
om, L. H. O., Zlatinov, M. B., Cao, G. & Smits, A. J. 2013 Turbulent pipe fow downstream of a 90. bend. J. Fluid Mech. 735, R7. 
Hof, B., van Doorne, C. W. H., Westerweel, J., Nieuwstadt, F. T. M., Faisst, H., Eckhardt, B., Wedin, H., Kerswell, R. R. & Waleffe, F. 2004 Experimental observation of nonlinear traveling waves in turbulent pipe fow. Science 305, 1594¨C1598. 
Hufnagel, L. 2016 On the swirl-switching in developing bent pipe fow with direct numerical simulation. Msc thesis, KTH Mechanics, Stockholm, Sweden. 
Jarrin, N., Benhamadouche, S., Laurence, D. & Prosser, R. 2006 A synthetic-eddy-method for generating infow conditions for large-eddy simulations. Int. J. Heat Fluid Flow 27, 585¨C593. 
¡§ 
Kalpakli, A. & Orlu,¡§ R. 2013 Turbulent pipe fow downstream a 90. pipe bend with and without superimposed swirl. Int. J. Heat Fluid Flow 41, 103¨C111. Kalpakli Vester, A., Orlu,¡§ R. & Alfredsson, P. H. 2015 POD analysis of the turbulent 
¡§ 
fow downstream a mild and sharp bend. Exp. Fluids 56, 57. 
¡§ 
Kalpakli Vester, A., Orlu,¡§ R. & Alfredsson, P. H. 2016 Turbulent fows in curved pipes: 
recent advances in experiments and simulations. Appl. Mech. Rev. 68, 050802. 
K¡§uhnen, J., Braunshier, P., Schwegel, M., Kuhlmann, H. C. & Hof, B. 2015 Subcritical versus supercritical transition to turbulence in curved pipes. J. Fluid Mech. 770, R3. 
K¡§
uhnen, J., Holzner, M., Hof, B. & Kuhlmann, H. C. 2014 Experimental investigation of transitional fow in a toroidal pipe. J. Fluid Mech. 738, 463¨C491. Lumley, J. L. 1967 The structure of inhomogeneous turbulent fows. In Atmos. Turbul. Radio Wave Propag. (ed. A. M. Yaglom & V. I. Tatarski), pp. 166¨C178. Moscow. Manhart, M. & Wengle, H. 1993 A spatiotemporal decomposition of a fully inhomogeneous turbulent fow feld. Theor. Comput. Fluid Dyn. 5, 223¨C242. Noorani, A., El Khoury, G. K. & Schlatter, P. 2013 Evolution of turbulence characteristics from straight to curved pipes. Int. J. Heat Fluid Flow 41, 16¨C26. Noorani, A. & Schlatter, P. 2016 Swirl-switching phenomenon in turbulent fow through toroidal pipes. Int. J. Heat Fluid Flow 61, 108¨C116. Poletto, R., Craft, T. & Revell, A. 2013 A new divergence free synthetic eddy method for the reproduction of inlet fow conditions for LES. Flow, Turbul. Combust. 91, 519¨C539. 
Poletto, R., Revell, A., Craft, T. J. & Jarrin, N. 2011 Divergence free synthetic eddy method for embedded LES infow boundary conditions. In 7th Int. Symp. Turbul. Shear Flow Phenom.. Ottawa. 
R¡§ oder, W. 2001 Large-eddy simulations of 90.
utten, F., Meinke, M. & Schr¡§ pipe bend fows. J. Turbul. 2, N3. R¡§ oder, W. & Meinke, M. 2005 Large-eddy simulation of low frequency 
utten, F., Schr¡§ 
oscillations of the Dean vortices in turbulent pipe bend fows. Phys. Fluids 17, 035107. 
Sakakibara, J. & Machida, N. 2012 Measurement of turbulent fow upstream and downstream of a circular pipe bend. Phys. Fluids 24, 041702. 
Sakakibara, J., Sonobe, R., Goto, H., Tezuka, H., Tada, H. & Tezuka, K. 2010 Stereo-PIV study of turbulent fow downstream of a bend in a round pipe. In 14th Int. Symp. Flow Vis.. EXCO Daegu, Korea. 
Sipp, D. & Lebedev, A. 2007 Global stability of base and mean fows: a general approach and its applications to cylinder and open cavity fows. J. Fluid Mech. 593, 333¨C358. 
Sirovich, L. 1987 Turbulence and the dynamics of coherent structures. Part I: coherent structures. Q. Appl. Math. 45, 561¨C571. 
Sudo, K., Sumida, M. & Hibara, H. 1998 Experimental investigation on turbulent fow in a circular-sectioned 90-degree bend. Exp. Fluids 25, 42¨C49. 
Tunstall, M. J. & Harvey, J. K. 1968 On the e ect of a sharp bend in a fully developed turbulent pipe-fow. J. Fluid Mech. 34, 595¨C608. 
Vashisth, S., Kumar, V. & Nigam, K. D. P. 2008 A review on the potential applications of curved geometries in process industry. Ind. Eng. Chem. Res. 47, 3291¨C3337. 

Title:Turbulent Flows in Curved Pipes: Recent Advances in Experiments and Simulations 
Abstract: Curved pipes are essential components of nearly all the industrial process equipments, ranging from power production, chemical and food industries, heat exchangers, nuclear reactors, or exhaust gas ducts of engines. During the last two decades, an interest on tur-bulent .ows in such conduits has revived, probably due to their connection to technical applications such as cooling systems of nuclear reactors (e.g., safety issues due to .ow-induced fatigue) and reciprocating engines (e.g., ef.ciency optimization through exhaust gas treatment in pulsatile turbulent .ows). The present review paper, therefore, is an account on the state-of-the-art research concerning turbulent .ow in curved pipes, natu-rally covering mostly experimental work, while also analytical and numerical works are reviewed. This paper starts with a historical review on pipe .ows in general and speci.-cally on .ows through curved conduits. In particular, research dealing with the effect of curvature on transition to turbulence, work dealing with pressure losses in curved pipes, as well as turbulence statistics are summarized. The swirl-switching phenomenon, a spe-ci.c structural phenomenon occurring in turbulent curved pipe .ows, which has interest-ing fundamental as well as practical implications, is reviewed. Additional complications, with respect to .ow through bends, namely, entering swirling .ow and pulsating .ow, are reviewed as well. This review closes with a summary on the main literature body as well as an outlook on future work that should be performed in order to tackle open questions remaining in the .eld. [DOI: 10.1115/1.4034135] 
1 Introduction 
Pipe .ows are the most common way to transport liquids and gases in technical and industrial processes. This is the case for transportation of oil and gas in pipelines and hot (cold) water for district heating (cooling), in all the types of process industry (such as pulp and paper industry, chemical plants, and nuclear power plants), for hydropower stations, etc. Also, for internal combus-tion engines, the so-called gas-exchange system is an intricate and compact system of piping where for a turbocharged vehicle, air is moved to and from the compressor to the cylinders through a pipe system and exhaust gases evacuated to a turbine and then through an after-treatment system and .nally to the exhaust. Also, biologi-cal systems rely on transportation of gases and liquids in ¡°pipelike¡± veins, arteries, and capillaries, such as the respiratory and cardiovascular systems in mammals. In all the aforemen-tioned cases, some parts of the piping system are curved due to the necessity to adopt to the geometrical or technical constraints of the system or to redirect the .ow. 
The .ow in pipes may be laminar, transitional, or turbulent depending on the geometrical, .uid, and .ow parameters of the system. The .ow itself can be steady or pulsatile, the latter is the case for the respiratory and cardiovascular systems, as well as, for instance, in engine .ows. For gas .ows through pipes, one may need to consider compressibility as well. Pipes themselves can have many different cross sections: round, oval, square, rectangu-lar, etc., the pipe wall can be rigid or .exible, smooth or rough, and there may be branches or junctions. In any pipe system, one would also expect to .nd bends of various angles and sharpness. Curved pipes or pipe bends are commonly associated with strong secondary .ow and increased pressure losses that are due to an instability set up by the centrifugal forces acting on the .uid as it passes through the bend. Also, cross-sectional mixing and heat 
1Corresponding author. 
Manuscript received January 29, 2016; .nal manuscript received July 11, 2016; published online September 19, 2016. Assoc. Editor: Herman J. H. Clercx. 
transfer may be strongly affected if a .uid passes through a curved pipe. 
As can be seen from the descriptions above, the .ow through pipes incorporates a vast area of various conditions. In this review, we will concentrate on aspects of turbulent .ows through curved pipes. There exist several reviews, book chapters, and books examining the effect of streamline curvature in general (see, for example, the extensive work by Bradshaw [1]) as well as speci.-cally within the area of pipe bends spanning from 1938 to 2008 [2¨C11], however, there exists quite substantial work done on tur-bulent .ows in curved pipes, which is not suf.ciently covered in the available literature. In the most well-cited2 
and extensive review on curved 
pipes 
[6], the authors state that ¡°laminar incom-pressible .ows will be our primary concern¡± and there is only a short section at the end of the review referring to some work on turbulent .ow as well as experimental work. However, especially in the last two decades, a number of studies exploiting advanced optical experimental techniques as well as (large-eddy/direct) numerical simulations (LES/DNS) have been conducted for the study of turbulent .ow characteristics in different bend con.gura-tions. In the present review, we will therefore focus on presenting state-of-the-art research related to turbulent .ow through pipe bends based on experiments and simulations and also give a his-torical background on original experimental and analytical studies relevant to the .eld. 

1.1 Some Basic Definitions 
Before proceeding, it is useful to give the de.nitions of a few basic parameters that are used when referring to the different stud-ies. The types of pipes, which will be discussed throughout the paper and are commonly used for numerical/experimental investi-gations in the main literature body, are the ones shown in Fig. 1 
along with their main geometrical parameters. 
2According to the Web of ScienceTM (www.webofknowledge.com), it has been cited today (April 2016) more than 200 times since 2010. 


Fig. 1 Different pipe con.gurations: (a) straight pipe, (b) 90 deg bend, (c) helix, and (d) torus, showing how some param-eters are de.ned. L, pipe length; D 5 2R, pipe diameter; Rc, cur-vature radius; c, coil radius; and 2pb, coil pitch. The torsion of a coil is de.ned as s 5 b=ec21b2T. 
We de.ne the Reynolds number as 
Re .qWbD=l (1) 
where q is the .uid density, D .2R is the pipe diameter (R being the pipe radius), Wb is the bulk velocity, and l is the dynamic viscosity. 
For curved pipe .ows, the so-called Dean number is often used, which takes into account both the Reynolds number and the effects of curvature. It was .rst introduced for laminar .ow and small curvatures by Dean [12,13], as will be discussed later on. This parameter took various forms in different studies through the years [14¨C16], sometimes based on the constant pressure gradient (mostly in theoretical/numerical studies) and sometimes on the mean velocity in the pipe (mostly in experimental studies) causing some confusion in the interpretation of the results from the litera-ture (see a discussion on this matter in Ref. [6]). Here, we de.ne the Dean number based on the curvature ratio c .R=Rc and the Reynolds number (based on the bulk velocity) as 
p...
De .cRe (2) 
The curvature radius, Rc, is here de.ned as the distance from the center of curvature to the centerline of the pipe, see Fig. 1 
The curvature ratio is within the limits 0 c 1, where for a straight pipe c .0 and for a sharp bend it is equal to 1 (sometimes sharp bends are referred to as elbows, typically when c is close to 1). 
Note that even though the Dean number is sometimes used in the literature for turbulent .ow (see, for example, Refs. [17] and [18]), c and Re should be viewed as independent parameters for (not only) turbulent .ows [19]. While in laminar .ow, the Dean number characterizes the integral quantities (for a detailed study on this matter, the interested reader is referred to Ref. [20]), the situation is, in particular, different for turbulent .ows where at least three different length scales play a role (pipe radius, curva-ture radius, and the turbulent viscous length scale). 
In this review, when referring to papers where the authors have used a different de.nition of the Reynolds and/or Dean numbers (for instance, using the radius as the length scale or de.ned the curvature ratio differently), we have recalculated them to adhere to the present de.nition.3 
It should also be noted that the notation 
3We have also rounded of numbers given to two signi.cant digits whenever needed. 

Fig.2 (a) Pipe bend with the cross-sectional plane A¨CA denoted as well as the outer and inner walls of the pipe bend. 
(b) Top view of the cross-sectional cut (A¨CA) at the exit of the pipe bend showing the de.ned Cartesian coordinate system. 
introduced here will be used throughout this review, unless other-wise stated such as when reprinting published .gures from the lit-erature. Finally, if not otherwise stated, we present results using the Cartesian coordinate system with z being the streamwise, x the horizontal, and y the vertical directions (as de.ned in Fig. 2) cor-responding to W, U, and V velocity components, respectively. Capital letters denote time-averaged quantities, small letters denote instantaneous quantities, whereas primed small letters denote r.m.s. quantities. 
Another important nondimensional parameter is the Fanning friction factor (f) de.ned as 
R dp
f .(3) 
qW2ds
b 
where dp=ds is the pressure gradient along the pipe. The Fanning friction factor is a way to nondimensionalize the pressure loss in a pipe of constant diameter, whereas the behavior of the curve in the f¨CRe diagram has traditionally been used to identify whether the .ow is laminar, transitional, or turbulent. 
Other parameters that will be useful to describe certain phe-nomena or .ow cases will be introduced in conjunction with their discussion. 
1.2 Outline. We start with a historical review on pipe .ows in general and speci.cally on .ows through curved conduits in Sec. 2. For curved .ows, we are here mainly referring to studies of laminar .ow. In Sec. 3, mainly experimental research dealing with the effect of curvature on transition to turbulence is reviewed although recently some DNS studies have been performed that give new insights. Section 4 
reviews both work dealing with pres-sure losses in curved pipes as well as turbulence statistics (mean velocity and Reynolds stresses), divided into sections with results obtained from experiments (Sec. 4.2.1), turbulence modeling (Secs. 4.2.2 
and 4.2.3), and DNS (Sec. 4.2.4). In Sec. 4.3, a spe-ci.c structural phenomenon is reviewed, namely, the so-called swirl switching which has interesting fundamental as well as prac-tical implications. Thereafter, we describe two additional compli-cations with respect to .ow through bends, namely, entering swirling .ow (Sec. 5) and pulsating .ow (Sec. 6). Finally, in Sec. 7, the main conclusions from the reviewed literature as well as suggested future work to be performed for turbulent curved pipe .ows are summarized. 
2 Historical Review 
Although pipes (or conduits) have been used to transport water at least since the construction of the .rst aqueducts in the Roman empire during the 4th century BC, the .rst scienti.c studies of pipe .ows are usually referred to around 1840 when Hagen [21] and Poiseuille [22] carried out experimental work on water .ow through straight pipes of various sizes to deter-mine the pressure losses, now known as Hagen¨CPoiseuille pipe .ow. Hagen observed that above a certain pressure head, the .ow became unstable, what he observed was probably the tran-sition to turbulence. 

Darcy [23], who was a hydraulic engineer, among others responsible for building the water supply system for Dijon 
[24], did similar studies. He investigated pressure drop in pipes of large dimensions (with a diameter of up to 0.5 m and more than 100 m long) where the .ow was certainly turbulent. He also studied the effect of roughness of the pipe walls on the pressure drop. 
Of course, a review on pipe .ows cannot exclude the pioneer-ing work on laminar¨Cturbulent transition by Reynolds in the late 19th century which was also published in his famous paper [25], where he de.ned the nondimensional number that we today know as the Reynolds number (Eq. (1)). Reynolds showed that transition to turbulence could not occur below a certain value, today known as the critical (or sometimes transition) Reynolds number. 
The .rst scienti.c notion of .ow through curved pipes of circu-lar and rectangular cross section is often attributed to the theoreti-cal work by Boussinesq [26]. His analysis showed that for laminar .ow through a curved channel of rectangular cross section, two symmetrical vortices exist. However, in some reviews, the work by Thomson in 1876 [27,28] is cited as the earliest effort to study .ows in curved pipes. Thomson4 
studied the windings of rivers and compared it with the .ow in pipe bends, however, he noted that a river (or curved channel) with a free upper surface in some respects is different from that of a .ow in a closed, .lled conduit. Thomson illustrated the secondary .ow set up in a river bend in a small laboratory model where he let particles show the inward motion at the bottom of the bend. His explanation of the particle motion was that pressure and centrifugal force set up by the curva-ture balance in the bulk .ow make an imbalance in the slow mov-ing .uid near the bottom and thus leads to an inward secondary .ow there. Nevertheless, even though usually neglected in histori-cal reviews on curved pipe .ows, the .rst work¡ªto the best of our knowledge¡ªdealing with .ow in curved pipes was the one by Weisbach in 1855 [30], who performed experiments to determine the pressure drop in sharp bends. 
Experiments in a curved pipe were also performed a bit later by Williams et al. [31] showing for the .rst time that the maximum mean velocity is shifted toward the outer wall at the outlet of a bend. Eustice [32,33] performed experiments, published around 1910, on .ow in various curved pipes and showed that the pres-sure drop increases as compared to a straight pipe but that the change of pressure drop observed in straight pipes at transition to turbulence was not as abrupt. The experiments by Eustice were followed by those of White [16], who clearly showed through pressure drop measurements that curvature stabilizes the .ow and that the transitional Reynolds number can be substantially increased for a curved pipe, compared to that of a straight one. 
Probably the most well-known name in curved-pipe-.ow research, Dean5 
(for a portrait see Fig. 3(a))6 
investigated analyti-cally the laminar .ow in a circular, curved pipe [12,13]by 
4James Thomson (1822¨C1892) was the 2 yr older brother of William Thomson, later Lord Kelvin, and was a hydraulic engineer, who became professor in Civil Engineering and Mechanics at the University of Glasgow and was elected as an FRS in 1877. A biography including his scienti.c writings can be found in Ref. [29], which can be accessed through the web for free. 
5William Reginald Dean (1896¨C1973) entered Trinity College at the University of Cambridge in 1919 and was elected a fellow of Trinity College 4 yr later. For some years, he was at the Imperial College London, where his .rst work on .ow in curved pipes and channels [12,13,34] was carried out in the late 1920s. In 1929, he moved back to Cambridge and stayed in Cambridge until 1952 when he was elected to the Goldsmid Chair of Mathematics at the University College of London from which he retired in 1964 [35]. Among other achievements, Dean was a co-winner of one of the most prestigious awards at the Cambridge University, the Adams Prize in 1950, along with G. K. Batchelor and L. Howarth, for their essays on the subject of ¡°Problems in the mechanics of solids and .uids (including elasticity, plasticity, and hydrodynamics).¡± The committee who awarded the Prize consisted of H. Jeffreys, S. Goldstein, and G. I. Taylor [36]. His last paper on curved pipe .ow was written in 1959 [37]. Generally, Dean has been described as a man ¡°with a well-developed gift of silence¡± and there is not so much information about his life and career apart from one paragraph and a portrait in the paper by Binnie [38]. 
6http://www.annualreviews.org 


Fig.3 (a) W. R. Dean (1896¨C1973). Reproduced with permis-sion of Annual Review of Fluid Mechanics, Volume 10, 1¨C11, 1978 [34] Copyright by Annual Reviews, http://www.annualre-
views.org 
(b) A schematic of the Dean vortices as sectional streamlines. Originally published in Dean, W. R., ¡°Note on the motion of .uid in a curved pipe¡±, Philosophical magazine, 1927, vol. 4, pp. 208¨C223 [12]. Reprinted by permission of the pub-lisher Taylor & Francis Ltd, http://tandfonline.com 

assuming that the cross-.ow velocity disturbance was much smaller than the streamwise velocity and that c was small. He showed that the important parameter was Re2c (the square root of this is nowadays known as the Dean number, see Sec. 1.1) but also that a cross-.ow velocity .eld was established for small cur-vature ratios. In Ref. [12], the cross .ow pattern, if projected on the cross-stream plane, was identi.ed to consist of two counter-rotating vortex cells, where the .ow in the center plane was directed toward the outer wall. These vortices are today called Dean cells or Dean vortices in honor of Dean¡¯s work, see Fig. 3(b).7,8 
A .uid particle in such a .ow would move downstream along a helical path and this analytical result con.rmed the .ow visualization picture obtained by Eustice, although the variation of the pitch of the helix with velocity seemed to be opposite that of the experiments. However, his expansion allowed only small values of the Dean number or rather that Re2c=1440 <1tobe 
7http://tandfonline.com 

8The term Dean vortices are originally referred to the small amplitude laminar .ow pattern shown in Fig. 3(b), however, similar vortices that exist in turbulent .ows are also denoted as Dean vortices. 


Fig. 4 The referee report by Taylor on Dean¡¯s 1928 paper on curved channel .ow [34]. The report should answer six ques-tion (only the .rst .ve were answered). Reprinted with permis-sion by the Royal Society. 
considered. Since the expansion was only taken to .rst order this resulted in that the pressure drop along the pipe became independ-ent of the pipe curvature. In his follow-up study [13], Dean increased the expansion to higher orders and showed that an increase in curvature gives rise to a lowering of the .ow rate at a given pressure drop and again he found that the governing param-eter was the Dean number (squared). 
Dean continued his work on curved .ows by studying the curved two-dimensional channel [34]. In contrast to the .ow in a circular pipe, he found that the mean .ow in a two-dimensional channel could be without secondary .ows if the value of De was small enough (here, the pipe diameter is replaced by the distance between the channel walls). However, he found that the .ow became unstable to streamwise oriented vortices if Re2c >36. This work was published in the Proceedings of the Royal Society of London, Series A, and the reviewer was no less than Taylor (see the referee report in Fig. 4). Curved channel .ow has been studied both through experiments and numerical simulations, speci.cally with regards to instabilities in the .ow (see, e.g., Refs. [39] and [40]), however, for curved pipes, there was no linear stability analysis available until recently with the work by Canton et al. [19]. 
Reviewing Dean¡¯s paper together with the concurrent work by White [16] may have inspired Taylor [15] to perform his own study of .ow in curved pipes.9 
He used .ow visualization by 
9It might be worth noting that both Taylor and Dean belonged to the ¡°Trinity Circus¡± [41], which ¡°meet on the golf course on Sunday mornings whenever it was convenient¡± and ¡°consisted of a small group of Rutherford¡¯s scienti.c friends, who at one stage or another were all fellows of Trinity College, Cambridge¡± [42]. 

Fig. 5 A drawing illustrating the tea-leaf effect. (Reproduced with permission from Einstein [43]. Copyright 1926 by Springer). 
injecting dye to determine the .ow structure in a glass helix and showed that the transition to turbulence was delayed to larger Reynolds numbers with increased curvature ratio. 
Flows in curved geometries caught the interest of many (famous) scientists, among them also Albert Einstein [43], who discussed .ows related to meandering of rivers. In his paper, he illustrated the centrifugal effect by the well-known tea-leaf obser-vation, i.e., that tea-leaves in a cup, if stirred into rotating motion, end up in the central part of the bottom due to the secondary motion set up by a centrifugal imbalance in the boundary layer at the bottom (see Fig. 5). 
Adler [44] in 1934 made measurements in curved pipes, all with a diameter of 1 cm, but with different small curvature ratios (c .0:005;0:01;and 0:02). The .ow medium was water, and a Pitot tube was used to measure velocity pro.les along seven dif-ferent chords on one side of the pipe (including the symmetry line of the pipe). The measurements were done for both laminar and turbulent conditions. From these measurements, he was also able to construct contour plots of the streamwise velocity. The contour plots all show the crest-shaped form that we associate with a sym-metric vortex pair. A year later, Wattendorf [45] measured mean velocities and pressure of fully developed turbulent .ow in a curved channel between two concentric circular walls. 
Since the two early pioneering papers by Dean [12,13], many other combined analytical and numerical studies have presented the laminar .ow in curved pipes from the 1960s on, see, e.g., Refs. [14] and [46¨C48]. Smith [49], for instance, looked at how the .ow upstream the bend was affected and showed that there was .ow toward the side of the inner wall of the bend creating a maximum of the shear there. Dennis and Ng [50] showed that for high Dean numbers, a four-vortex cell pattern was possible in addition to that of two vortices, whereas Dennis and Riley [51] looked at asymptotic solutions for high Dean numbers. In all these studies, the Dean number was found to be the governing parame-ter, however, we will not go into the details here since today they may be seen as obsolete as laminar .ow solutions of the Navier¨CStokes equations can now easily be obtained numerically through other methods without any severe assumptions. 
3 Transition to Turbulence 
The transition from laminar to turbulent .ow in curved pipes is a subject that has been investigated to a limited extent so far. Most of the existing studies regard helical pipes, whereas the main conclusions until a few years ago were based on pressure drop data, in which case the onset of turbulence was identi.ed by a change in the slope of the Fanning friction factor¨CReynolds number diagram. However, only a few studies give insight into how the transition is occurring. 
Early studies indicated that the transition to turbulence in a curved pipe appears at a lower Reynolds number when compared to a straight pipe. However, Dean [13] explained in his .rst ana-lytical work that the increase in resistance in a curved pipe hap-pens due to the secondary .ow, whereas Eustice [32] remarked that there is no critical Reynolds number for coiled pipes. Since the experimental studies by Taylor [15] and White [16] in helical 



Fig. 6 Data from the experiments by Taylor [15] and White [16] in helical pipes. 1 Speed at which White¡¯s curve indicates ¡°.rst appearance¡± of turbulence. These values were shortly after con.rmed through the experiments by Adler [42]. . Lowest speed at which the .ow appears completely turbulent accord-ing to Taylor¡¯s data. Highest speed at which the .ow is ¡°quite steady,¡± according to Taylor. The pipe diameter and curvature diameter are given as d and D, respectively, in the .gure (hence their ratio yields c). Reprinted with permission from Taylor [15]. Copyright 1929 by the Royal Society. 
pipes and by Adler [44] in curved pipes, it was made clear though that the onset of turbulence in curved pipes happens at a higher Reynolds number than in straight pipes, which is demonstrated in Fig. 6 
(taken from the work by Taylor [15] on transition to turbu-lence in coiled pipes). 
In fact, White [16] showed that for Reynolds numbers between 102 <Re <6 103, the resistance of a curved pipe (c .0:02) becomes progressively higher than for a straight pipe as Re increases and at Re .6 103, the .ow resistance was 2.9 times higher for the curved pipe. Similarly, Taylor [15] introduced dye through a small hole in a helical glass pipe and observed through .ow visualizations that a steady streamline motion persisted for a Reynolds number which was 2.8 times higher than the critical Reynolds number for straight pipes. These observations are sum-marized in Berger et al. [6] stating that the critical Reynolds num-ber for a curved tube might be larger by a factor of two or more compared to the one for a straight pipe (which is typically equal to Re .2 103). Moreover, as described in Ref. [4], it was anticipated from an early stage, that contrary to straight-pipe .ow, the critical condition in a curved pipe with small curvature depends on the disturbance level of the .ow, whereas for c >0:01 there is a unique critical Dean number for a given curvature ratio. 
An empirical relation for the critical Reynolds number was given by Ito [52], who compiled the critical Reynolds number given by earlier studies [15,16,44,53] together with his own data. The relation predicts a critical Reynolds number Recr .2:3 103 for c .0:001. Srinivasan et al. [54] also proposed relations for the critical Reynolds number for helices and spirals, which were later adapted by Ward-Smith and can be found in his book [4]. 
One of the most well-cited studies on transition in curved pipes is the one by Sreenivasan and Strykowski [55], who by means of .ow visualizations showed that the turbulent .ow entering from a straight into a coiled pipe becomes laminar and returns to its tur-bulent state after exiting the coil (see Fig. 7), see also Refs. [56] and [57]. The authors also observed that the .ow becomes time-periodic just before the onset of turbulence. The same observa-tions of relaminarization of the .ow in curved pipes were made later by Kurokawa et al. [58], who studied the laminarization of low Reynolds number turbulent .ow in curved and helically coiled pipes by means of smoke visualizations and velocity by means of hot-.lm anemometry measurements. The authors con-.rmed that the secondary .ow has a stabilizing effect delaying the transition to turbulence and observed similar to Ref. [55] that tur-bulent .ow in the upstream straight section becomes laminar as it 
Fig. 7 Visualization of relaminarization in coiled pipes for D 5 19.1 mm, Rc 5 90 mm, and Re 5 43103. Reproduced with permission from Sreenivasan and Strykowski [55]. Copyright 1983 by Springer. Caption of .gure in the original publication: ¡°Laminarization in coiled pipes. Pipe diameter 2a 5 1:91 cm, radius of curvature rc 5 9:0 cm, Reynolds number, R 5 4050¡±. 
passes through the coil. Sreenivasan and Strykowski are often given credit for being the .rst to make an effort to explain why the .ow in the coil is more stable than in a straight pipe. They showed that the upper critical Reynolds number (highest Re up to which the .ow remains laminar) measured at the end of 20 coils for 0 c 0:12 increases monotonically with c, however, the lower critical Reynolds number (the lowest Re at which the .ow is fully turbulent) reaches a maximum value and then suddenly drops. They also remarked that the .ow in the downstream straight section from the coil remains laminar for higher Re com-pared to the upstream straight pipe section. In another important study, Webster and Humphrey [59] found by means of laser Doppler velocimetry (LDV) measurements that periodic low-frequency perturbation waves arose at a Reynolds number Re .5 103 for a helical coil and c .0:054; a result that was later con.rmed 
by 
numerical 
simulations 
and 
.ow 
visualizations 
[60]. 
Today, it is generally known that the transition process from laminar to turbulent .ow in a curved pipe occurs more gradually as compared to a straight pipe (where the transition is known to happen abruptly in most cases). Recent DNS and experimental studies in helical and toroidal pipes have shown more speci.cally that the transition process can include many stages. The .rst study aiming to study the instability of the .ow is the one by Di Piazza and Ciofalo [61], who performed DNS in a torus for c .0:1 and 
0.3. The authors found that for c .0:1, there is a direct transition from steady to quasi-periodic .ow with symmetrical traveling waves perturbing the .ow, whereas for c .0:3 the transition starts with a Hopf bifurcation (supercritical transition) affecting mainly the Dean vortices and for higher Reynolds number (Re >5270), a secondary Hopf bifurcation appears leading to a quasi-periodic .ow. Therefore, the .ow is changing from station-ary to periodic, quasi-periodic, and .nally a chaotic state accord-ing to the authors, albeit their DNS were later found to be inaccurate [19,62]. For small curvatures, there is a subcritical tran-sition like the one occurring in a straight pipe, whereas for higher curvature values, a multistage supercritical transition occurs as described in Refs. [19] and [62] and will be discussed in more detail later on. In short, as summarized by K€
uhnen [63], the key features of the transition in curved pipes are the following: the curvature has a stabilizing effect, the transition happens gradually as opposed to a straight pipe, stable traveling waves arise in the transitional .ow, and turbulent .ow from a straight pipe can be fully or partially relaminarized by passage through a curved (heli-cal) pipe. K€
uhnen et al. [64] are the only ones to have performed experiments (SPIV, LDV) in toroidal pipes by using a steel sphere to drive the .ow to study the transition process. 
Cioncolini and Santini [65] provided a criterion for the critical Reynolds number as well. For curvature ratios below 0.042, they observed that before turbulence sets in, the f¨CRe curve depicted a 


Fig. 8 Comparison of available experimental and numerical data with the neutral curve by Canton et al. [19]. The envelope of families F4S and F5A is shown with the black line and inter-sect at d 5 0:016 (which here denotes the curvature ratio, c), and the two dotted lines highlight their continuation. The data by Cioncolini and Santini [65], which refer to the .rst discontinuity in their friction measurements, the data from Sreenivasan and Strykowski [55] which they refer to as ¡°conservative lower criti-cal limit,¡± and that by Noorani and Schlatter [67] referring to sublaminar drag are also shown. The interpolants by K€
uhnen et al. [62] are also shown: with a dashed (blue) line the .t for the supercritical transition and with a dotted (blue) line the .t for subcritical transition. Reproduced with permission from Can-ton et al. [19]. Copyright 2016 by Cambridge University Press. 
local minimum followed by an in.ection point and thereafter a local maximum. K€
uhnen et al. [62] performed LDV experiments both in toroidal and helical pipes with small pitch in order to detect disturbances in the .ow as the curvature ratio increased from 0.01 to 0.1 and found good agreement with the previous studies by Ito [52], Kubair and Varrier [66], and Cioncolini and Santini [65], who determined the critical Reynolds number from pressure drop data. From the critical Reynolds number¨Ccurvature ratio pairs, they could see a common trend of the critical Reynolds number to increase with increasing curvature ratio, although some scatter between the different studies is observed. Noorani and Schlatter [67] showed that close to Re .3:4 103 in a torus for c .0:01, a marginally turbulent .ow is established in which the friction drag reduces below the laminar solution for a straight pipe at the same Reynolds number, so-called sublaminar friction drag (see also Sec. 4.1). The most recent study on the matter which provided also the .rst instability analysis in a torus is the one by Canton et al. [19], where linear stability analysis and DNS were performed for various Re c. For the .rst time, the full range of curvatures (arriving to the upper limit of 1) was investigated 
(0:01 c 1). The authors con.rmed the results by K€
uhnen et al. [62] that the .rst instability occurring in a torus is a Hopf bifurcation (supercritical instability) as well as showed that previ-ous works indicated subcritical transition for curvature ratios below 0.02¨C0.04, see also Fig. 8. The authors found .ve families of eigenmodes (a family of eigenmodes shares similar characteris-tics) for different Re c pairs, see also Fig. 9; family F1A for high curvature and low Reynolds numbers which leads to a modi-.cation of the base .ow, a family of symmetric modes, F2S for medium curvature (0:54 c <0:64), a third family responsible for the onset of instability for 0:16 <c <0:48, and a fourth fam-ily (F4S) causing the instability for 0:016 <c <0:16. Finally, a family of antisymmetric modes was also detected for values of c below 0.0016. As it can be seen, previous studies agree on the existence of subcritical transition for very low curvatures (below 0.04), whereas the eigenmode family F4S was identi.ed in both Refs. [55] and [62]. 
4 Turbulent Flow 
4.1 Pressure Losses/Skin Friction. Since the pioneering work by Weisbach [30] in 1855, it is known that for turbulent 

Fig. 9 Neutral curve in the c ¨CRe plane (c is denoted in the .g-ure as d) formed by the envelopes of lines for 0:002 ¡ê c ¡ê 1by Canton et al. [19]. Each line corresponds to the neutral curve of one mode. Five families (black and blue) and three isolated modes (green) are shown. Continuous lines denote symmetric modes, whereas dashed lines indicate antisymmetric ones. The curves are not interpolated, i.e., they are segments connecting computed solutions with Dc 5 Oe1023T, and the uncertainty of the Reynolds number is 61024. Reproduced with permission from Canton et al. [19]. Copyright 2016 by Cambridge University Press. 
.ows, the pressure losses in curved pipes are higher than for straight pipes. This increase is due to centrifugal forces, which in turn (can) cause the .ow to separate at the inner wall of the curved pipe. The empirical relations put forward by Weisbach to express the increased pressure losses quantitatively for both elbow and curved pipes remained for half a century a reference as high-lighted in Ref. [68]. The increased pressure drop could through the .ow visualization studies by Eustice [32] later be coupled to the appearance of the secondary .ow. Since then, there has been a great interest to determine the pressure loss coef.cient used to compute pressure losses from the presence of pipe bends [69]. For example, investigators such as Nippert [70] and Richter [71] (among many others, see, e.g., Refs. [72] and [73]) performed tests on curved pipes to determine the pressure loss as function of various .ow and geometrical parameters, whereas Wasielewski 
[74] investigated the pressure losses in the bend of circular cross section as a function of the bend angle. 
As explained in the introductory part of the paper by Barua [75], a radial pressure gradient in the curved pipe is required to balance the centrifugal force applied from the curvature. Nowa-days, it is well-known that in a pipeline containing a bend, there will be a pressure increase along the outer bend wall and a decrease along the inner bend, and that this difference persists for 
Fig. 10 Pressure distribution along pipeline containing a 90 deg pipe bend (c 5 0:27), where lu and ld denote the pipe length upstream and downstream the bend, respectively. The Reynolds number is here denoted as R, whereas R is the curva-ture radius and r is the pipe radius. Reprinted with permission from Ito [76]. Copyright 1960 by ASME. 


Table 1 Experimental studies on turbulence statistics of curved pipe. ows. The last column shows the position downstream the bend where the various authors obtained data and where the.ow had not recovered from the effects of curvature. 
Author Type of Inlet pipe Exit pipe Inlet flow Quantities Diagnostic Length at which the flow 
4 
and year Re 10 c bend length length condition measured technique has not recovered 
Adler 1934 [44] 0.85, 1.17 0.01, 0.02 90 deg W Pitot tube Rowe 1970 [90] 24 0.042 180 deg, S 69D Fully developed Pt Pitot tubes, Conrad tubes, yawmeter 61D Enayet et al. 1982 [85] 4.3 0.17 90 deg 5D 10D Developing W, w0, PstLDV 6D 
0
Azzola et al. 1986 [86] 5.74, 11 0.15 180 deg 54.7D 54.7D Fully developed W, Vzw0;v LDV 5D Anwer et al. 1989 [88] 5 0.077 180 deg 96D 96D Fully developed W, U, V Miniature hot-wires 18D 
000
w ;u ;v wu, uv, wv Sudo et al. 1998 [87] 6 0.5 90 deg 100D 40D Fully developed W, U, V Rotating hot-wire 10D 
000
w ;u ;v Pst 
wu, vw 
Pww, Pwu, Pvw Sudo et al. 2000 [84] 6 0.5 180 deg 100D 40D Fully developed W, U, V Rotating hot-wire 10D 
000
w ;u ;v 
Pst wu, vw Pww, Pwu, Pvw 
Fig. 11 Fanning friction factor, f, as function of Reynolds num-ber, where . is for straight pipe data, and is for coiled pipe data for c 5 0:00964 (data obtained by Ref. [65]). Laminar and turbu-lent correlations by 
Ito 
[52] are shown by ---(for straight pipe) and ¡ª (for curved pipe). DNS results by Noorani et al. [81] are shown for fully developed turbulent .ow (Re 5 5:33103;
6:93103;and 123103) in straight pipe () and curved pipe with c 5 0:01 (w). DNS from Noorani and Schlatter are also shown: laminar .ows obtained in curved pipes are indicated with , whereas the symbol . illustrates sublaminar drag of the .ow in weakly turbulent curved pipe with Re 5 3:43103 and c 5 0:01. Reprinted with permission from Noorani and Schlatter [67]. Copyright 2015 by AIP Publishing. 
quite some distance downstream the bend leading into loss of pressure at downstream stations. This is depicted clearly in Fig. 10, which is reprinted from one of the pioneering studies on pres-sure losses in bends performed by Ito [76]. This study, together with an earlier one by the same author 
[52] on friction factor rela-tions for a wide range of curvature radii and Reynolds numbers, constitutes benchmark work which has been praised by other authors (e.g., see Ref. [77]), whereas the formulas they provided have been validated in recent studies [61,65]. In the .rst work by Ito [52], several experiments were conducted in order to establish a correlation between the friction factor and the .ow rate, which is one of the most crucial aspects of internal pipe .ow from an engineering point of view. The author compared his results with previous works [16,44,53]. As remarked by Ito, earlier published formulas for the bend-loss coef.cient provided results with con-siderable disagreement with more recent experiments 
[30,71,78]. The reason for the failure of previous studies was attributed later by 
Ward-Smith 
[4] to that the entry/exit conditions were not care-fully controlled, and the de.nition of the pressure drop was not precise. For a list of early relations of the bend-loss coef.cient as well as their performance, the reader is therefore referred to the work by 
Ito 
[76]. The in.uence of the degree of curvature, curva-ture ratio, Reynolds number, as well as recovery length of the downstream straight pipe section on the total pressure-loss coef.-cient has been investigated by various authors, and the results up to the early 1980s are summarized in a book chapter in Ref. [4]. In an earlier work, Ward-Smith [79] even showed that for the same Reynolds number range, turning angle, and curvature ratio, the same formulas for the pressure-loss coef.cient apply for square-and circular-cross-sectioned pipe bends without any con-siderable differences in the results. 
More recent work on pressure drop is mainly done in helically coiled pipes, such as the one by 
Ali 
[80], where an extensive table (Table 1 
therein) was given with many of the pressure drop corre-lations available in the literature. A similar effort to gather all the friction factor correlations for turbulent .ow in curved pipes was done a bit later also by Vashisth et al. [10] (Table 8 therein). As apparent from the aforementioned references, most of the work done on pressure drop in curved pipes/coiled tubes considers 



Fig. 13 Contours of longitudinal mean velocity, scaled with the bulk velocity on the horizontal axis along a 180 deg bend and its downstream straight pipe section until a downstream loca-tion of z=d 5 5. Minus sign denotes upstream location from the bend. The angles along the bend are also indicated. Reprinted with permission from Sudo et al. [84]. Copyright 2000 by Springer. Caption of .gure in the original publication: ¡°Contours of longitudinal mean velocity W =Wa on the horizon-tal plane¡±. 
laminar .ow and either helical or toroidal coils. Turbulent .ow has been generally neglected in the literature when considering the determination of the friction law in curved pipes with only a few exceptions [16,52,53]. Recently, Ref. [81] studied the effect of increasing Reynolds number on the Fanning friction factor for a constant curvature ratio, c .0:01, and they surprisingly showed that under some conditions, the friction factor is not higher for the curved pipe as compared to a straight one, as commonly believed. In fact, both substraight and sublaminar (friction) drag could be observed in some recent studies [65,67]. The most recent friction factor correlations are summarized together with the ones by Ito 
[52] in Fig. 11, reprinted from Ref. [67]. 
4.2 Turbulence Statistics and Flow Structures. In the fol-lowing sections (Secs. 4.2.1¨C4.2.4), experimental and numerical work which have been performed on turbulent .ows in curved pipes of different types (but of circular cross section) are pre-sented and discussed. Figure 12 
illustrates some common bend shapes that have been used in the literature, apart from the ones already shown in Fig. 1. 
4.2.1 Experiments. Between the early 1970s and late 1990s, the main literature body on mean quantities (velocity and pressure) and turbulence statistics along bends (mainly 90 deg or U-bends, which are typical in practical applications) was created. Measure-ment techniques employed mainly include LDV and hot-wire ane-mometry, whereas wall shear gauges, pressure taps, and Pitot tubes have also been used. Turbulence models have also been uti-lized almost at the same time as the experiments, mainly the most common ones (standard k e or k x) but even more sophisti-cated ones, as it will be outlined in detail in Secs. 4.2.2¨C4.2.3. 
One of the .rst studies that provided turbulence statistics of a curved pipe .ow through experiments was the one by Adler [44], showing mean velocity pro.les and contours for Reynolds 


Fig. 14 (a) Mean and (b) r.m.s. streamwise velocity pro.les measured along the horizontal axis by means of a single hot-wire probe, for three Reynolds numbers (based on pipe diame-ter), Re 5 1:43104,2:43104, and 3:43104 at 0.67D downstream of a 90 deg curved pipe. Reprinted with permission from Kalpa-kli Vester et al. [141]. Copyright 2016 by Springer. Caption of .g-ure in the original publication: ¡°Reynolds number effect on the a mean streamwise velocity and b r.m.s. pro.le normalized by the bulk velocity¡±. 
numbers range Re .2 12 103. Weske [82] presented velocity distributions from hot-wire measurements in various bends of cir-cular, square, and elliptical cross sections where in most of them separation occurred at the inner bend wall. Detra [83] presented mean axial velocity contours for high Reynolds numbers up to approximately Re .2:5 106 for different bends (a 21 deg bend with c .0:02 and a 42 deg bend with c .0:04). 
Since those early works, a number of studies on turbulence sta-tistics in pipe-bend .ows have been conducted, and the main results that were commonly found by different studies include the fact that the mean streamwise velocity becomes higher at the inner wall at 30 deg angle (see Fig. 13 
for clari.cation of the angle posi-tion along the bend) and at which point the Dean vortices start to form [84¨C87]. The mean .ow has been seen deviating from the fully developed turbulent pro.le at upstream distances of approxi-mately 1D [87,88]. At 90 deg, the maximum of the velocity was found to have shifted toward the outer wall and remains so down-stream the bend. On the other hand, the r.m.s. of the axial velocity is higher at the inner wall and lower at the outer wall (see, for example, Fig. 14). With increasing distances from the bend, the mean velocity pro.le becomes more uniform and the strength of the secondary .ow decreases. However, a distance of more than 50D is sometimes required for the .ow to return to its fully developed state, even for small curvatures [87¨C89]. It should be noted that the mean velocity, turbulence intensity, and Reynolds stresses were found to be similar for both 90 deg and 180 deg bends up to a bend angle of 60 deg [84]. Thereafter, it is shown that the effect of curvature on the .ow is stronger for a 180 deg bend, and a longer recovery length is required. Some of the most signi.cant experimental works which have been conducted on tur-bulence statistics in curved pipe .ows are summarized in Table 1. These will be described in more detail in the following paragraphs. 

Total pressure (Pt) measurements by means of Pitot tube as well as yaw angles by means of Conrad probes/claw-type yawmeters were made by Rowe [90], along a 180 deg bend for Re .2:4 105 and c .0:042. The .ow at the inlet was a fully developed turbulent pipe .ow, and it was shown that the second-ary motion becomes strongest 30 deg from the bend inlet, whereas for larger angles, it reduced in strength until it reached a constant value at 90 deg, i.e., the .ow pattern remained unchanged after that angle until the bend exit and it was therefore found to have reached a fully developed state. The author also detected second-ary .ow reversal along the bend symmetry plane between 90 deg and the 5D downstream station. The total pressure contours did not even recover from the effects of the curvature at a distance of 61D downstream the bend. 
Some measurements were also made in 45 deg S-bends, which, as the author remarks, give increased heat transfer compared to other bends. Compared to a 180 deg bend, the fully developed state was reached faster downstream of the S-bend. In fact, almost fully developed pipe .ow was achieved at approximately 50D shorter distance for the S-bend. However, this work showed con-.icting results with Ref. [88] and has been criticized by later stud-ies [86] due to the in.uence of the probes intruding the .ow and measurement uncertainties. 
Later on, Enayet et al. [85] performed LDV measurements in laminar and developing turbulent .ows (Re .4:3 104) at dif-ferent angles and up to 6D downstream a 90 deg bend (c .0:17). The mean streamwise velocity pro.les between the laminar and turbulent .ow cases were compared, whereas for the turbulent .ow case the corresponding turbulence intensity as well as static-pressure variation was presented. It was shown that a negative streamwise pressure gradient at the bend inlet exists on the inner wall which is twice the positive gradient existing on the outer wall. Those gradients disappear at an angle of approximately 25 deg and for bend angles up to 75 deg, a uniform pressure gradi-ent is established on both sides of the bend. However, for larger angles than 75 deg and until the straight pipe section downstream the bend, strong gradients were measured. The mean and turbu-lence .uctuations of the streamwise velocity were presented for 30 deg, 60 deg, 75 deg, and 90 deg bend angle as well as at 6D downstream and 0.58D upstream the bend. At 30 deg, the maxi-mum streamwise velocity has shifted to the inner wall since the bend alters the .ow direction and a cross-stream pressure gradient is set up, whereas at 90 deg, the maximum streamwise velocity is displaced toward the outer wall. 
LDV measurements of the turbulent developing .ow through a U-bend for Re .5.7 and 11 104 with c .0:15 were later per-formed by Azzola et al. [86], where the longitudinal and azi-muthal velocity components were measured. The experimental results were additionally compared with the results from simula-tions (standard k e model). As the .ow passes through the bend, the r.m.s. of the longitudinal and azimuthal velocities was found to increase due to the additional mean strain associated with the turning of the primary .ow and the secondary-.ow velocity gra-dients created. The agreement between numerical and experimen-tal data was found to be somewhat acceptable by the authors. The largest disagreement between experimental and numerical data was found for the .ow .eld downstream of the bend, even though secondary velocities are decreasing in strength as the .ow recov-ers downstream the bend. The measurement of the r.m.s. of the velocity .uctuations in this study was later found to be erroneous by Lee et al. [91] who performed hot-wire measurements for the same .ow parameters as Azzola et al. [86]. Two other studies pre-senting turbulence statistics as well as investigating sublayer bursting in a U-bend for Re .5 104 and c .0:077 are the ones by Anwer et al. [88] and Anwer and So [92]. Miniature hot-wire and wall-static pressure measurements were performed at various stations upstream and downstream the bend providing among others the Reynolds stress tensor in the horizontal and normal planes at various streamwise locations (z=D .18;1;1;6;10;18;30;and 49, where the negative and positive signs denote the location upstream and downstream the straight pipe sections connected to the bend, respectively). The mean streamwise veloc-ity pro.le at 18D upstream the bend was undisturbed from the bend by agreeing with other available data on the fully developed turbulent .ow. However, 1D upstream the bend the mean stream-wise velocity pro.le along the x-axis starts to deviate from the fully developed state and with respect to the vertical plane it is no longer axisymmetric. 
The mean streamwise velocity pro.le is not recovered to its ini-tial fully developed state even at 18D downstream the bend. Along the bend, the .ow never reaches a fully developed state in contrast to the results obtained by Rowe [90], whereas all the Reynolds normal stresses increase compared to a straight pipe until the bend exit is reached. The study also supports the exis-tence of an additional cell (apart from the Dean vortex) existing in the half pipe cross section of the bend, which as the authors claim was also found in Ref. [90]. The .ndings were supported by means of k e modeling and comparison of their results with pressure measurements [90]. That ¡°secondary¡± cell starts to appear somewhere between 67 deg and 112 deg bend angle and then gradually dissolves until it disappears at a distance of 1D downstream the bend. The authors concluded that since this cell could be found from previous total pressure measurements [90], it should be pressure driven just like the Dean vortices. The exis-tence of additional cells in the bend was later investigated by other authors by means of turbulence modeling and will be discussed in Sec. 4.2.2. In a follow-up study, Anwer and So [92] investigated the sublayer bursting in a turbulent .ow in a curved pipe using the same set up as in Ref. [88] and the variable interval time averag-ing technique¡ªsee, for example, Ref. [93]¡ªin order to identify burst events. Among others, it was veri.ed that convex curvature tends to stabilize the .ow [1], whereas concave curvature and deceleration enhance the bursting frequency. 
The most extensive and well-cited studies on turbulent statistics of .ows in 90 deg and 180 deg bends are the ones by Sudo et al. [82,88]. Hot-wire measurements in a 90 deg bend at Re .6 104 and c .0:5 were performed in Ref. [87] and provided the three components of mean and .uctuating velocities as well as Reyn-olds stresses at various cross sections along and downstream the bend by rotating a probe with an inclined hot-wire. The authors con.rmed that up to 30 deg bend angle the high mean streamwise velocity was shifted toward the inner wall, and at 30 deg bend angle the Dean vortices were formed, see Fig. 15. At the exit of the 90 deg bend, the main .ow was nonuniform with the high velocity toward the outer wall and the low velocity toward the inner wall. The distribution of the streamwise velocity becomes smoother as the distance from the bend increased, whereas the Dean vortices start to decay due to dissipation. However, the secondary .ow was observed even at 10D distance from the bend exit (see Fig. 15). The turbulence intensity increases in the outer part of the cross section between 0 deg and 30 deg, in response to an increase of the longitudinal velocity gradients in the radial direction. However, high values of turbulence intensity were found at the inner pipe wall at 60 deg. At 90 deg bend angle, the maxima of the turbulence inten-sity (w0=Wb) was settled at the inner bend. 
The data from Ref. [87] have served as a benchmark for the val-idation of computational data [94,95]. Similar results were shown in another study by Sudo et al. [84] for a U-bend, where the .ow characteristics are almost the same with a 90 deg bend for angles up to 60 deg curvature. At 90 deg, the secondary .ow starts to 


Fig. 15 Mean .ow velocity contours for locations on the upstream straight section (negative z/d, where d is the pipe diameter), different bend angles /, and locations downstream the bend (positive z/d). Streamwise velocity contours (top), scaled by the bulk velocity and secondary .ow velocity vectors (bottom), where the length of D=8 corresponds to Wb. The left and right sides of each .gure show the inside and outside walls in the bend, respectively. Reprinted with permis-sion from Sudo et al. [87]. Copyright 1998 by Springer. Caption of .gure in the original publication: ¡°Time mean .ow velocities in cross section. (Top .gure: Longitudinal velocity contours, where numerical values are for W =Wa. Bottom .g-ure: secondary .ow velocity vectors, where the length of d=8 corresponds to Wa. The left and right sides of each .gure show the inside and outside walls in the bend, respectively).¡± (a) z0/d 521, (b) u 50 deg, (c) u 530 deg, (d) u 560 deg, (e) u 590 deg, (f) z/d 51, (g) z/d 52, (h) z/d 55, and(i) z/d 510. 
weaken with the highest streamwise velocity being located at the upper and lower pipe walls, whereas the low velocity is located at the pipe center. Measurements at downstream stations from the bend showed the higher streamwise velocity being shifted to the outer wall at 1D and it remained so until 10D distance from the bend. Lee et al. [91] extended the study by Azzola et al. [86]by performing hot-wire measurements using a slanted wire which could be rotated into six orientations. The measurements were performed at different angles and downstream stations (up to 16D) from a 180 deg bend at the same .ow parameters as in Ref. [86]. The authors compared mean velocity and r.m.s. pro.les of the longitudinal and azimuthal components with the ones by Azzola et al., and the agreement was found to be good. 
4.2.2 Turbulence Modeling¡ªStatistics. Since the early 1970s, Reynolds-averaged Navier¨CStokes (RANS) models have been 


Fig. 16 Comparison of friction factors in fully developed turbu-lent .ow. Experimental data: , Schlichting [3]; ., Ito [52], c 5 0:038; ,Ito [52], c 5 0:06, and ¡ª predictions using k2e model by Patankar et al. [101], where a denotes thepiperadiusand R the curvature radius. Reproduced with permission from Patankar et al. [101]. Copyright 1975 by Cambridge University Press. 
employed in order to predict the .ow in curved pipes (see, for example, Refs. [96] and [97]), even though it is expected that those will not be successful due to the anisotropic nature of turbu-lence, the secondary motion imposed by the curvature, and separa-tion. Reynolds stress models (RSMs) are expected to perform better when turbulence is anisotropic and curvature is present than eddy-viscosity models (EVM) [98], however, even those have been shown to fail also for small curvature ratios (see, for exam-ple, the recent efforts by Noorani [99] and R€
ohrig et al. [100]). 
One of the .rst efforts to predict the .ow through a bend by means of turbulence modeling was made in 1975 by Patankar et al. [101] using the k e model. They compared the results with experiments by Rowe [90] for a developing .ow in a 180 deg bend, and with the data of, among others, Mori and Nakayama 
[102] for a fully developed .ow in a helical pipe. Friction factors for fully developed .ow were also compared with the experimen-tal data by Ito [52]. The agreement was quite poor and especially for the calculated friction factors (see Fig. 16), the disagreement between the computations and the experiments was 8%. 
Al-Rafai et al. [103], performed experiments and numerical cal-culations for a turbulent .ow at Re .3:4 104 in two bends with c .0:07 and 0.14. Measurements were performed by means of LDV, whereas a k e model was employed for calculating the .ow. The main results consisted of mean and r.m.s. streamwise velocities which showed the anticipated result that the secondary .ow is stronger in the bend with the higher c. The agreement of the results from the modeling with the experimental data was again not good, however, the authors remarked that it was ¡°encouraging¡± in view of the simpli.ed assumptions and short-comings of the model. Hilgenstock and Ernst [104] tested two common turbulence models (k e and renormalization group known as RNG) and provided acceptable results for a .ow in an out-of-plane bend (see, e.g., Fig. 12(d)), showing that among the tested models, the RNG model performed slightly better, however, at a four times higher computational cost. 
The performance of various turbulence models was tested also later on by Pruvost et al. [94] for 90 deg and 180 deg curved pipes, based on the .ows examined experimentally by Anwer and So 
[17] and Sudo et al. [87]. The results from various models with three types of near-wall strategies (wall-function, a two-layer zonal model, and low-Re k e model with boundary layer resolu-tion) compared to experiments showed again that the RSM model predicts the .ow more accurately compared to EVM. Compari-sons with the data by Sudo et al. [87] were also performed by Hellstr€om and Fuchs [95] using the RNG k e model but the agreement was rather poor, especially at the inner wall of the bend where the computed streamwise velocity was underestimated. 
Sugiyama and Hitomi [105] analyzed the development of turbu-lent .ow in a 180 deg bend using an algebraic RSM. The results were compared with the experimental data, and the authors found 

(b) turbulent .ow with fully developed inlet, c 5 0:077 and Re 5 53104. Reprinted with permission from Anwer and So [17]. Copyright 1993 by Springer. Caption of .gure in the original publication: ¡°Secondary .ow pattern in a curved pipe: a laminar .ow with parabolic inlet, a 5 0:077, De 5 277.5, and Re 5 1, 000; b turbulent .ow with fully developed inlet, u 5 0, 077, De 5 13, 874 and Re 5 50, 000¡±. 
that the model is capable of predicting quantitatively the mean .ow, however, the r.m.s of the velocity .uctuations was underpre-dicted. Pellegrini et al. [106] tested different turbulence models at transitional Reynolds numbers and showed that the performance of the models varies signi.cantly depending on the wall treatment chosen. Di Piazza and Ciofalo [107] compared pressure drop and heat transfer data from DNS and (available) experiments with results from various turbulence models (k e, shear stress trans-port k x, and RSM k x) for a coiled tube. The RSM x model was the only one found to be in good agreement with the experimental results for the friction factor for a fully turbulent .ow. 
By using DNS results [81], modeling shortcomings were exam-ined, as, for example, modeling the eddy-viscosity instead of the complete Reynolds-stress tensor, which is a simplicity used often, especially in industry. The turbulent viscosity constant for both straight and curved pipes and for different Reynolds numbers was computed and plotted. The values were quite constant along the radius for the straight pipe but for the bend they varied at the pipe core. From the results, it was shown that for small curvature ratios (c .0:01), the constant was distributed more homogeneously, and therefore, it is expected that the model in that case can pro-vide reasonable results, however, for higher curvatures it would most likely fail, since no explicit representation of turbulence ani-sotropy is present in the model. 
4.2.3 Turbulence Modeling¡ªSecondary Flows. A matter that has been studied through turbulence modeling in the early 1990s was the existence of turbulence-driven secondary .ow in a curved tube of circular cross section. In a straight duct with noncircular cross section, it is known that streamwise vortices are formed due to local variation in Reynolds stresses and are known as Prandtl¡¯s secondary .ow of the second kind or ¡°turbulence-driven¡± second-ary .ow [108]¡ªwhereas the secondary .ow of the .rst kind is induced by skewing of the mean .ow, such as in curved channels. There have been some studies which support the existence of such Reynolds-stress induced secondary .ow for curved pipes of circu-lar cross section as well. Experimental and numerical studies suggested that in the cross-stream half plane, there are three cells appearing when the .ow is turbulent: a Dean cell and a second cell formed near the pipe center as a result of a local imbalance of centrifugal force and radial pressure gradient as well as a third, turbulence-driven cell appearing in conjunction with the Dean cell. When the .ow is laminar, a separation cell exists at the inner wall, which, when the .ow is turbulent has been suggested to dis-appear. Lai et al. [109] performed computations using a RSM and showed the existence of three vortex pairs within a U-bend at Re .5 104 and c .0:077. The primary vortex pair was the Dean vortices, which is driven by the centrifugal forces and is always present in a curved pipe. A second pair existed near the pipe core as a consequence of local imbalance between the cen-trifugal force and the pressure gradient, and .nally, a third pair of vortices was found near the outer bend embedded in the Dean vor-tices, see Fig. 17. This pair of vortices started to appear around 60 deg from the bend entrance, reached a maximum strength at the bend exit, and disappeared at about 7D downstream distance from the bend. This additional pair or vortices was not found for developing laminar curved-pipe .ows, and consequently, it was believed that this is a secondary .ow driven due to the anisotropy of the turbulent normal stresses and their gradients. Those results agreed qualitatively with the experiments in Refs. [86] and [88], however, later on, it was found [17] that the experiments were of limited resolution and were not able to capture such structures. The existence of a third cell is not validated, and there have been no studies¡ªto the best of our knowledge¡ªperformed on the mat-ter during the last two decades. 


Somehow surprisingly, it is still a matter of debate, how effec-tively RANS models can be used for the study of .ows in curved pipes, where secondary .ow and separation are or can be present. The most recent study where an extensive comparative assessment of how RANS and LES can handle .ows with streamline curva-ture and potential separation regions is the one by R€
ohrig et al. [100]. Two models, an eddy-viscosity and a Reynolds-stress model, were compared to LES and particle image velocimetry (PIV) data by Kalpakli and €u[110]. The agreement between
Orl€the models and the experiments was again poor, with the RSM models performing slightly better, as apparent also from Fig. 18. The agreement between the experiments and LES, on the other hand, was fairly good, both regarding statistics and capturing the instantaneous .ow .elds (an example is shown in Fig. 19). 
The .rst effort to employ LES considering an in.nitely long curved pipe using periodic boundary conditions was, however, done much earlier, in the late 1990s by Boersma and Nieuwstadt [111,112]. The results for the mean .ow as well as the r.m.s. of the velocity .uctuations were presented for c .0:01 and 0.05 and Reynolds number of approximately Re .38 103. The simula-tions showed that the r.m.s. is enhanced at the outer wall of the bend and suppressed at the inner, whereas in contrast to straight pipe .ow, the Reynolds stresses were found to be larger at the center of the pipe than compared to straight pipes. 
Most LES studies in curved pipes that have been performed since then have shown how LES can capture the secondary .ow in time (Ref. 
[113], see also Sec. 4.3). A method combining also LES with a characteristic-based split scheme was proposed by Tan et al. [114] to investigate the turbulent .ow in a 90 deg bend with two curvature ratios (c .0:25 and 0.5). Separation at the inner bend wall was only observed for the sharper bend (c .0:5). The method was validated against experiments by Sudo et al. [87], but only for c .0:25 it was found to be successful. Detached eddy simulation which is computationally less expensive than LES has also been used by Wang et al. [115]. The authors com-pared results in terms of the pressure coef.cient along the 90 deg pipe with experimental data by Tan et al. [114] at different Reyn-olds and curvature ratios providing only a qualitative comparison. 
4.2.4 DNS. In this section, studies which have employed DNS for the study of curved pipe .ows are presented separately from other computational models. Even though the .rst efforts to employ DNS in curved pipe .ows started in the late 1990s-beginning of the 2000s [116,117], not many DNS studies in such .ows have been performed since then, probably due to the high computational cost. In any case, most of the studies considered either a helical or toroidal pipe, where the .ow is fully developed and streamwise homogeneous (contrary to the .ow in spatially developing curved pipes), using the orthogonal helical coordinate system introduced by Germano [118,119] to express the Navier¨CStokes equations for helical pipes. Helical pipes differ from curved pipes in a sense that apart from curvature effects, a torsion effect also exists, induced by the pitch of the coil (see Fig. 1). However, in most practical examples, the coil pitch is small compared to the diameter of the coil, and essentially, the coil can be considered as a torus. It has also been shown that mild torsion does not have a signi.cant effect on global quantities [120]. The .ow phenomena observed in a torus are similar to those observed in spatially developing bends (e.g., with respect to the secondary motion, see as an example Ref. [121], where cross-sectional instantaneous .elds in a torus are compared to those obtained downstream a 90 deg bend), and it has the advantage that the .ow is continuous and periodic boundary conditions can be applied, which is bene.cial for DNS.10 
The .rst DNS of a fully developed turbulent .ow in a curved pipe for Re .5 103 and c .0:2 were performed by Boersma and Nieuwstadt [116], mainly show-ing the capability of performing DNS in such .ows by presenting mean cross-sectional .elds obtained from two different initial conditions. 
A few years later, H€
uttl and Friedrich [117,122] performed DNS to investigate turbulence statistics in straight, curved, and helically coiled pipes, having as initial condition a fully developed turbulent .ow starting from a straight pipe. In the earlier study by the authors, a range of curvature ratios (c .0;0:01;and 0:1) and torsions (s .0;0:11;and 0:165) was applied in order to study their effect on the turbulence characteristics. The turbulent kinetic energy (TKE) was found to exhibit much lower values from the case with high curvature ratio compared to straight pipe .ow, whereas secondary motion became much stronger for high torsion. The TKE also increased with increased torsion for the same cur-vature ratio. Mean and r.m.s. of the azimuthal and radial velocities as well as Reynolds shear stresses and TKE along horizontal and vertical cuts in a pipe cross section were presented for Reynolds number around Re .6 103. Mean and r.m.s. velocities were the main results shown in Ref. [122] for Re .5:6 103 and c .0:1. 
10On the other hand, experiments in a closed torus are practically dif.cult to be realized due to limited technical possibilities to maintain the .ow in the torus, but in some cases it is still feasible (see, for example, the introduction in Ref. [61]). 


Fig. 19 Snapshots of the in-plane velocity .eld showing coun-terclockwise Dean cell. (a) Vortex as obtained from PIV for Re 5 3:43104.(b) Vortex captured by the LES of Ref. [100]for the same .ow parameters as the PIV shown to the left. Small-scale .uctuations have been suppressed in both cases using a spatial low-pass .ltering operation. Reprinted with permission from R€
ohrig et al. [100]. Copyright 2015 by Elsevier. 
The main conclusion from that work was that the turbulence intensity in a curved pipe is lower compared to a straight pipe. 
DNS of the fully developed turbulent .ow in straight and curved pipes (c .0:01 and 0.1) at moderate Reynolds numbers (Re .5:3 103 and 1:2 104) were studied by Noorani et al. [81]. For the .rst time, complete Reynolds stress budgets were presented in that study. The local axial wall shear stress was found to be signi.cantly higher at the outer wall compared to the inner side. For the highest Re and c case studied, the wall shear showed a plateau at the inner wall indicating relaminarization (Fig. 20), whereas distinct oscillations were found near the outer wall. The Dean vortices were observed in the mean for c .0:01 having an appearance similar to that of laminar .ow at low Dean numbers but for the higher curvature ratio case, the core of the vortices moved toward the side walls and a distinct ¡°bulge¡± region appeared in the pipe center, see Fig. 21. This region was found to be connected to enhancement of the TKE in the core of the pipe bend. However, the authors did not investigate this further in terms of how near-wall turbulence could be affected by the pres-ence of this region of high turbulence production. 
More recently, Di Liberto et al. [123] investigated the turbulent .ow in curved pipes for three cases of (c, Re) .e0;1:7 104), (0.1, 1:5 104), and (0.3, 1:2 104Tby DNS. It was shown that the overall turbulence levels decreased for a curved pipe 

Downloaded from http://asmedigitalcollection.asme.org/appliedmechanicsreviews/article-pdf/68/5/050802/6075086/amr_068_05_050802.pdf by Shanghai Jiaotong University user on 03 October 2020

compared to the straight one, whereas increasing the curvature ratio leads to a further reduction of the turbulence levels. It was also suggested that increasing the curvature ratio stabilized the Dean vortices since no .uctuations were found for that case in the core region as for the lowest examined curvature ratio. Finally, the most recent DNS study is the one by Noorani and Schlatter [121], however, this will be discussed in Sec. 4.3, as this work did not focus on statistics but concerned mainly the evolution of the Dean vortices in time and the so-called swirl-switching phenomenon. 
4.3 Swirl Switching: Rocking or Rolling?. The turbulent .ow in curved pipes and especially the behavior of the Dean vorti-ces is the subject of several studies, most of them conducted dur-ing the last decade [100,124¨C128]. It is notable that the .rst major study on turbulent .ow in bends was carried out in the late 1960s [129], and it took three decades for a second study to be per-formed [130] on the matter. In the study by Tunstall and Harvey (hereafter [129] referred to as TH68), almost .ve decades ago, for the .rst time it was shown that the turbulent .ow through sharp bends differs from the vortex pattern existing for laminar curved pipe .ow, i.e., the counter-rotating Dean cells being symmetrical to the plane of symmetry in time. Visualizations of the .ow in an elbow showed the existence of a single swirling vortex which was rotating alternatingly in the clockwise and anticlockwise direc-tions with a frequency corresponding to a Strouhal number St .0.002, de.ned here as 
St .fD=Wb (4) 
where f denotes the frequency. A .ag placed between golden-plated contacts was additionally mounted where the switching of the vortex was detected from the visualizations. The output from a transducer placed upstream of the .ag showed that the .ow was dominated by two distinct states that interchanged abruptly, an anticlockwise and a clockwise-rotating vortex dominating the .ow .eld alternatingly. Spectra of the axial velocity component and wall-pressure close to the outer and at the inner pipe wall con-cluded that this ¡°switching¡± was occurring with a frequency corre-sponding to St .0:002, agreeing with the visualizations. Finally, to account for Reynolds number and curvature ratio effects, a series of measurements were performed for various curvatures and a Reynolds number range Re .4 22 104. The switching was found for all the cases with frequencies increasing with the Reynolds number and corresponding to Strouhal numbers between 
0.001 and 0.004. The study .nally concluded that in order for the switching of the vortices to be triggered, the .ow must be turbu-lent and the bend should be sharp enough to cause separation, sug-gesting that the origin of the phenomenon lies on the .ow 


ucker [130]. (b) Reconstructed instantaneous .ow .elds from the .rst six POD modes downstream a 90 deg bend for Re 5 3.4 3 104 and c 5 0:32. Magnitude of the in-plane components is shown as the background contour map and their direction as vectors. The highest magnitude of the in-plane components on average is 0.3Wb, while instantaneously it is 0.5Wb. Reprinted with permission from Kalpakli and €u[110]. Copyright 2013 by Elsevier. (c) Flow .eld from the most
Orl€energetic POD modes and the mean, from DNS in a torus with Re 5 1.2 3 104 and c 5 0:3. Isocontours of the streamfunc-tion W projected on top of its pseudocolors where red represents positive and blue indicates negative values of the streamfunction. Reprinted with permission from Noorani and Schlatter [121]. 
upstream of the bend but also the separation bubble at the inner bend corner. 
The phenomenon observed by TH68 was overlooked from the .uid dynamics¡¯ community for several decades until the study by Br€
ucker in 1998 [130] where¡ªamong others¡ªthe term ¡°swirl switching¡± was assigned to it for the .rst time. Two-dimensional two-component PIV (2D2C) measurements were performed in a sharp bend (c .0:5) for Re .2 103 and 5 103. For the low-est Reynolds number, the switching of the secondary .ow as described by TH68 was not observed. However, for the higher Reynolds number case, at which the .ow is expected to be turbu-lent, the Dean vortices were found to alternatingly push one another toward the lower or upper half of the pipes with respect to the plane of symmetry, see Fig. 22. Three distinct peaks were identi.ed from power spectral density of the time signal of the tangential velocity component at a point close to the inner wall of the bend. Those frequencies corresponded to St .0:03, 0.12, and 
0.2. This observation supports the fact that such oscillatory motion exists only for turbulent .ows as suggested in TH68, however, the .ow pattern that was observed by Br€
ucker was different from the single dominating cell pattern observed in TH68. A similar pattern to the one captured by Br€
ucker was later on visualized by means of time-resolved stereoscopic PIV (TS-PIV) by Kalpakli et al. 
[131] (where the same motion as the one found by Br€
ucker was described as ¡°rocking¡± of the cells) as well as in a follow-up work 
[110] and a recent DNS study in a torus [121] (see Fig. 22). 
Until the beginning of the 21st century, only the studies by TH68 and Br€
ucker on the swirl switching were available¡ªwith the one by Br€
ucker [130] being the only one providing quantita-tive visualizations of the phenomenon¡ªand it was believed that separation and turbulence were necessary conditions for it to appear. In more recent years, three main concerns have driven the studies on the swirl switching, as it will be described in the fol-lowing. First, from the two early studies by TH68 and Br€
ucker [130], it was made clear that the Dean vortices oscillate at a low frequency, however, how this frequency is affected by .ow parameters such as the Reynolds number or the curvature of the bend was unknown. Spectra of either the time series [110] or the proper orthogonal decomposition (POD) time coef.cients [124,126,132] were employed in order to investigate this. The latter method which stands for proper orthogonal decomposition was used by many authors [110,124¨C126,132] in order to .lter the .ow .eld and decompose the large-scale structures from small-scale turbulence. POD is known to rank the .ow structures or so-called modes by energy content: the mean .eld accounts for more than half of the energy, typically more than 90% in steady .ows [133]. POD-based low-order reconstruction of turbulent pipe .ows has recently become quite popular, especially when PIV data are at hand [134] because it allows for a better visualization of coherent structures present in turbulent .ow .elds. For the swirl switching studies that applied POD, the observed modal pat-tern in most of the studies had a speci.c trend: the .rst two most energetic modes (excluding the one representing the mean) depict either the Dean vortices tilted or a single vortex with its center located at the pipe centerline, spanning the whole pipe cross sec-tion. Those modes have been shown to switch order (in terms of energy) with downstream distance from the bend exit [125,132]. Finally, whether the origin of the switching is connected to the separation at the inner bend corner or upstream .ow conditions/ structures is a matter that has been investigated during the last decade [128]. 

The concern about the frequency of the switching as well as the role of separation in the switching mechanism was investigated by means of LES by R€
utten et al. in two similar papers [18,113] with the second one being the most well-cited. Those investigations showed¡ªin contrast to TH68¡ªthat the swirl switching is not caused by the separation at the inner bend corner because spectra of the forces onto the pipe walls showed distinct peaks for both a sharp (c .0:5) and a mild (c .0:17) bend, where in the latter one separation was not observed. Spectral analysis was also per-formed on the time series of the position of the ¡°stagnation point¡± (the minimum of wh wr 2, where wh and wr are the azimuthal
2 t
and radial velocity component, respectively) at the inner wall to check whether a correlation exists between the forces on the walls, the stagnation point movement, and the alternating domination of the Dean vortices. Both spectra from the forces and the stagnation point showed low-frequency oscillations at St .0:0055 [18] and St .0:01 [113] for Re .2:7 104. That frequency was found to agree with the switching frequencies found in TH68 by extrapolat-ing those measurements to the Reynolds number employed in the LES.11 
Higher frequency peaks at St .0:2 and 0.3 were also present in the LES spectra which were interpreted as shear layer instabilities. It was additionally found that the stagnation point¡ª and therefore the Dean vortices¡ªwas not abruptly switching between two stable positions (as suggested by TH68 and Br€
ucker [130]) but rather moving smoothly between azimuthal positions within 640 deg [113]. 
LES of a curved pipe .ow with the same .ow parameters as the experiments by Sudo et al. [87] were also conducted by Hellstr€
om and Fuchs [95]. The LES agreed well with the experiments, and the swirl switching pattern visualized by the LES agreed well with the observations by R€
utten et al. [113]. The .rst POD analysis of the swirl switching, which was done in this LES study, showed a periodic variation of the most energetic modes which corresponds to St .0.5. However, this was probably a subharmonic of the sampling frequency, rather than a physical frequency. 
An effort to identify the switching motion of the Dean vortices was made by Sakakibara et al. [132] by applying POD on TS-PIV data, acquired at z=D .2 ¨C15 downstream the bend exit for Re .
1:2 105 and c .0:75. The swirl switching was reconstructed using the most energetic modes, being the mode representing the mean and the .rst two modes related to the .uctuating part of the .ow. It was shown that the most energetic mode related to the .uctuating part at the z=D .2 resembles the switching motion as 
11However, one should note that this comparison was misleading since the Reynolds number used in TH68 (Re .2:2 105) misinterpreted by R€
was utten et al. [18] (as Re .2:2 104). Therefore, the extrapolation made by the authors was invalid since the difference in Reynolds number between the study by TH68 and Ref. [18] is too large. 
described in TH68, i.e., a single vortex switching direction alter-natingly from anticlockwise to clockwise rotation. However, this mode decayed as the .ow evolved from 2D further downstream and became weaker than the second mode at 2D. This second mode was found to be responsible for a rotation of the plane of symmetry of the Dean cells as described in Ref. [130]. Further-more, spectra of the second temporal mode at 2D showed a peak at St .0:07. This study showed¡ªamong others¡ªthe capability of POD to .lter and suppress small-scale turbulence in PIV data (where spatial resolution is limited) and consequently, emphasize the swirl switching (being a large-scale phenomenon). POD has since then been used in numerous studies investigating the swirl switching mechanism, one of the earlier ones being the study by Kalpakli and €u[110], where it was shown that the switching of 
Orl€the Dean vortices could be reconstructed by using only the .rst six POD modes. 
In a follow-up paper, Sakakibara and Machida [128] also inves-tigated the connection between structures existing upstream of the bend and the swirl switching mechanism for Re .2:7 104 and c .0:5. The azimuthal displacement of the stagnation point was found to be correlated to streaks existing upstream of the bend that was 7¨C8D long, see Fig 23. Those streaks were suggested to be related to the very large-scale motions (VLSMs, see, for exam-ple, Ref. [135]), though being much shorter in length compared to the VLSMs as known for turbulent pipe .ow studies in straight pipes. The authors concluded that the streaks are responsible for the oscillatory motion of the vortices, which adds another possible explanation (apart from separation as claimed in TH68) for the cause of the swirl switching. 
A similar effort as the one made in Ref. [132], i.e., to identify the origin of the swirl switching through POD analysis of data obtained at different downstream stations from the bend, was done by Hellstr€
om et al. [126]. The Reynolds number in this study was Re .2:5 104 and c .0:5. Flow .elds at three downstream distances from the bend exit (5, 12, and 18D) were acquired by means of TS-PIV. In that case, the reconstruction of the swirl switching was done considering only the most energetic modes related to the .uctuating part of the .ow, i.e., the mode represent-ing the mean was not included. This was based on the fact that (even in an earlier preliminary work [89]), the Dean vortices were seen only in the mean vorticity maps and not in the instantaneous ones. The authors concluded that the Dean vortices are weak and they were, therefore, not considered in the reconstruction process. Additionally, the Dean vortices in the mean vorticity map were not symmetrical and as the authors remarked: ¡°the mean vorticity [¡­] shows the tilted Dean motion as mentioned by Ref. [130]. The mean .ow structures are, however, not evident among the much higher levels of instantaneous vorticity¡± [126]. It should, however, be kept in mind, that in such turbulent .ows as the ones described here, the levels of instantaneous vorticity will always be higher than in the mean. Furthermore, small turbulent scales which cannot be resolved due to the limited spatial resolution of the PIV will appear as noise in the instantaneous vorticity maps. Therefore, in order to obtain a meaningful representation of the .ow .eld, appropriate .ltering of the data, in order to suppress the small scales, is important. Additionally, the comparison of the two studies at this point is misleading since a mean quantity (vorticity) in the case of Ref. [124] is not comparable to instantaneous quan-tities shown in Ref. [130]. The .rst POD mode at all the down-stream stations depicted (similarly to the .rst POD mode seen by Sakakibara et al. [132]at z=D .2, see above) a single vortex spanning the entire cross section and rotating in the clockwise and anticlockwise direction as the .ow evolved downstream the bend (Fig. 24). This mode was found to be associated with St .0:33 through spectra of its temporal counterpart. The second and third mode depicted two Dean cells tilted with respect to the plane of symmetry with one of the cells being considerably suppressed. The second mode was associated with St .0:16, nd was referred to as ¡°tilted Dean vortices.¡± This is similar to the mode appearing as the most energetic one in Ref. [110] but also in recent LES and DNS studies which will be discussed later in this section. The authors concluded that the tilting of the Dean cells is a transitional state between two states of the swirl switching, during which one of the cells is being suppressed resulting in a single vortex (shown as the .rst mode in the POD analysis). The time-scale of those transitions corresponds to structures of length 0.25¨C2.5D. Finally, it was proposed that the suppression of one of the Dean cells was related to the shear .ow region at the inner bend corner which is in return sensitive to upstream conditions, supporting the .ndings by Sakakibara and Machida [128]. 


A similar modal pattern as the one described by Sakakibara et al. [132] and Hellstr€
om et al. [126] was observed by Kalpakli et al. [125]. The evolution of the POD modes downstream (1, 2, and 3D) a mild (c .0:14) and sharp (c .0:39) bend was investi-gated in this study. At all the downstream stations where data were acquired, the .rst mode also depicted a single vortex span-ning the whole cross section, whereas the second and third modes depicted the tilted Dean vortices with one of them also being sup-pressed. In the same study, the .rst effort to control the swirl switching was made; a honeycomb was mounted at the inlet of the bend with the aim to suppress any unsteady upstream structures and at the same time reduce the turbulence intensity. In the case when the honeycomb was mounted, the modal pattern changed substantially compared to when the honeycomb was not present, possibly indicating a diminishing or delay of the switching. 
The study by Carlsson et al. [124] addresses both the connec-tion between VLSMs and switching as well as the shape of the POD modes for a variety of curvature ratios at the same Reynolds number and downstream location from the bend as in Ref. [110] by means of LES. The agreement between the LES and the 


Downloaded from http://asmedigitalcollection.asme.org/appliedmechanicsreviews/article-pdf/68/5/050802/6075086/amr_068_05_050802.pdf by Shanghai Jiaotong University user on 03 October 2020 
om et al. [126]. Copyright 2013 by Cambridge University Press. 

€
from Kalpakli and Orl€
u[110]. Copyright 2013 by Elsevier. (b) Same as before but the results are from DNS data in a torus for c 5 0:3 and Re 5 1:23104. Reprinted with permission from Noor-ani and Schlatter [121]. 
experimental data [110] was rather poor with regard to the mean velocity pro.les, however, the POD analysis of the LES data iden-ti.ed the same modes as found from the POD analysis of the experimental data. The authors observed St .0.13 from their fre-quency spectra analysis in the upstream straight pipe section and interpreted that as existence of large-scale structures (VLSMs). They suggested that the swirl switching is connected to those structures. 
Recent DNS by Noorani and Schlatter [121] in a torus (i.e., in.nitely long curved pipe) indicate that the swirl switching can-not be caused by neither separation nor large-scale structures existing upstream from the bend and con.icts with some of the above studies [124,128] but also with the initial study by TH68. That is simply because the swirl switching was observed in the toroidal pipe as well, exactly in the same manner as in previous experimental studies (clearly depicted in Fig. 22) showing even the same modal pattern as in Refs. [110] and [124]. In a torus, there are no upstream conditions as well as no local separation, therefore this excludes those two as the necessary conditions for the swirl switching to be present. Nevertheless, it is still unclear what exactly is the role of separation or upstream structures in the way the switching is formed. The spatial mode that was found from the POD analysis in the DNS study by Noorani and Schlatter [121], but also earlier in experiments [110] and LES [124], resem-bles antisymmetric Dean vortices seen also as the second POD mode in Ref. [126], see Fig. 25 
for an illustration. It was shown that this is the dominant mode that might cause the swirl switch-ing and it is only related to the curvature itself, whereas the single-swirl-mode observed earlier [125,126] did not exist in that study. This suggests that this mode is not part of the swirl switching mechanism but could instead be related to the down-stream straight sections from the bend, which are not present in the case of the torus. The study by Kalpakli et al. [125] showed, for example, that in a mild bend (where most probably separation did not occur), the oscillations were weaker compared to a sharp bend. In the same study, the insert of a honeycomb upstream of the bend which is expected to damp any large-scale structures indicated delay of the switching. Therefore, it could be that some .ow features enhance or delay the swirl switching but are not nec-essarily the cause of it. 
At this point, it should also be mentioned that a series of stud-ies, which have been published during the last decade [136¨C138], investigated the behavior of the secondary motion in bends com-prising the cooling system of a nuclear reactor (Japan sodium-cooled fast reactor, JSFR), however, without assigning the term swirl switching to their observations. The results from these stud-ies dealt mainly with the vortex shedding from the separation region, and the frequency spectra showed only peaks at frequen-cies corresponding to Strouhal numbers higher than St .0.4. However, a similar phenomenon resembling the swirl switching motion as captured in Ref. [131] has also been revealed, for exam-ple, through PIV measurements in Ref. [127]. The PIV measure-ments were performed in two elbow bends resembling the cold-and hot-leg pipings existing in the cooling system of the JSFR, and measurements were taken both along the bends and at down-stream cross sections. The two Dean vortices were found to alter-natingly dominate the .ow .eld rotating in the clockwise and anticlockwise directions in a periodic manner, whereas separation was captured at the inner bend corner. It was believed that this periodical .ow, that looked similar to what other authors have referred to as swirl switching, affected the behavior of the separa-tion region near the inner wall. The above references have been largely neglected in the literature even though the phenomena observed could be connected to the so-called swirl switching and those are the only ones dealing with very high Reynolds number (e.g., Re .4:2 107 in Ref. [127]). 
A list of the aforementioned studies dealing with the swirl switching including some of the parameters under which they were conducted are summarized in Table 2. From the above, it is made clear that the swirl-switching phenomenon interests many research groups and as has been underlined, this oscillatory motion can cause fatigue, increase noise, and lead to failures in industrial systems which consist of bends and conduits and where the .ow is highly turbulent, such as in the cooling systems of nuclear plants. And even though the interest on this phenomenon has increased during the last decade, many open questions still remain. First of all, the pattern of the switching itself is unclear, i.e., are the Dean vortices ¡°rocking¡± in between two distinct states or does each one of them as a single vortex smoothly ¡°roll¡± 

Table 2 Summary of the studies related to the swirl switching. Note that the lowest Strouhal numbers are usually associated with the swirl switching. 
Author, year  4Re 10  c  z/D  St Diagnostic technique  
Tunstall 
and 
Harvey, 
1968 
[129] 
Br€ucker, 
1998 
[130] 
R€utten 
et 
al., 
2001 
[18] 
R€utten 
et 
al., 
2005 
[113] 
Hellstr€om 
and 
Fuchs, 
2007 
[95] 
Sakakibara 
et 
al., 
2010 
[132] 
Sakakibara 
and 
Machida, 
2012 
[128] 
Hellstr€om 
et 
al., 
2012 
[89] 
Kalpakli 
and 
€Orl€u, 
2013 
[110] 
Kalpakli 
et 
al., 
2013 
[180] 
Hellstr€om 
et 
al., 
2013 
[126] 
Fj€allman 
et 
al., 
2013 
[139] 
Kalpakli 
et 
al., 
2015 
[125] 
Noorani 
and 
Schlatter, 
2015 
[121] 
Carlsson 
et 
al., 
2015 
[124] 
 4¨C22 0.5 2.7 2.7 6 12 2.7 1.2, 1.8 1.4, 2.4, 3.4 2.4 2.5 2.4 2.4 1.17 3.4  1 0.5 0.5, 0.17 0.5, 0.17 0.25 0.75 0.5 1 0.31 0.4 0.5 0.31 0.14, 0.39 0.1, 0.3 0.32, 0.5, 0.7, 1  2.2 0, 1.5 2.5 2.5 0, 0.5 2, 10, 15, 25 2.5 5.3 0.67 0.2, 2 5, 12, 18 0.67 1, 2, 3 Torus 0.67  0.001¨C0.004 Visualization, gold-shim .ag 0.03, 0.12 2D2C PIV 0.0055, 0.014, 0.2, 0.3 LES 0.01, 0.2, 0.3 LES ¡ª LES 0.07 2D3C PIV ¡ª 2D3C PIV ¡ª 2D3C PIV 0.04, 0.12, 0.18 2D3C PIV ¡ª 2D3C PIV 0.16, 0.33 2D3C PIV ¡ª LES ¡ª 2D3C PIV 1, 0.087 DNS 0.003¨C0.01, 0.13, 0.5¨C0.6 LES  
Applied Mechanics Reviews  SEPTEMBER 2016, Vol. 68 / 050802-17  

Table 3 Studies of turbulent .ows in straight and curved pipes with and without swirl 
Flow in bends Swirling flow in straight pipes Swirling flow in bends 
Tunstall and Harvey, 1968 [129] White, 1964 [145] Binnie, 1962 [144] Enayet et al., 1982 [85] Murakami and Kikuyama, 1980 [146] Shimizu and Sugino, 1980 [161] Azzola et al., 1986 [86] Kikuyama et al., 1983 [150] So and Anwer, 1993 [157] Anwer et al., 1989 [88] Kikuyama et al., 1983 [148] So and Anwer, 1993 [158] So and Anwer, 1993 [158] Reich and Beer, 1989 [147] Anwer and So, 1993 [17] Boersma 
and 
Nieuwstadt, 
1996 
[112] Eggels, 1994 [152] Chang and Lee, 2003 [163] Br€ucker, 1998 [130] Imao et al., 1986 [149] Pruvost et al., 2004 [94] Sudo et al., 1998 [87] Orlandi and Fatica, 1997 [153] Orl€
Kalpakli and €u, 2013 [110] H€uttl and Friedrich, 2001 [122] Speziale et al., 2000 [155] Kalpakli Vester et al., 2016 [141] Pruvost et al., 
2004 
[94] Rocklage-Marliani et al., 2003 [151] R€Feiz et al., 2003 [154]
utten et al., 2005 [113] Kalpakli et al., 2012 [131] Facciolo et al., 2007 [144] Sakakibara and Machida, 2012 [128] Nygard and Andersson, 2009 [156] Fj€allman, 2013 [139] ¡­ Kalpakli and €u, 2013 [110]
Orl€Noorani et al., 2013 [81] Hellstr€
om et al., 2013 [126] Carlsson et al., 
2015 
[124] R€
ohrig et al., 2015 [100] ¡­ 
alternatingly in time. Furthermore, the origin of the switching of the Dean vortices is still unclear: is it connected to VLSMs, is sep-aration a possible trigger or could it just be a result of an instabil-ity mechanism in the bend itself. Finally, the possibility of control of the switching is an unexplored .eld which could provide possi-ble practical ways to avoid fatigue of piping systems. 
5 Swirling Flow 
When a swirling motion is superimposed on the main .ow, the balance between centrifugal, inertial, and viscous forces in the curved pipe changes. Turbulent swirling .ow is encountered in many technical applications such as in hydraulic plants, combus-tion chambers, and any machine that involves a turbine or fan or a combination of bends, e.g., double bends [140]. As apparent from a recent study [141], the work that has been done on swirling tur-bulent .ow in bends is not very extensive, compared to, for instance, work that has been done in swirling turbulent .ow in straight pipes (see as a reference Table 3 
extended from Ref. [141], where studies in straight and curved pipes with and without swirl are listed). 
For the generation of swirl, a number of different methods exist (pipe rotation, tangential injection, and vanes), which have a dif-ferent effect on the base .ow. For details, the reader is referred to Ref. [142]. This section will mostly refer to studies using the rota-tion method, i.e., through axially rotating a straight pipe section upstream the bend, since it provides a well-de.ned velocity pro.le and does not leave traces of the swirl generating mechanism (such as other means to introduce swirl on the .ow). 
In cases where the swirling motion is introduced by imposing a secondary .ow or by means of passive methods (guiding vanes), one needs to calculate an integral swirl number, de.ned as the ratio between the .uxes of angular momentum to streamwise momentum [142]. Such a swirl number has been de.ned by Gupta et al. [143] as follows: 
eR e2p WUhr 2 dr dh 
00
Sint .eR e2p (5) 
RW2rdrdh 00 
where Uh is the circumferential velocity, and r and h the radial and angular coordinates, respectively. 
When the .ow is in solid-body rotation, the imposed swirl intensity can be approximated in terms of the mean angular velocity (X) of the rotating motion related to the mean bulk veloc-ity as follows [94]: 
XR 
SSB .(6)
2Wb 
However, when the whole pipe is rotated to create the swirl motion and the .ow is turbulent, the .ow is not in solid-body rota-tion [144], see also below in the following paragraphs. In that case, the imposed swirling motion can be expressed through the swirl number de.ned here as 
Uw
S .(7)Wb 
with Uw being the pipe wall velocity. This is a quite common expression used in recent studies, and its de.nition is also adapted here. The above swirl numbers are connected to each other; when the mean axial velocity is constant for a plug .ow and Uh has a linear variation, the integral swirl number becomes equal to the swirl number for a .uid in solid-body rotation, whereas from Eqs. 
(6) 
and (7), we get S .2SSB. 
For turbulent .ow in straight pipes with imposed swirl, a num-ber of studies have been performed and it is now well-known that swirl decreases the pressure drop [145]. The mean streamwise velocity pro.le becomes less full and approaches the one of a lam-inar .ow [146,147]. It has been shown that in the case of turbulent swirling .ow, due to the cross-stream Reynolds stresses, the .uid is not in solid-body rotation but the azimuthal velocity pro.le is nearly parabolic [144,148,149]. Furthermore, an increase in the swirl intensity has been shown to decrease the turbulence intensity [149,150]. More recent three-dimensional LDV measurements of turbulent swirling straight pipe .ow by Rocklage-Marliani [151] showed an annihilating effect of the swirl on the Reynolds shear stresses, i.e., the Reynolds stresses were found to be low in the pipe core region, therefore suggesting that the swirl tends to lami-narize the .ow. One of the .rst LES performed on turbulent swirl-ing .ow [152] con.rmed the above and showed that the largest decrease in the turbulence intensity is observed for the streamwise velocity component. Early DNS [153] showed that the reduction in drag is responsible for a widening of the near wall structures in a turbulent swirling straight pipe .ow. Those data were used later in Ref. [154] in order to show that LES can reasonably well pre-dict phenomena in rotating turbulent pipe .ows. RSMs were developed by Speziale et al. [155] and were successful in captur-ing the main features of turbulent swirling .ow as described 


Fig. 26 Flow structures at increasing swirl numbers, S, as cap-tured by Stereo-PIV experiments. Background contour maps show the streamwise velocity component scaled by the bulk velocity, whereas the in-plane velocities are shown as the imposed vectors. Reproduced with permission from Kalpakli and €u[110]. Copyright 2013 by Elsevier. 
Orl€
above. More recent DNS on turbulent swirling .ow in a straight pipe [156] have shown the effect of swirl on near wall structures which appear as elongated streaks being tilted and shortened as the swirl intensity increases. 
The case of a turbulent swirling .ow through a 180 deg curved pipe at Re .5 104 with c .0:077 was examined experimen-tally in Ref. [157] and later in Ref. [17]. The effects of swirl with SSB .0.5 on the secondary .ow were examined through a compar-ison with nonswirling .ow data acquired in the same setup. Wall-static pressure distributions between the swirling and nonswirling cases showed opposite results in the two cases: the static pressure was lower at the outer bend as compared to the pressure at the inner for the swirling .ow whereas the opposite was found for the nonswirling case. The wall shear stress did not show differences between the outer and inner wall whereas for SSB .0, there was a signi.cant difference between the shear at the inner and outer walls due to the existence of the Dean cells in that case. From the mean velocity pro.les along the xz-plane, it was shown that the velocity distribution is more uniform and symmetric for the swirl-ing .ow case. The aforementioned observations suggested that a single dominating cell exists and that the curved-pipe .ow becomes fully dominated by the imposed solid-body rotation. An increase in turbulence production by the imposed swirling motion resulted in more uniform distributions of the Reynolds normal stresses in the radial and tangential directions. As a consequence, the radial and azimuthal gradients of these stresses are reduced, which explains why the secondary .ow created by the bend can-not be sustained. 
The follow-up work by So and Anwer [158] showed that in swirling .ow, the distance needed for the .ow to become fully developed in a curved pipe is shorter than that needed in the case of a straight pipe. Furthermore, it was found that the bend acceler-ates the decay of the swirl compared to swirling .ow in a straight pipe. 
Pruvost et al. [94] numerically studied the same .ow case as in Ref. [17] but extended the investigation for a range of swirl num-bers between SSB .0 and 0.5. An additional case for SSB .2.5 was also studied but no experimental data were available for vali-dation. Different numerical approaches were tested, and the results were compared to the experimental data by Anwer and So 
[17] both for the swirling and nonswirling cases. However, all the models gave poor agreement with the experiments for the mean velocity pro.le and TKE pro.le for SSB .0. For SSB .0.5 the low-Re, k e model gave satisfactory results for the mean veloc-ity, but again the agreement with the experimental data for the TKE pro.le was poor. Nevertheless, it was shown that there exists a complex interaction between the Dean vortices and the swirling motion for small S. As the swirling motion intensi.es, the Dean cells tend to merge into a single cell until the .ow .eld becomes completely swirl dominated. Similar observations about the inter-action between the Dean vortices and the swirling motion in tur-bulent .ow were made by means of PIV experiments by Kalpakli and €u[110] for S .0
Orl€1:2. The difference between that study and the one by Anwer and So [17] is that the swirling motion was created by rotating the whole straight pipe mounted upstream of the bend, whereas in the case of Ref. [17] only a pipe section rotated, therefore, the swirl was decaying downstream the rotated pipe section. Kalpakli and €u[110] showed that even for a low
Orl€swirl intensity, the swirling motion has a stabilizing effect on the .ow, moving the high speed .uid from the outer wall to the center of the pipe. As the authors claim ¡°axisymmetry of the .ow .eld is gradually achieved with increasing swirl number, as the Dean vor-tices merge resulting in a single large-scale motion located almost at the center of the pipe for S .1.2.¡± POD analysis was addition-ally performed in order to decompose the superimposed swirling motion from the turbulent .ow .eld. For S .1.2, the .rst POD mode depicted similar structures to the Dean vortices. Those observations¡ªfrom the .rst quantitative visualization experi-ments in swirling turbulent curved pipe .ow¡ªagreed qualita-tively with the aforementioned simulations in Ref. [94] and are illustrated in Fig. 26. Another aspect investigated by some authors for turbulent swirling .ow in curved pipes is the existence and shape of the so-called VLSMs, which are known to exist for tur-bulent straight pipe .ow [159]. For a curved pipe with c .0:36, 
€
Kalpakli and Orl€showed that .sh-bone-like
u[110] structures exist, similar to those found by Nygard and Andersson [156] for swirling turbulent .ow in a straight pipe, however, those were inclined due to the presence of the bend. Whether those structures were truly VLSMs was not investigated by the authors. The effect of increasing the swirl intensity was further shown to tear up the observed elongated structures, agreeing again with the .ndings by Nygard and Andersson [156] for a straight pipe. 
A database for both statistical and modal analysis for turbulent swirling .ow in a curved pipe was recently made available by Kalpakli Vester et al. [141]. Data from hot-wire and TS-PIV measurements taken at the same experimental conditions as in Ref. [110] were presented and analyzed for a range of Reynolds and swirl numbers. This is also a case more suitable for CFD vali-dations compared to previous data available in the literature [17,157] since the in.ow condition is well-de.ned by rotating the whole straight pipe section connected to the bend, and the swirl intensity remains the same along the pipe. The authors showed the in.uence of increasing swirl intensity on the .ow statistics such as the mean and r.m.s. velocity pro.les which are shown in Fig. 

27. The effects from the bend are clearly smeared out as the swirl strength increases, with the velocity pro.le becoming more uni-form and the higher velocities shifted from the outer pipe wall to the pipe core. 
Other well-known studies which did not use the rotation method to generate swirl should also be brie.y mentioned. One of the earlier studies on swirling .ow is the one in Ref. [160], where swirling .ow through a bend was visualized. Using a Pitot tube, pressure losses in a 180 deg bend were investigated in Ref. [161]. The difference in the formed .ow patterns in the case when the imposed swirling motion is a free-or forced-type vortex was also investigated. Those data were later used to validate a theoretical study of swirling curved pipe .ow [162]. The effects of swirl on the secondary .ow .eld along a bend were also studied in a later work by Chang and Lee [163] through planar PIV measurements for four different Re in the range 1:02:5 104. In that study, the swirling motion was created by tangential inlets. The main observation was that a two-cell pattern existed at the entrance of the bend which disappeared as the .ow developed along the bend with a decrease in swirl intensity. 
6 Pulsating Flow 
Pulsating (or pulsatile) .ow, i.e., the unsteady .ow12 
composed of a periodic and a steady component is typical in biological .ows, such as the respiratory and cardiovascular system (see, for example, the textbook by Pedley [5]). Therefore, the main body of unsteady .ow research has been devoted to physiological .ows [164¨C166]. However, pulsatile .ow occurs in and plays a major role in the performance of technical systems as well, such as reciprocating engines or heat exchangers. Stokes [167] was the .rst to do substantial work on unsteady .ows and today the boundary layer thickness in pulsating .ows bears his name. In a pipe, it is known that due to the nonslip condition at the wall, additional vorticity varying in time needs to be generated in the boundary layer in order to account for the mass-.ow variation under pulsating .ow conditions. In that sense, shear waves are generated, which propagate from the wall into the .uid and are attenuated as they do so [168]. In laminar .ow, the wave attenua-tion is characterized by a length scale, known as the Stokes
p...........
boundary layer and de.ned as ¡®s .2 =x, where x .2pf is the angular frequency of the pulsations and is the kinematic vis-cosity. Stokes showed that the amplitude of the waves are attenu-ated by a factor e .2.71 over a distance equal to ¡®s which means that the higher the pulsation frequency, the thinner the boundary layer. Womersley [169] de.ned a nondimensional parameter, which today bears his name and shows the counteraction between the transient inertial forces and the viscous forces 
r....
x 
a .R (8) 
but can also be seen as a ratio between an outer length scale (R) and the Stokes boundary layer thickness. 
According to the above, the boundary layer is expected to be thick for low Womersley numbers, whereas for large Womersley numbers it is expected to be thin. For example, it has been shown that for highly pulsatile turbulent .ow, the shear waves propagat-ing into the .uid from the wall are strongly attenuated and con-.ned mainly to the viscous sublayer region. The inner and outer layers are hence completely decoupled, and turbulence in the outer region is not affected by the pulsations and can be regarded as frozen, see the reviews in Refs. [168] and [170]. 
For physiological .ows, the Womersley number is small (of order 1) as is the Reynolds numbers whereas in technical applica-tions (such as the internal combustion engine), those numbers 
12It should be noted that unsteady .ows may be divided into two categories: pulsating .ow when the periodically time-averaged velocity is non-zero and oscillating .ow in which the periodically time-averaged velocity is zero. 

r.m.s. streamwise velocity pro.les normalized by the bulk velocity at Re 5 2:43104 and at 0.67D distance from the bend. Reprinted with permission from Kalpakli Vester et al. [141]. Copyright 2016 by Springer. Caption of .gure in the original publication: ¡°The swirl number effect on the a outer-scaled streamwise r.m.s. and b turbulence intensity at ReD 5 24;000¡±. 
may be of the order of 102 and 105, respectively. According to Ref. [171], for Womersley numbers smaller than approximately one, the .ow can be considered quasi-steady since the turbulent structures have time to accommodate to the slowly varying .ow rate. For 1 <a <30, the .ow is in an intermediate state (passage between quasi-steady and pulsatile .ow), whereas for a >30 the frequency of the pulsatile motion is high enough for turbulence not to be able to respond to the rapid changes in the .ow and the .ow becomes inertia dominated. Hence, turbulence becomes inde-pendent of the phase angle of the pulsations. In the following, we will focus on previous studies on pulsating turbulent .ow in curved pipes, which as we will see are limited. 
Pulsations are sometimes induced in heat exchangers since investigations have shown that they might increase the heat trans-fer rates [172]. In that respect, pulsatile turbulent .ow has been studied for different pipe con.gurations (mainly coiled tubes) in terms of enhancement of the heat transfer [173,174]. However, until recently, there were no studies¡ªto the best of our knowledge¡ªdealing with the effects of pulsatile .ow on turbulent curved pipe .ow with regard to the secondary .ow evolution or the dynamics of the .ow. Studies in laminar, low-frequency pulsa-tile .ow have already shown that under one cycle, the secondary .ow patterns vary due to competition between the centrifugal, inertial, and viscous forces [166,175,176]. Moreover, studies on the effect of high pulsations (for Womersley numbers a30) on 


Fig. 28 Top: Contour plots of the velocity .elds obtained through reconstruction using the four most energetic POD modes. z=D 5 0:2;Re 5 2:43104;c 5 0:4, and a 5 41. Five pulsating .ow .elds are shown for the corresponding phase angles indicated (1¨C5) in the centerline sig-nal of the streamwise velocity (middle subplot). Streamwise velocity scaled by the bulk speed is shown as the background contour map, whereas the in-plane components are shown as the vectors. Bottom: the .rst, second, and third POD modes (from left to right) shown as ¡°cross-sectional¡± streamlines. Reprinted with permission from Kalpakli et al. [180]. Copyright 2013 by Elsevier. 
turbulent curved pipe .ows encountered for example in the inter-nal combustion engine .ow environment are scarce. 
Kalpakli et al. [177] performed stereo PIV (SPIV) and instanta-neous mass .ow and pressure measurements in a unique .ow rig (CICERO rig, see Ref. [178]) that can be run both under steady and highly pulsatile .ow conditions (up to f .100 Hz) for engine-.ow-related studies. The secondary .ow downstream a sharp bend (c .0:4) was captured for a .30 revealing a complex .ow .eld during one pulse period and velocity magnitudes up to four times the bulk velocity. Instantaneous mass .ow and pressure measurements were also performed upstream and downstream a commercial passenger car turbocharger turbine with the bend used for the SPIV measurements mounted at its inlet and for various bulk velocities and 40 Hz pulsatile frequency. Comparisons with a similar study that used a straight pipe as the inlet to the same tur-bine [179] showed that the hysteresis loop in the turbine maps was signi.cantly damped in the case with the bend mounted at the inlet of the turbine. In a follow up study, Kalpakli et al. [180] investigated in more detail the effect of high pulsations (Womers-ley numbers a .41 and 71), on the secondary .ow. TS-PIV meas-urements were performed in a curved pipe of c .0:4, and the .ow .eld at cross sections 0.2D and 2D downstream locations was quantitatively visualized. The .ow structures were captured during one pulsation cycle, and it was shown that four main pat-terns existed: during acceleration and deceleration the .ow pattern resembled the one seen in steady .ow in the mean, i.e., two sym-metrical vortices could be observed. At the peak of acceleration weak secondary motion existed, whereas at the end of the deceler-ation phase the vortices overtake the whole cross section but with their centers located at the centerline of the pipe (see Fig. 28). POD was also for the .rst time applied on a highly pulsating turbulent .ow through a curved pipe, and it was shown that POD reconstruction of the .ow .elds is ¡°superior to phase-averaging, although the latter serves as the main processing tool for pulsatile .ow .elds so far in the literature.¡± The vortices seen in the phase-averaged and POD reconstructed .ow .elds were also observed as the most energetic spatial POD modes. 
A third work by the same group was also published recently 
[181] with its main focus, this time, on the statistical analysis and decomposition of highly pulsating turbulent .ow signals measured by means of a single hot-wire. Among others, the usability and shortcomings of a single hot-wire probe without temperature com-pensation in a highly pulsating .ow were demonstrated and explained. The Reynolds number was Re .2:4 105, and the Womersley number was a .80, which is the highest Womersley number reported¡ªto the best of our knowledge¡ªfor measure-ments in a curved pipe. Time series, phase-averaged, and low-pass .ltered signals, mean and r.m.s. pro.les, probability density function distributions, ensemble-averaged velocity pro.les at vari-ous phase angles, as well as reconstructed signals through singular value decomposition (SVD) were presented and discussed. The SVD was shown to be a more robust and ¡°user-independent¡± tech-nique than digital .ltering and phase-averaging, whereas the decomposition techniques employed were ¡°capable of separating the ¡°background¡± (small-scale) turbulence from the large-scale pulsatile motion. [¡­] indicates that the large-scale pulsatile motion, due to the wide scale separation, is merely superposed on the turbulence¡± [181]. 
7 Summary and Outlook 
Curved pipes are met in a variety of technical applications and give rise to strong secondary .ows, enhanced pressure drop, as well as sometimes low-frequency oscillations. Issues that have been puzzling the .uid dynamics¡¯ community for many decades regarding .ows in curved conduits are of both applied and basic scienti.c interest, namely, transition to turbulence and pressure drop for different bends of practical importance as well as how the secondary motion behaves when the .ow is turbulent, pulsat-ing, and/or swirling. The .ow environment in a curved pipe is challenging to investigate both for experimentalists and numeri-cists due to the .ow¡¯s three-dimensionality (therefore, all the velocity components need to be obtained to get a complete picture of the .ow .eld), occurrence of separation in the inner bend wall, as well as large velocity gradients. Modeling of such .ows is gen-erally favored with RANS models still being largely used in industry, however, it has been shown that due to the complexity of the .ow, simple turbulence models cannot predict such .ow .elds accurately. Instead, highly resolved LES or DNS need to be performed, which need to be validated against experiments and are computationally expensive and restricted to moderate/low Reynolds numbers, respectively. 

One of the matters that to a large extent is still an unexplored area is the transition from laminar to turbulent .ow in curved pipes. It is noticeable that the .rst instability analysis on curved pipe .ow was performed in 2016 [19], even though studies on the transition to turbulence in curved pipes are traced back to the beginning of the 1930s. It was shown already in the works by Tay-lor [15] and White [16] that the .ow in a curved pipe can remain laminar for substantial higher values of the Reynolds number compared to a straight pipe. The stabilization effect of curvature on the .ow was nicely visualized by Sreenivasan and Strykowski [55], who were the .rst ones to try to explain why the .ow in the coil is more stable compared to the downstream straight pipe sec-tion. The questions put forward in 1983 by the authors (¡°[¡­] what is the maximum Reynolds number for which laminarization is possible? [¡­] how precisely does the laminarized .ow return to a turbulent state when downstream of the coil the .ow is allowed to develop in another long straight pipe section?¡±) describe the puzzlement that prevailed in the community at that time about the transition from laminar to turbulent .ow in the presence of curva-ture. Speculations about the role of the secondary .ow or existing traveling waves [59,61] on the onset of instability were put for-ward through the years in the absence of a detailed instability analysis study. Pressure drop measurements, which were com-monly performed in order to relate the onset of turbulence to a deviation of the f¨CRe curve from the linear behavior of laminar .ow [52,65,76] gave little insight into the mechanisms of the tran-sition process as well. 
Instability analysis of the .ow in a torus (coil with zero pitch) became available only recently with the DNS study by Canton et al. [19] con.rming previous suggestions [62,65] that the transi-tion process in a curved pipe has several stages and is not as abrupt as in the straight pipe. It is con.rmed that for curvature ratios below 0.02¨C0.04, subcritical transition occurs and that the .rst instability in a torus is a Hopf bifurcation. Canton et al. could .nally give an explanation for the nonmonotonic behavior of the lower-critical Reynolds number that puzzled Sreenivasan and Strykowski 
[55] in the early 1980s, as a switch from subcritical to supercritical transition. It would be interesting, however, to extend such studies for the case of a coil and a wide range of coil pitches as well as investigate the effect of torsion on transition. 
Experiments designed to explore the velocity distribution and turbulence statistics in curved pipe .ows are maybe fewer than one would expect and were mainly performed with pressure probes, hot-wires, and LDV in mainly 90 deg and 180 deg bends. The results in these studies are seldom directly comparable since they are done at different parameter values (Re and c) but there are some common conclusions that can be drawn. All the studies show that the .ow is affected up to one diameter upstream the bend and also that the relaxation distance to fully developed pipe .ow downstream the bend is much longer. Furthermore, at the inlet of the bend the .ow accelerates on the inner (convex) side of the bend and decelerates on the outer side up to about 30 deg from the entrance, where after there is a shift of the high velocity region to the outer (concave) side of the bend. All the studies also show the existence of a pair of Dean cells. Various authors have tried to predict turbulent .ow in curved pipes by using RANS models (either eddy-viscosity or RSMs), however, those failed to capture the .ow correctly. 
The behavior of the Dean vortices under turbulent .ow condi-tions has met a revived interest during the last two decades, since the studies were initiated in 1968 by Tunstall and Harvey [129]. What is known today as the swirl-switching phenomenon or mechanism describes the temporal movement of the Dean vortices for turbulent .ow in a curved pipe; each of the Dean vortices takes over the .ow .eld changing direction of rotation from clockwise to anticlockwise alternatively. As remarked in 2005 by R€
utten et al. [113]: ¡°[¡­] this phenomenon could cause fatigue in piping systems due to low-frequency oscillations, [but] not much atten-tion seems to have been focused on it in the literature. It is for example, completely ignored in the review articles on curved pipes.¡± During the last 15 years, the swirl switching has attracted investigations both from numericists and experimentalists within the .uid dynamics¡¯ community. The most popular tools used so far are either LES (but also recently the .rst DNS to study this phenomenon were performed [121,182]) and PIV, which can pro-vide the three velocity components as well as a quantitative pic-ture of the secondary motion. At the same time, proper orthogonal decomposition started to become a popular tool in the hands of .uid dynamicists dealing with the swirl switching [110,124,126,132]. The main contributions in the literature the past two decades to unravel the mystery of this phenomenon are the following in chronological order: R€
utten et al. [113] found by means of LES that the oscillatory motion of the Dean cells is not caused by separation at the inner bend corner as initially believed by Tunstall and Harvey [129]. They also remarked that the switch-ing motion is not abrupt from one state to another (i.e., a dominant cell rotates clockwise and at another moment rotates in the oppo-site direction) but it is a smooth transition between different states. Kalpakli et al. [110] reconstructed (by means of POD analysis) the three states between which the two Dean vortices are switch-ing; symmetric, as well as dominant clockwise and counterclock-wise Dean cell. Sakakibara and Machida [128] made, through conditional averaging, the important observation that elongated structures existing upstream from the bend are correlated to the movement of the Dean cells downstream the bend. Those .ndings were later supported also by Hellstr€
om et al. [126] by means of POD analysis, and they additionally suggested that the .ow .eld in a curved pipe is governed by two states, the swirl switching (a sin-gle cell spanning the whole cross-section and appearing as mode 1) and the Dean motion (tilted Dean vortices with one of them being suppressed, those were seen as modes 2 and 3). The same modes were seen later also in the POD analysis by Kalpakli et al. [125], however, the physical meaning of them is still unclear. 
All the studies (from the initial one by Tunstall and Harvey to the more recent ones) agree on one thing that the swirl switching is happening at low frequencies that correspond to Strouhal num-bers of the order 10 2, however, peaks at higher St are also pres-ent in the spectra but are usually associated to other phenomena such as vortex shedding [110,113,121,130]. Despite the agreement on the frequency range, it is still unclear how this phenomenon is triggered; the most recent study and the .rst DNS data on the switching [121] showed that the cause of the oscillatory motion of the Dean cells can be neither separation nor upstream structures. The DNS were performed for a torus with periodic boundary con-ditions; therefore, there was no upstream straight section con-nected to the curved part where separation could occur, therefore the assumptions by previous investigators become obsolete. Fur-thermore, the POD modes from the DNS data did not depict the single swirl mode found in Ref. [126], but rather the POD modes from this study were very similar to the ones shown in Refs. [110] and [124]. Once more, it is still a blur what those modes can tell us or how important separation or upstream large-scale structures are for the triggering of the swirl switching, however, as indicated in Kalpakli et al. [125], those factors could still enhance or delay the oscillatory motion of the vortices. It appears from the above that in order to fully explore the mechanism and determine a phys-ical understanding of the swirl switching one should probably obtain the full three-dimensional .ow .eld upstream, along, and downstream a spatially developing bend, e.g., 90 deg, where both separation and upstream structures can be triggered and con-trolled. This is probably dif.cult to obtain experimentally, but a DNS of a 90 deg bend with upstream and downstream straight pipe sections could give some answers. Such a DNS is feasible and has been started recently [183,184]. The latter reference includes pre-liminary three-dimensional POD analysis of swirl switching. 

On the other hand, how the Dean vortices behave when an addi-tional motion (swirl and pulsation) is superimposed on turbulence is an unexplored area with a limited number of studies being available [17, 94,110,141,180]. In Ref. [180], PIV experiments of highly pulsating turbulent .ow in sharp bends revealed a vortex pattern varying from a single vortex at the end of deceleration to a ¡°steady-.ow-like¡± pattern at the peak of acceleration. Pulsating turbulent pipe .ow is generally neglected but when it comes to curved pipes, the available information is even more scarce. There is clearly an interest to study this complex .ow .eld for wider ranges of Reynolds and Womersley numbers as well as curvature ratios both in terms of statistics and secondary .ow behavior. 
From all the above, it is made clear that turbulent .ow in curved pipes is far from being fully understood, despite the great efforts done in the last decades. It is hence of both engineering and fundamental points of interest that further investigations are initiated. 
Acknowledgment 
The starting point of this review has been the doctoral thesis by the .rst author [185], which was supported by CCGEx and the Linne Flow Center. The .nal version was completed by the .rst author together with the co-authors as part of the postdoctoral period within the Fordonsstrategisk Forskning och Innovation (FFI) project entitled ¡°Experimental Investigation of the In-Cylinder Flow During the Intake Stroke.¡± This project is a cooper-ation between the Swedish Energy Agency, Scania CV AB, Volvo GTT, and KTH. The authors are thankful to Professor Philipp Schlatter, Dr. Azad Noorani, and MSc. Jacopo Canton from KTH Mechanics for useful discussions on curved pipe .ows. 
References 
[1] Bradshaw, P., 1973, ¡°Effects of Streamline Curvature on Turbulent Flow,¡± AGARDograph 
No. 
169. 
[2] Goldstein, S., 1938, Modern Developments in Fluid Mechanics, Oxford Uni-versity Press, Oxford, UK. 
[3] Schlichting, H., 1979, Boundary-Layer Theory, McGraw¨CHill, New York. 
[4] Ward-Smith, A. J., 1980, Internal Fluid Flow. The Fluid Dynamics of Flow in Pipes and Ducts, Clarendon Press, Oxford, UK. 
[5] Pedley, T. J., 1980, The 
Fluid 
Mechanics 
of 
Large 
Blood 
Vessels, Cambridge University Press, Cambridge, UK. 
[6] Berger, S. A., Talbot, L., and Yao, L. S., 1983, ¡°Flow in Curved Pipes,¡± Annu. 
Rev. 
Fluid 
Mech., 15(1), pp. 461¨C512. 
[7] Ito, H., 1987, ¡°Flow in Curved Pipes,¡± Bull. 
JSME, 30(262), pp. 543¨C552. 
[8] Spedding, P. L., and Benard, E., 2004, ¡°Fluid Flow Through 90 Degree Bends,¡± Dev. 
Chem. 
Eng. 
Miner. 
Process., 12(1¨C2), pp. 107¨C128. 
[9] Naphon, P., and Wongwises, S., 2006, ¡°A Review of Flow and Heat Transfer Characteristics in Curved Tubes,¡± Renewable 
Sustainable 
Energy 
Rev., 10(5), pp. 463¨C490. 
[10] Vashisth, S., Kumar, V., and Nigam, K. D. P., 2008, ¡°A Review on the Poten-tial Applications of Curved Geometries in Process Industry,¡± Ind. 
Eng. 
Chem. 
Res., 47(10), pp. 3291¨C3337. 
[11] Kundu, K. P., Cohen, M. I., and Dowling, R. D., 2012, Fluid Mechanics, 5th ed., Elsevier, Oxford, UK. 
[12] Dean, W. R., 1927, ¡°Note on the Motion of Fluid in a Curved Pipe,¡± Philos. 
Mag., 4(20), pp. 208¨C223. 
[13] Dean, W. R., 1928, ¡°The Stream-Line Motion of Fluid in a Curved Pipe,¡± Philos. 
Mag., 5(30), pp. 671¨C695. 
[14] McConalogue, D. J., and Srivastava, R. S., 1968, ¡°Motion of a Fluid in a Curved Tube,¡± Proc. 
R. 
Soc., 307(1488), pp. 37¨C53. 
[15] Taylor, G. I., 1929, ¡°The Criterion for Turbulence in Curved Pipes,¡± Proc. 
R. 
Soc., 124(794), pp. 243¨C249. 
[16] White, C. M., 1929, ¡°Streamline Flow Through Curved Pipes,¡± Proc. 
R. 
Soc., 123(792), pp. 645¨C663. 
[17] Anwer, M., and So, R. M. C., 1993, ¡°Swirling Turbulent Flow Through a Curved Pipe¡ªPart 1: Effect of Swirl and Bend Curvature,¡± Exp. 
Fluids, 14(1), pp. 85¨C96. 
[18] R€oder, W., 2001, ¡°Large-Eddy Simulations of
utten, F., Meinke, M., and Schr€90 deg Pipe Bend Flows,¡± J. 
Turbul., 2, p. N3. 
[19] Canton, J., Schlatter, P., and €u, R., 2016, ¡°Modal Instability of the Flow in
Orl€a Toroidal Pipe,¡± J. 
Fluid 
Mech., 792, pp. 894¨C909. 
[20] Cieslicki, K., and Piechna, A., 2012, ¡°Can the Dean Number Alone Character-ize Flow Similarity in Differently Bent Tubes?,¡± ASME 
J. 
Fluids 
Eng., 134(5), p. 051205. 
€R€
[21] Hagen, G., 1839, ¡°Uber die Bewegung des Wassers in engen zylindrischen 
ohr,¡± Poggendorff¡¯s Ann. Phys., 46, pp. 423¨C442. 
[22] Poiseuille, J. L. M., 1842, ¡°Recherches experimentales sur le mouvement des liquides dans les tubes de tres-petits diameters,¡± C. R. Chim., 11, pp. 961¨C967. 
[23] Darcy, H., 1857, Recherches experimentales relatives au mouvement de l¡¯eau dans les tuyaux, Mallet-Bachelier, Paris. 
[24] Darcy, H., 1856, Les fontaines publiques de la ville de Dijon, Dalmont, Paris. 
[25] Reynolds, O., 1883, ¡°An Experimental Investigation of the Circumstances Which Determine Whether the Motion of Water Shall Be Direct or Sinuous, and of the Law of Resistance in Parallel Channels,¡± Proc. 
R. 
Soc. 
London, 35(224¨C226), pp. 84¨C99. 
[26] Boussinesq, M. J., 1868, ¡°Memoire sur l¡¯in.uence des frottements dans les mouvements reguliers des .uids,¡± J. Math. Pures Appl., 13(2), pp. 377¨C424. 
[27] Thomson, J., 1876, ¡°On the Origin of Windings of Rivers in Alluvial Plains With Remarks on the Flow of Water Round Bends in Pipes,¡± Proc. 
R. 
Soc., 
25, pp. 5¨C8. 
[28] Thomson, J., 1877, ¡°Experimental Demonstration in Respect to the Origin of Windings of Rivers in Alluvial Plains, and to the Mode of Flow of Water Round Bends of Pipes,¡± Proc. 
R. 
Soc., 26(179¨C184), pp. 356¨C357. 
[29] Thomson, J., 1912, Collected Papers in Physics and Engineering, Cambridge University Press, Cambridge, UK. 
[30] Weisbach, J., 1855, Experimentelle Hydraulik, Engelhardt, Freiberg, Germany. 
[31] Williams, G. S., Hubbell, C. W., and Fenkell, G. H., 1902, ¡°Experiments at Detroit, MI, on the Effect of Curvature Upon the Flow of Water in Pipes,¡± Am. Soc. Civ. Eng., 47, pp. 1¨C196. 
[32] Eustice, J., 1910, ¡°Flow of Water in Curved Pipes,¡± Proc. 
R. 
Soc., 84(568), pp. 107¨C118. 
[33] Eustice, J., 1911, ¡°Experiments on Stream-Line Motion in Curved Pipes,¡± Proc. 
R. 
Soc., 85(576), pp. 119¨C131. 
[34] Dean, W. R., 1928, ¡°Fluid Motion in a Curved Channel,¡± Proc. 
R. 
Soc., 
121(787), pp. 402¨C420. 
[35] Nature, 1952, ¡°Mathematics at University College, London: Prof. W. R. Dean,¡± Nature, 169(4306), p. 779. 
[36] Faculty of Mathematics, University of Cambridge, Centre for Mathematical Sciences (personal communication), 2016. 
[37] Dean, W. R., and Hurst, J. M., 1959, ¡°Note on the Motion of Fluid in a Curved Pipe,¡± Mathematika, 6(1), pp. 77¨C85. 
[38] Binnie, A. M., 1978, ¡°Some Notes on the Study of Fluid Mechanics in Cam-bridge, England,¡± Annu. 
Rev. 
Fluid 
Mech., 10(1), pp. 1¨C11. 
[39] Finlay, W. H., Keller, J. B., and Ferziger, J. H., 1988, ¡°Instability and Transi-tion in Curved Channel Flow,¡± J. 
Fluid 
Mech., 194, pp. 417¨C456. 
[40] Matsson, O. J. E., and Alfredsson, P. H., 1992, ¡°Experiments on Instabilities in Curved Channel Flow,¡± Phys. 
Fluids 
A, 4(8), p. 1666. 
[41] Cohen, M., 2010, ¡°My Dear Eve...: The Remaining Letters From Eve¡¯s Ruth-erford File,¡± Fontanus, 12, pp. 1¨C10. 
[42] Batcheloris, M., 2006, ¡°Rutherford and the Trinity Golf Circus,¡± Phys. 
World, 19(1), p. 48. 
[43] Einstein, H. A., 1926, ¡°Die Ursache der M€aanderbildung der Flu.l€aufe und des sogenannten Baerschen Gesetzes,¡± Naturwissenschaften, 14(11), pp. 223¨C224. 
[44] Adler, M., 1934, ¡°Str€in ummten Rohren,¡± Z. 
Angew. 
Math.
omung gekr€Mech., 14(5), pp. 257¨C275. 
[45] Wattendorf, F. L., 1935, ¡°A Study of the Effect of Curvature on Fully Devel-oped Turbulent Flow,¡± Proc. 
R. 
Soc., 148(865), pp. 565¨C598. 
[46] Greenspan, D., 1973, ¡°Secondary Flow in a Curved Tube,¡± J. 
Fluid 
Mech., 
57(1), pp. 167¨C176. 
[47] Collins, W. M., and Dennis, S. C. R., 1975, ¡°The Steady Motion of a Viscous Fluid in a Curved Tube,¡± Q. 
J. 
Mech. 
Appl. 
Math., 28(2), pp. 133¨C156. 
[48] Van Dyke, M., 1978, ¡°Extended Stokes Series: Laminar Flow Through a Loosely Coiled Pipe,¡± J. 
Fluid 
Mech., 86(1), pp. 129¨C145. 
[49] Smith, F. T., 1976, ¡°Fluid Flow Into a Curved Pipe,¡± Proc. 
R. 
Soc., 351(1664), pp. 71¨C87. 
[50] Dennis, S. C. R., and Ng, M., 1982, ¡°Dual Solutions for Steady Laminar Flow Through a Curved Tube,¡± Q. 
J. 
Mech. 
Appl. 
Math., 35(3), pp. 305¨C324. 
[51] Dennis, S. C. R., and Riley, N., 1991, ¡°On the Fully Developed Flow in a Curved Pipe at Large Dean Number,¡± Proc. 
R. 
Soc., 434(1891), pp. 473¨C478. 
[52] Ito, H., 1959, ¡°Friction Factors for Turbulent Flow in Curved Pipes,¡± ASME J. Basic Eng., 81(2), pp. 123¨C134. 
[53] Keulegan, G. H., and Beij, K. H., 1937, ¡°Pressure Losses for Fluid Flow in Curved Pipes,¡± J. 
Res. 
Natl. 
Bur. 
Stand., 18(1), pp. 89¨C114. 

[54] Srinivasan, P. S., Nandapurkar, S. S., and Holland, F. A., 1970, ¡°Friction Fac-tors for Coils,¡± Trans. Inst. Chem. Eng., 48(4¨C6), pp. 156¨C161. 
[55] Sreenivasan, K. R., and Strykowski, P. J., 1983, ¡°Stabilization Effects in Flow Through Helically Coiled Pipes,¡± Exp. 
Fluids, 1(1), pp. 31¨C36. 
[56] Viswanath, P. R., and Narasimha, R., 1978, ¡°Visualisation of Relaminarizing Flows,¡± J. Indian Sci., 60, pp. 159¨C165. 
[57] Narasimha, R., and Sreenivasan, K. R., 1979, ¡°Relaminarization of Fluid Flows,¡± Adv. 
Appl. 
Mech., 19, pp. 221¨C308. 
[58] Kurokawa, M., Cheng, K. C., and Shi, L., 1998, ¡°Flow Visualization of Rela-minarization Phenomena in Curved Pipes and Related Measurements,¡± J. 
Vis-
ualization, 1(1), pp. 9¨C28. 
[59] Webster, D. R., and Humphrey, J. A. C., 1993, ¡°Experimental Observations of Flow Instability in a Helical Coil (Data Bank Contribution),¡± ASME 
J. 
Fluids 
Eng., 115(3), pp. 436¨C443. 
[60] Webster, D. R., and Humphrey, J. A. C., 1997, ¡°Traveling Wave Instability in Helical Coil Flow,¡± Phys. 
Fluids, 9(2), p. 407. 
[61] Di Piazza, I., and Ciofalo, M., 2011, ¡°Transition to Turbulence in Toroidal Pipes,¡± J. 
Fluid 
Mech., 687, pp. 72¨C117. 
[62] K€uhnen, J., Braunshier, P., Schwegel, M., Kuhlmann, H. C., and Hof, B., 2015, ¡°Subcritical Versus Supercritical Transition to Turbulence in Curved Pipes,¡± J. 
Fluid 
Mech., 770, p. R3. 
[63] K€uhnen, J., 2012, ¡°Experimental Investigation of Transition to Turbulence in a Torus,¡± Ph.D. 
thesis, Technical University Wien, Wien, Austria. 
[64] K€uhnen, J., Holzner, M., Hof, B., and Kuhlmann, H., 2014, ¡°Experimental Investigation of Transitional Flow in a Toroidal Pipe,¡± J. 
Fluid 
Mech., 738, pp. 463¨C491. 
[65] Cioncolini, A., and Santini, L., 2006, ¡°An Experimental Investigation Regard-ing the Laminar to Turbulent Flow Transition in Helically Coiled Pipes,¡± Exp. 
Therm. 
Fluid 
Sci., 30(4), pp. 367¨C380. 
[66] Kubair, V., and Varrier, C. B. S., 1961, ¡°Pressure Drop for Liquid Flow in Helical Coils,¡± Trans. Indian Inst. Chem. Eng., 14, p. 93. 
[67] Noorani, A., and Schlatter, P., 2015, ¡°Evidence of Sublaminar Drag Naturally Occurring in a Curved Pipe,¡± Phys. 
Fluids, 27(3), p. 035105. 
[68] Alexander, C. W. L., 1905, ¡°The Resistance Offered to the Flow of Water in Pipes by Bends and Elbows,¡± Minutes Proc. Inst. Civ, Eng., 159, pp. 341¨C364. 
[69] Grindley, J. H., and Gibson, A. H., 1908, ¡°On the Frictional Resistances to the Flow of Air Through a Pipe,¡± Proc. 
R. 
Soc., 80(536), pp. 114¨C139. 
€
[70] Nippert, H., 1929, ¡°Uber den str€ummten kan€omungsverlust in gekr€alen,¡± Ph.D. thesis, Technischen Hochschule der Freien Stadt Danzig, Danzig, Germany. 
[71] Richter, H., 1930, ¡°Der Druckabfall in gekr€ummten glatten Rohrleitungen,¡± VDI Forschungsh., 338. 
[72] Jeschke, H., 1925, ¡°W€arme€ubergang und Druckverlust in Rohrschlangen,¡± VDI Z., 506, pp. 24¨C28. 
[73] Schmidt, E. F., 1967, ¡°W€arme€ubergang und Druckverlust in Rohrschlangen,¡± Chem. 
Ing. 
Tech., 39(13), pp. 781¨C789. 
[74] Wasielewski, R., 1932, ¡°Verluste in glatten Rohrkr€ulmmern mit kreisrundem Querschnitt bei weniger als 908 Ablenkung,¡± Mitt. Hydraul. Inst. Techn. Hochsch. M€unchen, 5, pp. 53¨C67. 
[75] Barua, S. N., 1963, ¡°On Secondary Flow in Stationary Curved Pipes,¡± Q. 
J. 
Mech. 
Appl. 
Math., 16(1), pp. 61¨C77. 
[76] Ito, H., 1960, ¡°Pressure Losses in Smooth Pipe Bends,¡± ASME 
J. 
Basic 
Eng., 82(1), pp. 131¨C140. 
[77] Beck, C., 1960, ¡°Discussion: Pressure Losses in Smooth Pipe Bends,¡± ASME 

J. Basic 
Eng., 82(1), pp. 140¨C142. 
[78] Pigott, R. J. S., 1957, ¡°Losses in Pipe and Fittings,¡± Trans. ASME, 79, pp. 1767¨C1783. 
[79] Ward-Smith, A. J., 1971, Pressure Losses in Ducted Flows, Butterworth, London. 
[80] Ali, S., 2001, ¡°Pressure Drop Correlations for Flow Through Regular Helical Coil Tubes,¡± Fluid 
Dyn. 
Res., 28(4), pp. 295¨C310. 
[81] Noorani, A., El Khoury, G. K., and Schlatter, P., 2013, ¡°Evolution of Turbu-lence Characteristics From Straight to Curved Pipes,¡± Int. 
J. 
Heat 
Fluid 
Flow, 41(ETMM9), pp. 16¨C26. 
[82] Weske, J. R., 1948, ¡°Experimental Investigation of Velocity Distributions Downstream of Single Duct Bends,¡± Report 
No. 
NACA-TN-1471. 
[83] Detra, R. W., 1953, ¡°The Secondary Flow in Curved Pipes,¡± Ph.D. 
thesis, The Swiss Federal Institute of Technology, Zurich, Switzerland. 
[84] Sudo, K., Sumida, M., and Hibara, H., 2000, ¡°Experimental Investigation on Turbulent Flow Through a Circular-Sectioned 180 Bend,¡± Exp. 
Fluids, 28(1), pp. 51¨C57. 
[85] Enayet, M. M., Gibson, M. M., Taylor, A., and Yianneskis, M., 1982, ¡°Laser-Doppler Measurements of Laminar and Turbulent Flow in a Pipe Bend,¡± Int. 

J. Heat 
Fluid 
Flow, 3(4), pp. 213¨C219. 
[86] Azzola, J., Humprey, J. A. C., Iacovides, H., and Launder, B. E., 1986, ¡°Developing Turbulent Flow in a U-Bend of Circular Cross-Section: Measure-ment and Computation,¡± ASME 
J. 
Fluids 
Eng., 108(2), pp. 214¨C221. 
[87] Sudo, K., Sumida, M., and Hibara, H., 1998, ¡°Experimental Investigation on Turbulent Flow in a Circular-Sectioned 90-Degree Bend,¡± Exp. 
Fluids, 25(1), pp. 42¨C49. 
[88] Anwer, M., So, R. M. C., and Lai, Y. G., 1989, ¡°Perturbation by and Recovery From Bend Curvature of a Fully Developed Turbulent Pipe Flow,¡± Phys. 
Flu-
ids 
A, 1(8), p. 1387. 
[89] Hellstr€om, L. H. O., Zlatinov, M., and Smits, A. J., 2011, ¡°Turbulent Pipe Flow Through a 90 deg Bend,¡± 7th International Symposium on Turbulence and Shear Flow Phenomena, Ottawa, Canada, July 28¨C31. 
[90] Rowe, M., 1970, ¡°Measurements and Computations of Flow in Pipe Bends,¡± J. 
Fluid 
Mech., 43(4), pp. 771¨C783. 
[91] Lee, G. H., Choi, Y. D., and Han, S. H., 2007, ¡°Measurement of Developing Turbulent Flow in a U-Bend of Circular Cross-Section,¡± J. 
Mech. 
Sci. 
Tech-
nol., 21(2), pp. 348¨C359. 
[92] Anwer, M., and So, R. M. C., 1990, ¡°Frequency of Sublayer Bursting in a Curved Bend,¡± J. 
Fluid 
Mech., 210, pp. 415¨C435. 
[93] Alfredsson, P. H., and Johansson, A. V., 1984, ¡°On the Detection of Turbulence-Generating Events,¡± J. 
Fluid 
Mech., 139, p. 325. 
[94] Pruvost, J., Legrand, J., and Legentilhomme, P., 2004, ¡°Numerical Investiga-tion of Bend and Torus Flows¡ªPart I: Effect of Swirl Motion on Flow Struc-ture in U-Bend,¡± Chem. 
Eng. 
Sci., 59(16), pp. 3345¨C3357. 
[95] Hellstr€
om, F., and Fuchs, L., 2007, ¡°Numerical Computations of Steady and Unsteady Flow in Bended Pipes,¡± AIAA 
Paper No. 2007-4350. 
[96] Wilcox, D. C., 1994, ¡°Turbulence Modeling for CFD,¡± DCW Industries, La Ca~
nada Flintridge, CA. 
[97] Durbin, P. A., and Pettersson-Reif, B. A., 2011, Statistical 
Theory 
and 
Model-
ing 
for 
Turbulent 
Flows, 2nd ed., Wiley, Chichester, UK. 
[98] Wallin, S., and Johansson, A. V., 2002, ¡°Modelling Streamline Curvature Effects in Explicit Algebraic Reynolds Stress Turbulence Models,¡± Int. 
J. 
Heat 
Fluid 
Flow, 23(5), pp. 721¨C730. 
[99] Noorani, A., 2015, ¡°Particle-Laden Turbulent Wall-Bounded Flows in Moder-ately Complex Geometries,¡± Ph.D. 
thesis, KTH Mechanics, Stockholm, Sweden. 
[100] R€ohrig, R., Jakirlic, S., and Tropea, C., 2015, ¡°Comparative Computational Study of Turbulent Flow in a 90 deg Pipe Elbow,¡± Int. 
J. 
Heat 
Fluid 
Flow, 
55(ETMM10), pp. 102¨C111. 
[101] Patankar, S. V., Pratap, V. S., and Spalding, D. B., 1975, ¡°Prediction of Turbu-lent Flow in Curved Pipes,¡± J. 
Fluid 
Mech., 67(3), pp. 583¨C595. 
[102] Mori, Y., and Nakayama, W., 1967, ¡°Study of Forced Convective Heat Trans-fer in Curved Pipes (2nd Report, Turbulent Region),¡± Int. 
J. 
Heat 
Mass 
Trans-
fer, 10(1), pp. 37¨C59. 
[103] Al-Rafai, W. N., Tridimas, Y. D., and Woolley, N. H., 1990, ¡°A Study of Turbu-lent Flows in Pipe Bends,¡± Proc. 
Inst. 
Mech. 
Eng., 
Part 
C, 204(6), pp. 399¨C408. 
[104] Hilgenstock, A., and Ernst, R., 1996, ¡°Analysis of Installation Effects by Means of Computational Fluid Dynamics¡ªCFD Versus Experiments?,¡± Flow 
Meas. 
Instrum., 7(3¨C4), pp. 161¨C171. 
[105] Sugiyama, H., and Hitomi, D., 2005, ¡°Numerical Analysis of Developing Tur-bulent Flow in a 180 deg Bend Tube by an Algebraic Reynolds Stress Model,¡± Int. 
J. 
Numer. 
Methods 
Fluids, 47(12), pp. 1431¨C1449. 
[106] Pellegrini, M., Endo, H., and Ninokata, H., 2011, ¡°Numerical Investigation of Bent Pipe Flows at Transitional Reynolds Number,¡± Prog. 
Nucl. 
Energy, 53(7), pp. 916¨C920. 
[107] Di Piazza, I., and Ciofalo, M., 2010, ¡°Numerical Prediction of Turbulent Flow and Heat Transfer in Helically Coiled Pipes,¡± Int. 
J. 
Therm. 
Sci., 49(4), pp. 653¨C663. 
[108] Bradshaw, P., 1987, ¡°Turbulent Secondary Flows,¡± Annu. 
Rev. 
Fluid 
Mech., 19(1), pp. 53¨C74. 
[109] Lai, Y., So, R. M. C., and Zhang, H. S., 1991, ¡°Turbulence-Driven Secondary Flows in a Curved Pipe,¡± Theor. 
Comput. 
Fluid 
Dyn., 3(3), pp. 163¨C180. 
[110] Kalpakli, A., and €u, R., 2013, ¡°Turbulent Pipe Flow Downstream a 90 degOrl€Pipe Bend With and Without Superimposed Swirl,¡± Int. 
J. 
Heat 
Fluid 
Flow, 41(ETMM9), pp. 103¨C111. 
[111] Boersma, B. J., and Nieuwstadt, F. T. M., 1996, ¡°Direct Numerical Simulation of the Flow in a Bend,¡± 3rd International Symposium on Engineering Turbulence Modelling and Measurement, Heraklion, Greece, May 27¨C29. 
[112] Boersma, B. J., and Nieuwstadt, F. T. M., 1996, ¡°Large-Eddy Simulation of Turbulent Flow in a Curved Pipe,¡± ASME 
J. 
Fluids 
Eng., 118(2), p. 248. 
[113] R€oder, W., and Meinke, M., 2005, ¡°Large-Eddy Simulation ofutten, F., Schr€Low Frequency Oscillations of the Dean Vortices in Turbulent Pipe Bend Flows,¡± Phys. 
Fluids, 17(3), p. 035107. 
[114] Tan, L., Zhu, B., Wang, Y., Cao, S., and Liang, K., 2014, ¡°Turbulent Flow Simulation Using Large Eddy Simulation Combined With Characteristic-Based Split Scheme,¡± Comput. 
Fluids, 94, pp. 161¨C172. 
[115] Wang, Y., Dong, Q., and Wang, P., 2015, ¡°Numerical Investigation on Fluid Flow in a 90-Degree Curved Pipe With Large Curvature Ratio,¡± Math. 
Probl. 
Eng., 2015, p. 548262. 
[116] Boersma, B. J., and Nieuwstadt, F. T. M., 1997, ¡°Non-Unique Solutions in Turbulent Curved Pipe Flow,¡± Direct 
and 
Large-Eddy 
Simulation 
II, Springer, Dordrecht, The Netherlands, pp. 257¨C266. 
[117] H€uttl, T. J., and Friedrich, R., 2000, ¡°In.uence of Curvature and Torsion on Turbulent Flow in Helically Coiled Pipes,¡± Int. 
J. 
Heat 
Fluid 
Flow, 21(3), pp. 345¨C353. 
[118] Germano, M., 1982, ¡°On the Effect of Torsion on a Helical Pipe Flow,¡± J. 
Fluid 
Mech., 125, pp. 1¨C8. 
[119] Germano, M., 1989, ¡°The Dean Equations Extended to a Helical Pipe Flow,¡± 
J. Fluid 
Mech., 203, pp. 289¨C305. 
[120] Yamamoto, K., Akita, T., Ikeuchi, H., and Kita, Y., 1995, ¡°Experimental Study of the Flow in a Helical Circular Tube,¡± Fluid 
Dyn. 
Res., 16(4), pp. 237¨C249. 
[121] Noorani, A., and Schlatter, P., 2015, ¡°Swirl-Switching Phenomenon in Turbu-lent Flow Through Toroidal Pipes,¡± 9th International Symposium on Turbu-lence and Shear Flow Phenomena, TSFP9, Melbourne, Australia, June 30¨CJuly 3. 
[122] H€uttl, T. J., and Friedrich, R., 2001, ¡°Direct Numerical Simulation of Turbu-lent Flows in Curved and Helically Coiled Pipes,¡± Comput. 
Fluids, 30(5), pp. 591¨C605. 
[123] Di Liberto, M., Di Piazza, I., and Ciofalo, M., 2013, ¡°Turbulence Structure and Budgets in Curved Pipes,¡± Comput. 
Fluids, 88, pp. 452¨C472. 

[124] Carlsson, C., Alenius, E., and Fuchs, L., 2015, ¡°Swirl Switching in Turbulent Flow Through 90 deg Pipe Bends,¡± Phys. 
Fluids, 27(8), p. 085112. 
[125] Kalpakli Vester, A., €u, R., and Alfredsson, P. H., 2015, ¡°POD Analysis of
Orl€the Turbulent Flow Downstream a Mild and Sharp Bend,¡± Exp. 
Fluids, 56(3), pp. 57¨C15. 
[126] Hellstr€
om, L. H. O., Zlatinov, M., Cao, G., and Smits, A. J., 2013, ¡°Turbulent Pipe Flow Downstream of a 90 deg Bend,¡± J. 
Fluid 
Mech., 735, p. R7. 
[127] Ono, A., Kimura, N., Kamide, H., and Tobita, A., 2010, ¡°In.uence of Elbow Curvature on Flow Structure at Elbow Outlet Under High Reynolds Number Condition,¡± Nucl. 
Eng. 
Des., 241(NURETH-13), pp. 4409¨C4419. 
[128] Sakakibara, J., and Machida, N., 2012, ¡°Measurement of Turbulent Flow Upstream and Downstream of a Circular Pipe Bend,¡± Phys. 
Fluids, 24(4), p. 041702. 
[129] Tunstall, M. J., and Harvey, J. K., 1968, ¡°On the Effect of a Sharp Bend in a Fully Developed Turbulent Pipe-Flow,¡± J. 
Fluid 
Mech., 34(3), pp. 595¨C608. 
[130] Br€
ucker, C., 1998, ¡°A Time-Recording DPIV-Study of the Swirl Switching Effect in a 90 Bend Flow,¡± 8th 
International 
Symposium 
on 
Flow 
Visualiza-
tion, Sorrento, Italy, Sept. 1¨C4, p. 171. 
[131] Kalpakli, A., €u, R., and Alfredsson, P. H., 2012, ¡°Dean Vortices in Turbu-
Orl€lent Flows: Rocking or Rolling?,¡± J. 
Visualization, 15(1), pp. 37¨C38. 
[132] Sakakibara, J., Sonobe, R., Goto, H., Tezuka, H., Tada, H., and Tezuka, K., 2010, ¡°Stereo-PIV Study of Turbulent Flow Downstream of a Bend in a Round Pipe,¡± 14th International Symposium on Flow Visualization, EXCO, Daegu, South Korea, June 21¨C24. 
[133] Berkooz, G., Holmes, P., and Lumley, J. L., 1993, ¡°The Proper Orthogonal Decomposition in the Analysis of Turbulent Flows,¡± Annu. 
Rev. 
Fluid 
Mech., 
25(1), pp. 539¨C575. 
[134] Raiola, M., Discetti, S., and Ianiro, A., 2015, ¡°On PIV Random Error Minimization With Optimal POD-Based Low-Order Reconstruction,¡± Exp. 
Fluids, 56(4), p. 75. 
[135] Guala, M., Hommema, S. E., and Adrian, R. J., 2006, ¡°Large-Scale and Very-Large-Scale Motions in Turbulent Pipe Flow,¡± J. 
Fluid 
Mech., 554, pp. 521¨C542. 
[136] Shiraishi, T., Watakabe, H., Sago, H., and Yamano, H., 2009. ¡°Pressure Fluc-tuation Characteristics of the Short-Radius Elbow Pipe for FBR in the Postcrit-ical Reynolds Regime,¡± J. 
Fluid 
Sci. 
Technol., 4(2), pp. 430¨C441. 
[137] Takamura, H., Konno, H., Hashizume, H., Aizawa, K., and Yamano, H., 2011. ¡°PIV Measurements of a Complex Turbulent Flow in a Short Elbow Piping Under a High Reynolds Number Condition,¡± 9th International Symposium on PIV, PIV¡¯11, Tsukuba, Japan, July 21¨C23. 
[138] Yamano, H., Tanaka, M., Murakami, T., Iwamoto, Y., Yuki, K., Sago, H., and Hayakawa, S., 2011. ¡°Unsteady Elbow Pipe Flow to Develop a Flow-Induced Vibration Evaluation Methodology for Japan Sodium-Cooled Fast Reactor,¡± J. 
Nucl. 
Sci. 
Technol., 
48(4), pp. 677¨C687. 
[139] Fj€allman, J., Mihaescu, M., and Fuchs, L., 2013, ¡°Analysis of Secondary Flow Induced by a 90 deg Bend in a Pipe Using Mode Decomposition Techniques,¡± 4th 
International 
Conference 
on 
Jets, 
Wakes, 
and 
Separated 
Flows, Nagoya, Japan, Sept. 17¨C21. 
[140] Yuki, K., Hasegawa, S., Sato, T., Hashizume, H., Aizawa, K., and Yamano, H., 2011, ¡°Matched Refractive-Index PIV Visualization of Complex Flow Structure in a Three-Dimensionally Connected Dual Elbow,¡± Nucl. 
Eng. 
Des., 241(11), pp. 4544¨C4550. 
[141] Kalpakli Vester, A., Sattarzadeh, S. S., and €u, R., 2016, ¡°Combined Hot-
Orl€Wire and PIV Measurements of a Swirling Turbulent Flow at the Exit of a 90 deg Pipe Bend,¡± J. 
Visualization, 19(2), pp. 261¨C273. 
[142] €u, Studies Jet and Zero Pressure-
Orl€R., 2009, ¡°Experimental in Flows Gradient Turbulent Boundary Layers,¡± Ph.D. 
thesis, KTH Mechanics, Stock-holm, Sweden. 
[143] Gupta, A. K., Lilley, D. G., and Syred, N., 1985, Swirl Flows, ABACUS Press, Cambridge, MA. 
[144] Facciolo, L., Tillmark, N., Talamelli, A., and Alfredsson, P. H., 2007, ¡°A Study of Swirling Turbulent Pipe and Jet Flows,¡± Phys. 
Fluids, 19(3), p. 035105. 
[145] White, A., 1964, ¡°Flow of a Fluid in an Axially Rotating Pipe,¡± J. 
Mech. 
Eng. 
Sci., 6(1), pp. 47¨C52. 
[146] Murakami, M., and Kikuyama, K., 1980, ¡°Turbulent Flow in Axially Rotating Pipes,¡± ASME 
J. 
Fluids 
Eng., 102(1), pp. 97¨C103. 
[147] Reich, G., and Beer, H., 1989, ¡°Fluid Flow and Heat Transfer in an Axially Rotating Pipe¡ªI: Effect of Rotation on Turbulent Pipe Flow,¡± Int. 
J. 
Heat 
Mass 
Transfer, 32(3), pp. 551¨C562. 
[148] Kikuyama, K., Murakami, M., Nishibori, K., and Maeda, K., 1983, ¡°Flow in an Axially Rotating Pipe: A Calculation of Flow in the Saturated Region,¡± Bull. 
JSME, 26(214), pp. 506¨C513. 
[149] Imao, S., Itoh, M., and Harada, T., 1996, ¡°Turbulent Characteristics of the Flow in an Axially Rotating Pipe,¡± Int. 
J. 
Heat 
Fluid 
Flow, 17(5), pp. 444¨C451. 
[150] Kikuyama, K., Murakami, M., and Nishibori, K., 1983, ¡°Development of Three-Dimensional Turbulent Boundary Layer in an Axially Rotating Pipe,¡± ASME 
J. 
Fluids 
Eng., 105(2), pp. 154¨C160. 
[151] Rocklage-Marliani, G., Schmidts, M., and Vasanta Ram, V., 2003, ¡°Three-Dimensional Laser-Doppler Velocimetry Measurements in Swirling Turbulent Pipe Flow,¡± Flow 
Turbul. 
Combust., 70(1), pp. 43¨C67. 
[152] Eggels, J., 1994, ¡°Direct and Large Eddy Simulation of Turbulent Flow in a Cylindrical Geometry,¡± Ph.D. thesis, Delft University of Technology, Delft, The Netherlands. 
[153] Orlandi, P., and Fatica, M., 1997, ¡°Direct Simulations of Turbulent Flow in a Pipe Rotating About Its Axis,¡± J. 
Fluid 
Mech., 343, pp. 43¨C72. 
[154] Feiz, A. A., Ould-Rouis, M., and Lauriat, G., 2003, ¡°Large Eddy Simulation of Turbulent Flow in a Rotating Pipe,¡± Int. 
J. 
Heat 
Fluid 
Flow, 24(3), pp. 412¨C420. 
[155] Speziale, C. G., Younis, B. A., and Berger, S. A., 2000, ¡°Analysis and Model-ling of Turbulent Flow in an Axially Rotating Pipe,¡± J. 
Fluid 
Mech., 407, pp. 1¨C26. 
[156] Nygard, F., and Andersson, H. I., 2009, ¡°DNS of Swirling Turbulent Pipe Flow,¡± Int. 
J. 
Numer. 
Methods. 
Fluids, 64(9), pp. 945¨C972. 
[157] So, R., and Anwer, M., 1993, ¡°Fully-Developed Turbulent Flow Through a Curved Pipe With and Without Swirl,¡± ASME 
Publ. 
FED, 146, pp. 29¨C43. 
[158] So, R. M. C., and Anwer, M., 1993, ¡°Swirling Turbulent Flow Through a Curved Pipe¡ªPart 2: Recovery From Swirl and Bend Curvature,¡± Exp. 
Fluids, 
14(3), pp. 169¨C177. 
[159] Hellstr€
om, L. H. O., Sinha, A., and Smits, A. J., 2011, ¡°Visualizing the Very-Large-Scale Motions in Turbulent Pipe Flow,¡± Phys. 
Fluids, 23(1), p. 011703. 
[160] Binnie, A. M., 1962, ¡°Experiments on the Swirling Flow of Water in a Verti-cal Pipe and a Bend,¡± Proc. 
R. 
Soc., 270(1343), pp. 452¨C466. 
[161] Shimizu, Y., and Sugino, K., 1980, ¡°Hydraulic Losses and Flow Patterns of a Swirling Flow in U-Bends,¡± Bull. 
JSME, 23(183), pp. 1443¨C1450. 
[162] Kitoh, O., 1987, ¡°Swirling Flow Through a Bend,¡± J. 
Fluid 
Mech., 175, pp. 429¨C446. 
[163] Chang, T. H., and Lee, H. S., 2003, ¡°An Experimental Study on Swirling Flow in a 90 Degree Circular Tube by Using Particle Image Velocimetry,¡± J. 
Visual-
ization, 6(4), pp. 343¨C352. 
[164] McDonald, D. A., 1955, ¡°The Relation of Pulsatile Pressure to Flow in Arteries,¡± J. 
Physiol., 127(3), p. 533. 
[165] Chandran, K. B., and Yearwood, T. L., 1981, ¡°Experimental Study of Physiological Pulsatile Flow in a Curved Tube,¡± J. 
Fluid 
Mech., 111, pp. 59¨C85. 
[166] Glenn, A. L., Bulusu, K., Shu, F., and Plesniak, M. W., 2012, ¡°Secondary Flow Structures Under Stent-Induced Perturbations for Cardio-vascular Flow in a Curved Artery Model,¡± Int. 
J. 
Heat 
Fluid 
Flow, 
35(TSFP-7), pp. 76¨C83. 
[167] Stokes, G. G., 1851, ¡°On the Effect of the Internal Friction of Fluids on the Motion of Pendulums,¡± Trans. 
Cambridge 
Philos. 
Soc., 9,p.8. 
[168] He, S., and Jackson, J. D., 2009, ¡°An Experimental Study of Pulsating Turbu-lent Flow in a Pipe,¡± Eur. 
J. 
Mech. 
B/Fluids, 28(2), pp. 309¨C320. 
[169] Womersley, J., 1957, ¡°Oscillatory Flow in Arteries: The Constrained Elastic Tube as a Model of Arterial Flow and Pulse Transmission,¡± Phys. 
Med. 
Biol., 2(2), pp. 178¨C187. 
[170] Scotti, A., and Piomelli, U., 2001, ¡°Numerical Simulation of Pulsating Turbu-lent Channel Flow,¡± Phys. 
Fluids, 13(5), pp. 1367¨C1384. 
[171] Carpinlioglu, M. €undogdu, M. Y., 2001, ¡°A Critical Review on Pul-
O., and G€satile Pipe Flow Studies Directing Towards Future Research Topics,¡± Flow 
Meas. 
Instrum., 12(3), pp. 163¨C174. 
[172] Zohir, A. E., Abdel Aziz, A. A., and Habib, M. A., 2015, ¡°Heat Transfer Char-acteristics and Pressure Drop of the Concentric Tube Equipped With Coiled Wires for Pulsating Turbulent Flow,¡± Exp. 
Therm. 
Fluid 
Sci., 65, pp. 41¨C51. 
[173] Guo, L., Chen, X., Feng, Z., and Bai, B., 1998, ¡°Transient Convective Heat Transfer in a Helical Coiled Tube With Pulsatile Fully Developed Turbulent Flow,¡± Int. 
J. 
Heat 
Mass 
Transfer, 41(19), pp. 2867¨C2875. 
[174] Ramezani Kharvani, H., Ilami Doshmanziari, F., Zohir, A. E., and Jalali-Vahid, D., 2016, ¡°An Experimental Investigation of Heat Transfer in a Spiral-Coil Tube With Pulsating Turbulent Water Flow,¡± Heat 
Mass 
Transfer, 52(9), pp. 1779¨C1789. 
[175] Timite, B., Castelain, C., and Peerhossaini, H., 2010, ¡°Pulsatile Viscous Flow in a Curved Pipe: Effects of Pulsation on the Development of Secondary Flow,¡± Int. 
J. 
Heat 
Fluid 
Flow, 31(5), pp. 879¨C896. 
[176] Jarrahi, M., Castelain, C., and Peerhossaini, H., 2010, ¡°Secondary Flow Pat-terns and Mixing in Laminar Pulsating Flow Through a Curved Pipe,¡± Exp. 
Fluids, 50(6), pp. 1539¨C1558. 
€
[177] Kalpakli, A., Orlu,€R., Tillmark, N., and Alfredsson, P. H., 2012, 
¡°Experimental Investigation on the Effect of Pulsations on Exhaust Manifold-Related Flows Aiming at Improved Ef.ciency,¡± 10th International Conference on Turbochargers and Turbocharging (IMechE), London, May 15¨C16, pp. 377¨C387. 
[178] Laurantzon, F., Tillmark, N., €u, R., and Alfredsson, P. H., 2012, ¡°A Flow
Orl€Facility for the Characterization of Pulsatile Flows,¡± Flow 
Meas. 
Instrum., 26, pp. 10¨C17. 
[179] Laurantzon, F., €u, R., Segalini, A., Tillmark, N., and Alfredsson, P. H.,
Orl€2012, ¡°Experimental Analysis of Turbocharger Interaction With a Pulsatile Flow Through Time-Resolved Flow Measurements Upstream and Down-stream the Turbine,¡± 10th International Conference on Turbochargers and Tur-bocharging (IMechE), London, May 15¨C16, pp. 405¨C415. 
€
[180] Kalpakli, A., Orlu,€R., and Alfredsson, P. H., 2013, ¡°Vortical Patterns in Turbulent Flow Downstream a 90 deg Curved Pipe at High Womersley Numbers,¡± Int. 
J. 
Heat 
Fluid 
Flow, 44, pp. 692¨C699. 
[181] Kalpakli Vester, A., €u, R., and Alfredsson, P. H., 2015, ¡°Pulsatile Turbu-
Orl€lent Flow in Straight and Curved Pipes¡ªInterpretation and Decomposition of Hot-Wire Signals,¡± Flow 
Turbul. 
Combust., 94(2), pp. 305¨C321. 
[182] Noorani, A., and Schlatter, P., 2016, ¡°Swirl-Switching Phenomenon in Turbu-lent Flow Through Toroidal Pipes,¡± Int. J. Heat Fluid Flow (in press). 
[183] Yongmann, C., 2015, University of Warwick, Coventry, UK (personal communication). 
[184] Hufnagel, L., 2016, ¡°On the Swirl-Switching in Developing Bent Pipe Flow With Direct Numerical Simulation,¡± MSc. thesis, KTH Mechanics, Stock-holm, Sweden. 
[185] Kalpakli Vester, A., 2014, ¡°Vortices in Turbulent Curved Pipe Flow¡ª Rocking, Rolling and Pulsating Motions,¡± Ph.D. 
thesis, KTH Mechanics, Stockholm, Sweden. 



International Journal of Fatigue 

A study on the effects of machining-induced residual stress on rolling contact fatigue 
Youngsik Choi * 
Department of Mechanical and Aerospace Engineering, Florida Institute of Technology, Melbourne, FL 32901, USA 
article info abstract 
Article history: 
Received 29 January 2009 Received in revised form 28 April 2009 Accepted 1 May 2009 Available online 9 May 2009 
Keywords: 
Rolling contact fatigue Residual stresses Hard machining Machining parameters Optimization 
ABSTRACT
This study investigates the effects of machining-induced residual stress on rolling contact fatigue of hard machined components. The rolling contact fatigue life predictions demonstrate that the fatigue life is influenced by residual stresses significantly and that tool nose radius can contribute to the optimization of machining parameters to enhance the fatigue performance of hard machined components. The rolling contact fatigue test results show that the prediction accuracy of the fatigue life model can be enhanced signi.cantly by incorporating residual stresses. It is further noted that residual stresses in.uence the roll-ing contact fatigue life by more than 40%. The results demonstrate that the rolling contact fatigue perfor-mance of hard machined 
components can be enhanced signi.cantly by optimizing the residual stress distribution below the machined surface.
﹝ 2009 Elsevier Ltd. All rights reserved. 
INTRODUCTION
1. Introduction 
Hard turning is an alternative process for making precision components. Compared with grinding, it has many advantages in terms of the number of processes, processing time, material han-dling time, set-up time, equipment cost, and the .exibility of sys-tem. However, the implementation of hard turning as a .nishing process demands an extensive understanding of surface integrity aspects of hard turning. 
It was reported that an increase in the depth of cut yields larger compressive residual stresses on the surface [1]. According to K.nig et al. [2], a larger in-feed increases the depth of the affected zone and the level of compressive residual stresses. An experimen-tal study [3] showed that hard turning induces more compressive residual stresses than grinding. Tool edge geometry was found to be the dominant factor deciding the residual stress pro.le [4]. Re-cently, Hua et al. [5] argued that an optimal tool edge geometry and aggressive feedrate increase compressive residual stresses. 
Based on a residual stress model, an optimal residual stress dis-tribution was put forward by Mittal and Liu [6] 
to enhance the roll-ing contact fatigue performance of super.nish hard turned surfaces. A .nite element simulation demonstrated that residual stresses of hard turned surface can be changed to a better state by optimizing the second cut [7]. Recently, Zhang et al. [8] demon-strated that a model based on back-propagation neural network can predict residual stress pro.les of hard turned surfaces more accurately than conventional linear regression method. 
It was reported that the maximum modi.ed equivalent stress is a better predictor for the rolling contact fatigue life of hard ma-chined surfaces than the maximum Hertzian stress [9]. Guo and Yen [10] found that the slope of a compressive residual stress pro-.le is an important factor for rolling contact fatigue damage. Addi-tionally, a .nite element simulation [11] showed that multi-axial fatigue damage parameters can characterize the relative fatigue damage under the in.uence of machining-induced residual stress pro.les. 
A rehardening burn layer was observed after cutting a steel of hardness 53 HRC by using a ceramic chamfered tool, but this layer was not found in the case of machining with a sharp tool [12,13]. The rehardening burn layer is resistant to the etching in contrast to the bulk material, hence called commonly as a ＆＆White Layer§. It was also noted that white layers were not formed if the tool wear was below 0.15 mm [14]. The alteration of the chemical composition of ASTM 5115 steel was investigated when white layers were produced by hard machining, and no element concentration transition from the over-tempered layer to white layers was found [15]. Recently, it was reported that white layer forms whenever the austenitisation temperature of the material is exceeded on the machined surface [16]. Surface .nish of hard turned surfaces was found to be at least as good as that of ground surfaces as a result of minimal plastic .ow and the absence of a built up edge due to the high hardness of material [17]. This .nding was further investigated in hard turning of AISI 52100 steel, and surface .nish was found to deteriorate with the increase of tool .ank wear [18]. 
The feasibility of replacing abrasion-based super.nishing with hard turning was demonstrated [19,20]. Agha and Liu [21] found that the deterministic nature of hard turning results in more con-sistent repeatability of the fatigue life in rolling contact. It was fur-ther reported that the selection of machining parameters can greatly affect the rolling contact fatigue life [22]. 
The .ndings are due to the deterministic nature of the residual stress distribution below hard turned surfaces. Since the fatigue life is in.uenced by the residual stress distribution, hard turned surfaces show more consistent repeatability of the rolling contact fatigue life. Furthermore, the machining parameters affect the roll-ing contact fatigue life signi.cantly as the residual stress distribu-tion is in.uenced by the machining parameters. 
Although the in.uence of residual stresses on the fatigue life of hard machined surfaces has been investigated analytically [6,9每 
11,22], an experimental and analytical study on the effects of resid-ual stresses induced at various machining parameters on the fati-gue parameters has not been explored. 
This study aims to investigate the effects of residual stresses in-duced at various machining parameters on the fatigue parameters, such as crack initiation depth, maximum shear stress at crack ini-tiation depth, crack initiation life, crack propagation life, and fati-gue life, by both experimental and analytical work. The residual stress distributions below hard machined surfaces are investigated. The fatigue parameters are then computed based on the residual stress distribution to investigate the effects of machining-induced residual stress on rolling contact fatigue performance. Lastly, the rolling contact fatigue tests are performed to substantiate the computations. 
METHOD
2. Experimental 
2.1. Specimen preparation 
Specimens of through hardened AISI 1053 steel were prepared for the experiment, since this steel is used for bearing applications. The composition of AISI 1053 steel is given in Table 
1. The dimen-sions of the specimen were selected to minimize de.ection by the chucking forces of a standard jaw and to be uniformly through hardened [23]. The selected dimensions are shown in Fig. 1. 
2.2. Machining parameters 
The cutting tool used in the experiment was a cubic boron nitride (CBN) tool with a tool nose radius of 0.79, 1.59, or 
4.76 mm. Three different tool chamfer angles were used: 0∼,15∼, 20∼. The machining parameters, such as cutting speed, feedrate, depth of cut, and coolant type, were selected according to Taguchi＊s orthogonal array [24] 
that was employed to distribute the effect of machining parameters on surface integrity evenly. 
Ten different combinations of machining parameters were determined, and the specimens were numbered sequentially according to the combinations of machining parameters (Table 2). 

2.3. Residual stress measurement 
X-ray diffraction method was used to measure the residual stress distribution in the subsurface of specimens. A Denver-Proto 
Table 1 
Composition (%) of specimens. 
Material C Mn P S 

Circumferential direction (Cutting direction) 
Radial direction (Feed direction) 


Dimensions in mm 

Fig. 1. Dimensions of specimen and directions of residual stress measurement. 
XRD 3000 residual stress analyzer was used with a Cr Ka radiation tube. The sin2 w technique, which is elaborated by Noyan and Co-hen [25], was applied to compute the residual stress value. Nine w angles were used for the computation: 20∼, 15∼, 10∼, 5∼,0∼, 5∼,10∼,15∼, and 20∼. Residual stresses were measured in two direc-tions: circumferential and radial. Fig. 1 shows the directions of the residual stress measurement. Residual stresses were measured at the exposed surface and .ve different depths: 5.08, 12.7, 25.4, 50.8, and 127 lm. 
A particular amount of a layer was removed with the help of an electrolytic etcher, saturated NaCl solution, to measure residual stresses at different depths. The machined surface was masked by using an etching mould made with a silicone rubber to avoid etching the whole surface. Since pure NaCl leaves a patterned appearance on the surface by a corrosive effect, a little amount of soap was added to the electrolyte to prevent this effect. The thick-ness of the specimen was measured to check the removed amount of a layer after each etching. 

2.4. Rolling contact fatigue test 
A special test rig was used to perform rolling contact fatigue tests. The tests were performed in a temperature-controlled room that was set to 25 ∼C. The thrust ball bearing, which has Grade 25 balls of 3.69 mm diameter, was used in the test. This bearing was inserted between two specimens. The upper specimen was rotated at 1840 rpm, and the lower specimen was .xed in the test rig. An axial load that produces a maximum Hertzian stress of 2720 MPa was imposed on the upper specimen. 
SAE-30 lubrication oil was circulated through a 0.25 lm .l-tered-pump feed system at a rate of 56.8 cm3/min. The bearing and the specimens were immersed in this lubricant while a test was run. Fig. 
2 shows a schematic diagram of the rolling contact fa-tigue test rig. The experimental lives by this test rig showed less than 10每17% differences with those by the Falex multi-specimen rolling fatigue tester [21]. 
The rolling contact fatigue test was monitored by an accelerom-eter connected to a vibration meter. The vibration signal, which is in.uenced by both the upper and lower specimens, was acquired by the data acquisition system during the test. The vibration 

Table 2 
Machining parameters. 
Specimen  Tool nose radius (mm)  Tool chamfer angle (∼)  Cutting speed (m/s)  Feedrate (mm/rev)  Depth of cut (mm)  Coolant type  

MQL = minimum quantity lubrication. 

threshold level was set to 0.2 g to detect the initiation of a fatigue spall. To enable a reliable comparison, this level was .xed for all the tests. 

METHOD
3. Prediction of rolling contact fatigue life 
For properly lubricated conditions, rolling contact fatigue nor-mally begins with crack initiation in the subsurface. The cracks then propagate by a continued rolling, eventually reaching the ex-posed surface. Subsequently, surface fragments are dislodged, which results in the formation of a spall on the surface. The in-stance is generally de.ned as the initiation of a fatigue spall. 
Choi and Liu [26] proposed a crack initiation life model and a crack propagation life model to predict the initiation of a fatigue spall, and veri.ed the models experimentally [27]. Liu and Choi 
[28] later developed a crack initiation life model based on a dislo-cation model, and they found that this model shows better accu-racy than their previous model. In this paper, the crack initiation life model based on a dislocation model [28] and the crack propa-gation life model [26] are combined to predict the fatigue life. 
3.1. Crack initiation life model Tanaka and Mura [29] proposed a dislocation model for fatigue crack initiation. The simpli.ed form of the model can be expressed as follows: 
Load Shaft 


Accelerometer 
Upper Specimen 

Lubricant In
Bearing Retainer 
Bearing Ball
Lower Specimen 


Lubricant Out 
Fig. 2. Schematic diagram of a rolling contact fatigue test rig. 
where Ni is the number of loading cycles to crack initiation, Wc the speci.c fracture energy for a unit area, Ds the range of local shear stress, sk the frictional stress, and A is a function depending on the material properties and the type of initial cracks. A can be writ-ten as [30]: 

where Gh is the shear modulus of homogeneous media, Gi the shear modulus of inclusion, m the Poisson ratio, l the semi-length of slip band, m the semi-minor length of elliptical slip band area, and R is the radius of inclusion. 
Subsurface cracks typically initiate from inclusions, which act as a stress raiser [31每34]. The inclusions intensify the local stress, and the periphery of the inclusion is strained, eventually initiating cracks [35]. Fractographic examination of high strength steels shows that the initiation sites for subsurface cracks are always inclusions located in the interior of the material [36]. 
Accordingly, it is assumed that a crack initiates along the inter-face of the inclusion. To further simplify the model, the semi-length of a slip band is assumed to be equal to the semi-minor length of the elliptical slip band area [30,36]. With these assump-tions, the crack initiation life model can be rewritten as follows: 

The local shear stress is replaced by the maximum shear stress, and the crack initiation point is assumed to be where the ratio of maximum shear stress to micro-hardness is highest based on the previous .nding [37]. Based on the measurement of inclusions in Fig. 3 [28], the crack initiating inclusion size is assumed to be 30 lm. Accordingly, the crack initiation life model can be simpli-.ed as follows: 
where I is the initiation constant (I . GiR ) and Dsmax is the range of maximum shear stress during a cycle at the initiation point. Accordingly, the crack initiation life can be predicted by using the maximum shear stress at the initiation point, provided that the initiation constant and the frictional stress of the crack initiation life model are determined. 


3.2. Crack propagation life model 
The crack propagation life model was developed by using the modi.ed Paris＊ formula [26]. A formula was proposed by Paris 

Fig. 3. Cross-sectional view of specimen. 

and Erdogan [38] for the correlation between the crack propaga-tion rate and the stress intensity factor range: 

where a is the half length of the crack, N the number of loading cy-cles, C the material constant, DK the stress intensity factor range, and n is the slope index. 
Paris＊ formula, Eq. (5), is modi.ed, since the crack propagates relatively faster as the local hardness is lower if the stress .eld is identical [39]: 

where Hb and Hl are the Knoop hardness number at the bulk mate-rial and the local Knoop hardness number, respectively. 
Hearle and Johnson [40] 
calculated the mode II stress intensity factors at each end of the crack in the half-space, where the stress .eld over the whole crack is known. The crack geometry used for the calculation of the stress intensity factor is shown in Fig. 4. The corresponding stress intensity factor at the leading tip (KL)is as follows: 
where L is the crack length, sc the net shear stress, and xL is the po-sition of the leading tip. 
The net shear stress (sc) is the stress available to cause stress intensities at the crack tip. The driving force for crack propagation in rolling contact has been postulated to be related to the maxi-mum shear stress, and it was experimentally veri.ed [39,41]. The maximum shear stress is thus assumed to be the net shear stress. 

Fig. 4. Geometry of a crack moving under a stationary load. 

The stress intensity factor range is calculated from the variation of KL at different crack tip positions. The crack propagation life can be calculated by integrating the crack propagation rate from the initial crack size to the .nal crack size. The .nal crack size is de-rived from the crack size when the crack reaches the surface. The crack propagation life model can then be written as follows: 
Z a21


where Np is the crack propagation life in loading cycles, a1 the half length of the initial crack size, a2 the half length of the .nal crack size, and n is the slope index, which is taken as three for bearing steels [42]. 
The half length of the crack can be expressed by using the depth of the leading tip position and the crack propagation angle as follows: 

where zi is the crack initiation depth, z the depth of the leading tip position, and h is the angle that the crack propagation direction makes with the surface, which is taken as 17.2∼ based on the exper-imental observations [23,37]. 

3.3. Rolling contact fatigue life model 
The crack initiation life model is combined with the crack prop-agation life model to predict the fatigue life as follows: 

where N is the fatigue life in number of loading cycles. 

RESULT and DISSCUSSION
4. Results and discussion 
4.1. Residual stresses 
Most specimens typically showed compressive stresses in cir-cumferential and radial directions, while specimen 9 showed ten-sile stresses around the surface in circumferential direction. The residual stress distributions of specimens 3 and 9 are shown in Figs. 5 and 6. 
The peak compressive residual stress was typically located in the range of 5 and 25 lm, and compressive residual stresses were reduced as it goes deeper. The peak compressive residual stress ranged between 400 and 1600 MPa, while the residual stress below the depth of 100 lm ranged between 100 and 400 MPa. 
Residual stresses of some specimens showed a similar distribu-tion between circumferential and radial directions. However, residual stresses of other specimens showed an appreciable differ-ence between circumferential and radial directions, especially around the surface. 

4.2. Crack initiation depth 
The crack initiation depth was computed based on subsurface stress distribution. First, the crack initiation depth is computed be-fore residual stresses are taken into account. The crack initiation depth is then computed after residual stresses are taken into ac-count. The computed crack initiation depths are listed in Table 3. 
The crack initiation depth is changed to a deeper location up to 40.6 lm after residual stresses are taken into account, since com-pressive residual stresses move the crack initiation depth deeper by lowering stresses. The effect of residual stresses on crack initia-tion depth is shown to be signi.cant considering that the crack initiation depth is 18.6 lm before residual stresses are taken into account. 
Since the crack initiation depth determines the initial spall size and the crack propagation length required to reach the exposed surface, it will affect the fatigue performance considerably. If the initial spall size is reduced, a greater number of continued loading cycles are required to increase the spall size until the .nal failure. If the crack propagation length required to reach the exposed surface increases, the crack propagation life increases as long as the stress .eld is identical. 

4.3. Maximum shear stress at crack initiation depth 
The maximum shear stress at crack initiation depth was com-puted. First, the maximum shear stress at crack initiation depth 
Table 3 
Crack initiation depth comparison. 
Specimen  zi 
zi = crack initiation depth before residual stresses are taken into account. 
zi,r = crack initiation depth after residual stresses are taken into account. 
is computed before residual stresses are taken into account. The maximum shear stress at crack initiation depth is then computed after residual stresses are taken into account. The maximum shear stresses at crack initiation depth are listed in Table 4. 
The maximum shear stress is generally reduced after residual stresses are taken into account, while the maximum shear stress is increased for specimen 9 due to tensile residual stresses in cir-cumferential direction. The maximum shear stress is reduced up to 421.9 MPa, while the maximum shear stress is 853.8 MPa before residual stresses are taken into account, which demonstrates that about 50% of maximum shear stress can be reduced by residual stresses. The effect of residual stresses on crack initiation life is ex-pected to be signi.cant considering that the maximum shear stress is a key value for crack initiation life. 


4.4. Crack initiation life 
The crack initiation life was predicted by using the crack initia-tion life model. First, the crack initiation life is predicted before residual stresses are taken into account. The crack initiation life is then predicted after residual stresses are taken into account. The predicted crack initiation lives are listed in Table 5. The crack initiation life is generally increased after residual stresses are taken into account, while specimen 9 shows the con-trary, as expected from the maximum shear stresses at crack initi-ation depth. The crack initiation life is increased up to 4.73E+06, while the crack initiation life is 3.66E+05 before residual stresses are taken into account: residual stresses can increase the crack ini-tiation life by more than 12 times. 
Since the crack initiation life is dominant for high strength steels, it is conceivable that the residual stress distribution should be controlled to maximize the crack initiation life of rolling contact components made of high strength steels. 


4.5. Crack propagation life 
The crack propagation life was predicted by using the crack propagation life model. First, the crack propagation life is predicted before residual stresses are taken into account. The crack propaga-tion life is then predicted after residual stresses are taken into ac-count. The predicted crack propagation lives are listed in Table 6. 
The crack propagation life is generally increased after residual stresses are taken into account, while specimens 7 and 9 show the contrary. Since the crack propagation life is in.uenced by crack initiation depth and stress .eld, a deeper crack initiation depth does not always increase the crack propagation life. 
The crack propagation life is increased up to 4.34E+05, while the crack propagation life is 4.92E+04 before residual stresses are ta-ken into account: residual stresses can increase the crack propaga-tion life by more than eight times. 
Table 4 
Maximum shear stress comparison. 
Specimen  smax 

Table 5 
Crack initiation life comparison. 
Specimen  Ni 


Table 6 
Crack propagation life comparison. 
Specimen  Np 

Np = crack propagation life before residual stresses are taken into account. 

b 
Np,r = crack propagation life after residual stresses are taken into account. 


4.6. Prediction of rolling contact fatigue life 
The rolling contact fatigue life was predicted by using the roll-ing contact fatigue life model. First, the fatigue life is predicted be-fore residual stresses are taken into account. The fatigue life is then predicted after residual stresses are taken into account. The pre-dicted fatigue lives are listed in Table 
7. 
The fatigue life is generally increased after residual stresses are taken into account. The fatigue life is increased up to 4.98E+06, while the fatigue life is 4.15E+05 before residual stresses are taken into account: residual stresses can increase the fatigue life by around 12 times. The results demonstrate that the rolling contact fatigue performance of rolling contact components can be en-hanced signi.cantly by optimizing the residual stress distribution at the subsurface. 
Depending on the function of rolling contact component, crack initiation life or crack propagation life is dominant for the fatigue 
Table 7 
Predicted rolling contact fatigue life comparison. 
Specimen  Na 

N = predicted rolling contact fatigue life before residual stresses are taken into account. 
b 
Nr = predicted rolling contact fatigue life after residual stresses are taken into account. 
Table 8 
Predicted and experimental rolling contact fatigue lives. 
Specimen  Na 

a 
N = predicted rolling contact fatigue life before residual stresses are taken into account. 
b 
Nr = predicted rolling contact fatigue life after residual stresses are taken into account. 
c 
Ne = experimental rolling contact fatigue life. 

performance. Accordingly, an optimal residual stress distribution varies with the function of rolling contact components. 
From this perspective, if a methodology to optimize the residual stress distribution depending on the function of rolling contact components is established, it will provide an ef.cient way to opti-mize machining parameters to produce an optimal residual stress distribution at the subsurface. 
The fatigue lives of specimens 1, 2, 5, and 6 are shown to be longer than those of other specimens. The machining parameters show that those specimens were machined by using a tool with a tool nose radius of 0.79 mm (smallest tool nose radius in this study), while other machining parameters were not identical. It is thus conceivable that tool nose radius can contribute to the opti-mization of machining parameters to enhance the rolling contact fatigue performance of hard machined components. 

4.7. Rolling contact fatigue test 
The predicted and experimental rolling contact fatigue lives are listed in Table 
8. The ratio of predicted life to experimental life was calculated for the predicted life before residual stresses are taken into account and the predicted life after residual stresses are taken into account (Table 8). 
It is noted that the predicted life before residual stresses are ta-ken into account shows signi.cant under-predictions. The overall average in the ratios for the predicted life after residual stresses are taken into account is 0.80, while that for the predicted life be-fore residual stresses are taken into account is 0.38. 
The assumptions used in the rolling contact fatigue life model contributed to a portion of the prediction inaccuracy. The non-uni-formity of the material properties and tribological conditions need to be addressed by further work to improve the prediction accuracy. 
The .nding demonstrates that the prediction accuracy of the rolling contact fatigue life model can be enhanced signi.cantly by incorporating residual stresses. It is further noted that residual stresses in.uence the rolling contact fatigue life by more than 40% considering that the rolling contact fatigue life model shows rea-sonable accuracy. 

CONCLUSION
5. Conclusions 
Generally, the residual stress distributions of hard machined specimens showed compressive stresses. The peak compressive residual stress was typically located in the range of 5 and 25 lm, and compressive residual stresses were reduced as it goes deeper. 
Y. Choi / International Journal of Fatigue 31 (2009) 1517每1523 
Residual stresses of some specimens showed a similar distribution between circumferential and radial directions, while residual stres-ses of other specimens showed an appreciable difference between circumferential and radial directions, especially around the surface. 
The effect of residual stresses on crack initiation depth was shown to be signi.cant: the crack initiation depth was changed to a deeper location up to 40.6 lm after residual stresses were ta-ken into account, while the crack initiation depth was 18.6 lm be-fore residual stresses were taken into account. The maximum shear stress at crack initiation depth was reduced up to about 50% after residual stresses were taken into account. 
Residual stresses could increase the crack initiation life by more than 12 times, while those could increase the crack propagation life by more than eight times. Consequently, residual stresses could increase the rolling contact fatigue life by around 12 times. 
The predicted lives of specimens machined by using a tool with a tool nose radius of 0.79 mm were longer than those of other spec-imens, which demonstrates that tool nose radius can contribute to the optimization of machining parameters to enhance the rolling contact fatigue performance of hard machined components. 
The rolling contact fatigue test results showed that the predic-tion accuracy of the rolling contact fatigue life model can be en-hanced signi.cantly by incorporating residual stresses and that residual stresses in.uence the rolling contact fatigue life by more than 40%. 
The results demonstrate that the rolling contact fatigue perfor-mance of hard machined components can be enhanced signi.-cantly by optimizing the residual stress distribution below the machined surface. 
References 
[1] Brinksmeier E. Residual stresses in hard metal cutting. In: Conference on residual stresses in science and technology; 1986. p. 839每46. 
[2] K.nig W, Klinger M, Link R. Machining hard materials with geometrically de.ned cutting edges 每 .eld of applications and limitations. Ann CIRP 1990;39(1):61每4. 
[3] Abr.o AM, Aspinwall DK. The surface integrity of turned and ground hardened bearing steel. Wear 1996;196:279每84. 
[4] Matsumoto Y, Hashimoto F, Lahoti G. Surface integrity generated by precision hard turning. Ann CIRP 1999;48(1):59每62. 
[5] Hua J, Umbrello D, Shivpuri R. Investigation of cutting conditions and cutting edge preparations for enhanced compressive subsurface residual stress in the hard turning of bearing steel. J Mater Process Technol 2006;171(2):180每7. 
[6] Mittal S, Liu CR. A method of modeling residual stress in super.nish hard turning. Wear 1998;218:21每33. 
[7] Liu CR, Guo YB. Finite element analysis of the effect of sequential cuts and tool-chip friction on residual stresses in a machined layer. Int J Mech Sci 2000;42:1069每89. 
[8] Zhang JY, Liang SY, Zhang G, Yen D. Modeling of residual stress pro.le in .nish hard turning. Mater Manuf Process 2006;21:39每45. 
[9] Agha SR, Liu CR. On modeling the rolling contact fatigue performance based on residual stresses generated by super.nish hard turning. ASME Med 2000;11:1047每53. 

[10] Guo YB, Yen DW. Hard turning versus grinding 每 the effect of process-induced residual stress on rolling contact. Wear 2004;256:393每9. 
[11] Guo YB, Barkey ME. Modeling of rolling contact fatigue for hard machined components with process-induced residual stress. Int J Fatigue 2004;26:605每13. 
[12] Matsumoto Y, Barash MM, Liu CR. Residual stress in the machined surface of hardened steel. High speed machining; 1984. p. 193每204. 
[13] Matsumoto Y, Barash MM, Liu CR. Cutting mechanism during machining of hardened steel. Mater Sci Technol 
[14] Ogata M. Einsatz von PCBN 	每 Werkzeugen in der production. VDI Berichie
[15] Tonshoff 	HK, Wobker HG, Brandt D. Hard turning 每 In.uences on the workpiece properties. Trans 
[16] Guo YB, Janowski GM. Microstructural characterization of white layers by hard turning and grinding. Trans NAMRI/SME 
[17] Nakayama K, Aria M, Kanda T. Machining characteristics of hard materials. Ann CIRP  
[18] Davies MA, Chou Y, Evans CJ. On chip morphology, tool wear, and cutting mechanics in .nish hard turning. Ann CIRP 
[19] Liu CR, Mittal S. 	Single-step super.nish hard machining: feasibility and feasible cutting conditions. Robot Comput Integ Manuf 
[20] Liu CR, Mittal S. Single-step super.nishing using hard machining resulting in superior surface integrity. J Manuf Syst  
[21] Agha SR, Liu CR. Experimental study on the performance of super.nish hard turned surfaces in rolling contact. Wear 
[22] Liu 	CR, Mittal S. Optimal pre-stressing the surface of a component by super.nish hard turning for maximum fatigue life in rolling contact. Wear 1998;219:128每40. 
[23] Agha SR. Fatigue performance of super.nish hard turned surfaces in rolling contact. Ph.D. Thesis, Purdue University; 2000. 
[24] Taguchi G. 	Taguchi on robust technology development: Bringing quality engineering upstream. ASME Press; 1993. 
[25] Noyan 	IC, Cohen JB. Residual stress: measurement by diffraction and interpretation. Springer-Verlag; 
[26] Choi Y, Liu CR. Rolling contact fatigue life of .nish hard machined surfaces: Part 1. Model development. 
[27] Choi Y, Liu CR. Rolling contact fatigue life of .nish hard machined surfaces: Part 2. Experimental veri.cation
[28] Liu CR, Choi Y. A new methodology for predicting crack initiation life for rolling contact fatigue based on dislocation and crack propagation. Int J Mech Sci 2008;50:117每23. 
[29] Tanaka K, Mura T. Dislocation model for fatigue crack initiation. J Appl Mech 
[30] Zhou RS, Cheng HS, Mura T. Micropitting in rolling and sliding contact under mixed lubrication. J Tribol 
[31] Kaynak C, Ankara A, Baker TJ. Initiation and early growth of short fatigue cracks at inclusions. Mater Sci Technol 
[32] Melander A. A .nite element study of short cracks with different inclusion types under rolling contact fatigue load. Int J Fatigue 
[33] Nelias D, Dumont ML, Champiot F, Vincent A, Girodin D, Fougeres R, et al. Role of inclusions, surface roughness and operating conditions on rolling contact fatigue. J Tribol 1999;121:240每51. 
[34] Bormetti E, Donzella G, Mazzu A. Surface and subsurface cracks in rolling contact fatigue of hardened components. Tribol Trans 
[35] Harris TA. Rolling bearing analysis. New York: Wiley-Interscience; 
[36] Wang QY, Bathias C, Kawagoishi N, Chen Q. Effect of inclusion on subsurface crack initiation and gigacycle fatigue strength. Int J Fatigue 
[37] Leng X, Chen Q, Shao E. Initiation and propagation of case crushing cracks in rolling contact fatigue. Wear 
[38] Paris P, Erdogan F. A critical analysis of crack propagation laws. J Basic Eng 
[39] Chen Q, Leng X, Shao E. In.uence of microstructure and residual stress on the stages of case crushing. Wear 
[40] Hearle AD, Johnson KL. Mode II stress intensity factors for a crack parallel to the surface of an elastic half-space subjected to a moving point load. J Mech Phys Solids 
[41] Jiang B, Zheng X, Wang M. Calculation for rolling contact fatigue life and strength of case-hardened gear materials by computer. J Test Evaluat 
[42] Averbach BL. Fracture of bearing steels. Metal progress; December 





Simulation of curing process of carbon/epoxy composite during autoclave degassing molding by considering phase changes of epoxy resin 
Strain monitoring of a carbon/epoxy composite cross-ply laminate during thermoforming was conducted by using .ber Bragg grating (FBG) sensors. The entire process was simulated by employing .nite element analysis (FEA) by taking into consideration the phase changes of the epoxy resin. For the precise simulation of the curing process, a dielectrometry sensor was used to detect the epoxy-resin dissipation factor, which in turn was used to identify the curing point. To investigate the phase changes and consolidation of the composite laminate by employing FEA, modulus changes with tem-perature were measured by dynamic mechanical analysis (DMA), and the permeability was estimated by measuring the .ber volume fraction according to the curing temperature. As the epoxy resin changed from a liquid to solid phase, the strain generated along the carbon .bers dynamically changed, and the analysis results generally predicted the strain variation quite well. To apply this simulation technique to practical structures, a composite-aluminum hybrid wheel was analyzed and experimentally verfied. 
1. Introduction 
The excellent mechanical properties of .brous composites have enabled them to be applied to the design of various structures, in which their performance has been closely investigated. However, a variation in some factors, particularly, residual stressda crucial factordduring the forming process may ultimately affect the performance of the .nal product. In order to investigate the effect of material behavior during a forming process on the per-formance of the .nal product, signi.cant research on thermo-forming processes has been conducted, e.g., on a smart cure cycle for controlling temperature overshoot [4], simulation techniques for composite laminates, incorporation of consolidation and thickness variation [5,6], and forming techniques that take the resin viscosity into account [7e9]. 
In addition, intensive research into different forming processes has also been carried out. An optimal cure cycle to minimize residual stress and strain has been previously proposed, with strain variation during thermoforming being observed and closely 
investigated using .ber Bragg grating (FBG) optical sensors. This cycle takes into account the effect of thermal property differences between the composite laminate and the mold, the micro-structures of fabric composites, boundary conditions, and differences in the coef.cient of thermal expansion between different materials. Health monitoring of composite structures
and cure monitoring of composite-metal hybrid structures 
using various sensors have also been conducted to detect any material failures during service or forming conditions. Recently, the .eld of application of FBG sensors has been broadened to include real-time health monitoring of structures such as airplanes and ships. The performance validation of FBG sensors, including their endurance limit, was also veri.ed by comparing them with conven-tional sensors by performing various mechanical tests. 
This paper presents the simulation of the curing of a carbon/ epoxy laminate by taking into consideration the phase changes of the epoxy resin and the corresponding material property changes in the laminate. The strain and temperature change in the com-posite laminate was monitored during the curing process using FBG sensors, and the residual thermal strain also was measured. The simple technique for simulating the entire curing process with simple mechanical properties was developed using .nite 
element analysis (FEA) with a user's subroutine, which predicted well the overall change in the generated strains in the composite laminate during the curing process. This technique may provide design guidance for composite structures, allowing durability to be enhanced by reducing the residual stress and strain of structures. 
2. Experiments for determining parameters for the simulation 
2.1. Materials and sensors 
A carbon/epoxy prepreg (USN125, SK Chemical, Korea) was used to fabricate a laminate specimen. Vacuum bag degassing molding was then used to perform cure monitoring of the composite lami-nate, and all the materials used in the experiment are listed in Table 
The material properties were obtained from a previous research. 
Two types of sensors were used: FBG .ber optic sensors with an interrogator (Micron optics instrument, USA) were used for moni-toring the strain and temperature of composite laminates during curing, and a dielectrometry sensor 
1a; [11]) was used for detecting the degree of cure of the laminate. In order to accurately measure the strain variation with temperature in the composite laminate during curing, the sensor's coef.cient of thermal expan-sion was compensated for in calculating equations: 
2.2. Measurement of the .ber volume fraction 
To estimate the amount of resin extracted during vacuum bag degassing molding, the .ber volume fraction according to tem-perature was measured. Darcy's law
predicts the resin .ow during the curing of composites based on the permeability (ki)of the material, and it is closely related to the material void ratio
strain change, temperature change of the FBG, Bragg grating's 
thermo-optical coef.cient, sensitivity coef.cient of strain, and where rf and ci are the .ber radius and KozenyeCarman constant, sensitivity coef.cient of temperature, respectively. The respectively. The permeability of the composite laminate 
Material properties and measured values of various materials. 
Materials Thermal conductivity (W/mK) Speci.c heat capacity (J/kg K) Density (kg/m3) 
Mechanical properties Epoxy 0.2 1740 1210 Fiber 85.0 700 1750 Steel 60.0 450 8000 Vacuum bag 0.24 1670 1140 Breather 0.007 1350 260 Te.on .lm 0.4 1050 2200 
Materials Density (kg/m3) Young's Modulus (GPa) Poisson's ratio Elongation (%) 
Mechanical properties USN125 1480 E1 131.0 n12 0.0226 1.8 E2 10.5 n23 0.0226 E3 10.5 n31 0.4700 
Aluminum 2700 68.9 0.33 17.0 
Materials 125C 105C 85 C 65 C 45 C 25 C 
CTE for temperature (10 6/ C) USN125 L 2.21 1.85 1.66 1.49 1.31 1.09 T 62.82 48.42 37.44 29.79 25.20 23.80 Aluminum 23.76 23.49 23.29 23.09 22.91 22.86 Steel 12.82 12.40 12.24 12.02 11.72 11.56 
Temperature ( C) Fiber volume fraction Permeability 
Vf and permeability for temperature 25 0.55 0.8181 55 0.56 0.7857 80 0.58 0.7241 100 0.60 0.6666 125 0.61 0.6339 
Image of Fig. 1
Property variation of carbon/epoxy prepreg: (a) dielectrometry sensor and corresponding equivalent circuit and (b) variation of moduli and dissipation factor with temperature. 
corresponding to the forming temperature was calculated using Eq. 
2.3. Dynamic mechanical analysis (DMA) (5), and the values are listed in Table 
1. 
This permeability was used to simulate the thermoforming process of the composite laminates, Storage and loss moduli of the composite laminates at different based on previously identi.ed resin .ow behavior. The .ber temperatures were measured by a Dynamic Mechanical Analysis volume fraction was saturated at a value of 0.61 when the test (DMA) machine (Triton Technology, UK). A short beam specimen of temperature reached 125 C, generating a saturated permeability of stacked carbon/epoxy prepregs was mounted on the machine, and 0.6339. Beyond this point, the epoxy resin was regarded as having the test temperature was raised to 160 C at a rate of 1 C/min, with entered the gelation phase. a driving frequency of 1 Hz. The measured moduli are plotted in Fig. 
1b, and this data was subsequently used to simulate the forming process. 
2.4. Strain measurement of a carbon/epoxy laminate during thermoforming 
Real time in-situ strain variation in a composite laminate dur-ing vacuum bag degassing molding was investigated experimen-tally using FBG .ber optic sensors. The stacking sequence of the laminate was [05/905]s, and sensors were inserted between the third and the fourth plies along the 0 .bers, and 10th and 11th plies along the 90 .bers, as illustrated in Fig. 
2a. A dielectrometry sensor was also inserted at the edge of the laminate. A pair of steel plates was then applied to the laminate, and various accessories such as a Te.on .lm were applied to both surfaces as shown in Fig. 
2a. In order to ensure easy demolding of the composite laminate, a releasing agent was sprayed onto the surfaces of the steel mold. Wavelength shifts, which can be used to calculate strain using Eqs. 
(1) 
and 
(2), were measured for different forming time by an interrogator. In addition, the equivalent resistance and capacitance of the epoxy-impregnated dielectrometry sensor. 
1a) were measured by an LCR meter, and these values were substituted in Eq. 
(3) 
to calculate DF. The temperature was also measured by the FBG sensor in the composite laminate. The rec-ommended cure cycle is shown in Fig. 
2b. These measured strains were used for the veri.cation of the simulation technique and its accuracy. 
3. Simulation of the curing process using .nite element analysis 
To estimate the strain variation during the thermoforming of a carbon/epoxy laminate, FEA was conducted using the experimen-tally acquired data introduced in Section 
2, such as the permeability (ki) and modulus at various forming temperatures. These results were later being compared against the cure monitoring test results for verifying the appropriateness of the simulation technique. The .nite element model of the laminate is shown in Fig. 
3a. A com-mercial .nite element codedABAQUS 6.11 with C3D8HT ele-.ow based on Darcy's law was used to determine the material state when the composite laminate was in liquid state. According to the permeability variation of the epoxy resin at a certain temperature appropriate storage modulus was assigned in each FE element by VOIDRI code and the iterative calculation was carried out as shown in Fig. 
3b. 
In Phase III, changes in the coef.cient of thermal expansion and Young's modulus of the cured composite laminate were used in the FEA for precise estimation of strain as done in a related study. Simple Hooke's law was used to calculate material strains because in this phase the laminate was solid state. 
A tie condition between the laminate and steel mold was imposed for Phase I and II analyses resulting in the same strain at the interface, and a contact condition with a friction coef.cient of 
0.2 was applied at the interface between the laminate and steel mold, taking into account the partial demolding that occurred under abrupt temperature changes. And one node at the edge of the FE model was .xed in all directions for the convergence. As a loading condition hydrostatic compression of 0.1 MPa (vacuum) was applied to the FE model. 
4. Veri.cation of the simulation technique 
4.1. Experiments for strain monitoring during curing process 
shows the results of cure monitoring experiments and analyses, in which the strain was measured in 0 and 90 plies along the .bers. To identify the epoxy-resin phase changes corresponding to the curing time, DF was plotted and analyzed along with the strain variation around the onset of phase change of the epoxy resin. The DF and the measured strain were subsequently used to classify the resin phases, as shown in Fig. 
4aeb. The border between Phase I (liquid regime) and Phase II (gelation regime) is identi.ed by a peak in the DF value, which is indicative of large-scale cross-linking in the epoxy resin. 
4.1.1. Phase I 
In Phase I, the lique.ed epoxy resin shrinks under vacuum (hydrostatic compression of 0.1 MPa), generating a negative strain 
mentdwas used, with the number of elements set to 14,400. The of 230m and 223m 
analysis process itself was divided into three phases (Phases I, II, III) based on the different phases of the epoxy resin: Phase I (heating: 25 Cto125 C) is for the liquid phase, Phase II (holding: 125 C) is for the gelation of the epoxy resin, and Phase III (cooling: 125 Cto room temperature) is the solidi.cation phase of the epoxy resin, including vitri.cation. In Phase I, the deformation behavior of the composite laminate was investigated by taking into account the variation in permeability and the modulus of the laminate. To determine the change in permeability by employing FEA, the VOI-DRI code provided by ABAQUS was used. Iterative calculation of the generated strain. 
3b) was carried out by varying the permeability and modulus of the composite laminates. The mate-rial properties and permeability corresponding to the forming temperatures were updated, and once the permeability reached a value of 0.6339, the gelation phase of the epoxy resin was consid-ered to have commenced and the iteration process was terminated. As gelation proceeded in Phase II, the Poisson's ratio of the epoxy follows a trend similar to that observed in previous studies [11,12]. The gradual reduction in the viscosity of the epoxy resin with increasing temperature induces a corresponding negative strain, and this trend abruptly reverses with the instantaneous curing of the epoxy resin; i.e., around the peak DF value, the strain exhibits a minimum value, as shown in Fig. 
4aeb. 
4.1.2. Phase II 
In Phase II, the gelated epoxy resin behaves like rubber stuck to the mold surface, causing compressive strain in the thickness direction and in-plane tensile strain under vacuum conditions. At a constant temperature (125 C), the strain linearly increased with time up to the vitri.cation of the epoxy resin, as shown in Fig. 
4aeb. Consolidation of the laminate continued to occur during this period, which reduced the thickness and resulted in extension along the in-plane direction due to the high Poisson's ratio (0.48). In experiments, maximum tensile strain values of 
resins increased 
to a point (0.48) at which they could be 300m and 330m were measured in the 0 and 90 
regarded as a rubber-like hyperelastic material. The permeability and Young's modulus at a temperature of 125 C were used for calculating strains in the laminate. In those phases (Phase I and Phase II) MooneyeRivlin model was used to simulate the strains to cope with non-linear behavior of epoxy resin as a hyperelastic material and storage moduli according to temperature measured by DMA were used. The VOIDRI code which is able to express resin respectively. 
4.1.3. Phase III 
In Phase III, vitri.cation was observed around 15 min prior to the commencement of the cooling process. However, this short regime of vitri.cation was suf.cient for the epoxy resin to completely so-lidify and become hardened. During this phase, the rate of strain 
Image of Fig. 2
Fig. 2. Details of cure monitoring of a composite laminate and sensor con.guration: (a) preparation for thermoforming of a laminate and (b) recommended cure cycle. 
increase .rst reduced and then abruptly decreased with the onset In Phase III, the thermal residual strain was de.ned by the dif-of cooling. This is presumably caused by stick-slip motion (partial ference between the peak strain in Phase II and the strain when the delamination) at the interface between the composite laminate and cooling process was terminated. As shown in Fig. 
4, the laminate the steel mold, due to the signi.cant difference in thermal expan-strain decreased as the cooling process proceeded, eventually 
sion coef.cients of both materials ( 0.9 mm/m C for 0 ply and reaching 157m and 72m in0 and90 
33
plies, respectively. 
13 mm/m C for a steel mold) under abrupt cooling conditions Consequently, the generated thermal residual strain was 253m 
Fig. 3. Finite element analysis of the curing process of a composite laminate: (a) .nite element model and (b) analysis procedure. Fig. 4. Strain monitoring results: (a) 0 ply along the .ber direction, (b) 90 ply along the .ber direction, and (c) strain variation in materials during cooling (Phase III). 
4.2. Simulation of strain variation during the curing process 5. Engineering practice: simulation of thermoforming of a composite-aluminum hybrid wheel 
4.2.1. Phase I Finite element analysis predicted well the behavior of the 5.1. Simulation 
laminate under compression, showing the minimum strains of 210m and 154m in the 0 and 90 plies, respectively. The rela-To apply the simulation technique to a real composite structure, 
tively substantial difference between the experimentally deter-mined and calculated strain values of the 90 ply that was located in the center of the laminate (see Fig. 
2a), presumably comes from the large degree of deformation caused by resin .ow during this period when the analysis could not accurately simulate the precise mass transfer (see Fig. 
4b). Consequently, FEA with a linear element underestimates the deformation of epoxy in this phase. 
4.2.2. Phase II 
In analyses, a maximum tensile strain of 221m and 172m was 
a composite-aluminum hybrid wheel for a passenger car was selected and the strain variation during thermoforming was measured and evaluated. It is hoped that the results will provide a design guideline for reducing or optimally utilizing the residual stress and strain for realizing better performance in such structures [9,33]. The overall shape and dimensions of the structure are shown in Fig. 
5, and the material properties are listed in Table 
For the simulation of strain during the thermoforming of the hybrid wheel, only a quarter section of the wheel circumference was modeled with surface symmetry at the cutting surfaces. All simulation techniques and procedures were the same as those introduced in 
section 
for the composite laminate. measured in the 0 and 90 plies, respectively. The relatively large error (36% and 48%) between the experimentally measured and 
analyzed results comes from the simpli.ed analysis that only considers the laminate's mechanical behavior corresponding to changes in permeability and does not consider factors such as the 
chemical behavior of the epoxy resin and the interaction between 
the mold and the laminate. However, the overall trends of 
strain variation as per the simulation and measured results were 
5.2. Experiments 
For the experimental evaluation, 16 plies of carbon/epoxy pre-
pregs (USN125, SK Chemical, Korea) were stacked inside the 
aluminum part of a wheel along the hoop direction (0 ) along the 
circumference, as shown in Fig. 
5a. An FBG sensor was inserted 
along the direction of the .bers between the 14th and 15th plies 
similar. 
4.2.3. Phase III 
from the aluminum surface, and a dielectrometry sensor was also inserted into the composite part to monitor the strain and DF. 
5aeb). The forming conditions were the same as those in the case of the composite laminate, with the composite and aluminum parts being bonded by co-cure bonding. The major differences be-
In Phase III, the terminal strains calculated by FEA were 218m 
and 170m in0 and90 
plies, respectively. Consequently, the 
simulated thermal residual strain was 229m and 188m in the tween these two cases were the topology of the structure and the 
bonding characteristics; i.e., the hybrid wheel had a closed form 
0 and 90 plies, respectively, which were quite well estimated by 
the simulation. 
with co-cure bonding between the composite and aluminum, while 
the composite laminate had an open structure with a very weak 
4.3. Evaluation of the simulation in terms of the residual strain 
The 90 plies had a relatively low residual strain owing to their location in the center part of the laminate, which was little affected by steel molds having a much higher CTE (13 mm/m C); on the other hand, the CTE of the .ber direction of carbon/epoxy prepreg is 
0.9 mm/m C. On the other hand, the 0 plies were signi.cantly affected by the mold, since they were in direct contact with it. 
4c). The composite laminate with a stacking sequence of [05/ 905]s has an averaged CTE of 2.52 mm/m C in orthotropic directions; when this averaged CTE is used to calculate strains, it is expected to produce low magnitudes of strain in FEA, as shown in Fig. 
4c. However, from a microscopic point of view, there was strain vari-ation along the thickness direction based on the stacking angle of each ply. 
4c). Therefore, even though the FBG sensor embedded along the .bers tends not to deform the adjacent layers, a difference in CTE along a certain direction (including the steel mold) signi.cantly affects the interaction of certain plies with the sensor, thus resulting in the thermal residual strain in each ply that is shown in Fig. 
From the results of this experimental work and the FEA, the type and level of generated strain during curing was identi.ed and this information may be used to develop a means to reduce thermal residual stress or strain of composite structures by modifying their cure cycle. Through this study, the validity of the simulation tech-nique based on FEA was veri.ed, and its suitability for application to the design of any composite structure was established. 
bonding with the steel mold because of the releasing agent used. These differences generated different behaviors in the composite part during the cooling phase (Phase III). 
5.3. Evaluation and discussion 
6a shows the measured and analyzed strain during the thermoforming of the hybrid wheel, from which it can be seen that in Phases I and II, the trend of strain along .bers is the same as that in the case of composite laminate, as shown in Figs. 
and 
6. The analysis predicted well the generated strain by considering the variation in permeability with temperature. As consolidation of the stacked prepregs proceeds, the strain gradually increases due to the high Poisson's ratio of the gelated epoxy under vacuum conditions. In Phase III, the factors affecting the variation in the magnitude and type of strain generated were many, including contact conditions, anisotropic thermal properties of the com-posite, interactions with the aluminum part, and chemical re-actions such as the cure shrinkage of epoxy resin [34]. The initial strain peak in the cooling phase presumably comes from the negative CTE ( 0.9 mm/m C) of the composite, which was abruptly exposed to room temperature by opening of the auto-clave. Beyond this point, the strain gradually drops due to the shrinkage of the outer aluminum rim with a high CTE (23.76 mm/ m C), which generates radial pressure between the composite and rim parts. 
6b). The strain peak at the beginning of the cooling phase (Phase III) was not accurately predicted by the analysis because the analysis could not predict the local micro-scopic deformation of the composite but simply estimated the macroscopic behavior by considering the interaction between the 
Fig. 5. Cure monitoring of a composite-aluminum hybrid wheel: (a) shape and dimensions and (b) details of cure monitoring. 
aluminum and composite parts. On the other hand, in experi-ments, an FBG sensor was inserted into the outer layers of the composite, thereby allowing local deformation of adjacent layers of the composite laminate to be detected with minimal effect on the aluminum part. This is currently a limitation of the current FE model, which needs to be resolved for accurate analysis especially in the case of complex structures. However, from a qualitative point of view the results reveal that the composite part exhibited tensile strain under a compressive stress environment, which means that the entire structure expanded while the outer aluminum rim still provided pressure to the composite part. Such conditions enhance the fatigue strength of the structure, and therefore this simulation technique can be used to predict the level of residual strain. Furthermore, it also allows for optimal forming processes to be developed, thus leading to better struc-tural design. 
6. Conclusion 
Strain variation in a carbon/epoxy composite laminate ([05/ 905]s) during thermoforming was simulated by .nite element analysis, and for veri.cation, the generated strain was also moni-tored by employing .ber Bragg grating sensors. The generated strain varied dynamically as the phase of the epoxy resin changed from liquid to solid via a gel. The onset of the gelation point was detected by a dielectrometry sensor and indicated by the variation in strain. To simulate the strain variation accurately, the modulus variation with temperature of carbon/epoxy prepregs was measured by a performing dynamic mechanical analysis, and the variation in .ber volume fraction with temperature was measured to calculate the permeability of the composite. Based on the vari-ation in strain and the dissipation factor, the cure process was classi.ed by the three phases: Phase I (liquid), Phase II (gelation), and Phase III (vitri.cation and solidi.cation). Overall, the simulated strain agreed well with the measured values, but a relatively big difference was observed at the phase changing points (liquid to gel), especially with the 90 ply. This difference is presumed to have come from the large deformation of the ply caused by resin .ow during this period, when the analysis could not simulate the exact mass transfer and substantial deformation. In addition, the sub-stantial difference observed between Phase II and Phase III (gel to solid) was because of the simpli.ed analysis that does not consider the chemical behavior of the epoxy resin or the interaction between the mold and the laminate. This is a drawback of the current FE model, which is yet to be addressed. 
Applying the simulation technique to a composite-aluminum hybrid structure produced results similar to those for the com-posite laminate in Phases I and II; however, an abrupt increase in strain was observed from the beginning of cooling phase (Phase III), which is opposite to the trend observed in the laminate. This dif-ference in behavior came about from the difference in the topology of the two structures: i.e., an open shape for the laminate, and closed shape for the wheel. Again, the abrupt strain change at the beginning of cooling was not effectively simulated for the same reasons as those in the case of the composite laminate. However, the simulation did successfully estimate the trend of strain varia-tion and the level of residual strain, which affects the static and fatigue strengths of composite structures. This simulation tech-nique may therefore provide a useful means to reduce residual stress by changing the forming process, and enhancing the struc-tural integrity of composite structures. 
International Journal of Fatigue 

Single-Gear-Tooth Bending Fatigue of HDPE reinforced with short natural fiber
Philippe Blais, Lot. Toubal. 

Innovations Institute in Ecomaterials, Ecoproducts and Ecoenergies 每 Biomass Based, Mechanical Engineering Department, Universit谷 du Qu谷bec 角 Trois-Rivi豕res, 3351 boul. des Forges, Trois-Rivi豕res G9A 5H7, Canada 
ARTICLE INFO ABSTRACT 
Keywords: 
Fatigue Short natural .ber High density polyethylene Single gear tooth Life prediction
ABSTRACT
In this study, the mechanical behavior of high-density polyethylene (HDPE) reinforced with short natural .ber was evaluated using a test bench designed to monitor the high-cycle bending-fatigue characteristics of gear teeth. High-resolution imaging and acoustic emission (AE) were used to document crack initiation and propa-gation against the number of cycles for all specimens. Fatigue versus number of cycles was modeled using S每N curves, damage indices and a linearized Weibull distribution. Correlations between the variables are discussed and compared with published data. Crack progression, residual load, and AE show consistent results that track the progressive damage to the material. 
INTRODUCTION
1. Introduction 
Developers of new materials can no longer ignore the impact that these will have on the environment. Product life expectancy, the quantity of each component and its origin must be taken into con-sideration. Most consumer items are currently made with virgin plas-tics, which are usually derived from petroleum. For example, metal gears have been replaced by plastic gears, which o.er a better price/ performance ratio. Despite numerous bene.ts, the intensive use of plastics raises sustainability issues because of the depletion of non-re-newable petroleum resources and the pollution that is generated. 
Reinforcement with short natural .bers is proposed as a way of enhancing the mechanical properties of low-cost plastics and limiting their environmental impact. E.orts to produce this type of composite material date back to the 1980s. It was soon realized that coupling agents were needed in order to obtain performances comparable to those of technical plastics [1,2]. For a proposed composite to serve its purpose, it must be recyclable. Furthermore, the properties must be conserved after multiple injection cycles [3]. 
Optimizing the formulation of composite .llers has been the subject of more recent research [4每12]. Yellow birch is interesting because of its abundance, especially in regions such as Mauricie in the Canadian province of Qu谷bec, where paper production has been a major industry for years [13,14]. It is rarely used in this industry, and its cost would not be a.ected by demand for paper products. Its use in composites would be facilitated by .ber processing technology developed in the 



pulp and paper industry. High-density polyethylene (HDPE) is the preferred matrix for composites because of its low cost compared to other thermoplastics [6] and the possibility of producing it from bio-mass [7,8,12]. The basic mechanical properties and behavior of HDPE/ birch .ber composite have been studied [5每10]. Tension tests have shown the bene.ts of using a coupling agent [6]. In addition to bending tests and acoustic emission, creep and fatigue tests have been used to quantify durability and identify the mechanisms by which the material is damaged [5,7,8]. The four principal modes of damage identi.ed in tensile and bending tests are: matrix micro cracking, matrix/matrix friction, .ber/matrix de-bonding and matrix/.ber friction. Residual load, acoustic energy and crack propagation have been recorded [10]. Correlations are observed between these three damage indices and material lifetime. In terms of lifetime, HDPE/birch .ber composite compares advantageously with polyamide 6 [10]. 
One application suggested for composites is plastic gearing, based on the current uses of technical plastics and the growth of these markets over the years [15]. Gear thermal behavior [16], root stress [17] and critical damage mode [18] have been evaluated. Carbon particles have been found to enhance the friction performance of POM gears [19] by changing the critical fatigue mode at lower material cost. However, carbon is not comparable to short natural .bers. Little information has been published on short-.ber composite gears. The use of natural .ber composites in structural applications is being delayed pending further safety and reliability studies. 
Like carbon-reinforced POM, .ber-reinforced composite costs less 


Received 22 May 2020; Received in revised form 16 July 2020; Accepted 24 July 2020 
Available online 28 July 2020

Fig. 1. Sequence of tests in the evaluation of a new material for gearing applications. 
than the unreinforced plastic while o.ering basic mechanical properties better than those of the plastic, based on thermal behavior and lifetime in gear rotation tests [12]. Mixed damage modes are observed, such as root cracking, thermal surface wear and general thermal degradation. The thermal behavior of the HDPE/birch composite has been evaluated [12], but bending behavior has been examined only using test samples [10]. The next step is to observe how wear evolves in a gear made of the material [20]. In the case of gears, the next most signi.cant cause of failure after thermal e.ects is tooth root fatigue [18]. No studies have been conducted on the single-gear-tooth fatigue behavior of natural .ber composites. To focus on root fatigue, tooth bending is proposed as an intermediate test between three-point-bending [10] and rotating gear tests [12]. 
Fig. 1 shows the progression of the speci.city of gear material fa-tigue tests. Compared to three-point-bending, tooth bending focuses the stress on the tooth base and adds a shear stress component. It di.ers from rotating tests by involving no slippage and almost no thermal e.ect. Plastic gears are used usually in low-torque and long-run appli-cations, where wear and root fatigue are more likely to occur, if the gear train is properly designed. 
Gear tooth deterioration due to bending is di.cult to assess in a high rotation test because of the speed of movement. In addition, some of the deterioration will be due to heat associated with friction. The thermal component is negligible when a single non-moving tooth is tested. Numerous studies show that material fatigue occurs in single teeth in bending tests [21每25] 
using a bench based on SAE standard J1619 [26]. In every case, tooth life ends with root cracking, which is the focus of this study. 
To observe the bending behavior of composite reinforced with birch short .ber in gearing applications, a test bench was designed for op-eration in static and fatigue modes. Material deterioration (i.e. gear tooth root cracking) was monitored throughout the fatigue tests. Two non-destructive evaluation techniques (acoustic emission and digital image correlation) have proven their e.ectiveness in the monitoring of gear tooth root fatigue damage. A Weibull distribution was then used to model the material lifetime probabilities. 
In the sections that follow, we describe the process used to make the gears, the test bench, the static test procedure and the fatigue tests. Static test results for the composite and the unreinforced HDPE are compared. Fatigue data, including damage progression and gear life-time, are compared with published values. Finally, the Weibull dis-tributions are shown for each displacement and compared with the literature. 
METHOD
2. Experimentation 
2.1. Materials and sample production 
Yellow birch thermomechanical wood pulp .ber (Fig. 2) was pre-pared at the Innovations Institute in Ecomaterials, Ecoproducts and Ecoenergies, biomass based (I2E3) at Universit谷 du Qu谷bec 角 Trois-Rivi豕res (UQTR). Wood .ber was dried at 80 ∼C in an air-circulating oven for 24 h and then ground to 20每60 mesh size before use. The .ber aspect ratio (mean length divided by average diameter) classes were obtained by mechanical re.ning and screening and characterized using an OpTest .ber quality analyzer (Table 1). 
The bio-composite consists of a matrix (HDPE), reinforcing material (yellow birch short .ber, average length 0.49 mm, average thickness 24.7 米m) and a coupling agent (maleic-anhydride-grafted polyethylene, aka MAPE) (Fig. 2). Their proportion in mass are 57% of HDPE, 40% of .bers and 3% of MAPE. The material base mechanical properties are shown in Table 
1. Those are experimental mechanical properties obtain during previous studies [6每8,27] at the exception of the MAPE prop-erties and those marked with an asterix. MAPE properties are approx-imate to generic low density polyethylene (LDPE) in which the maleic hydride is dissolved, and the ones marked with an asterix are evaluated using general rule of mixtures. The adapted rule of mixture of La-vengoof-Goettler [27] gives a Young＊s modulus value of 3.31 GPa. This value is lower than the experimental value obtained. Birch .ber Poisson ratio and tensile strength are calculated with the standard rule of mixture since their length doesn＊t allow standard characterization. The HDPE and MAPE were mixed on heated rollers at 190 ∼C (the production process is shown schematically in Fig. 3) and the .bers were added slowly to keep the mixture in a molten state. Once out of the rollers, the composite was ground to granules 1每3 mm in diameter. 
The gear mold was .lled with granules and heated to 190 ∼C. The melted composite was mixed thoroughly before applying 5.5 MPa of pressure for ten minutes. The press was then water cooled. A central hole was drilled in the de-molded gear (Fig.4a). The thickness was 6.35 mm, the diameter was 76.2 mm and the 30 teeth had a diametral pitch of 10 per inch. A tooth was removed to facilitate mounting on the test bench. 
2.2. Mechanical tests 
2.2.1. Quasi-static bending tests 
The main purpose of the static tests was to evaluate the elastic properties, .exural strength and fracture mode of the composite. Three loading speeds were chosen to represent the progression between quasi-static speed and a fatigue test loading speed. A 10 Hz [10] test at 0.5 mm displacement has a speed of 942 mm/min, which is over the maximum of 500 mm/min of our static test machine. The speeds were therefore 500 mm/min, 50 mm/min and 5 mm/min to assess the strain hardening e.ect. These speeds are in accordance with ISO 178:2019 standard practice for plastics determination of .exural properties. An Instron LM-U150 equipped with a 150 kN load cell was used. 
Adjustable to multiple gear sizes and contact points, the single-tooth test bench (Fig. 5) is designed to ensure that the test tooth fails before the locking tooth. The test bench applies the load in a linear motion instead of a circular motion as proposed by the SAE J1619 standard. The main reason is the allowance of future testing on di.erent gear sizes that is easier and cheaper to design with a linear motion. A tri-angular abutment in contact with the former and a square abutment next to the latter oppose the clockwise rotation of the gear. The locking tooth abutment is mounted on a mechanical jack to allow adjustment. A one-tooth separation ensures that the pressure points do not interact. The test tooth abutment is supported horizontally by a bearing to prevent lateral load due to the angle of action relative to the load cell. Therefore, 9 teeth out of 30 can be tested as shown on Fig. 4 
b). The tooth number 3 is tested .rst with tooth number 1 as the locking tooth, 

Fig. 2. Base materials of the proposed composite.  
Table 1 Base material mechanical properties.  Birch 
Elastic modulus (GPa) 13.9 1.27 0.12每0.55 4.48 Elongation at break (%) 每 590 450每810 2.44 Poisson Ratio 0.17* 0.46 每 0.33 Tensile Strength (MPa) 149.67* 22.0 16 38.05 Mean length, L (mm) 0.49 每每 每 Mean width, D (米m) 24.7 每每 每 Aspect ratio (L/D) 19.8 每每 每 
then tooth number 6 with tooth number 4 as the locking tooth and so on. 


2.2.2. Bending fatigue tests 
The same bench was used with an MTS hydraulic machine equipped with a 100 kN load cell. Fatigue tests were performed using displace-ment control. They were performed using sinusoidal displacement control at a 10 Hz loading frequency. The displacement ratio (R = 汛min/汛max) was 0 for these tests. This ratio is chosen because a real gear tooth has its load removed between each gear revolution. The choice of frequency was based on experimental fatigue tests conducted on composite gears at rotational speeds ranging from 500 rpm to 1500 rpm [12] 
and corresponding respectively to 8 Hz and 25 Hz. The minimum value is chosen to obtain an endurance limit and the other values are chosen with the minimal increment. An increment of 
0.03 mm was necessary to ensure that the displacements were distinct, since the low reaction force from the gear lowered the precision of the position reset. The resulting .ve cases are: 0.46 mm, 0.49 mm, 
0.52 mm, 0.55 mm and 0.58 mm. 
The objectives of these tests were to investigate the mechanical behavior of and damage to bio-composite under fatigue loading and to estimate the endurance limit of single gear tooth. A target of 15 samples by load case is .xed to allow further statistical analysis of the cumu-lative damage and its evolution during the fatigue tests. A high-speed charge-coupled device (CCD) camera with a 2D digital image correla-tion system (StrainMaster, LaVision Inc.) and acoustic emission sensors were used to assess crack initiation and evolution. Two acoustic sensors were attached to the gear, as shown in Fig. 
6. An acoustic threshold level set at 30 dB was used to .lter background noise. The quality of the acoustic data mainly depends on the choice of the waveform system timing parameters, namely the peak de.nition time, hit de.nition time and hit lockout time, which were set respectively at 40 米s, 80 米s and 200 米s [5]. Cumulative damage was recorded. The CCD camera was used to record macro-cracks on the gear tooth surface (Fig. 6). The Fig. 3. Schematic summary of the composite production process. 
P. Blais and L. Toubal International Journal of Fatigue 141 (2020) 105857 

Fig. 4. A test gear made of HDPE/birch .ber composite; (a) test gear side view, (b) schematic gear test dimensions and test order. 
camera sensor dimensions were 7.2 mm ℅ 5.4 mm, the resolution was 1628 ℅ 1236 pixels and maximal update rate was 14 frames per second. Images were captured until complete failure of the gear tooth. A precision of 0.02 mm was possible with the camera, but a precision of 
0.2 mm was obtained by the program transferring the data from the images to numerical values. Tooth lifetime was based on crack initia-tion, because of the speed of propagation to 50% of the tooth width. Since crack length was measured only on one side of the gear, it could have been longer below the surface or on the other side, making crack initiation a more objective threshold. 
The three damage indices measured in the fatigue test were crack length, residual load and acoustic emission. Their purpose is to show the progression of damage before and after the crack initiation. Thermal readings were obtained in preliminary tests at each displacement, but were not distinguishable from the thermal camera variance, which is about 3 ∼C. As in previous studies [10], self-heating was neglected. 
The end of the second phase of the residual load was evaluated as a criterion for determining life limit for comparison purposes. The max-imum base tooth stress (考) was estimated using the AGMA equation [28] shown below (1). This equation is accessible since data on each factor are abundant and a single gear tooth is loaded over a small range of its normal use. 
考 is the calculated stress at the base of the tooth, Wt is the tangent load, KO is the overload factor, KV is the dynamic factor, KS the dimensional factor, Pd is the diametral pitch (10 teeth/inch), F is the tooth width 
(0.25 in.), Km is the load distribution factor, KB is the rim thickness factor and J is the form factor. The overload factor is set to compensate for any impact that could occur form the controlled machine or power source. For our experiment, where the gear is .xed and the ※controlled machine§ is a hydraulic fatigue machine. KO is then set to 1. The dy-namic factor is a function of loading speed and material .nish. It gets closer to 1 a smoother .nish and a slower loading speed. It is therefore .xed as 1 [29]. The dimensional factor takes into consideration the tooth width, Lewis geometrical factor and diametral pitch. The calcu-lated value of the dimensional is 0.944, it is then set to 1 according to Budynas and al. [29]. The gear geometry has a diametral pitch of 10 teeth/inch and a width of 0.25 in. The load distribution factor takes into consideration the load non-uniformities over the span of the line of action. Since our test bench loads the tooth at only one point of the line of action, the load distribution factor is set to 1. The gear is not a rim gear, so the rim thickness factor is neglected, and the form factor is equal to 0.26. 
RESULT AND DISCUSSION
3. Results and discussion 
3.1. Single-tooth quasi-static loading 
Complete static results are shown in Table 
2. A strain hardening e.ect can be inferred from the increase of the force as the loading speed increases whereas the displacement at failure decreases. These e.ects appear on the typical curve for each loading speed and material shown in Fig. 7. Two materials are being tested for comparison purposes. The composite is the main subject of the study and pure HDPE represents the composite matrix, which is the base material we are trying to en-hance. 
Both materials, either the composite or the pure HDPE show dif-ferent behavior for the same static test and geometry. Pure HDPE shows non-linear behavior before failure, whereas the composite shows fragile behavior. A reason for this e.ect is the pure HDPE plastic deformation. Pure HDPE undergoes considerable plastic deformation, which reduces the concentration of stress at the tooth base by increasing the base 

Fig. 5. The test bench with a sample mounted for fatigue testing (side view). 
Fig. 6. Acoustic emission sensors (left) and side view of the CCD camera setup. 
Table 2 	Table 3 
Single gear tooth results: Static .exural max load and displacement at failure. Quasi-static stress for unreinforced and reinforced HDPE [11]. 



The same e.ect is seen on the stress maxima (Table 3). Unreinforced shows a higher stress at failure than the composite and the three-point-bending equivalent. In addition to elastic deformation reducing the stress concentration, since the stress at the tooth base reaches yield levels, the stress concentration decreases even more as plastic de-Average formation increases. In the case of unreinforced HDPE, the evaluated stress is above the real stress. The evaluated stress doesn＊t consider the plastic deformation and the resulting decrease of the concentration Unreinforced HDPE 
 factor. Reinforced samples show a higher stress maximum than three-point bending, partly due to the indistinguishable local plastic zone that reduces the stress concentration. However, this is not factored into stress calculation. 
 In terms of sti.nesses (Fig. 8), adding .ber to the HDPE matrix  seemed to have little to no impact on the strain hardening e.ect. As mentioned above, birch .ber increased the overall sti.ness of the 
Average thermoplastic. The sti.ness was measured using the linear elastic sec-tion of the load curves obtained for each type of sample. 

3.2. Gear tooth lifetime and damage progression 
The experimental results are summarised in the displacement cycles (汛-N) charts in Fig. 
9, which shows the progression of the number of cycles at crack initiation for each displacement and HDPE content. This 
radius. This leads to a displacement at failure a lot higher than the composite. On the other hand, the composite shows a load nearly twice that of the pure HDPE for the same displacement and loading speed. No concentration relief takes place before the composite resistance is reached, because near to no plastic deformation takes place at the tooth 
Fig. 9. Distribution of gear tooth fatigue at crack initiation. 
standard semi-log fatigue diagram is built by plotting the amplitude of the imposed displacement level 汛 against the number of cycles to crack initiation Nf. The dots represent individual samples and the dotted lines represent Weibull 10%, 50% and 90% failure distribution marks. The displacements are presented as a percentage of the corresponding static failure displacement. An arrowhead at 106 cycles indicates that these specimens did not fail at that displacement level. The high displacement at failure of unreinforced HDPE sets its relative displacement well below that of the reinforced material. The high cycle fatigue strength of reinforced HDPE is about 39% of its ultimate displacement (U汛) and about 18% for unreinforced. The inherent variability of fatigue lifetime may explain in part the rather large degree of scatter apparent in the data. 
For further comparison with previous studies, gear tooth life esti-mated on the basis of the end of the residual load stable phase was considered as the equivalent stress that the tooth would have endured if it had been tested under constant force rather than constant displace-ment. Typical three damage index graphs for both reinforced and un-reinforced material are shown on Fig. 
10. The red circles indicate the end of the residual load second phase end for both materials. Fig. 11 and Table 4 
show that the lifetimes were shorter than estimated pre-viously. This is due to the stress and load measurement methods. 
Table 4 
Second lifetime evaluation and comparison of reinforced samples. 
Stress  Lifetime  Author  

Assuming the use of a full gear matched with a rack or another gear of which the tooth is loaded over its entire length, the AGMA equation underestimates the real stress on the tooth. The test bench used in the present study loads the test tooth only at its tip to ensure that it breaks before the locking tooth, which represents the worst loading case [30]. In addition, the steady phase recorded previously [10] is much .atter than those recorded in the present tests, which would further under-estimate the stress calculated at the end of the phase. 
Since all three 0.46 mm tests reached 106 cycles and showed little to no damage, analysis was limited to the four larger displacement cases, plotted in Fig. 12. The damage index is plotted as the average number of cycles needed to reach a damage level. Error bars show the standard deviation of the damage index at di.erent points of the indices. Pre-vious work by Bravo and al. [12] 
on functional gear train shows that the main cause of failure is root fatigue. The present study emphasizes on root fatigue and the load being applied at the tooth tip makes the lifetimes shorter than the ones recorded by Bravo and al. [12]. The principal reason why this study takes place is to characterize the da-mage progression to root failure and Fig. 11 shows that the root failures obtained during this study show a trend close to the one of a func-tioning gear train. 
The cracking index represents the crack length relative to its max-imum possible length: 


where ICracking is the damage index, tc is the crack length and td is the tooth thickness at the crack location. 
The residual load index is given by: 
RLi (3) where IRL is the residual load damage index, RLN is the current residual load value and RLi is the residual load of the .rst cycle. 

The acoustic emission index is the cumulative number of counts relative to the total number of counts: 

where IAE is the acoustic emission damage index, CN is the current emission number of counts and Cf is the .nal number of counts. 
All three indices obtained are between 0 and 1. A damage index is a dimensionless number which varies from zero to one, and the failure should occur when the index reaches one. The use of indices between 0 and 1 is arbitrary and means an adjustment of the values measured on di.erent scales to a common scale. That will help the comparison be-tween the three indices and with other samples. For a literature com-parison, values up to 50% in sti.ness reduction are used as a failure criterion in long .bre composite materials [31]. 
Some interactions between the indices can be observed. The longer the crack, the lower the sti.ness and the smaller the reaction load. When a crack reached 80% of its possible length, the reaction load was very low, and progression sometimes stopped. If an advancing crack met a bunch of suitably oriented .bers when the residual load was low, it probably stopped [32每34]. If it met few or no such .bers, it grew to more than 90% of the possible length. This is why cracking standard deviation gets larger around 80% of progression. The 0.58 mm case 
P. Blais and L. Toubal does not show this trend because a larger displacement yielded to a larger load and imposed the continuity of the cracking process with a lower probability of stoppage. 
In reinforced HDPE, acoustic emission occurred principally during crack progression. Unlike in unreinforced HDPE, it took place before and during plastic deformation, as shown in Fig. 
10. Fig. 13 shows crack paths in both materials. In reinforced HDPE (b), cracks changed di-rection more, which requires more energy than propagating in a straight line. Much acoustic activity also occurred after the end of crack progression. This is due to friction between crack surfaces. Since ma-trix/matrix friction is known as one of the four damage modes visua-lizable by acoustic emission [5,7], the texture of the crack suggests an abundance of friction. 
Residual load changed in 3 phases: rapid initial decay, a constant phase and a .nal decrease. The initial drop in residual load is due to the cyclic accommodation of HDPE [35] and is followed by degradation at a lower rate. The end of the second phase coincided with crack initia-tion. The third phase started with crack initiation and .nished along with crack progression. 
These phases appear to progress di.erently in three-point bending tests [10]. Crack initiation may span about 30% of the total life of the sample, and the initial and rapid phase may be much shorter. Another stage characterized by the crack progressing faster to failure may occur near the end of the test. No such stage was observed in the present study. 
The main goal is not to enhance the material durability, but to get a better understanding of the damage process. The composite gives an indication of the approaching failure of the tooth. The crack propaga-tion to 40% is fast, but the overall progression is slower than the un-reinforced HDPE. Moreover, the start of acoustic emission correlates with crack initiation. giving another indication of the failure＊s ap-proach. 
3.3. Weibull distribution plotting 
The Weibull distribution is commonly used to describe the prob-ability of material failure. The cumulative Weibull distribution is de-.ned as: 


Where N is the number of load cycles, PR is the probability that the sample is fractured at N cycles, 汐 is the Weibull shape coe.cient and N0 is the characteristic lifetime in load cycles. 
The linearized Weibull distributions illustrated in Fig. 14 show a linear .t with its correlation coe.cient. The dots represent the ex-perimental results. The Weibull shape coe.cient and lifetime are shown in Table 5. A shape coe.cient below 2 for all four cases indicates a broad distribution of lifetime. The values are similar, the lowest corre-sponding to 0.49 mm displacement, at which fatigue was slowest and (logically) the most variable. The characteristic lifetime increased with decreasing displacement, as expected. Distribution width is associated with the material being short .ber composite, due to the major impact of random .ber orientation and local concentration on the mechanical properties and hence the fatigue-determined lifetime [32,36]. 


Table 5 Weibull distribution parameters.  0.49 mm  0.52 mm  0.55 mm  0.58 mm  
N0 (charac. lifetime) 汐 (shape)  132 145 ㊣ 35 233 1.23 ㊣ 0.17  66 664 ㊣ 7 766 1.73 ㊣ 0.10  40 005 ㊣ 10 031 1.57 ㊣ 0.20  23 707 ㊣ 3 222 1.54 ㊣ 0.11  

Fig. shows the experimental (dots) and Weibull (smooth line) cumulative distributions. The four displacement cases are shown and identi.ed with their absolute displacement value in mm and their re-lative to ultimate displacement value in percentage. A two-shoulders distribution can be interpreted from one or two of the distributions. However, the standard Weibull distribution seems satisfactory for the current study since the coe.cient of determination R2 is at least 0.85. For distributions with 12 samples, namely 0.49 mm and 0.55 mm, R2 is respectively 0.85 and 0.86. For those with 15 samples, namely 0.52 mm and 0.58 mm, R2 is respectively 0.96 and 0.95. The fatigue behavior is thus considered with at least 85% con.dence to be of Weibull type. 
CONCLUSION
4. Conclusion 
The HDPE/birch .ber composite material tested in this study could be used to manufacture ecofriendly machine parts at lower cost. The .bres cost is around 1.07 USD$/kg [6], HDPE is at 1.20 USD$/kg, PA66 is around 1.85 USD$/kg, ABS is around 2.50 USD$/kg and PEEK is around 41.00 USD$/kg. The addition of .bers to HDPE reduces the cost of the material which is already lower than the cost of other high re-sistance plastics. Blends containing up to 40% short natural .bers by weight have static mechanical properties comparable to those of poly-amide 66. The material has been tested in bulk form and in rotating gear tests. In this study, it was subjected to the single-gear-tooth bending test, which allows evaluation of bending behavior without friction and thermal degradation. 
The customized test bench allowed static testing at di.erent speeds and comparison of the composite and the pure thermoplastic in terms of fracture behavior and other properties. Fatigue tests run on the same bench with four displacement factors showed the evolution of three damage indices: crack length, residual load and acoustic emission. A large number of specimens were tested, and three damage indices were evaluated to study the fatigue behaviour of single-gear-tooth bending of HDPE reinforced with short natural .ber. Crack progression, residual load and acoustic emission show consistent results that track the pro-gressive damage to the material. Crack initiation appears more suitable for evaluating gear tooth lifetime since subsequent crack progression may be very fast. Cracking slows down when the displacement is .xed, and the reaction load thus decreases as the crack process advances. 
Although the trends were comparable with published results, var-iations in material lifetime were noted. Lifetime was modelled using a linearized form of the Weibull distribution. The model shows a good .t and provides insight into the lifetime distribution of the material under fatigue. Material variability was likely the source of the wide range of responses, since the .ber distribution and orientation are random. 
Future studies will focus on designing an industrial molding process that reduces material variability and brings HDPE/birch .ber compo-site material closer to market readiness. All improvements thus ob-tained will be tested by comparing the composite to pure technical plastic using the gear tooth test described in this study. 
Declaration of Competing Interest 
The authors declare that they have no known competing .nancial interests or personal relationships that could have appeared to in.u-ence the work reported in this paper. 
Acknowledgements 
The authors would like to acknowledge the .nancial support of the Natural Sciences and Engineering Research Council (NSERC) of Canada. 








Surface roughness effects on the fatigue strength of additively manufactured Ti-6Al-4V 
Jonathan Peguesa,b, Michael Roachc, R. Scott Williamsonc, Nima Shamsaeia,b,. 

a Department of Mechanical Engineering, Auburn University, Auburn, AL 36849, United States b National Center for Additive Manufacturing Excellence (NCAME), Auburn University, Auburn, AL 36849, United States c Department of Biomedical Materials Science, University of Mississippi Medical Center, Jackson, MS 39216, United States 
ARTICLE INFO ABSTRACT 
Keywords: 
Additive manufacturing Fatigue behavior Surface roughness Size effects Geometry 
ABSTRACT
Additive manufacturing has become an increasingly popular advanced manufacturing technique, however, many questions concerning the reliability of parts fabricated by methods such as laser powder bed fusion must be addressed. In this research, the e.ect of surface roughness and size is investigated by designing various addi-tively manufactured Ti-6Al-4V specimen geometries. These as-built specimens were designed to speci.cally determine the e.ect of surface area and part diameter on the fatigue behavior of specimens fabricated diagonally from the substrate. Results indicate that the fatigue behavior is more sensitive to part diameter than surface area. Parts with diameters of 4.90 mm or less showed higher surface roughness on the down-skin surface. This var-iation diminished, however, for specimens with diameters larger than 4.90 mm. Additionally, as part diameter decreased, the di.erence between the load-bearing and nominal stress amplitudes, caused by surface roughness, increased, resulting in signi.cant scatter in the high cycle fatigue data. 
INTRODUCTION
1. Introduction 
Additive manufacturing has become an increasingly popular man-ufacturing technique offering large design freedoms and a significant potential for fabricating optimized and customized parts with complex geometries. In the aerospace industry, topology optimization can be utilized to create complex geometries that can reduce weight of struc-tural parts, and thus, e.ectively increasing fuel e.ciency. These com-plex geometries often cannot be fabricated through traditional sub-tractive methods, however, additive manufacturing has shown the ability to produce such geometries [1]. In the biomedical industry, additive manufacturing promises customized parts, per patient/injury, that can be readily tailored to the patient ensuring a better .t. While titanium alloys such as Ti-6Al-4V are commonly used in both aerospace parts and biomedical implants due to their high strength-to-weight ratio, excellent mechanical properties, and biocompatibility [2每5], they are also compatible with additive manufacturing processes because of their weld-ability. One of the major obstacles facing full implementa-tion of additive manufactured (AM) Ti-6Al-4V parts, however, is the concern with the reliability, especially under cyclic loading conditions. While extensive research has been conducted on the fatigue behavior of traditional wrought Ti-6Al-4V [5每12], there are needs for further in-vestigating the fatigue behavior of AM counterparts due to their structural imperfections such as defects/pores and increased surface roughness [13每16]. 
The reliability and durability concerns for AM parts have driven a recent surge of research into the fatigue behavior of AM parts [13每15,17,18]. Defects associated with AM parts such as porosity and lack of fusion can reduce the fatigue life leading to lower endurance limits compared to traditional wrought alloys. Additionally, as-built AM parts have been shown to have even lower fatigue limits when com-pared to parts that underwent post-process machining and polishing [14,15]. This is attributed to the high surface roughness of the as-built parts in which the micro-notches associated with partially melted powders on the surface act as stress concentrators resulting in earlier crack initiation [19]. Parts fabricated diagonally have been shown to have increased surface roughness on the down-skin surface, which can further reduce the fatigue strength of AM parts [20,21]. 
There have been many investigations that attempt to characterize the e.ect of surface roughness on the fatigue behavior of both wrought and AM materials with both conditions showing mixed results [22每27]. Removal of the surface roughness through machining and polishing is necessary for AM parts to achieve fatigue limits that are more com-parable to wrought materials. However, one of the most bene.cial promises of additive manufacturing technology is the ability to manu-facture net shape parts without the need of further subtractive 

machining or polishing. For this reason, it is imperative to better un-derstand the fatigue performance of AM parts fabricated in the net shape condition without any surface enhancements before testing. Additionally, the range of AM part sizes can vary from small medical implants to large structural components in which the fatigue behavior of the net shape parts may di.er between these contrasting sizes. 
The objective of this research is to investigate the e.ect of part size in relation to surface roughness, surface area, and gage diameter on the fatigue behavior of laser-based powder bed fusion (L-PBF) specimens in as-built condition. Directly correlating these specimen properties to the corresponding fatigue performance for various part sizes is critical to reliably design AM parts with the ability to withstand the service loadings associated with a given application. 
METHOD
2. Materials and methods 
A series of .ve specimen geometries were designed to investigate the e.ect of part size on the fatigue behavior of L-PBF Ti-6Al-4V. The five specimen geometries are shown in Fig. 1 and can be separated into two distinct groups. The .rst of these groups maintains constant gage volume while incrementally increasing the gage surface area by 25%. These geometries are referred to as the constant gage volume (CGV) specimens and shown as Geometries 1每3in Fig. 1. In order to maintain the constant volume between Geometries 1每3 while increasing the surface area, the gage diameter and gage lengths were adjusted accordingly. The second specimen group maintains constant gage dia-meter while incrementally increasing the gage surface area by 25% as shown for Geometries 4 and 5 in Fig. 1. These specimens are referred to as the constant gage diameter (CGD) and are designed to account for any e.ect that the change in the gage diameter may have on the fatigue behavior. The sub-sized geometries were chosen to allow each group to be fabricated during the same build to limit variability between tests. 
Parts were fabricated on an EOS M290 (EOS, Krailling, Germany) AM machine with a maximum laser power of 400 W. The process parameters used for fabrication were the suggested performance para-meters provided by EOS for Ti-6Al-4V components. This set of process parameters include a laser power of 280 W, a scan speed of 1200 mm/s, and a hatch distance of 0.140 mm. The powder was supplied by LPW Technology Inc. (LPW, Runcorn, UK) and had a particle size range of 15每45 米m. All specimens were fabricated at an angle of 45∼ from the substrate. Before removal from the substrate, the parts were stress re-lieved at 700 ∼C for one hour, and air cooled to room temperature. The time and temperature were selected to maintain the as-built micro-structure while minimizing the e.ects of residual stress that arise from the high cooling rates associated with L-PBF. 
The surface roughness of the up-skin and down-skin surfaces of each specimen was measured at two areas within the gage section using a Keyence VHX-1000 (Keyence, Osaka, Japan) digital microscope. The down-skin and up-skin surfaces are shown in Fig. 2 (not to scale) and are termed in relation to the build direction. Down-skin refers to sur-faces in which the vertical component of the vector normal to the surface is in the opposite direction of the build progression. This occurs in overhangs or, as in this case, surfaces that are diagonal and directed opposite of the build direction. Up-skin refers to surfaces where the vertical component of the vector normal to the surface is in the same direction as the build progression. Representative measurements for a down-skin surface are schematically shown in Fig. 3 where the 3D surface map was collected from each end of the uniform gage section. Triplicate line measurements were taken from each surface map, re-sulting in six total measurements for each up-skin and down skin surface. The arithmetic mean roughness value (Ra) was calculated for each line measurement and an average for each up-skin or down-skin surface was obtained. Statistical ANOVA (analysis of variance) (汐 = 0.05) was performed on the surface roughness measurements taken from each geometry to determine if the roughness between the given geometries were statistically di.erent. 
Microstructure analysis was performed on representative samples within the gage section. The general microstructure was revealed using Kroll＊s reagent and optical microscopy. Additionally, bulk x-ray dif-fraction (XRD), scanning electron microscopy, and electron backscatter di.raction (EBSD) analyses were performed to determine the phase fraction and equilibrium grain shape and size. Bulk XRD was performed on a Scintag XDS2000 (Scintag, Franklin, MA) di.ractometer and the EBSD analysis was performed on a Zeiss Supra 40 FEG (Zeiss, Oberkochen, Germany) scanning electron microscope (SEM) with EDAX OIM (EDAX, Draper, UT) analysis software. 
Fatigue testing was conducted on an MTS servo-hydraulic testing frame (MTS, Eden Prairie, MN) with a 100 kN capacity. For each spe-cimen type, a set of four fully reversed (R = .1) stress amplitudes were selected to provide low-cycle fatigue (LCF), intermediate-cycle fatigue, and high-cycle fatigue (HCF) lives. Before testing, the cross-sectional area was initially measured with calipers to select an appropriate loading force for the desired nominal stress amplitude, 考a. These initial measurements did not provide an adequate description of the load-bearing cross-sectional area due to the high surface roughness. Upon failure, fracture surfaces were removed and prepared for analytical fractography, which was conducted on a Zeiss Supra 40 FEG SEM. In addition to fractography, the fracture surfaces were measured using digital imaging and ImageJ software to give a more accurate mea-surement of the load-bearing cross-sectional area. Fig. 4 
gives a re-presentation of the initial caliper measured cross-section in red1 and the more representative load-bearing cross-sectional area in blue1, which was then used to calculate a more representative applied stress ampli-tude for the stress-life curves. 
RESULT
3. Experimental results 

3.1. Microstructure 
The general microstructure of the AM Ti-6Al-4V parts consisted of semi-columnar prior 汕 grains with the major axis parallel to the build direction. The columnar microstructure had abrupt changes in the prior 汕 grain orientation resulting in the broken columnar microstructure observed. A scaled representation of the microstructure is given in the zoomed in area of Fig. 
5. As can be seen, the major axis of the prior 汕 grains are oriented at an angle of 45∼ from the loading direction, in a broken epitaxial manner along the build direction. The prior 汕 grains have abrupt interruptions in the epitaxial growth which results in new grains with di.erent orientations, referred to as stray grains by the welding community [28每30]. Anderson and Dupont [29] 
concluded that, during the welding process of a Ni-based super alloy, stray grain formation increases with increasing travel speed and power, which is related to the high thermal gradient. The L-PBF process inherently re-sults in large thermal gradients due to the extremely high cooling rates contributing to the stray columnar prior 汕 grain structure. 
In addition to optical microscopy, EBSD and XRD were conducted to better analyze the microstructure. The results show a fully martensitic microstructure as indicated by the phase map, acicular grain structure, and absence of the equilibrium 汕 phase, as detailed in Fig. 6(b). In general, most wrought Ti-6Al-4V alloys have an equilibrium 汐 + 汕 phase in which the 汐 phase grows from the 汕 phase upon relatively slow cooling during thermal treatment. The absence of the equilibrium 汕 phase indicates that there was not enough time during cooling in L-PBF process to allow the 汐 phase to di.use from the 汕 phase, and thus, the di.usionless 汕 ↙ 汐∩ martensitic phase transformation occurred [31,32]. The 汐∩ grain size was measured using the EBSD data and OIM software giving an average width of 1.5 米m with an average aspect ratio of 0.55. The inverse pole .gure map, in Fig. 6(a), con.rms that most grains are acicular, which is commonly related to the martensitic phase in tita-nium alloys. 


3.2. Fatigue behavior 
The stress-life fatigue behavior of wrought Ti-6Al-4V has been well studied and shown to have a fatigue limit in the range of 550每750 MPa [33]. The stress-life curves for both CGV and CGD specimens with the appropriate stress amplitudes based on the load-bearing cross-sectional area are given in Fig. 7(a) and (b), respectively. The data is plotted on a semi-log scale to give a more consistent representation of the variation between data points that would diminish on a log每log plot due to the low stress amplitude range (75每450 MPa) tested. The fatigue limit for these specimens falls between 75 and 120 MPa which is in stark con-trast to the ranges observed for wrought Ti-6Al-4V alloys (550每750 MPa). The reduced fatigue limit is likely due to the high surface roughness, which results in many prime crack nucleation sites at the e.ective notch-like areas along the rough surface highlighted in red1 in Fig. 4. Additionally, the martensitic microstructure has also been shown to have a detrimental e.ect on the fatigue behavior of ti-tanium alloys [34]. A combination of these two e.ects can result in the drastic reduction of the fatigue limit for L-PBF produced Ti-6Al-4V parts in the as-built condition. 
The stress-life fatigue behavior of CGV specimens is presented in Fig. 7(a), in which the hollow symbols indicate tests that reached failure, while the solid symbols indicate tests that did not fail before 107 cycles. The stress-life data shows a slight increase in fatigue life for Geometry 1 which had the smallest surface area; however, Geometries 2 and 3 exhibited similar fatigue lives for nominal stress amplitudes larger than 100 MPa. Furthermore, the nominal stress amplitude of 100 MPa resulted in fatigue lives longer than 107 cycles for specimens in Geometries 1 and 2, while Geometry 3 required a reduction to 75 MPa to achieve a fatigue life longer than 107 cycles. These results indicate that the di.erence in fatigue behavior across all geometries may be related to other factors besides surface area, as was described by Pegues et al. [35]. 
The .nal two specimen sets, CGD Geometries 4 and 5, were de-signed to evaluate any size e.ects as a result of the decreasing cross-sectional diameter in CGV specimens. The diameters of these CGD specimens (Geometries 4 and 5) were held constant while the gage length was increased from 9.75 mm in Geometry 4 to 12.00 mm in Geometry 5 producing an increased surface area of 25%. Therefore, the CGD geometries have di.erent surface areas and similar diameters to determine if the di.erences in fatigue behavior between Geometry 1每3 were related to surface area or cross-sectional diameter. The CGD stress-life fatigue data set was generated in a similar fashion to the CGV specimens and is plotted in Fig. 
7(b). The stress-life behavior of Geo-metries 4 and 5 were similar to Geometry 3 in which a stress amplitude of 75 MPa was required to obtain a runout past 107 cycles. There was an unexpected large amount of scatter for Geometry 4 compared to the other geometries for stress amplitudes around 100 MPa. Interestingly, the Geometry 4 specimen exhibiting the longest fatigue life had slightly lower 考a, as determined by the load-bearing cross-section, compared to the other two specimens at this nominal stress amplitude. 
Fig. 8 presents the comparison of all fatigue data along with the stress-life fatigue curves, determined by Basquin＊s equation [36]. The fatigue behavior is similar between geometries 2每5 except for the high cycle fatigue tests (考a ≒ 100 MPa) in which there is a signi.cant amount of scatter among the specimens. Geometries 1 and 2 both had fatigue limits at a nominal stress amplitude of 100 MPa while all other geometries resulted in failure at this stress level. As the stress amplitude approaches 200 MPa, however, the data sets between Geometries 2每5in Fig. 8(a) show consistent overlap. Comparing the stress-life curves in Fig. 8(b), there is an observable shift in fatigue life for Geometry 1 while Geometries 2每5 all show overlap amongst each other. The in-consistent amount of scatter in the HCF regime cannot be adequately explained without further investigation into the fracture surfaces. 


3.3. Fractography 
Fractography was performed on select specimens to investigate potential diffferences in failure mechanisms between sample geometries. Specifically, the failure mechanisms of Geometries 2每5 were in-vestigated to determine the source of variability observed in the HCF regime. All specimens had cracks originate at the surface and, more importantly, along the down-skin surface. Fig. 9 shows a representation of the typical fracture surface showing three distinct crack growth zones. The crack growth zones included the initial stable crack growth stage (1, shown in blue1), a quasi-stable crack growth stage (2, shown in red1), and a .nal unstable crack growth stage (3, shown in white1). 
The .rst stage of crack growth includes the crack nucleation and the subsequent stable crack growth. Further investigation revealed that cracks initiated at sharp micro-notches where partially melted powder particles were attached to the specimen surface. A representative crack initiation site is detailed in Fig. 10(a), where the arrows point out two micro-notches at the intersection of partially melted powder particles. As the crack continues to develop in the microstructurally short crack growth stage, crack propagation appears to be a mixture of transgra-nular crack growth across the martensitic 汐∩ grains highlighted in red1 in Fig. 10(a) as well as 汐∩ intergranular crack growth as a result of the .ne acicular 汐∩ grains. In this stage, many cracks initiate on the surface as shown by the blue arrows in Fig. 10(b). Each crack grows in-dependently on a separate plane until they begin to interact with each other and eventually coalesce into a single crack. Once the cracks coalesce to form a large crack, very distinct tear ridges are left behind from the merging cracks on di.erent planes. These tear ridges are in-dicated in Fig.10(b) by the white arrows and extend into the interior of the facture surface in the direction of crack growth. The second stage of crack growth typically occurred immediately after crack coalescence. Once the cracks coalesce into a single large crack, the driving force increases and the stability of the crack growth begins to deteriorate. This crack growth stage consisted of a combina-tion of cyclic and static failure modes resulting in a more tortuous crack path than the .rst crack growth stage. Fig. 11 details a portion of this crack growth stage in which stable fatigue striations, highlighted in blue1, can be seen next to an area that is characterized by ductile tearing across a plane, highlighted in red. There are some secondary cracks that can be seen around this grain, shown by arrows, indicating a mixture of 汐∩ transgranular and prior 汕 intergranular crack growth. As the crack continued to develop in the second stage, the stability of the crack front decreased until it resulted in the .nal unstable crack growth stage. 
1 For interpretation of color in Figs. 4, 9, 10, 11 and 14, the reader is referred to the web version of this article. 

The .nal crack growth stage is characterized by highly unstable crack growth and final rupture of the specimen. Fig. 12 details re-presentative characteristics that occur during this .nal stage. Fig. 12(a) presents an area where the end of a large prior 汕 grain was pulled entirely from the opposite fracture surface indicating a continuing shift to intergranular failure along prior 汕 grains that was .rst observed in the second stage. Secondary cracking and cleavage facets were also observed in this region, as shown in Fig. 12(a). This combination of prior 汕 intergranular failure, cleavage, secondary cracking, and ductile tearing creates an even more tortuous fracture surface than the second crack growth stage. The stability of the crack eventually deteriorated such that .nal rupture occurred causing a small shear lip at the edge of the specimen. This ductile tearing in shear mode is indicated by the equiaxed dimples shown in Fig. 12(b). 
DISCUSSION
4. Discussion 
The initial CGV fatigue results indicated a possible e.ect of surface area on the stress-life for AM specimens in the as-built condition. The fracture surfaces, however, showed a di.erent mechanism behind the shorter fatigue lives of specimens with larger surface area. Speci.cally, the number of initiation sites and their proximity to each other appear to have an e.ect on the fatigue lives among the di.erent geometries. Fig. 13 presents representative fracture surfaces of Geometries 1每3in order of decreasing diameter, (a) being the largest diameter (smallest surface area) and (c) being the smallest diameter (largest surface area). It is worth reminding here that to increase the surface area while holding the volume of the gage section constant, the diameter of the specimen needed to be decreased (see Fig. 
1). Fig. 13 shows that as the diameter of the gage section decreases, the number of initiation sites increases while the distance between the initiation sites decreases. The closer the initiation sites are to each other, the faster the respective cracks begin to interact with each other, eventually coalescing into a single large crack with more driving force. For Geometry 1 (Fig. 13(a)) there is only a single distinct tear ridge with two initiation sites, shown with arrows, relatively far from each other. Comparing this to Fig. 13(b) for Geometry 2, there is still just a single distinct tear ridge, however, the two initiation sites are slightly closer to each other. Finally, Fig. 13(c) for Geometry 3 shows at least four distinct tear ridges with more initiation sites that are even closer in proximity to each other than the Geometry 2 specimen in Fig. 13(b). Additionally, the tear ridges in Fig. 13(c) appear larger in depth resulting in a much more torturous fracture surface in the .rst crack growth stage than in Fig. 13(a) and (b). These observations explain the nearly perfect correlation of the fatigue data for Geometries 4 and 5. For specimens with similar dia-meter, the fatigue lives are mostly consistent for all stress levels, re-gardless of surface area, indicating that the diameter may be more in-.uential to the fatigue behavior than surface area. 
There were also observable di.erences in the crack growth zone of the fracture surfaces between the di.erent geometries. Fig. 14(a) gives a representation of the largest diameter/smallest surface area specimen (Geometry 1) and Fig. 14(b) represents the smallest diameter/largest surface area specimen (Geometry 3), both at a nominal stress amplitude of 300 MPa. In both specimens, the stability of the crack begins to de-teriorate soon after the cracks begin to coalesce which occurs near the end of the tear ridges. The blue stable crack growth zone in Fig. 14(a) is relatively large compared to the total fracture surface area and is ob-servably di.erent than the smaller diameter specimen in Fig. 14(b). The smaller diameter specimen (Fig. 14(b)), on the other hand, has a much smaller ratio of stable crack growth zone (blue) compared to the total fracture surface. Both geometries result in main and secondary cracks that coalesce and transition into the second quasi-stable crack growth zone, however, the size of the stable crack growth zone is smaller than for the smaller diameter specimen. For the smaller diameter specimens, the main and secondary cracks initiate much closer to one another, and therefore, they coalesce and become less stable quicker than the larger diameter specimens. 
The quasi-stable crack growth zone (red) shows somewhat similar behavior for both specimens, beginning at the tear ridge left behind from the two merging cracks and increases in tortuosity up until the unstable crack growth region. The unstable crack growth zone (white), however, is strikingly di.erent between the two specimens. While the crack path in the unstable region appears to be much more tortuous for the smaller diameter specimen, the most notable di.erence is the ratio between the unstable crack growth area and fracture surface area. The smaller specimen has a much larger ratio of unstable crack growth as compared to the larger specimen, indicating that the stability of the crack deteriorates much more quickly for smaller diameter specimens. These results indicate that the diameter of the specimen may a.ect the crack growth behavior as well as the initiation behavior for a given geometry further con.rming that the specimen diameter may have more detrimental e.ects on the fatigue behavior of AM parts than the surface area, which again explains the observations in Figs. 7 and 8. Specimens built at some angle from the vertical direction generally have higher surface roughness on their down-skin surfaces compared to the up-skin surfaces [20,21]. The di.erence in surface roughness may be related to a di.erence in the solidi.ed part volume below the re-spective layer edges which can a.ect the thermal history [21]. It should be noted that the solidi.ed part has signi.cantly higher thermal con-ductivity than the powder bed meaning the majority of heat will be dissipated through the part. This e.ect is visually shown in Fig. 2, where there is more solidi.ed part volume beneath the layer along the up-skin edge to dissipate heat. Conversely, on the down-skin edge, much less solidi.ed part volume exists to dissipate heat, resulting in the layer edge remaining hotter for a longer period of time. The prolonged increased temperature/time can result in additional partially melted powders adhering to the surface which causes the down-skin surface to have a rougher surface [21]. 

Numerical simulations conducted using the model developed by Masoomi et al. [37] 
showed the temperature on the down-skin edge remained above the melting point of Ti-6Al-4V (1604 ∼C) for approxi-mately 0.42 ms. In contrast, the up-skin edge only remains above the melting point for 0.17 ms. These observations and simulation results indicate that the higher roughness on the down-skin surface compared to the up-skin surface is a result of the down-skin edge having less so-lidi.ed part volume underneath to dissipate the supplied heat. 
Additionally, the simulation results indicate that this e.ect can be somewhat alleviated by increasing the part diameter. The increased solidi.ed part volume below the down-skin edge can dissipate the heat more e.ciently which would be evident in the surface roughness measurements amongst the geometries. Parts with su.ciently large diameters should have lower down-skin surface roughness compared to smaller diameter parts. 
Table 1 lists the mean surface roughness values, Ra, obtained for each geometry along with the standard deviations. The results indicate that the roughness for the up-skin surfaces is mostly similar for all geometries ranging from approximately 12每15 米m. The mean down-skin surface roughness, however, shows consistency between Geome-tries 2每5 while Geometry 1 has a surface roughness more comparable to the up-skin surface. In fact, the mean surface roughness is slightly higher for the up-skin surface while the standard deviation for the down-skin roughness is higher. Observation of the down-skin and up-skin surfaces for Geometry 1 also indicates that there are more partially fused powder particles on the down-skin surface, which is believed to a.ect the geometry of the micro-notches introduced by the intersecting particles. The sharpness of the micro-notches of intersecting powder particles as shown in Fig. 10(a) increases the likelihood of crack 
Table 1 
Average roughness and gage diameter measurements for as built geometries considered in this study. 
initiation occurring on the down-skin surface, even if the surface roughness is similar to the up-skin surface. Increasing roughness values on the down-skin surface, however, proves to be even more detrimental to the fatigue resistance as indicated by the stress-life curves for Geo-metries 2每5 as compared to Geometry 1 in Fig. 8(b). 
An ANOVA with post-hoc Tukey analysis of the surface roughness data showed a signi.cant di.erence between the down-skin and up-skin surfaces for Geometries 2每5. Similar up-skin roughness values were shown for all geometries, but only Geometry 1 had a similar down-skin roughness compared to the up-skin roughness (Table 1). Geometries 2每5 exhibited signi.cantly di.erent surface roughness values between the up-and down-skin surfaces. This .nding validated the previously described thermal simulation results for larger diameter sections. These results are visually presented in the box plot in Fig. 15 showing the spread of the Ra measurements for each specimen geometry and a re-latively larger diameter (9.5 mm) obtained from the grip section of an untested specimen. The boxes for each surface indicate the interquartile range of the surface roughness measurements, such that they can be a visual indication of statistical di.erence between two data sets. From this analysis the larger grip diameter was similar to Geometry 1 in which the down-skin surface roughness was statistically similar to the up-skin surface roughness while the up-skin and down-skin surfaces for Geometries 2每5 were mostly statistically di.erent. Geometry 3 does show some overlap between the up-skin and down-skin measurements, however, the trend of the large di.erences in the roughness between each surface is still apparent. Additionally, the box plot indicates that diameters above 4.90 mm begin to have down-skin surface roughness more similar to the up-skin surface which is most likely related to the lower amount of time spent above the melting temperature for larger diameter parts. 
The statistical surface roughness analysis explained some key dif-ferences in the low cycle fatigue behavior (nominal 考a ≡ 200 MPa) for the given geometries tested. Geometry 1, which has the smallest surface area and coincidentally the largest gage diameter, showed better fatigue performance as compared to Geometries 2每5, as previously shown in Figs. 7 and 8. Therefore, the lower number of crack initiation sites observed for Geometry 1 specimens with larger gage diameter, can most likely can be explained by the decreased di.erence in surface roughness between the up-skin and down-skin surfaces of the specimen. Geometries 2每5, on the other hand, had similar roughness values on their down-skin portions which explains the similar low-cycle fatigue behavior observed amongst these geometries. 
The increased surface roughness on the down-skin surfaces for Geometries 2每5, compared to Geometry 1, was attributed to an in-creased number of partially fused powder particles on the down-skin surfaces, resulting in lower fatigue resistance. As previously discussed, a combination of partially fused powder particles on the surfaces may create a series of sharp micro-notches across these surfaces. The in-creased number of micro-notches created on the down-skin surfaces, as compared to the up-skin surfaces, is believed to be the reason for the decreases shown in fatigue life. Essentially, sharp micro-notches will initiate cracks earlier or in greater quantities for surfaces that have a higher surface roughness (i.e. Geometries 2每5). In this case, this in-creased surface roughness for Geometries 2每5 is related to the smaller diameter specimens and is attributed to the amount of time the layer edge remains above the melting point. 
Interestingly, while the low cycle fatigue behavior can be explained well by the surface roughness, the scatter and reduced fatigue strength in the high cycle regime (nominal 考a ≒ 100 MPa) were not addressed. From Fig. 8(a) it appears that AM parts in the high cycle regime are more sensitive to diameter. Geometries 2每5 all have similar roughness values and, by the previous argument, should therefore have similar fatigue lives, if only the surface roughness is considered. Considering the fact that the initial cross-sectional measurements may not give the actual representation of the load-bearing cross-section, the e.ect of the gage diameter can be easily understood in regards to the surface roughness. As the specimen＊s diameter decreases, the di.erence be-tween the load-bearing cross-section and the initial measured cross-section becomes larger. The di.erence in load-bearing cross-section and initial measured cross-section was previously shown on a fracture sur-face in Fig. 
4. This over-estimation of the cross-section results in in-creasingly higher applied stress amplitudes (as compared to the nom-inal 100 MPa fatigue limit) which yields fatigue failure before 107 cycles. Fig. 16 presents fatigue lives for tests that had an initial esti-mated (nominal) stress amplitude of 100 MPa based on the initial measured diameter using a caliper. The hollow data points in this .gure represent failed specimens and the .lled data point was a runout test. As the applied stress amplitudes approach 110 MPa, a sharp bend in the stress life data occurs resulting in the large scatter observed in the high cycle fatigue regime. Essentially a fatigue limit of approximately 110 MPa exists above which failure typically occurs before 107 cycles. Geometries with smaller cross-sectional diameters are more a.ected by the higher surface roughness resulting in an applied stress amplitude that is considerably higher than the preset nominal stress amplitude of 100 MPa. These results explain the scatter observed for the HCF data showing surface roughness can be more detrimental to smaller diameter specimens where the over-estimation of the cross-sectional area results in a greater applied stress. 
The fatigue behavior of AM Ti-6Al-4V specimens fabricated diag-onally were mostly a.ected by the down-skin surface roughness. For geometries with cross-sectional diameters larger than 4.90 mm, the down-skin surface roughness was more comparable to the up-skin surface roughness, resulting in an improved fatigue performance at all stress levels. Furthermore, smaller cross-sectional diameter specimens were more in.uenced by the surface roughness in the HCF regime re-sulting in higher applied stresses and lower fatigue lives. The fatigue behavior of the di.erent geometries was also shown to be a.ected by the crack growth and crack coalescence characteristics. Larger diameter specimens had larger portions of stable crack growth as multiple crack initiation sites were further apart requiring more fatigue cycles before crack coalescence occurred. The smaller diameter specimens, on the other hand, had crack coalescence occurring earlier due to the close proximity of the crack imitation sites. 
CONCLUSION
5. Conclusions 
The effects of surface area, surface roughness, and gage diameter on the fatigue behavior of as-built L-PBF Ti-6Al-4V parts fabricated at 45∼ 
(i.e. diagonal direction) were investigated in this study. Through sev-eral experimental observations and measurements of the micro-structure, surface characteristics, fatigue behavior, and fracture sur-faces, the following conclusions can be made: 
1. Initial results indicated that surface area may a.ect the fatigue re-sistance of the varying geometries. However, when the applied stress was corrected by only considering the load-bearing cross-section, no signi.cant e.ect of increasing surface area on the fatigue behavior was observed for the as-built additive manufactured spe-cimens within the diameter range examined in this study. 
2. The crack initiation behavior was found to be highly sensitive to surface roughness, where all cracks initiated on the rougher down-skin surface. Specimens with lower down-skin surface roughness showed a superior fatigue resistance as compared to specimens with higher down-skin surface roughness. This result is important for parts that are built diagonally or with overhangs as the higher down-skin surface roughness will be more susceptible to crack in-itiation causing a lower fatigue resistance. 
3. The crack growth behavior was shown to be a.ected by the cross-sectional diameter with smaller diameter parts having multiple crack initiation sites closer to each other. The proximity of the multiple cracks results in much earlier crack coalescence which deteriorates the crack growth stability resulting in a lower fatigue life. 
4. A gage diameter larger than 4.90 mm resulted in significantly lower surface roughness for down-skin surfaces due to the larger solidi.ed part volume beneath the layer edge, thus dissipating the supplied heat more e.ciently. This result indicates that the e.ect of the high down-skin surface roughness can be minimized by increasing the part thickness/size at diagonally built surfaces. 
5. Thee.ect of surface roughness on the high cycle fatigue behavior was found to be greater for specimens with smaller gage diameters. Similar surface roughness on decreasing gage diameters resulted in an increasing overestimation of the load-bearing cross-section. Therefore, the applied stress was higher and yielded shorter fatigue lives in the high cycle fatigue regime. 
The fatigue limit of the as-built additive manufactured Ti-6Al-4V 
specimens was approximately 110 MPa which is signi.cantly lower than the 550每750 MPa range for wrought Ti-6Al-4V. The surface roughness, introducing many micro-notches along the surface, most likely has signi.cant contribution to the reduced fatigue strength of additive manufactured Ti-6Al-4V specimens. 
These results highlight the importance of understanding the spe-cimen property to part performance relationships of AM parts subjected to cyclic loading. Fatigue data is typically generated in the laboratory on standard size specimens with uniform gage sections, while the in-service parts can have di.erent size, geometry, layer orientation, sur-face roughness, and more. Not only can a structural part experience very di.erent thermal history than the standard specimen, but the thermal history may even vary from point to point of a part depending of its geometry complexity. Any variations in thermal history can a.ect the resulting microstructure, porosity level and surface roughness, which in turn will a.ect the part performance. This raises serious concerns regarding applicability of mechanical properties generated in the laboratory from standard size specimens to design and evaluate load-bearing structures and components. Therefore, there is a critical need to better understand the specimen property to part performance relationships as well as to develop appropriate standards speci.cto additive manufacturing technology and challenges. 
Acknowledgement 
Partial funding for this work was provided by the National Science Foundation under Grant No. 1657195. Supports from the Naval Air Systems Command (NAVAIR) is also greatly appreciated. Authors also acknowledge the support of Mohammad Masoomi from Auburn University for performing all the simulations of this study. 








1. Introduction 
Inrecenttimes,XCThasbeenwidelyusedforqualitativeandquan-titativeassessmentsinthe.eldsofaerospace,automotive,defense,and medicalapplications[1每3].Inparticular,theuseofXCTfornon-destructiveinspectioninaerospacecompositeshasincreasedconsider-ably[4,5].Thisisduetoitshighspatialresolutionandtheabilitytopre-ciselycapturethemultiscalestructurenon-destructively[6].XCT providesinformationontheinternalcompositestructureinboth2D and3Dformats.ThisisoneofthemainadvantagesofusingXCTcom-paredtootherdestructiveand2Dvisualizationtechniques,suchas scanningelectronmicroscopy,opticalmicroscopy,etc.,[7].Moreover, othernon-destructivetechniques,suchasultrasonicC-scanandinfra-redthermography,cannotcapturethehigh-resolutiondamagemecha-nismsthatcanbedetectedusingXCT[8]. 
ThedevelopmentofXCTforobserving.ber-reinforcedpolymer (FRP)compositesiscrucial.ThisisbecauseFRPcompositesareaniso-tropicandheterogeneous,sincethepropertiesofindividualconstitu-entsaredifferent,andthedeformationbehavioroftheseisdirection dependent[9].TheaccumulationofdefectsinFRPcompositestakes theformofcomplex3Dpatterns.Therefore,investigatingmicrostruc-turalchangesinFRPcompositesduringmanufacturingandunderload-ingscenariorepresentativeofin-serviceconditionsisdif.cult. Furthermore,thereisatendencyfortheformationofprocess-induced defectsduringthemanufactureofFRPcomposites,particularlywhen manufacturingcomplexgeometries[10].These.awscanbeinthe formoffabricwrinkles,resinpockets,inter-andintra-towvoids,matrix microcracks,delaminationanddebonding,etc.,[11,12].Manyofthese defectsarebarelyvisibleorinvisibletothenakedeye[13].Asaconse-quence,themechanicalperformanceofcompositecomponentsmay besigni.cantlyreduced.Hence,obtaininginformationduringthe 
differentstagesofmanufacturingandpost-processingofcomposites usingXCTisinvaluable[3,14,15].TheinformationobtainedfromXCT imagescanbeusedtocreategeometricalmodelsoffabricsandprepregs forperformingnumericalsimulations[16].Thereareseveralnumerical simulationtoolsavailable,suchasGeodict,Avizo,Voxtex,etc.,topredict processparametersfromthecreatedgeometricalmodelsoftherein-forcements[17每19].Inthisway,theXCTcanbevaluableforunder-standingandimprovingthemanufacturingprocess. 
Inrecenttimes,FRPcompositeshavebeenwidelyused,notonlyin primaryandsecondarystructuralaircraftapplications,butalsoinen-ginenacellesandcowlings,duetotheirhighstrengthtoweightratio comparedtometals[20每22].TheAirbusA350,Boeing787and,Bombar-dierC-SeriesaircraftarefabricatedwithFRPcompositeshavingN53%, 50%,and47%ofcompositesbyweight,respectively[23每25].Itisclear fromFig.1thatallofthecompositecomponentsintheAirbusA380in-volvetheuseofadvancedcompositemanufacturingtechniques,suchas autoclave,resintransfermolding(RTM),resininfusion(RI),resin.lm infusion(RFI),automated.berplacement(AFP)andautomatedtape laying(ATL)[26每28].Also,inrecenttimes,AFPandATLtechniques havebeenwidelyusedtomanufacturethewingstructureoftheMS-21aircraftasanalternativetousinghand-laidprepregs[29,30].More-over,mostunmannedaerialvehicles(UAVs)anddronesarecurrently beingfabricatedusingFRPcomposites[31,32].Thisincreaseduseof compositesisduetoadvancesincompositesmanufacturingtechniques, enablingcomplexshapedstructuralcomponentstobefabricatedeasily andrapidly. 
Inthisreview,wefocusprimarilyontheuseofXCTincommonly-usedaerospacemanufacturingtechniques(i)OoAprocessing,suchas VacuumBagOnly(VBO)andLiquidCompositeMolding(LCM),and (ii)theautoclaveprocessing.TheOoAtechniquesrequirelowcapitalin-vestmentsandprovide.exibilityindesignandsize[35].However, 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
Rudder  
Vertical Tail Plane  (ATL/Autoclave)  
(ATL/Autoclave)  Radome  
Glare (Autoclave)  (RFI/RTM)  
Flaps  
(ATL/Autoclave)  
Stringers  
Horizontal Tail  (ATL/Autoclave)  
Plane  
(ATL/Autoclave)  
Engine Cowlings  
(AFP/Autoclave)  
Rear pressure bulkhead (RFI/RTM)  Tail cone  Ailerons (ATL/Autoclave)  
Fig. 1. Airbus A380 aircraft components produced using advanced composite manufacturing techniques [26]. 
there are issues of the void generation that are directly associated with low-pressure processing. In contrast, the autoclave process enables original equipment manufacturers (OEMs) to fabricate components with a low void content (b2%) by applying suf.cient pressure and tem-perature to the laminate during consolidation. However, there are size limitations, and capital investment and maintenance costs associated with this process are also high. [36,37]. 
There are some challenges in achieving high-quality parts using liq-uid composite molding (LCM) techniques. The .ow through the dual scale porous media (dry reinforcement) is often challenging [38,39]. As a result, complete saturation of the reinforcement is vital in conven-tional LCM techniques. Incomplete saturation may lead to dry spots and high void content in the part [40,41]. However, complete saturation with a low void content can be achieved by further enhancing LCM tech-niques [42每47], through the use of advanced processing and character-ization techniques [48每50], and enhancing the permeability of .ber preforms using .ber surface treatment and ultraviolet (UV) curable resin [51], leading to successful injection with low void content. In such processes, the visualization of microstructural variations in the .ber preform at different stages of resin infusion (before, during or after) is a challenging task. 
OoA prepreg possessing offers good control over .ber alignment and enables the use of high viscosity toughened resins. However, thermoset prepregs have a low shelf-life, therefore, the protection of thermoset prepregs against environmental conditions in accordance with the manufacturer's data sheet is important. On the other hand, thermoplas-tic prepregs offer a range of bene.ts over thermoset prepregs, such as recyclability, room temperature storage, long shelf-life, weldability, high fracture toughness, fast production rate, good chemical resistance, etc., [52]. Nevertheless, the melting temperatures and viscosity of typi-cal thermoplastic prepregs are comparatively high, requiring high pres-sures and temperatures for consolidation [53,54]. On the other hand, resin movement cannot be tracked during laminate consolidation in au-toclave and VBO prepreg curing techniques. 
The issues mentioned above and the challenges associated with aerospace composite manufacturing techniques can be resolved through the use of in-situ XCT techniques [55,56]. Recently, the use of in-situ XCT for studying the LCM process parameters at every stage of the manufacturing cycle (before, during, and after resin infusion) has in-creased 
[57每59]. Further, the data is used to develop XCT-based numer-ical models for studying the OoA and autoclave process parameters, which can potentially reduce characterization time, cost, and the associ-ated number of experiments. The literature related to composites manufacturing using OoA (LCM and VBO curing) and autoclave, partic-ularly process modeling approaches using XCT are limited. Some recent studies have discussed void generation in FRP composites [60], and in prepregs 
during 
OoA 
processing 
[61]. However, it is important to inves-tigate further the fabricating quality using XCT imaging for faster pro-cess characterization with lower costs. 
In this review, XCT-based modeling approaches are discussed for the complete understanding of the OoA and autoclave manufactur-ing processes, and the associated challenges during the different stages of manufacturing. The procedure to optimize the process pa-rameters via numerical modeling is given for minimizing the number of experiments. The scope of this article is shown in Fig. 
2 
with the aim of reviewing the effect of various process conditions on key pro-cessing parameters in OoA and autoclave manufacturing processes. In the .rst section, we discuss the working principle, advantages, and challenges associated with the XCT procedure. The signi.cance of synchrotron and laboratory-based XCT for time-lapse studies to generate numerical models for reducing the processing time and cost is discussed. In the second section, XCT data-driven model gen-eration techniques are discussed. In the third and fourth sections, the application of XCT in understanding and improving the OoA and au-toclave manufacturing processes are reviewed. The role of XCT in characterizing the compaction behavior, resin permeability through reinforcing fabric, the air permeability of prepregs, and void forma-tion and removal are discussed. Finally, we highlight the potential areas in different composites manufacturing processes, where ex-situ and in-situ XCT techniques could be useful in better understand-ingand improvingthe process. 
2. X-ray computed tomography of FRP composites 
2.1. Working principle and data processing 
The XCT technique is commonly used as a non-destructive evalu-ation (NDE) tool. A laboratory-scale XCT usually consists of an X-ray source that converts electrical input power to X-rays, a rotary stage where the specimen is mounted, and a detector, as shown in Fig. 
3 
(a, b). The resolution achieved with XCT is relatively high compared to other NDE techniques (e.g., ultrasonic C scan), due to its high 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
OoA 
Prepreg Hand layup 
. Prepreg Air 
XCT 
Permeability 
. Fabric Compaction . Fabric Permeability Machine . Prepreg Compaction 
. Degree of Impregnation . Resin Flow Patterns . Porosity . Void characterization 
Fig. 2. Schematic representation of the aerospace manufacturing techniques and the potential role of XCT in characterization of the process parameters. 
frequency [62,63]. The XCT principle is based on generating 2D ra-receives a large number of radiographic (projection) images at dif-diographs by impinging a beam of X-rays from the X-ray tube ferent angular positions during a 360∼ rotation of the sample. After through a sample. Further, a CCD (Charge Coupled Device) detector that, these XCT scanned images are reconstructed using 
(a) X-ray beam (b) 
Specimen X-ray tube 
X-ray tube focal spot 
Detector 
Specimen 
X-ray tube 
Rotary stage 
Lx Ly 
(c) 
Fig. 3. (a) A typical laboratory XCT setup, (b) schematic representation of the setup and (c) stepwise XCT data analysis procedure. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
reconstruction algorithms (matrix inversion, .ltered back-projection algorithm, etc.) and software such as CT Pro [64], Phoenix datos|x [2], NRecon, Octopus Reconstruction, etc. A .ltered back-projection algorithm is a commonly-used procedure [4,65]. The .eld of view is initially cropped to reduce the size of the .le for re-construction. The reconstruction methods for XCT are reported in detail elsewhere [66]. 
2.1.1. Voxel geometry creation and data analysis procedure 
The procedure to generate the voxel geometry for running simula-tions to study various composite process parameters is shown in Fig. 
(c). The overall XCT data analysis procedure can be divided into three fundamental steps: (i) pre-processing, which includes .ltering and smoothing of the XCT data to enrich image quality. These reconstructed XCT images are then used as input data to create a 3D structure using software packages such as, VGStudio Max, Geodict, Fiji, VoxTex, Avizo, etc., (ii) Analysis, which includes the segmentation and feature extrac-tion (component analysis), etc. The segmentation procedure is the es-sential step in data analysis that distinguishes the different constituents in FRP composites, such as the .bers, resin, air, etc., based on their greyscale values. The .ber orientations in unidirectional, woven, and 3D fabrics such as warp, weft, and Z-binder yarns can be ob-tained from the segmentation method. After segmentation, the model can run in Abaqus, Ansys, LS-Dyna, etc., to predict the compaction re-sponse of the fabric numerically. The permeability simulation can run in .ow modeling software packages, such as Fluent, Geodict, Ansys/ CFX, etc., based on computational .uid dynamic (CFD) techniques [67], and the mold .lling, dry spot prediction, and cure simulation can run in Liquid Injection Molding Simulation (LIMS), PAM RTM and Con-vergent Compro software [68]. 
(iii) Visualization of data, which includes mapping and rendering of the results and then verifying the results with reference methods [6]. Further details related to the XCT data analysis process are reported elsewhere [69]. 
2.1.2. In.uence of process parameters on XCT image quality 
The XCT image quality can be reduced due to issues such as artifacts, noise, and image blur. Some of the common types of artifacts are beam drift, beam hardening, ring, and partial volume artifacts. Beam drift can be caused by the thermal expansion of the X-ray tube. Beam hardening can be caused due to the mismatch in real and assumed X-ray attenua-tion coef.cients of the material, which results in streak and cupping ar-tifacts [70]. These issues can be overcome through the use of pre and post-processing 
techniques 
and 
the 
dual-energy 
method 
[71]. Ring arti-facts may occur due to the mechanical misalignment between the X-ray source and the detector or the low sensitivity of the detector. Partial vol-ume artifacts are due to the limited .eld of view, i.e., only part of the specimen is projected. Noise is mainly induced by photon noise, whereas image blur is associated with geometric or motion blurring ef-fects [72]. Realistic XCT projections without any artifacts, noise, and image blurring can be obtained by using the SimCT software package [73]. SimCT is an X-ray imaging simulation tool which enhances the image quality by optimizing the scan parameters automatically [74]. 
Image resolution and contrast have to be optimized for obtaining high-quality images of FRP composites. In order to do that, the XCT pa-rameters, such as X-ray voltage and current, spatial resolution, image acquisition time, rotation steps, .lters, etc. need to be studied [75]. 
(a) Voltage and current 
X-ray characteristics can be controlled by the applied voltage and current to the X-ray tubes. The wavelength of the X-rays emitted can be reduced by increasing the voltage, while the X-ray intensity can be increased 
by 
increasing 
the 
current 
at 
a 
constant 
voltage 
[76]. 
(b) Spatial resolution 
The ability to show .ne structural details or the least separation at which two points are distinguished in a scanned image is de.ned as the spatial resolution. It depends on several factors, such as the perfor-mance of the X-ray tube and its focal spot size, detector characteristics (sensitivity, dynamic range, noise, quantum ef.ciency, etc.), capacity of the computer, data enhancement techniques, etc. The nano-sized focal tube is used for lighter samples, owing to its smaller spot size, while the micro-sized focal tube is used for highly absorbing materials, where the same voltage and .lament current are used. The exposure time can be adjusted from 33 milliseconds to 5 s in the former case to obtain a good spatial resolution by obtaining a high signal to noise ratio [77]. However, the tube power cannot increase, due to the smaller focal size of the nano-sized tube. The spatial resolution can be measured using either the Rayleigh or the Sparrow criteria [78,79]. 
(c) Acquisition time and rotation steps 
In general, a higher image acquisition time (scan time) gives high contrast images, which vary for each XCT system. Some scanners use a stepwise scanning approach, while some use a continuous scanning ap-proach, which includes continuous rotation and image acquisition. There is a tendency for slight vibrational moments to occur in the spec-imen in the step-wise rotation approach. It is advisable to use a skip function to avoid this effect, by not considering the .rst image in each step. However, a continuous scanning approach may not introduce any 
vibrational 
moments 
[80]. Multiple images can be captured and av-eraged to reduce the noise and improve the image quality in each step in the stepwise rotation approach. The number of steps depends on the size of the sample based on the magni.cation. A greater number of steps, involving a larger quantity of images, is bene.cial for a good re-construction, only if a higher magni.cation is required. 
(d) Filtration 
Filters are used for minimizing image artifacts. There are two types of .ltering approaches commonly employed during image acquisition: 
(i) 
beam .ltering and (ii) detector .ltering. If .lters can be applied be-tween the X-ray tube and the specimen, referred to as beam .ltering, or between the specimen and the detector, referred to as detector .lter-ing. The function of beam .ltering is to protect the specimen from the beam hardening effect, such that streak artifacts can be ignored. Detec-tor .ltering is used to reduce the noise from the remitting secondary X-rays, which may be produced when the specimen is denser or nano-structured 
[77]. 
(e) 
Image binarization 
Image binarization is an essential step in the .eld of optical character recognition (OCR) [81]. Within binarization, thresholding is an impor-tant step for identifying different materials in the XCT images based on their grayscale values [82]. Otsu's and Gaussian Ostu's methods are widely used thresholding algorithms for measuring the grayscale values in XCT images [83,84]. It is widely recognized that between the two methods, Gaussian Ostu's method provides more accurate results than Otsu's method [85]. 
2.2. Classi.cations of XCT 
There are three types of commonly-used XCT system that are based on frame rate, namely (a) medical XCT (up to 10 projections/s), 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
(b) synchrotron XCT (up to 10,000 projections/s) and (c) laboratory XCT (approximately 1 s 每 10 
min/projection) 
[86]. In medical XCT, the X-ray source and detector rotate around the patient. In contrast, the specimen rotates while the X-ray source and detector are stationary in the synchrotron and laboratory XCT. In the .eld of engineering, the lat-ter two are mainly used, and therefore, their use in understanding and improving aerospace composite manufacturing techniques will be discussed in this manuscript. 
2.2.1. Synchrotron XCT vs. laboratory XCT 
A synchrotron XCT (SXCT) offers a relatively high resolution, ranging from 30 nm to 20 米m, which can be attributed to the use of a coherent source with a parallel monochromatic beam. It is more expensive than the laboratory XCT (LXCT). It also offers a better signal to noise ratio, al-though limitations in the sample size (between 20 米m and 50 mm) is a major drawback. In contrast, the maximum sample size possible with the LXCT varies from 50 米m to 300 mm, and the resolution range varies from 50 nm to 100 米m[8]. The lower resolution and higher acquisition time are due to the use of a polychromatic light source and cone beam. The LXCT technique can further be categorized into two based on the focal spot size of the X-ray tube: (i) low-.ux re.ection type with a focal spot size of b1 米m, and (ii) high-.ux re.ection type with the focal spot size N1 米m[87]. However, compared to SXCT, the LXCT tech-nique has a low X-ray .ux [88]. As a result, the specimen scanning time using SXCT is fast. Also, SXCT provides a higher brightness than LXCT. Further, resin .ow tracking inside .ber tows is dif.cult with the LXCT technique due to its low spatial resolution. However, it could be possible to 
view 
this, 
to 
some 
extent, 
in 
the 
SXCT 
[59,89]. 
2.2.2. In-situ vs. ex-situ loading 
XCT can be further classi.ed as in-situ and ex-situ for time-lapse studies. In-situ imaging can be performed inside the XCT, and the scan-ning process can be interrupted or uninterrupted. The application of LXCT is mostly limited to interrupted scans, due to its low .ux and high acquisition characteristics; otherwise, high image contrast is not possible 
[3]. 
For fracture mechanics, ex-situ imaging can be performed after re-moving the specimen from a mechanical test machine and placing it in-side the XCT for further analysis. There are two major drawbacks to this approach. For example, consider the case of understanding crack behav-ior: 
(i) 
a 
crack 
may 
close 
after 
unloading 
the 
specimen 
[90]. (ii) Contrast agents are usually added to improve the detectability of cracks, which may change the mechanical performance of the composite during fur-ther testing [91]. However, the crack closure effect can be quanti.ed using in-situ XCT [92]; therefore, the contrast agents are not usually used here. 
2.3. Advantages and limitations of XCT 
The ability to generate a 3D voxel geometry based on 2D radial slices is the main advantage of using XCT. The 3D information includes .ber waviness, .ber orientation, individual volume fractions of the constitu-ents in FRP composites, morphological information regarding voids, such 
as 
their 
shape 
and 
roundness, 
etc., 
[88]. The XCT provides detailed 3D information even for complex 3D geometries as compared to other destructive and non-destructive methods. However, apart from these advantages, there are certain disadvantages of the XCT. These are mainly related to high capital costs, the need for skilled operators, and the time required for post-processing. One of the reasons for the high costs of the XCT machine is due to the custom-built X-ray tube, which is the major component in the XCT setup. The function of the X-ray tube is to accelerate the electrons to high energies for generating X-rays. In doing this, the machine consumes a lot of electrical energy, out of which only a fraction of its energy is converted to X-rays and the remainder is turned into heat. To deal with the high heat, the X-ray tubes need to be designed to accommodate this effect [93]with periodic maintenance after the operation. Otherwise, X-ray tubes are prone to failure, such as normal aging (.lament burnout, arcing, etc.), process failures, application mismatch, improper drive by the power supply and tube enclosure considerations, etc., [94]. These failure modes result in extra costs. 
The other signi.cant disadvantage of XCT is that a high capacity computer data storage device is required to store the XCT scanned and reconstructions data since the .le size for each scanned specimen is usually several gigabytes (GB) [95]. In particular, the quantity of data (sum of experimental data and images) for in-situ XCT scans is usually greater than that for ex-situ scans. However, the former gives more ac-curate results. Also, SXCT scans consume more data than LXCT scans [96]. 
2.4. Challenges associated with XCT of FRP composites and remedies 
(a) Low X-ray attenuation coef.cient 
The major challenge associated with XCT of composites, mainly when using LXCT during acquisition and post-processing, is achieving a high contrast image (a high X-ray attenuation coef.cient), particularly for those materials which have a low density and a low atomic number. Such materials exhibit a low X-ray attenuation coef.cient (米) when the X-ray impinges on it, as 米 is directly proportional to these two parame-ters [14]. For example, the attenuation coef.cient of a carbon/epoxy composite is low since it has a low density. Also, the difference in den-sity between the carbon .ber 
and 
the 
epoxy 
matrix 
is 
small 
[97]. As a re-sult, it is dif.cult to identify intra-tow voids at low resolutions in such types of composites. A high resolution can be obtained using a small sample [98]. 
(b) Dif.culties in capturing the .nest damage mechanisms 
Another challenge is identifying transverse ply (90∼) cracks in com-posites using ex-situ XCT. It is dif.cult to identify such damage since transverse ply cracks result in only a slight reduction in the overall stiff-ness of the structure, and tend to change its failure mechanisms while handling and placing the specimen in the XCT for scanning [12,99]. For example, the NASA X-33 launch vehicle program was interrupted due to a leakage found in the liquid nitrogen tanks induced by trans-verse ply cracking [100]. However, these narrow cracks can be seen via 
a 
combination 
of 
phase 
contrast 
and 
in-situ 
XCT 
loading 
[101,102]. 
The .nest damage mechanisms or defects (matrix micro-cracks, transverse ply cracks, .ber breaks, etc.) can be detected by employing propagation-based phase contrast in the SXCT [7,103,104]. The propagation-based phase-contrast method is suited only to coherent or semi-coherent sources. Phase-contrast can be enhanced by maintain-ing the greatest distance between the detector and the sample, or the smallest 
distance 
between 
the 
sample 
and 
the 
X-ray 
source 
[105]. This distance depends on the sample size and the wavelength of the X-rays [3]. Another way to enhance image contrast and render micro-cracks more visible is through the use of gratings [106,107] and by staining [65]. However, staining could induce defects in the form of matrix crack growth, etc., such details are reported elsewhere in the literature [3]. 
(c) Sample size restrictions 
A good spatial resolution can be achieved with a smaller sample size, as mentioned earlier. It is dif.cult to measure the yarn volume fraction in a larger sample, due to lower spatial resolution. This is the reason in most of the cases, solid inter-tow regions are considered for .ow 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
modeling [108,109]. A remedy for accurately measuring the yarn vol-ume fraction is to choose a smaller voxel size. 
XCT machines struggle to scan large .at, or planar composite panels, due to sample size restrictions. However, a modi.ed XCT technique called computed laminography, can be used to scan these types of panels 
[100,101]. 
3. Modeling using XCT generated material models 
XCT data contains valuable information that can be generated at dif-ferent stages of the manufacturing process, and that information can be extracted in the form of a material model and applied to better under-stand and improve the process. The potential of the XCT for generating numerical models to predict the effect of varying LCM process parame-ters on the dry .ber compaction behavior, the permeability, and voids may lead to a reduction in the processing time and an enhancement in 
the 
component 
quality 
[17,110每112]. Three-dimensional geometrical models are well-suited to perform simulations, characterization, and modeling 
operations 
[16]. Fig. 
4 
shows the key steps required to develop XCT-generated material models for virtual design and processing. 
In the .rst step, reconstructed XCT images of fabric/prepregs are converted into a three-dimensional voxel geometry. As a result, a voxel-based micro-/meso-scale geometrical model is created. The size of the voxel geometry depends on the selected representative volume elements (RVE). Several RVEs at different locations in the fabric/Prepreg are chosen before taking the average. In the next step, XCT based mate-rial model is generated and the segmentation procedure is performed on the geometrical model to identify the anisotropic-threshold value, density, etc., based on the gray values of each constituent in dry fabric or prepregs. The obtained information from the segmented volume can be used to create a computational model. The computational model can be used to perform simulations (such as compaction, .ow, curing, etc.) for virtual manufacturing. Here, it should be emphasized that real XCT generated materials models have not been widely used in the literature. Instead, pseudo-material models are commonly uti-lized to understand the design and process modeling. Pseudo-material models ignore some of the microstructural features, such as .ber diam-eter, distribution, and orientation within the yarn and assume that the tows are solid. In this review, we mainly focus on reviewing how these pseudo-material models can be used in understanding and im-proving the design and modeling process. 
3.1. Compaction models based on XCT 
Recently, several researchers 
[17,113] have developed realistic unit cell models based on tow data obtained from the XCT and used them for simulating the compaction behavior of real textile composites. Such models are based on the multi-chain digital element method (MCDEM) or the digital element method (DEM) [114,115]. This ap-proach has great potential for predicting the compaction behavior of fabrics. In this method, each yarn is represented as multiple frictionless 1D digital chain elements. These chain elements act like .bers in a real fabric. The advantage of the DEM approach is that the .nal fabric con.g-uration is independent of the choice of the initial cross-sectional shape [18,115]. However, unlike the real yarn, which has several thousand micro-.bers, offering resistance to tow spreading in the form of friction and .ber entanglement, the FE modeled yarn has few macro-.bers. Consequently, frictional forces between the .bers are ignored. This leads to the spreading of the tows, attributed to a lack of resistance to deformation 
[116]. Besides, the DEM approach is feasible only for cap-turing periodic deformations (constant unit cell) due to its high compu-tational cost. Variations in inter-tow and intra-tow pore geometries can also be more accurately simulated for different loading conditions using this approach. Any errors are because the number of .bers in a simu-lated yarn is different (lower) compared to experimental results from the XCT. 
The Virtual Textile Morphology Suite (VTMS) [17] or LS-Dyna [18] can be used to run the DEM based simulations. In general, the area of each .lament is greater in the simulated yarn than in the experiment to match the local intra-tow .ber volume fraction, since the simulated tow has a smaller number of .bers. 
DEM does not accurately predict the mechanical response of the fab-ric, due to the smaller number of beam element chains in the yarn and the absence of any interaction between them. However, it accurately predicts the deformation of the fabric architecture during compaction. This can be overcome by using the same number of digital element chains as the number of .bers in the yarn. However, this leads to a high computational cost and a longer run time. In contrast, high .delity modeling 
[117] successfully captures both the kinematic and mechani-cal behavior of the fabric. In this method, a meso-geometry is initially adopted from the DEM and meshed with hexahedral elements. Further, the micro-scale yarn properties obtained from the experiment are used as an input in the .nite element modeling (FEM) software using a 
Preprocessing Block of XCT Generated Material Models 
Characteris cs Building Block 
Virtual Design and Processing Block 
Simulaon Results 
Fig. 4. Virtual design and processing models using XCT generated material models. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
hyperelastic constitutive model. XCT data-driven compaction models are not extensively studied in the literature, and there is still a need to develop compaction models to better understand the tow behavior and associated geometrical and process parameters at different levels of compaction. Besides, there is also a need to develop a strain-rate de-pendent XCT based compaction model for understanding the fabric be-havior from quasi-static to dynamic loading conditions. Strain rate dependent compaction modeling will be useful to predict the global and localized failure regions in a fabric. These failure mechanisms occur when sudden loads (e.g., tool drops) act on the fabric structure. 
3.2. Permeability models based on XCT 
The Geometric Permeability Estimator (GPE) [111] and the hybrid (XCT 
and 
CFD) 
[118,119] approaches are commonly used for predicting permeability numerically. GPE requires different stages of resin infusion data as input through in-situ XCT. Hence, employing GPE is expensive, 
Table 1 
though it provides closer results to CFD. On the other hand, the hybrid approach can run in Math-to-Market's Geodict software or any .ow analysis software after generating the voxel mesh using XCT recon-structed images. Such software predicts the inter-yarn permeability using the Navier-Stokes approach and intra-yarn permeability using ei-ther Darcy's approach or the Brinkman approach [120]. The hybrid ap-proach is CFD-based and has recently demonstrated an ability to compute fabric permeability [56]. 
3.3. Void prediction models based on XCT 
Unlike several numerical models used in the case of compaction and permeability of .ber preforms, numerical models, using XCT data as input for predicting the void formation, movement and characterization are very limited. However, the LIMS and PAM RTM software packages are well suited to predicting dry spots using a hybrid approach (exper-imental data, including XCT, and FEM) [68,121,122]. The geometry of 
Process parameters in OoA and Autoclave manufacturing techniques References 
Dry .ber Compaction Prepreg Compaction Permeability Air Permeability Void formation and characterization 
Thompson et al. [117] Desplentere et al. [112] Naouar et al. [125]RTM----

Green et al. [18, 113] Yousaf et al. [17, 170] Yan et al. [179, 230] Zeng et al.[120] --RTM --Tahir et al. [231] Straumit et al. [109] RTM -RTM --Salvatori et al. [197] RTM ---RTM Schell et al. [122] --RTM -RTM Schell et al. [68] Sisodia et al. [62]
----RTM Suzuki et al. [223] Zeng et al. [232]RTM-RTM --Ali et al. [56, 108, 119] Soltani et al. [129, 130] Laghaei et al. [118]--RTM --
Caglar et al. [208] Bodaghi et al. [39, 233] RI ----Saidet al.[19] RI -RI --Hemmer et al. [57, 58] Vila et al. [59]
--RI -RI Larson et al. [111, 194] Bossio et al. 
Awaja et al. [234] ----RFI Leonard et al. [235] --AFP/RTM/RI --Endruweit et al. [110] AFP/RTM/RI -AFP/RTM/RI --Aziz et al. [29] Zhang et al. [236, 237] ---HLU/VBO HLU/VBO Schechter et al. [238] Centea et al. [239] ---HLU/VBO -Kratz and Hubert [240] -HLU/VBO -HLU/VBO HLU/VBO Kratz and Hubert [241] -HLU/VBO -HLU/VBO -Cender et al. [242] Torres et al. 
[243] Centea and Hubert [244, 245] ----HLU/VBO Grunenfelder et al. [246] Little et al. [247] Lambert et al. [248] AFP 
----Belnoue et al. [249]
/Autoclave 
AFP 
---AFP/Autoclave Pearson et al. 224, 250]
/Autoclave Stamopoulos et al. 
[124, 251] Tserpes et al. [252] ----HLU/Autoclave Aratama et al. [253] Landro et al. [254] Plessix et al. [255] Mehdikhani [131, 222]
----ATL/Autoclave 
Comer et al. [256] AFP/RI and ATL Nguyen et al. [257, 258]--AFP/RI and ATL/Autoclave -
/Autoclave Straumit et al. [259] 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
the micro-pores and longitudinal .ber bundles can be created using 1D and 2D elements, respectively. Gourichon et al. [123]usedLIMSto gen-erate models for micro-void prediction. In contrast, several post-processing tools that support the XCT images, such as VGStudio Max, Avizo, Simpleware, ImageJ, iAnalyse, etc. [4,105,124] in analyzing voids. Using XCT images as input data, some of the recent compaction, permeability, and void characterization models are presented in Table 
1. 
4. The application of XCT based material models in OoA processing 
4.1. Liquid composite molding (LCM) 
LCM techniques have been commonly used in the manufacture of aircraft structural components, such as wing box structures [133,134] and the Mitsubishi Regional Jet (MRJ) empennage box structure [133], automotive components for example doors, roofs and side panels, etc. [135,136], helicopter unitized structure (aerodynamic beanie) [137] and marine applications [138]. 
In the LCM process, during preforming (draping and compaction), geometrical variations, such as changes in the fabric internal architec-ture and cross-sectional shape of the tows are known to occur. These variations are much more likely to occur in dry fabrics than in prepregs, due to absence of resin. As a consequence, nesting of the layers is more signi.cant, leading to a reduction in permeability [139]. 
For observing void generation and transportation in both the RTM and resin infusion techniques, the tracking of the .ow front is important as the resin .ows through the micro and macro gaps in the .brous pre-forms. In particular, the tracking of the .ow-through 3D fabrics is dif.-cult due to their complex internal structure. Some in-situ techniques, such as direct .ow 
observation 
using 
a 
camera 
[63,140], dielectric sen-sors [141], digital image correlation (DIC) [142], and ultrasonic C-scan 
[143] are effective in capturing the .ow through the macro-pores in a fabric. However, these techniques fail to monitor the .ow through the micro-pores, due to their insuf.cient spatial resolution. X-ray radiogra-phy is capable of capturing the resin .ow front, however, it is not suit-able for capturing micron level porosity [144]. In contrast, Magnetic Resonance Imaging (MRI) is a useful technique for studying .ow prop-agation in .ber tows [145,146]. The advanced in-situ SXCT technique not only facilitates tracking the .ow in both micro-and macro-pore re-gions, but it also measures the size, shape, and spatial distribution of the voids [91]. 
Therefore, understanding the compaction, permeability, and void distribution of fabrics in the LCM process is essential with the aid of XCT. Using numerical modeling, a number of attempts have been made 
that 
involve 
changing 
various 
process 
parameters 
[147] to attain an 
autoclave 
quality 
component, 
in 
less 
time 
and 
for 
less 
cost 
[148,149]. 
4.1.1. Compaction characterization 
An understanding of transverse compaction response of the rein-forcement in the RTM or resin infusion process is essential, given that it directly in.uences the resin permeability through the fabric, the .ber volume fraction, the thickness of the laminate, and ultimately the mechanical properties of the composite [150]. Several researchers [55,151,152] have studied the compaction response of preforms and re-ported that the degree of compaction depended on the weave architec-ture of the fabric (2D and 3D), the thread density in stitched fabrics, mesoscale geometrical features of yarns, the number of layers, dry and wet fabrics, and most importantly, the nesting of the layers within the fabric. 
Despite previous use of several 2D visualization techniques for ob-serving the microstructural changes of preforms after compaction and resin infusion [136,153], there has been little focus on the 3D response of preforms during or after compaction using XCT. The XCT is a promis-ing tool for capturing variations in the yarn-geometry in-situ. Further, a number of software packages (Avizo, Geodict, VoxTex, etc.) are avail-able for creating the 3D geometry from XCT sliced images, as discussed earlier in [29,108]. In particular, relevant information, such as the de-gree of anisotropy, .ber orientation, average gray value (AGV), etc., can be obtained using the VoxTex software package after the voxel model is generated using the XCT images [154]. The anisotropy-thresholded value is the precise border between the different materials (e.g., .ber, air, etc.) in the scanned image, which can be obtained from the anisotropy curve in a 1D histogram or the anisotropy vs. density curve from a 2D histogram. The position of the .bers and air gaps in a dry fabric and the matrix in addition to the .berand airgaps incompos-ites can also be obtained in 3D, based on the anisotropy-thresholded value, after exporting the voxel geometry in the ParaView software package, which is in-built in VoxTex. 
4.1.1.1. Mesoscale compaction models. Determining the compaction be-havior of preforms experimentally is expensive, as it requires man-power, several specimens for carrying out a suf.cient number of trials and advanced instruments. Moreover, it is a lengthy process. To avoid this, it is important to accurately model the compaction behavior of fab-rics numerically. In 
doing 
so, 
some 
researchers 
[125,155] have generally used three different mesoscale unit cell models based on the textile ge-ometry. One is directly created using textile geometrical modeling soft-ware packages, such as TexGen [156] or WiseTex [157], by making several assumptions, such as straight warp and weft yarns (no crimp), a constant cross-section (no waviness), etc., thereby creating an ideal-ized geometry. The second model (realistic model) is a simulated geom-etry by DEM, which can also be created using textile geometrical modeling software. However, the geometry is considered the same as those of actual geometry obtained from XCT scanning after reconstruc-tion. The third technique is an analytical approach, where the geometry created from the theoretical approach can be re-created after de.ning the center points, and section points based on the cross-sections of the warp and .ll yarns using sinusoidal functions and measurements from the actual fabric. The methodology for creating the analytical geometry and its formulation are reported elsewhere [158每160]. The simulation time for an idealized geometry is less than that associated with the DEM and analytical methods. However, the idealized solution is less ac-curate and ignores realistic features, such as yarn waviness and yarn pinching [125]. A superposition of the ideal, DEM, and analytical model results (scanned specimens from a 3D woven orthogonal carbon fabric) is shown in Fig. 
5(i)(a-c), respectively. 
It is clear from Fig. 
5(i) that the idealized approach gives poor results compared to the other models. The DEM and analytical models offer re-sults that are closer to the experimental (XCT) results. A comparison of the deformed cross-sectional area between XCT and for different meso-scale models in each section [.ll (ZX1 and ZX2) and warp (ZY1 and ZY2)] including the global (sum of ZX1, ZX2, ZY1, and ZY2) are shown in Fig. 
5(ii). It is clear that the predicted cross-sectional areas from the DEM and analytical approaches correlate well with the XCT results. 
4.1.1.2. Meso-macro-scale compaction models. The use of mesoscale models is restricted to the unit cell level, particularly for complex inter-action geometries and 3D fabrics, due to their high computational cost and inability to capture tool geometry effects at this scale during the compaction step in an LCM process. A meso-and macro-scale model was proposed by Said et al. [19] to overcome the limitations of meso-scale models and, for numerically capturing the deformation of fabrics, at both the meso-and macro-scales during LCM, based on a tessellation algorithm. In this process, the mesoscale DEM geometry was used as an input in the .nite element modeling (FEM) software to create the macro-scale geometry 
[126]. 
Unlike the periodic conditions in the case of a single unit cell meso-scale model, large structures can be modeled using hundreds of unit cells to simulate non-periodic deformations based on tessellation. In this case, the slave and master yarn concepts are used for creating a non-periodic unit cell from one layer to another layer throughout the stack 
[127]. This approach predicts the same results as those found in 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
(a) XCT vs. Ideal approach (b) XCT vs. DEM (c) XCT vs. Analytical approach 
ii 
Fig. 5. (i) 
Superposition 
of 
warp, 
weft 
and 
binder 
yarns 
of 
different 
mesoscale 
models 
on 
the 
XCT 
results 
[155], Note: ZX1 and ZX2 are .ll, ZY1 and ZY2 are warp. (ii) Comparison of the cross-sectional area for each section between XCT and for different mesoscale modeling approaches. 
the XCT, such as empty spaces (resin pockets) at the apex (1) and tool macro-modeling, and a hyperelastic constitutive law [163]for corners (2), yarn overlaps (3), etc., as shown in Fig. 
6. meso-modeling by importing the yarn properties obtained from 
Common defects, such as wrinkling, yarn buckling, and distor-the experiments. Despite the shorter running time of the macro-tions, occur when the textile fabric is draped over the mold. Most im-draping model compared to MMZ, it failed to capture the local distor-portantly, these defects typically occur when placing the fabric over tions. The authors did not use XCT data as an input for MMZ simula-a 3D mold due to the presence of corners, edges, and curvatures. In a tions. However, XCT offers great potential for reducing the modeling recent study, Iwata et al. [161,162] proposed a coupled meso-macro time and gives more information related to defects that occur during zoom (MMZ) simulation method to simulate these features, includ-the draping process. Moreover, XCT images can be used for validat-ing local distortions, and observed good agreement with experi-ing the MMZ results. Therefore, further studies are required to inves-ments. They used a membrane-shell continuous approach for tigate the use of XCT for MMZ. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
Fig. 6. Defects in vacuum infused carbon/epoxy composite (XCT) vs. macro-scale 
model 
[19]. 
4.1.1.3. Compaction of .ber reinforcements. The compaction behavior of .brous preforms in the LCM process, speci.cally the compaction of the .bers from the vacuum bag to the tool side, can be simulated using the viscoelastic response of the fabric [150]. The viscoelastic re-sponse of the fabric depends on the applied pressure, temperature, lu-brication, strain-rate, creep, relaxation (single and multiple loading), and cyclic 
loading 
[164每166]. A higher load is required for compacting a dry preform due to more gaps being available compared to wet (lubri-cated) 
preforms 
[167]. 
Fig. 
7(a) and (b) show the change in warp and weft tow areas re-spectively in a single layer plain-woven E-glass fabric, as the pressure is increased from 10 kPa 每 100 kPa, for experimental (XCT) and numer-ical (DEM) approaches. The area of the warp and weft tows decreases in the experimental results with increasing pressure, due to a decrease in tow thickness, though the tow width remains constant. The area of the warp and weft tows is smaller in the DEM simulations than the ex-perimental results. Furthermore, the decrease in tow area in the DEM data over this pressure range is not only in.uenced by the change in thickness, but also by a slight decrease in tow width [17]. 
In general, structural components for use in most engineering appli-cations, especially aerospace and automotive structural members, are fabricated using more than one layer or ply. Therefore, understanding the compaction behavior of multi-layer preforms is important. Nesting and shifting of layers are the main factors for differences in the compac-tion 
response 
of 
dry 
single 
and 
multi-layer 
preforms 
[168]. 
The compaction behavior of single and multilayer fabric preforms was studied [169] by performing in-situ compaction tests inside the XCT. It was found that the number of nesting increases in multilayer preforms if the .oat length of the yarn decreases and the yarn spacing and the number of layers increases. There may be an absence of nesting in some layers in multi-layer preforms when an insuf.cient load is ap-plied, in this case, the channels remain open for void transportation, this can be seen in the XCT image in Fig. 
8. It is worth noting from the .gure that the larger channels in the non-nested layers are associated with the regions of higher permeability. The ability of the XCT to capture this nesting effect is attractive. Differences in the order of magnitude exist between the permeability of fabrics exhibiting nesting and zero-nesting [39]. The nesting effect was numerically studied through a high .delity model by Thompson et al. [117]. XCT images were used as input for developing the high .delity model. This model requires minimum input data compared to other macro-scale models. This model can predict both the kinematic and mechanical properties of the fabric. These results were found to be in agreement with the exper-imental results (XCT). 
When manufacturing structural components for high-performance applications, a fabric should have high permeability for fast processing, adequate drapability and should exhibit good mechanical properties, which is possible with a high .ber volume content. However, the per-meability decreases with increasing .ber volume fraction (Vf)as shown by Eq. (1) 
[171]. 
.汕Vf
K . 汐 e e1T 
where 汐 and 汕 are permeability constants; The lower permeability of multi-layer 2D fabrics that is associated with nesting can be overcome by using 3D fabrics. However, further studies are required to understand the compaction behavior of 3D fabrics. 
The need to use multiple thin 2D fabric plies for achieving the target thickness can be eliminated, and the fabrication time can be reduced by using 
3D 
fabrics 
[136]. The 3D fabrics have through-thickness .bers (Z-
Fig. 7. Comparison of tow widths between experimental results (XCT) and DEM for different compaction levels: (a) Warp tows and 
(b) 
Weft 
tows 
[17]. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
Fig. 8. Effect of nesting and non-nesting on the resin .ow channels in multilayer stack using XCT [170]. 
binder yarns) running through at different interlocking angles. These binder yarns prevent the fabric from nesting and increase the trans-verse compaction properties (strength and stiffness) and permeabil-ity by maintaining a high Vf [172]. 3D fabrics also offer a high intra-laminar shear strength and impact resistance, which is attributed to stiffer binder yarns protecting the structure from delamination [173每176]. Therefore, 3D fabric composites are being increasingly used in the manufacture of aerospace structural components, partic-ularly for fabricating engine fan blades, due to their excellent out-of-plane properties [150,177]. The maximum applied pressure of 1 atm in resin infusion leads to a lower Vf in 3D fabrics. Therefore, manufacturing 3D composites by RTM is bene.cial compared to resin infusion [178]. 
Blunt 
The main factors that govern the compaction behavior of 3D fabrics are yarn stack shifting, tow .attening, and bending [172]. These geo-metrical variations were observed using an XCT by Yan et al. [179]. The XCT is a versatile tool for investigating both the compaction behav-ior of 3D fabrics and its composites and for generating numerical models [112,180,181]. Liu et al. [154] studied the linear and non-linear behavior of 3D orthogonal woven fabric composites with the help of XCT. Saleh et al. [182] developed a micro-mechanics based damage mechanics model for 3D orthogonal woven fabric composites using a mesoscale voxel model created through XCT. 
In general, a structure tensor approach is used for 2D fabrics, since the yarns are positioned in all directions [125]. However, the Texture Analysis for Image Segmentation method is well suited to 3D fabrics [128]. This method is used to split and analyze the warp, weft, and Z-binder yarns separately in XCT images. The numerical model based on DEM is well suited for predicting the .nal geometry and compaction re-sponse of 3D fabrics, as shown in Fig. 
9. The geometric variability of 3D fabrics in DEM simulations agrees well with XCT experiments for differ-ent compaction levels based on their target .ber volume fractions. How-ever, there was a slight deviation found in the compacted geometry of the Z-binder yarn between these two cases, particularly for low com-paction (Vf =45% andVf = 48.75%) levels. In Fig. 
9,the topsurface of the Z-binder yarn was found to be .at and the shape of the tows under this top surface was elliptical in XCT scans. In contrast, the top surface of the Z-binder yarn was found to be sharp and the circular shape of tows under this top surface was seen, in DEM simulations. However, yarn crimping and waviness followed the trends that were observed in the experiments [18]. 
4.1.2. Permeability characterization 
The XCT technique can play an essential role in permeability predic-tions of dry fabrics during the injection stage of LCM manufacturing. Permeability is an essential parameter in the LCM process, and its value depends on the .ber architecture/volume fraction, resin viscosity, porosity, and the other processing parameters (injection pressure, .ow rates, temperature, etc.) [183]. In particular, the permeability of UD .-brous structures depends on several parameters, such as the thickness of the fabric, .ber randomness in the fabric, .ber diameter, and the 
Elliptical Circular Sharp 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
Dimensionless Permeability(K11/r2) 
Permeability ratio (non-uniform/hexagonal) 
Tomadakis & Robertson (Overlapping-Analy cal) Tomadakis & Robertson (Non-overlapping-Analy cal) Drummond & Tahir (Analy cal) Westhuizen & Plesis (Analy cal) Sangani &Yao (Analy cal) Roy et al. (Experimental) Anderson & Warburton (Experimental) 
Sullivan & Hertel (Experimental) 
+ Williams et al. (Experimental) Tomography-based method (Moderately Aligned -CFD) Current work (Experimental) Current work (CFD) 
Porosity (汍) 
Fig. 10. Comparison of axial permeability of randomly UD .brous structure between experimental, numerical (CFD), hybrid (XCT and CFD) and analytical approaches [118]. 
porosity 
[129,130]. On the one hand, for a known porosity, the perme-ability decreases with increasing .ber diameter. On the other hand, the permeability increases with increasing porosity for a constant .ber diameter 
[118]. Fig. 
10 
shows the variation of the axial permeability of random UD .brous structures with porosity for different approaches, namely experimental, numerical (CFD), hybrid (XCT and CFD) and ana-lytical 
[184每187]. 
It can be observed from Fig. 
10 
that the CFD results agree very well with the experimental results and the tomography-based method. However, the relative standard deviation of experimentally measured permeability from the literature is N30%, due to the adoption of non-standard experimental setups. On the other hand, the analytical methods do not fully agree with the experimental results and CFD. This may be due to the underlying assumptions adopted in deriving the equations. 
The Geometric Permeability Estimator (GPE) is a new computational tool for predicting the permeability of .ber 
beds 
[111]. Fig. 
11 
indicates that the predicted permeability ratio using GPE compared with CFD 
[188] and unit cell models based on an area-weighted average, available in the literature [189每192]. The detailed modeling approach for Voronoi KC 
and 
Shou 
were 
reported 
elsewhere 
[191,193]. Good agreement with b6% deviation was obtained between the GPE and CFD approaches. In contrast to the GPE and CFD methods, unit cell models, such as Voronoi KC, Voronoi Shou, and sliding cell Shou underestimate and Delaunay KC method overestimates the GPE predictions. The drawback associated with these methods is that they assume that the .ber arrangements are either square or hexagonal. 
However, the problems associated with GPE are complicated since it requires inputs (.ow channeling and .ber positions before, during, and after impregnation, void, and resin information) in the form of images from in-situ XCT. In-situ XCT studies for permeability measurements are limited due to the high cost of the .xture and space limitations in-side the XCT. As a consequence, this restricts sample size [194,195]. 
Recently 
[57,58], and for the .rst time, continuous capturing of the deformation behavior of quasi-unidirectional (UD) non-crimp fabrics during resin infusion was achieved with the aid of the in-situ SXCT. The downsized setup is designed inside the XCT, such that the dry state of the fabric before infusion and saturated state after infusion are successfully captured. It is worth noting that the geometry changes that have been captured through this setup, include the vertical dis-placement of the tows, through-thickness tow swelling, and the in-crease in stack thickness. 
Vila et al. [59] examined tow level impregnation using in-situ SXCT. The ability of in-situ SXCT to capture detail at the tow level is highly valuable, due to its high resolution compared to LXCT. Fig. 
12 
represents the transverse .ow perpendicular to the E-glass tow captured simulta-neously at three different locations. A high local .ber volume fraction is seen in the regions of high capillary pressure. The wet region is highlighted in navy blue (Fig. 
12a). Transverse .ow is reduced in re-
3.5 3.0 2.5 2.0 1.5 1.0 0.5 0.0 

Sliding cell Shou Sliding cell KC Delaunay Shou Delaunay KC Voronoi Shou Voronoi KC GPECFD 
Fig. 11. Comparison of Permeability ratio with CFD, GPE and area-weighted average six 
gions where high local .ber volume fraction is apparent in the .gure, this leads to a decrease in permeability. It is worth noting that the per-meability and capillary pressures are contradicting each other. 
In general, for viscous resins, the optimum capillary number (Ca) can be obtained when the .ow propagates at an extremely low velocity, which leads to a high impregnation time. A high impregnation time is a major concern in composites manufacturing. To produce a high-quality product in a low impregnation time, further studies are required. A low impregnation time can be achieved through a detailed XCT analysis of the .brous preform. Figs. 
13 
(i) and (ii) correspond to unsaturated and saturated .ow through quasi-unidirectional non-crimp fabrics dur-ing resin infusion captured using in-situ SXCT. The capillary number in-creases from 4 ℅ 10.5 to 2 ℅ 10.3 as the applied pressure increases from 0 to 552 kPa. As a result, the impregnation time decreases from 
11.3 min (when both capillary forces drive the .ow and no pressure is applied) to 5.4 min (when the .ow is driven by both capillary forces, and 552 kPa applied pressure), as indicated in Fig. 
13(ii). Complete sat-uration occurs in the latter case 
[111]. 
For most aerospace and defense applications, 2D fabric composites are frequently used due to their high energy absorption capability com-pared 
to 
unidirectional 
composites 
[196]. Hence, there is a need to study the permeability of 2D fabrics. The saturated permeability of G-Ply (NCF) is one order of magnitude higher than G-weave (woven fabric) 
different unit 
cell 
models 
[111]. [197]. Hence, the former is better suited for high-viscosity resins, 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
especially to thermoplastic resins [198]. Differences in the mesoscale geometries of the two fabrics are the reason for the higher permeability of G-ply. Three different mesoscale geometries are observed in G-ply, based on variations in the number of layers as the level of compaction is different for each case, which can be seen from XCT and binary con-verted images shown in Fig. 
14. The meso-channel shapes in the unit cells are rectangular, hybrid (the combination of rectangular and trian-gular) and triangular, for 4, 5, and 6 layers, respectively. The shape change from rectangular to triangular occurs with an increase in the number of layers from 4 to 6. This is attributed to a combination of shrinkage and bending of the .ber bundles in the transverse direction. The rectangular pores cover a wider area than their hybrid and triangu-lar counterparts, and as a result, the saturated permeability is higher in this case. Such studies can be conducted on virtually any type of fabric with the aid of XCT geometric modeling to better understand the satu-ration behavior of laminates. 
Auniform .ow front is usually not possible with woven fabrics [38]; therefore, further studies are needed to attain a uniform .ow front and, most importantly, enhance the permeability of woven glass fabrics. Thin random fabrics (isotropic .ber preforms) have a single macro or bulk permeability as the micro-tows contain hundreds of .bers, which are loosely packed. As a result, the .ow through the preform is considered as homogeneous, i.e., the gaps (micro and mesopores) behind the .ow front are assumed to be .lled. Therefore, the continuity equation and Darcy's law are suf.cient to determine the macro-permeability [41]. For such cases, low-resolution XCT data is suf.cient to predict the permeability. 
In contrast, the .ow through tows cannot be ignored in the case of woven fabrics, due to their multi-scale .brous structure [199]. An XCT data-driven geometrical model can be developed using Geodict or VoxTex software packages. Having detailed features of the structure of the dual-scale fabric is desirable to compute the permeability accu-rately. Dual-scale .brous media rely on a sink effect at high capillary numbers, while the sink effect is negligible at low capillary numbers. As a consequence, .ow through a dual-scale .brous media resembles a single-scale 
medium 
[200]. However, to exactly model .ow through woven fabrics, the Navier-Stokes equation can be used for open regions to predict the macro permeability, and Darcy's equation can be used to model .ow through the .ber tows (micro-permeability). In the partic-ular case of a low pore Reynolds number, which is common in compos-ites, the model can be further simpli.ed to the Stokes equation. However, such coupling is inappropriate for solving the system of equa-tions when one uses Darcy's equation for analyzing the tow regions, since this equation does not have a shear stress term. Therefore, for modeling tows, researchers have replaced Darcy's equation with the Brinkman equation 
[201,202]. Shou et al. [189] proposed an analytical 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
(i) Capillary number of 4x10-5 and zero (ii) Capillary number of 2x10-3 and the applied pressure applied pressure of 522 kPa 
model for the quick prediction of in-plane permeability in a dual-scale .brous media. The input parameters such as .ber volume fraction, number of .laments, and .lament diameter are required for this model. Several researchers [16,119,203,204] have used the Kozeny-Carman equation for predicting the permeability of fabrics. More details about the modeling approaches are reported elsewhere 
[205]. 
A number of researchers [206,207] have also used experimental techniques to study resin .ow through dual pore .brous assemblies and determine the permeability with the help of a camera. XCT data-driven geometrical modeling was recently adopted to enhance the per-meability of fabric by introducing glass beads of different diameters and volume fractions (Vp) in between the .ber preform layers, which also enhances the mechanical properties. However, there is a critical size (100每200 米)and Vp = 10%, or optimum value (Fig. 
15c) below which, there is not much change in permeability, though the mechanical prop-erties 
increase 
compared 
to 
pristine 
fabric 
composite 
(Fig. 
15a). Above this value (Fig. 
15d) the mechanical properties decrease due to exces-sive out-of-plane tow bending and pore opening, although the perme-ability was increased. Such observations were made by Caglar et al. 
[208] using a laboratory XCT with the region of interest (ROI) of 1244 ℅ 1244 ℅ 304 voxels and smaller crop size of 304 ℅ 304 ℅ 304 voxels. 
Compared to experimental and analytical approaches, numerical ap-proaches are somewhat cheaper and require less time to calculate the permeability. The unit cell geometry for a numerical approach can be generated automatically (ideal approach) [209] or through XCT (realis-tic 
approach), 
using 
textile 
software 
packages 
[210], as mentioned ear-lier. The overall process, ranging from textile model creation to .ow simulation, requires little time (4.7 min or 282.37 s) for a woven roving of a single layer of E-glass fabric shown in Fig. 
16(a), which con.rms the advantage of the numerical approach over the experiment. 
As previously discussed, incomplete saturation of the micro-tows is the major drawback of 2D woven fabrics, due to their dual scale nature. As a result, there is a compromise between high permeability and .ber volume fraction (Eq. 1). However, permeability can be enhanced by the use of 3D fabrics. In contrast, the cost of 3D fabrics is high, and experi-mentally measuring the permeability is time-consuming and tedious, due 
to 
their 
complex 
geometry 
[212]. 
However, in recent times, there has been a growing interest in predicting the permeability of 3D fabrics using the hybrid approach, which is a combination of experimental and numerical approaches to create 3D fabric digital twins [16]. The hybrid approach requires XCT images as input to create the voxel geometry using software, such as Geodict, VoxTex, etc. and run the simulations using the same or any 
Fig. 14. XCT (left-hand side) and binary-converted (right-hand side) images indicating cross-section views of meso-channel shapes for different layers of G-ply fabric stack: (a, b) 4, (c, d) 5 and 
layers 
[197]. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
(d) 400每800 米m[208]. 
other .ow analysis software, such as Fluent, Ansys/CFX, etc., with a CFD method [120]. Ali et al. [119] studied the in-plane permeability of 3D fabrics with different approaches, as shown in Fig. 
17(a, b), namely ex-perimental and hybrid modeling (XCT-numerical based). The hybrid method was performed in Math-to-Market's Geodict software, after creating the unit cell geometry in a digital twin format using XCT im-ages. The predicted permeability values from the hybrid approach agreed well with the experimental results. The permeability in the warp direction (K22) was found to be lower than in the weft direction (K11) in their case. 
4.1.3. Void characterization 
The formation of voids during the LCM process depends on several parameters, such as resin degassing procedure, volatiles in the resin, the .ow front velocity, mold clamping pressures, temperature, areal density of fabric and stacking sequence, resin viscosity, insuf.cient cur-ing, 
and 
surface 
tension, 
etc. 
[213]. The voids act as stress concentrations and crack initiators in the composite [214], causing the mechanical properties to reduce signi.cantly. The voids can be reduced by correctly 
Texle image 
positioning the injection and vent gates, degassing the resin before in-jection, increasing the packing pressure during curing, optimizing the .ow velocity, etc. [215,216]. 
Void formation between the tows and inside the tows in the LCM process was predicted 
by 
Park 
et 
al. 
[217] using theoretical modeling and simulations. These predictions were correlated with the experi-mental results from the literature [218], and good agreement was ob-tained. It was found that the void content relies not only on resin velocity but also on vacuum pressure. Further details regarding void size, 
shape, 
and 
distribution 
can 
be 
found 
in 
[219]. In general, optical mi-croscopy, scanning electron microscopy (SEM) and ultrasonic C-scan are widely used for characterizing void morphology in composites, since they are less expensive techniques [143,220每222]. However, as discussed earlier, the XCT gives better 3D information about voids than 2D imaging [14,55,124]. Suzuki et al. [223] introduced the dual-energy XCT concept for determining the size, shape, and distribution of voids in FRP composites. Nikishkov et al. [14] developed the density-based void contouring method to quantify the void dimensions (shape and porosity content) even in low contrast materials (e.g. 
Tex le Model Dry Fiber Mesh Compac on simula on Create Voxel Mesh 
Prepare Mesh Flow Simula ons 
(b) shows the step by step procedure for the permeability prediction of a single layer fabric unit cell. The textile model takes 2.77 s to run, followed by the dry .ber mesh creation phase (38. 8 s) using Texgen software after uploading the textile .le from Matlab. The compaction simulation can be performed in Abaqus software using a Python script. Following this, the voxel Mesh can be created using Matlab, and then the mesh size was reduced using Hyper Mesh software. Finally, .ow simulations can be performed using Ansys CFX software to predict the permeability with an overall computation time of 282.37 s. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
(a) In-plane weft permeability (K11) (b) In-plane warp permeability (K22) 
carbon/epoxy) investigated by XCT. The robustness of this method was evident, which minimized the error in void estimation. The porosity maps can be obtained from XCT data visualization software packages such as VGStudio Max, Avizo, Simpleware, ImageJ, etc. [4], after reconstructing 
the 
XCT 
images 
in 
CT 
Pro 
[64] or Phoenix datos|x [2]. In particular, VGStudio Max and Avizo software packages offer great po-tential for segmentation, surface determination, and porosity/inclusion analysis of XCT images [224,225]. The voids in XCT images can also be segmented using a trainable segmentation tool known as Fiji [226]. 
XCT is not generally employed for tracking voids during the resin in-fusion process, owing to dif.culties (sample size restriction and high cost) associated with the in-situ setup. Little work has been performed on void characterization using in-situ XCT, except for a small number of papers [57每59,111]. However, the Synchrotron-Radiation Computed Laminography (SRCL) setup can be used to capture void generation and transportation, even in large-sized samples [227]. SRCL enables one to capture the images with different phase contrasts in different materials (e.g., air, .ber, resin, etc.) during resin in.ltration. However, the use of XCT has increased rapidly in efforts to identify and analyze both macro and micro-voids and their content in composites processed using LCM [62,228,229]. 
The transportation of voids in LCM relies on void size, shape, and lo-cation. The equivalent diameter (De) of each void is represented based on its area (A) which can be measured using ImageJ software or any other 
image 
analysis 
software 
packages 
and 
is 
given 
by 
[104,105]. 
r...... 
4A
De .e2T
羽 
The voids can be classi.ed in Eq. (2) 
as small (De b 50米m), medium (150 b De ≡ 50 米m) or large (De ≡ 150 米m) voids [48]. The void shape can be classi.ed as spherical or irregular. Spherical void shapes can be further classi.ed based on their shape roundness (Sr) which is de.ned as, 
De 2 
Sr .e3Tdmax 
where dmax is the maximum diameter of the void. Spherical voids can be categorized using Eq. (3) 
as circular (1 ≒ Sr N 0.9), elliptical 
(0.2 b Sr ≒ 0.9) or highly elongated (Sr ≒ 0.2). The assessment of voids using XCT can help to predict the failure region and enhance the resid-ual properties of the composite [60]. 
4.2. OoA processing using vacuum bag only (VBO) 
The placement of prepregs on the mold surface during VBO prepreg curing can be achieved in two ways: (i) Hand-layup (HLU) which is a method in which prepregs are placed manually and (ii) Automated Fiber Placement (AFP) and Automated Tape Laying (ATL) 每 in these two techniques, prepregs can be placed automatically and oriented as desired using a computer-controlled poly-articulated robot. AFP is used for placing narrow prepreg strips (width between 3.175 mm and 
12.175 mm) while ATL is used for placing wide prepreg tapes (width be-tween 75 mm and 350 mm) [249,260]. Stacked prepregs produced using HLU and automated techniques can be cured using a hot air oven. The maximum applied pressure possible in the case of VBO pre-pregs is 1 atm. The major difference between the consolidation process of VBO prepregs and autoclave prepregs is the applied pressure. The OoA VBO processing parameters, such as air permeability and the devel-opment of porosity/voids during and after processing, are discussed in Section 
5, along with the autoclave processing parameters. Also, Table 
2 
represents a comprehensive summary of the literature available on determining process parameters associated with OoA and autoclave techniques using XCT. 
5. Application of XCT based material models in autoclave processing 
It is well known that the void content in a component fabricated by autoclave molding is low, due to the high applied pressures as com-pared to that applied to OoA grade prepreg systems. However, OoA con-solidation of prepregs can produce void-free components (b 1%) 
[52], if the correct cure cycle and air evacuation procedures are adopted [261,262]. Moreover, the cost of autoclave-fabricated components using both HLU and automated prepreg placement techniques is signif-icantly higher when compared to OoA fabricated components [263]. Therefore, a comprehensive discussion of HLU and automated tech-niques in terms of cost, production time, and ef.ciency is necessary. Fig. 
18 
presents the product cost for fabricating two types of CFRP pre-pregs (intermediate modulus (IM) and high strength (HTA)) compos-ites manufactured by the autoclave process. From the .gure, it is clear that the labor and material costs (including material wastage) are high in the HLU process, while equipment costs (capital investment) are high in the automated (AFP and ATL) technique. The tooling cost (including cooling of the equipment) is almost the same in both cases. However, automated techniques offer the advantage that the material wastage and manpower are lower as compared to HLU. The ATL place-ment head cost is higher than the AFP head, as a consequence, the equipment and material costs associated with ATL is higher than AFP [260]. 
As compared to the HLU technique, AFP and ATL are widely used in high production industries due to their ability to reduce part cost, pro-duction time, [30] and yield a low void content in fabricated compo-nents [264]. In particular, ATL is used for manufacturing aircraft structural components, such as ribs, spars, I-beam stiffeners, etc. Though ATL is very effective in manufacturing large .at parts, it is dif.cult to use for manufacturing more complex geometries (double curvature sur-faces) such as wing spar C, fuselage barrels, engine cowls, etc. [249,265,266]. In contrast, AFP can be used for manufacturing complex-shaped components [267]. The layup rate for ATL is in the range of 25每50 kg/h for producing simple .at geometries [268]and 12 kg/h for manufacturing relatively complex pro.les [269]. In contrast to ATL, the layup rate for AFP is slow i.e., approximately 5.54 kg/h, which 

K. Naresh et al. / Materials and Design 190 (2020) 108553 
Table 2 
Key process parameters in OoA and Autoclave using XCT, as reported in the literature. 
Manufacturing Technique Process parameters in OoA and Autoclave References 
Dry .ber compaction Prepreg compaction Permeability Air permeability Void formation and characterization 
Thompson et al. [117] Desplentere et al. [112] Naouar et al. [125]
can be attributed to predominantly accelerating/decelerating the head movement 
in 
curved 
surfaces 
or 
interfaces 
[270]. 
Although AFP and ATL techniques are widely used in high produc-tion applications, the challenges associated with these techniques such as, tow gaps and overlaps occurred in the part, which can be attrib-uted to .ber placement head movement/backlash, tow width varia-tions, dif.culties in steering the tows [249,260]. To overcome this, process parameters, such as lay-up temperature, compaction roller pressure, lay-up speed, etc., need to be studied with the use of in-situ defect monitoring NDT techniques. Further, the development of FEM-based XCT aided geometrical models of the material using images ex-tracted from the XCT is a topic of great interest, and can help in develop-ing cure simulation models of the complete cure cycle for understanding the material behavior at each stage during processing [271]. 
The curing behavior of prepregs can be often treated using two .ow models, namely, percolation [272] and shear stress models [273]. The resin .ows through the .ber bed in the former case, whereas both the resin and .bers .ow together in the latter case, as a result of deviatoric stresses. Percolation models are generally used for predicting the con-solidation behavior of thermoset prepregs, and shear .ow models are used 
to 
predict 
the 
curing 
behavior 
of 
thermoplastic 
prepregs 
[274,275]. 
It is important to consider the magnitude of the compaction force acting on the .ber bed and the .uid pressure during consolidation of FRP composites. Gutowski et al. [276] applied the effective stress theory developed by Terzhaghi [277] and Biot [278], to the consolidation of laminated composites using Eq. (4) 
[279], 
e4T
考a . 考 f t Presin 
where 考a and 考 f are the applied stress and .ber bed effective stress; Presin is the hydrostatic resin pressure. 
Therefore, to study autoclave and VBO prepreg curing processes, three characterization parameters, namely compaction, air permeability 
K. Naresh et al. / Materials and Design 190 (2020) 108553 

and porosity/void characteristics of prepregs can be determined through XCT characterization setups. 
5.1. Compaction of prepregs 
The prepreg compaction process is important for the consolidation of laminates fabricated using autoclave/VBO processes. Inter-ply and intra-ply porosities can be eliminated by reducing the thickness of the stack during the compaction process [250,280]. Wang and Gutowski 
[281] developed analytical models to remove gaps in a stack, based on an idealized geometry. However, removing the gaps is not suf.cient, since accounting for other defects associated with vacuum consolida-tion of prepreg is also important. Bleeding and squeezing effects are also important parameters to consider during compaction, which have asigni.cant impact on air permeability, as the through-thickness com-paction 
of 
prepregs 
dictates 
the 
part 
thickness 
. Belnoue et al. [249] deliberately created gaps and overlaps when laying down IM7/8552 prepreg using AFP and consolidated the prepreg stack under vacuum. The specimen geometry was then analyzed using XCT. The FEM geome-try was created using Abaqus and validated with the XCT geometry. The thickness variation and defect area predicted by the FEM model corre-lated well with the experiments. 
Fiber misalignment is a common cause of defects and occurs mainly during the compaction process due to applied pressure. The degree of .ber misalignment in carbon/epoxy prepreg laminates fabricated using ATL followed by the autoclave process was investigated by Nguyen et al. [258] with the help of XCT, and they found that the max-imum ply disorientation was approximately 1.2∼. The maximum in-plane .ber misalignment was in the range of 2∼每4∼, and the out-of-plane .ber misalignment was 1.25∼. The ImageJ software can be used to measure tow and .ber misalignments in a woven 
fabric 
[181] after curing. However, the degree of .ber misalignment can also be measured by performing in-situ compaction experiments using XCT. 
Similar to dry fabrics, the compaction response of prepregs follows a viscoelastic, non-linear, stress-strain relationship. Any increase in the compaction load leads to an increase in .ber volume fraction, which can be attributed to the domination of .ber bed compaction compared to resin [275]. Therefore, the compaction response of the un-cured pre-preg relies on the viscoelastic or inelastic properties (stress relaxation, creep, and cyclic properties), studying these properties is very impor-tant. In general, the viscoelastic or inelastic properties of the prepreg can be studied using a dynamic mechanical analyzer (DMA), differential scanning calorimetry (DSC), and a universal testing machine (UTM) [282每285]. The tracking of resin .ow is not possible with these instru-ments, which is a major drawback of these characterization techniques. In contrast, the overall process can be adequately characterized and vi-sualized with the aid of in-situ compaction using XCT, by incorporating a compaction .xture with an environmental chamber inside the XCT machine. 
There are challenges associated with studying the viscoelastic re-sponse of prepregs using in-situ XCT. The relaxation behavior of pre-pregs depends on temperature, .ber orientation or stacking sequence, rate effects, strain amplitude, sample size, frequency, etc. The stress re-laxation response of prepregs decreases with increasing temperature and decreasing % strain. Also, temperature has a signi.cant effect on the thickness of prepregs during stress relaxation tests [250]. Besides, unidirectional fabrics undergo higher compaction than cross-ply fabrics, which can be attributed to more friction between the .bers in the latter case 
[286]. Nevertheless, XCT can help to better understand rate effects in the compaction behavior of prepregs and associated parameters. 
To mimic the autoclave process, numerous compaction tests need to be performed at different pressures, therefore, un-interrupted in-situ XCT is ideal, but achieving good image contrast using LXCT is dif.cult, due to its high acquisition time and low .ux. On the other hand, performing such experiments using SXCT is very expensive, and there are sample size limitations. Further, SXCT may also damage the speci-men during continuous temporal studies, due to the high .ux. 
5.2. Air permeability through prepregs 
OoA/Autoclave prepregs can be fabricated by applying the resin .lm on both sides of the fabric, such that the dry fabric areas (partial impreg-nation) are maintained at the center in most of cases for removing the entrapped air during lay-up at room temperature. Air permeability is the primary parameter in prepregs that determines the component void content. Air permeability mainly depends on the degree of impreg-nation (DOI). The DOI is de.ned as the ratio between the .ber saturated volume to the total volume of the stack [61]. DOI can be measured by two methods; (i) manual method by performing the water pick-up test, and (ii) non-destructive testing (NDT) method with the help of two instruments, namely XCT and infrared (IR) thermography. Out of these two methods, the water pick-up test is the simplest and avoids the use of costly NDT instruments and post-processing tools. In this method, prepreg specimens are immersed in water; DOI is measured 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
based on the weight change of the specimen before and after immer-sion. Measured DOI based on the XCT approach using Otsu's method and water pick-up test correlates well than IR thermography based ap-proach [287]. 
The higher the degree of impregnation, the lower is the air perme-ability. As a result, the entrapped air locks within the plate. On the con-trary, the lower degree of impregnation results in unsaturated areas in the .ber bed, due to insuf.cient time to cover all areas before resin ge-lation 
[61]. Hence, there should be a balance between these two. How-ever, the latter is less cumbersome than the former, since it can be controlled by choosing a proper dwell time. 
Compaction pressure and temperature play a vital role in determin-ing the air permeability of prepregs. Determination of the air permeabil-ity is the .rst step for developing process models to predict the evacuation of entrapped air and the vacuum hold time [288]. Air perme-ability is higher at room temperature de-bulking, where the molecules in the resin are immobile in that state. However, the air permeability starts to decrease at elevated temperatures as the resin starts .lling the gaps due to the higher mobility of molecules at just slightly below or above the glass transition temperature (Tg). This can be attributed to a lower resin viscosity at this temperature. The .rst isothermal dwell period in a cure cycle is usually designed slightly below or above the Tg value of the resin for possible air removal from the compo-nent 
[243]. Further, any increase in temperature beyond the .rst dwell, the molecules again have become immobile due to resin starting to gel. However, there is no change in air permeability at the end of the curing process 
after 
resin 
gelation 
[289,290]. Xin et al. [291] compared the air permeability of prepregs fabricated through autoclave and VBO pro-cesses. It was found that the percentage of voids are independent of the air permeability in autoclave-fabricated parts, due to the high com-paction pressures. Conversely, entrapped air is naturally dif.cult to evacuate completely during debulking at 1 atm pressure in VBO pro-cessing prepregs. Therefore, voids in this process rely on the air perme-ability, particularly in the case of thick laminates. The lack of air permeability leads to a high void content in the VBO process. 
The falling pressure method has been used by a number of re-searchers [289,292] for measuring the in-plane and through-thickness air permeability of prepregs. In-plane air permeability can be measured using this method by using the vacuum bag consumables, mainly plac-ing the edge breathing dams around the prepreg stack to ensure air ex-traction in the in-plane direction and using a non-perforated release .lm to prevent air.ow in the through-thickness direction [288]. On the other hand, vacuum bag assembly, including the porous release ply or perforated release .lm, can be used to perform through-thickness air permeability experiments [240]. However, measuring the air permeability using the current experimental methods is time-consuming and labor-intensive. The movement of voids is challenging to track during the experiment. Instead, the uncured prepregs can be compacted dynamically using in-situ XCT. Further, voxel-based material modeling approaches can be developed by using XCT images of compacted uncured prepregs as an input in .ow simulation software, such as Geodict, VoxTex, etc., to predict the air permeability of prepregs. 
The air permeability of prepregs varies with .ber architecture and orientation effects. Kratz and Hubert [240] investigated the air permeability of three different OoA prepregs, these being unidirec-tional, plain weave, and satin weave, based on their porosity net-works and dry tow area using XCT. Out of three prepreg systems, the plain weave prepreg exhibited higher in-plane and transverse air permeabilities, owing to their high crimp, which can be attributed to the interlacement of every warp tow with a weft tow or vice-versa. In contrast, unidirectional prepregs do not possess any crimp, resulting in the lowest air permeability amongst the three prepregs. The interlayer air permeability of carbon/PEEK (AS4/ APC2) thermoplastic prepreg stacks based on different orientations, namely 0∼/0∼, 0∼/90∼ and 90∼/90∼ was studied using XCT by Zhang and Gillespie [293]. It was found that the air permeability was higher for the 0∼/0∼orientation (.bers and .ow direction are same) and least for 90∼/90∼ orientation. 
Moreover, it is well known that the transverse air permeability of prepregs is lower than the in-plane air permeability, due to the anisot-ropy of prepregs [294]. Several researchers [266,295,296]madeat-tempts to enhance the transverse air permeability of prepregs. As compared to continuous .lm prepreg, the semi-prepreg (partially resin-impregnated areas throughout the .ber bed surface) exhibit three orders of magnitude higher transverse air permeability [297]. This is because the volatiles can escape through the air evacuation chan-nels in the semi-prepreg. However, semi-prepreg materials require a low viscosity resin, due to the large dry areas. As a result, resin bleed out is dif.cult to control. 
Enhancing the transverse air permeability and reducing surface po-rosity issues in OoA, which are the major issues in woven prepregs, could be possible by using USCpreg, developed by the University of Southern California (USC) research group [298]. The USCpreg (Fig. 
19a) can be fabricated by creating dry gaps over the fabric using a pair of resin-coated rollers [299]; with this, through-thickness air transport is possible. Moreover, zero surface porosity and defect-free components are possible without the need for edge breathing dams. Fig. 
19(b) presents the through-thickness air permeability values for different prepregs, namely, control, one-sided, USCpreg and traditional VBO prepreg. It is clear from the .gure that the USCpreg exhibits a higher through-thickness air permeability, irrespective of the number of plies compared to the other prepregs. 
As with the USCpreg approach, a polymer .lm dewetting method 
enhances the transverse air permeability of the prepreg by dewetting the resin below the cure temperature on a substrate. It leads to a discontinuous resin pattern, which creates additional path-ways in the through-thickness direction for gases to evacuate, resulting in porosity-free laminates, even at low pressures associated with the VBO curing process. The transverse air permeability of dewetted pre-preg (Prepreg-104-120) was 36每52 times higher than that for the UD control prepreg. Therefore, the former exhibited only .ow-induced po-rosity (intra-ply voids), while the control prepreg exhibited both air (gas) induced (interply voids) and .ow-induced porosity, which can be seen in the XCT images in Fig. 
20. The intra-ply voids are due to in-complete wet-out, which can be attributed to the adoption of either im-proper cure cycle, i.e., an insuf.cient dwell time at low viscosity, or air that is not entirely removed between the .bers inside the tows during the initial stage. 
5.3. Void characterization in prepregs 
Void characterization is usually done at different stages of the manufacturing cycle to understand the effects of process conditions on the development and movement of the voids. Moreover, it is also very important to characterize the spatial distribution of voids in cured com-posites and to understand the performance of the manufactured part. The size and shape of voids depend on .ber architecture, type of resin used, and stacking sequence [300]. The void volume fraction (Vv)is a commonly-used void characterization parameter in FRP composites [261], which can be determined for layer-wise and global sample using VGStudio Max software, after post-processing XCT images [124,224,254]. The ability of VGStudio Max software to obtain layer-wise void distribution is highly valuable. 
Extensive studies have been performed on post-cured samples using XCT 
to 
characterize 
the 
presence 
of 
voids 
[14,132,244]. Detailed discus-sions on void characterization and its effect on the performance of the composites can be found in recent literature reviews [60,243]. Kastner et al. [301] studied different types of void content in carbon/epoxy pre-preg samples ranging from 1 to 10% by volume, using XCT. Spherical-shaped voids were found in samples having a void volume content of b3%, while .at-shaped voids were observed in samples with a void vol-ume content of above 3%. Such voids degrade the mechanical properties 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
(a) Manufacturing process of USCpreg (b) Through-thickness air permeability 
of composites and reduce component ef.ciency. The effect of voids on the mechanical property degradation of autoclave fabricated compos-ites has been studied by many researchers [124,132,253,302,303] using XCT. Tserpes et al. [252] developed a progressive damage model based on data obtained from XCT scans to analyze the effect of void con-tent effect on the transverse tensile properties of carbon/epoxy prepreg composites fabricated using the autoclave process. The results highlighted a small decrease in stiffness, and a signi.cant decrease in tensile strength, as the void content increases from 1.56% to 3.43%. 
In general, four possible causes of void formation in prepregs are: 
(i) 
air entrapment between or in .ber tows during prepreg manufactur-ing, (ii) moisture absorption in the prepreg during storage, (iii) air en-trapment between the plies during lay-up, and (iv) volatiles generated during resin cure [36,304,305]. Approaches to minimize issues (ii) to 
(iv) 
during prepreg manufacturing are discussed below. 
Absorbed moisture can be predicted using a diffusion-based void model [306] and removed by optimizing the effect of prepreg out time [261]. Void growth takes place only when the concentration of moisture in the resin (Cresin) is higher than the concentration of moisture at the void surface (Cvoid). The parameter Cvoid is directly proportional to the square of the pressure [261,307]. Therefore, void growth would not take place during autoclave consolidation of prepregs, due to their high operating pressures, when compared to the VBO consolidation of prepregs. Air entrapment can be eliminated by choosing an optimum DOI during prepreg production and interrupted debulking during lay-up, 
which 
relies 
on 
the 
thickness 
of 
the 
plate 
[308,309]. However, com-plete void removal is not possible at room temperature; for that, under-standing the effect of heating rate, degree of cure, resin viscosity, applied pressure, temperature ramp, dwell time, and post-cure time on cure cycle is important. In particular, choosing the appropriate dwell temperature is essential to enable resin .ow into the porous areas 
[310]. 
These 
details 
can 
be 
found 
in 
[311]. The cure cycle is usually designed in such a way that a lower resin viscosity and, therefore, a lower degree of cure are maintained until the end of the .rst dwell cycle for possible void transport [243]. The resin viscosity can be mea-sured using a rheometer, whereas the degree of cure (汐) can be mea-sured using differential scanning calorimetry. These are given by [312,313], 
G0 iG}
米 .e5T
肋 
忖Hresidual 
汐 . 1. e6T
忖Htotal 
Prepreg type  2D Visualization  3D Visualization  
Control Prepreg  
Prepreg 104-120  
Fig. 20. Visualization of microstructure in 2D and 3D for different UD prepregs treated with ammonia using XCT prior to cure 
[238].                          
where 米 . is the complex viscosity; G∩ and G" are the storage and loss moduli, respectively; 肋 is the angular frequency; 忖Hresidual and 忖Htotal are the residual and total reaction heat, respectively. 
The effect of these parameters can be studied on un-cured prepregs at different pressures and temperatures by performing in-situ experi-ments using XCT to mimic the complete cure cycle. For example, the vacuum bag mold closing process can be replicated by performing com-paction and stress-relaxation experiments while the dwelling process can be mimicked by performing compaction creep experiments. The consideration of residual stress/strain before designing the cure cycle is also essential, since these properties can be affected by the aforemen-tioned 
cure 
cycle 
parameters 
[314]. 
XCT is a promising tool for measuring all four possible ways of voids discussed above. However, the in-situ XCT can be effectively used only to monitor the fourth possible void type during composite laminate cur-ing. Plessix et al. [255] performed in-situ ultra-fast SXCT experiments to study the porosity behavior of carbon/epoxy prepregs by varying the pressure to replicate the autoclave cure process. They used two cure cy-cles with one of them, similar to the OoA prepreg process performed at a constant pressure of 0.1 MPa, and the other is shown in Fig. 
21(d). The dramatic decrease in void content with the increase in pressure from 
0.1 MPa to 0.8 MPa is evident in Fig. 
21(b). The void content increases again when the pressure decreases from 0.8 MPa to 0.1 MPa (Fig. 
21c). These results emphasize the value of using XCT for in-situ real-time monitoring of the voids in laminates. 
Post-cured laminates generally contain two types of porosity: 
(i) surface porosity and (ii) internal porosity. The reason for the forma-tion of these porosities and possible ways to minimize or eliminate them during the consolidation of the composite from placing the pre-preg on the mold surface to the end of the composite curing process are discussed below. 
5.3.1. Surface porosity 
Air entrapment between the interface of the .rst prepreg ply and the mold surface causes surface porosity. There are several parameters, such as prepreg out-time, freezing, and process effects, which cause surface porosity. Amongst these, vacuum hold time plays an important role in determining the level of surface porosity, since longer vacuum hold time gives lower levels of porosity. Details about these effects are re-ported in [315]. However, the effect of surface porosity on the structural capability of a component is less compared to internal porosity, al-though it reduces the aesthetic appearance of the product. Conse-quently, it may result in the extra costs for the surface .nishing of the component. These surface porosities can be estimated and then mini-mized using statistical analysis tools and with the help of VGStudio Max software, after reconstructing the XCT images [124,225]. 
Extensive surface porosity in woven prepreg components leads to surface roughness (58.6 米m). In contrast, UD prepregs typically exhibit a smooth surface (12.7 米m). This is attributed to different .ber architec-tures of the woven and UD prepregs [316]. The weaving patterns of woven fabrics are such that there are regions (overlaps and underlaps) for easy air entrapment; this phenomenon has been studied using XCT [240]. Surface porosity in woven prepregs can be minimized using a sil-icone rubber contoured roller instead of a conventional hard rubber roller 
At the same time, this porosity can be entirely eliminated by 
the 
usage 
of 
USCpreg, 
as 
discussed 
earlier 
[299]. 
5.3.2. Internal porosity 
Unlike surface porosity, internal porosity occurs between adjacent plies and .ber tows. Centea and Hubert developed a representa-tive model to predict tow impregnation and validated the model with the results from scanned prepreg samples using XCT. It was reported that if the cure cycle involves a high initial resin degree of cure and a low temperature, .ow-induced voids can occurr, particularly for tows with more circular geometries (high .ber volume fraction) due to the long tow .ow region. Voids between .ber tows as a function of prepreg out time were studied using XCT by Grunenfelder et al. [246]. They found that the .ber tow porosity of a prepreg increases, as the room temperature aging time increases than the prepreg stated out life ac-cording to the manufacturer datasheet. Besides, such porosity remains in the .ber tow when using a high degree of cure resin. The internal po-rosity degrades the mechanical properties of the component. Therefore, understanding resin cure kinetics and rheology is important when try-ing to eliminate internal porosity. 
5.3.2.1. Resin cure kinetics and rheology. OoA prepreg processing requires a long cure cycle to ensure that the resulting properties are equivalent to autoclave quality. This is due to the fact that a lower consolidation pres-sure is applied [318]. However, it is clearly desirable to reduce the manufacturing time without compromising the product quality when using OoA resins. These resins should normally react (high viscosity) at room temperature for the air evacuation channels remain open and to prevent ※cold .ow,§ on the other hand, these should have low viscos-ity and less volatile generation at elevated temperatures for complete saturation of the porous areas before gelation and .nal cure. ※Cold .ow§ is also induced by long debulking cycles, which leads to intra-tow voids [304]. However, it is clear from the XCT images that one hour debulking signi.cantly reduces the presence of inter-ply voids in prepregs compared to samples without any vacuum hold, as indicated in Fig. 
22 
[244]. The inter-ply voids are represented by a gray color. This decrease is attributed to tow nesting and ply adhesion, due to compaction. 
To date, the majority of resins used in aerospace-grade prepregs are toughened epoxy resins [284], which induce exothermic reactions at high temperatures. Consequently, localized heat (thermal overshoot) occurs in the center of the laminate, or where the thickness of the lam-inate is high [319]. Besides, there is a tendency for components to fail catastrophically owing to less impact energy of aerospace-grade epoxy resins compared to thermoplastic polymers. Therefore, researchers should focus on other resin systems as well, to overcome this. Prepregs manufactured based on a hybrid resin (thermoset and thermoplastic) system possess excellent impact energy compared to thermoset pre-pregs [320]. 
The bleeder geometry also determines the resin volume fraction in a laminate, which possesses a signi.cant effect on the cure cycle. The in-.uence of pore gaps in the bleeder, and its thickness on the degree of cure, and thermal overshoot of graphite/epoxy prepreg composites was studied by Ganapathi et al. [321]. A thin bleeder generally has fewer pores and absorbs less resin, which exhibit a high thermal over-shoot at the center of the laminate while the thick bleeders cause lower thermal overshoot and delay the degree of cure. For further de-tails about the voids induced by the thermal gradients during prepreg consolidation 
studied 
using 
XCT, 
is 
given 
elsewhere 
[239]. An decrease in void content with increasing temperature from 30 ∼C to 90 ∼C during the consolidation of aerospace-grade prepreg systems (Hexcel) was found using XCT, coupled 
with 
VGStudio 
Max 
software 
[224]. 
5.3.3. Effect of cure cycle on void formation/removal 
The entrapment of voids in the micro and macro-regions of a com-posite are dif.cult to control. These parameters rely on the compaction mechanism 
(room 
temperature 
vacuum 
hold) 
and 
the 
cure 
cycle 
[305]. A comparison of interply and intraply voids for different cured condi-tioned laminates (fresh, t1,t2,t3,t4, and cured) by hand-layup and auto-mated .ber placement (AFP) is shown in Fig. 
23 
[243]. It can be seen from the .gure that most of the voids in the fresh state are intraply voids, due to the air evacuation channels in both HLU and AFP. These intraply voids are completely removed after the .rst ramp (t1) in AFP processing and after .rst dwell (t2) in HLU. Most of the interply voids are eliminated after the .rst dwell phase in AFP while the interply voids are removed after the .rst dwell in HLU, though complete interply void removal is not possible. The resin starts gelling after the .rst dwell; therefore no residual voids can be removed after this stage [322]. The 
(a) No vacuum hold (b) 1 hour vacuum hold 
increase in interply voids after t2 in both AFP and HLU is due to a change in pressure. Therefore, changing the pressure after the .rst dwell is not desirable in the OoA process. The overall void content is lower in the case of AFP than in HLU, though the intraply void content in the fresh state is slightly higher for AFP (6.01%). Hence, it emphasizes the robust-ness of AFP over HLU. 
Fig. 
24(a) shows the void spatial distribution in unidirectional car-bon/epoxy prepreg laminates fabricated according to four different cur-ing cycles. Fig. 
24(b) shows the rod-like structure, which includes the coalescence of voids along the .ber direction (Z-axis) [313], since the air permeability is higher in the .ber direction relative to perpendicular to the .ber direction [323]. Amongst these four laminates, LAM-C2 (Fig. 
24a) is optimum in terms of achieving a low void volume fraction (Vv = 0.4%), due to maintaining a lower viscosity for a longer time, be-fore resin gelation. However, the time available for void suppression or mitigation in LAM-C1 is less, due to the higher viscosities that lead to a higher Vv (2.9%), compared to the other laminates. However, the resid-ual porosity (1.1%) is lower in LAM-C3 compared to LAM-C1 and LAM-C0 (Vv = 2.7%), due to higher .rst dwell (130 ∼C) after the initial .ash. The void content in LAM-C1 can be reduced by varying the pres-sure instead of maintaining it constant and reducing the initial dwell temperature, which is equivalent to LAM\\C2. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
The XCT is a powerful tool for analyzing voids, not only in thermoset leads to higher levels of void formation (Vv = 3.81%). On the other prepreg composites but also in thermoplastic prepreg composites. In hand, no voids were present when the laminate edges are open to vac-contrast to thermoset prepregs, the void characterization of thermo-uum [236]. plastic prepregs mainly relies on (i) through-thickness air diffusion and (ii) a combination of air diffusion to a single layer and through the permeable interlayer regions. Such void reduction mechanisms in 6. Future directions carbon/poly-ether-ether-ketone (PEEK) composites can be seen in the XCT images in Fig. 
25. It is clear from the .gure that through-thickness XCT studies are useful in designing and developing material models/ air diffusion only occurs when the laminate edges are sealed, which twins for manufacturing advanced aerospace composite structures. 
(a) 72-layer with edge sealed  (b) 72-layer with perimeter open to vacuum  
3D volume  Voids in sample  3D volume  Voids in sample  
Fig. 25. Void distribution in Carbon/PEEK composites: (a) Edge sealed and (b) Perimeter open to vacuum 
[236]. 
K. Naresh et al. / Materials and Design 190 (2020) 108553 
However, there are speci.c challenges that need to be addressed during composites manufacturing and modeling, which are discussed below. 
Several XCT parameters during image acquisition and post-processing parameters (including reconstruction, voxel geometry crea-tion, and analysis, segmentation) need to be optimized, given that they play a vital role in accurate process characterization and modeling. Moreover, these XCT based approaches deal with mesoscale geometries based on RVE. XCT-aided material modeling approaches for under-standing the process behavior of macroscale geometries are very lim-ited, owing to dif.culties in achieving a high resolution and the high computational cost. As a result, it is challenging to mimic the exact pro-cess behavior of real structures. The following lists further areas of im-provement and potential future directions. 
Further studies are required to explore several in-situ XCT set-up to continuously monitor .ow in three dimensions and generate the as-sociated numerical models. 
Creep compaction studies using XCT to characterize un-cured pre-pregs at different rates of loading and temperatures. 
Further studies on hybrid prepregs using in-situ XCT are also needed to clearly understand temperature and pressure effects. 
The microstructural variation of .ber tows and ply distortion charac-teristics, resin bleed void coalescence, and their movement during dif-ferent stages of creep can be investigated with the help of XCT. 
A combination of arti.cial intelligence (AI) tools and XCT technology can resolve issues associated with handling big data during compos-ites characterization and process modeling. 
7. Conclusions 
In this review, procedures for improving different characterization stages of aerospace composites manufacturing processes using XCT generated material models have been discussed. The review demon-strates that modern XCT equipment and procedures can provide useful information about reinforcing fabrics and prepregs during process char-acterization that can potentially reduce the number of experiments, time, and manpower. The in.uence of XCT parameters on image quality during scanning and post-processing, including data enhancement for reconstruction and voxel geometry creation, has been reviewed. The computation procedure based on reinforcement image segmentation and boundary condition assignment for simulations has also been discussed in detail. The limitations of meso models over the meso-macro and macro models and vice versa have been highlighted. Under-standing the effect of nesting during compaction on single and multi-layer dry fabrics using XCT for LCM characterization has been shown to be useful for predicting mold clamping forces and the .nal part thick-ness. A comparison of different CFD approaches with analytical and XCT based hybrid methods for predicting reinforcement permeability has also been reviewed in detail. A review of literature has highlighted promising results can be obtained from XCT based material models, suc-cessfully predicting the permeability of several types of reinforcement. The effective cure cycle design procedures for VBO prepregs and auto-clave techniques based on optimization of several processing parame-ters (debulking time, heating rate, degree of cure, dwell period, etc.) showed that void generation and mitigation can be better understood using XCT-based models. The technical aspects required to enhance the through-thickness air permeability and the void reduction mecha-nisms in both the thermoset and thermoplastic prepregs have been rig-orously reviewed. 
Recent developments in aerospace manufacturing techniques coupled with advances in the modeling approach through the use of XCT and commercial FEM and CFD software packages were discussed in this study. These show that XCT based material models will help de-sign better materials for the next generation aerospace components. 
CRediT authorship contribution statement 
K. Naresh:Writing -original draft, Software, Validation, Formal anal-ysis, Visualization, Investigation, Resources.K.A. Khan:Conceptualiza-tion, Methodology, Data curation, Writing -review & editing, Visualization, Resources, Investigation, Supervision, Project administra-tion, Funding acquisition.R. Umer:Conceptualization, Methodology, Data curation, Writing -review & editing, Resources, Visualization, In-vestigation.W.J. Cantwell:Resources, Writing -review & editing. 
Acknowledgment 
This publication is based on work supported by the Khalifa Univer-sity of Science and Technology under Award No. CIRA-2018-15. 

1. Introduction 
Current trends in composite manufacturing technologies for the aerospace industry are focussed on the automatic deposition of composite tapes and new multi-component materials systems, such as toughened prepregs, for improved component perfor-mance, damage tolerance or added functionality. The automation and material technologies enabling these advances have pro-gressed much further than the capability to understand, predict, and optimise the manufacturing processes [1,2]. As a result, even though automatic .bre deposition technologies promise higher manufacturing rates, in practice lack of understanding of the pre-preg behaviour often leads to limited improvement in laydown rates. The complexities in processing present a high risk of defect generation and requires substantial investments in empirical optimisation. 
Two of the main technologies for automated deposition of pre-preg material are Automated Tape Laying (ATL) and Automated Fibre Placement (AFP) [3每5]. The two techniques use very similar machines, consisting of a computer-controlled poly-articulated robot, with a placement head that lays bands of prepreg strips onto a mould in order to construct the layup (Fig. 
1). ATL is employed to 
. Corresponding author. E-mail address: jonathan.belnoue@bristol.ac.uk 
(J.P.-H. Belnoue). 
deliver wide prepreg tapes onto a surface whilst automatically removing the ply backing and is well adapted for the manufactur-ing of large parts of relatively simple geometry, e.g. an aircraft wing skin. AFP is similar to ATL but utilises narrow prepreg strips 
(the width of which can vary from 18 to 12 ), which are collimated on the head and then delivered together. As narrower tapes can steer over sharply curved surfaces better than wider tapes (that cannot be placed without buckling some of the .bres), AFP can be used to manufacture much more complex geometries e.g. wing spar C sections. In order to obtain good layup quality, it is essential to control all the process parameters (heating temperature, com-paction pressure, placement speed, etc) [6每10] 
and the machine trajectories. Over the years, design rules have been implemented to optimise these trajectories for maximum .nal mechanical prop-erties (stiffness and strength) of the component being constructed [11,12]. This is, however, insuf.cient to ensure the production of defect free components as the tolerance in the .bre placement head movement, steered .bres, and tow width variation all con-tribute to the introduction of gaps and overlaps within the lami-nate [3,13]. In addition, the optimisation process of the machine trajectory can itself be responsible for the introduction of further defects. What is not often taken into account in manufacturing practices and analysis of such defects is that the in.uence on .nal .bre path and ply geometry from gaps and overlaps is not their nominal as-deposited position, but a function of what happens to 
1359-835X/ 2017 The Author(s). Published by Elsevier Ltd. 
This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/). Fig. 1. (a) An AFP machine laying into a female mould [3] 
每 (b) Automated .bre placement head, redrawn from [11]. 
Unlabelled image
the laminate in the subsequent processes, such as debulking or consolidation. The through-thickness deformation of the laminate with the gaps or overlapping tapes may lead to an additional .bre crimp or wrinkles, which can have a major impact on composite properties. 
In recent years there have been a number of experimental and numerical studies carried out to predict and characterise the knockdown effect of gaps and overlaps on .nal mechanical proper-ties of components manufactured by AFP. Amongst others, Sawicki and Minguet [14] 
have shown the reduction of compressive strength of samples with various distributions and sizes of gaps in 90 plies; Turoski [15] 
studied the knockdown effect under both tensile and compressive loading of isolated and interacting gaps with different stagger repeats; Croft et al. [16] 
investigated the effect of tensile, compressive and in-plane shear loading on the strength of laminates with a gap, an overlap and a half gap/overlap embedded in the through-thickness symmetry plane; and Elsher-bini and Hoa [17] 
showed similar trends with respect to fatigue loading. More recently, Li et al. [18] 
generated sophisticated mesh-ing tools which allowed them to easily create a series of .nite ele-ment models with various combinations and permutations of gaps and overlaps. They were, then, able to systematically investigate the in.uence of defect size and distribution on the strength knock-down of the modelled specimens. The main conclusion of all these studies was that the introduction of gaps and overlaps during the layup process results (after consolidation and curing of the lami-nate) in the introduction of out-of-plane waviness of the load bear-ing 0 plies (i.e. wrinkles) adjacent to the gapped or overlapped layers and to local thickness variation which, in turn, are responsi-ble for the decreased mechanical properties. This was later con-.rmed by Lan et al. [19,20] 
who showed that the use of a caul plate favours resin .ow thus leading to reduced thickness variation and improved mechanical properties. 
To limit the variation in strength of composite components made by automated manufacturing, it is absolutely imperative to establish more clearly how wrinkle severity and thickness varia-tion can be reduced and, in the best case scenario, mitigate against it. With application to ATL of thermoplastic-based systems, Wang and Gutowski [21] 
studied analytically the possibility to remove (through processing) gaps and laps embedded in a lay-up. Although these studies were very informative, the authors only considered a very idealised geometry where the overlaps were described as cubical blocks of material superimposed with each other. Moreover, the evolution under processing conditions of only one embedded defect (i.e. one gap or one overlap) was considered. A real component manufactured by AFP, however, contains a very large number of different and complex combinations and permuta-tions for gaps and overlaps, which can be of different sizes. A full exploration of the possibility to mitigate, through processing, defects in realistic engineering structure laid-up by AFP would therefore necessitate a very large and costly test plan. In compar-ison, composite process modelling offers the possibility to test a much larger range of possible combinations in a virtual sense and to optimise the processing parameters in order to minimise the defect sizes in a cured component. Process modelling could also advantageously replace or at least reduce empirical methods 
and imaging techniques (which necessitates specimen/com-ponent manufacture) [22,23] 
for the generation of the internal ply geometry that is used as input to failure models. This would open the way for a fully virtual optimisation and strength predic-tion tool for composite components produced by AFP. 
Most of the models available in the literature for the .ow of resin in beds of reinforcement are based on Darcy＊s law [24,25]. These methods work very well to describe the evolution of the average thickness and .bre volume fraction in a large piece of pre-preg when the edge effects can be neglected [26每29]. The principal modelling assumption is that bleeding .ow (i.e. the pressure gradi-ent causes resin .ow relative to the .bre) is the main mechanism leading to the thickness variation of the laminate when processed. This is however not always fully representative as it has been known for a number of years [30] 
that squeezing .ow (i.e. the lam-inate behaves as a highly viscous incompressible .uid) can also occur. Recently, Nixon-Pearson et al. [31] 
have shown that this can greatly affect the deformability of toughened thermoset pre-preg under compaction. They highlighted that the smaller a pre-preg volume with unconstrained boundaries (external or internal as in the case of tapes with gaps), the bigger the effect of squeezing .ow on thickness evolution. This is important for the study of the consolidation of laminate produced by AFP as unconstrained nar-row strips of prepreg and gapped and overlapped regions can locally affect the tape thickness to width ratio. This has been shown [31] 
to greatly in.uence the deformability of prepreg stacks. Hence, numerical tools for consolidation-induced deforma-tion of laminate produced by AFP need to be able to capture these phenomena. The model recently proposed by Belnoue et al. [32] 
takes account of both bleeding and squeezing .ow. It was shown to predict accurately the evolution of thickness and width over time of laminates subjected to complex pressure and pressure rate cycles. It is also able to capture some of the interesting experimen-tally observed effects [31] 
such as the convergence to a compaction limit at high temperature and high pressure or the size effects observed in the through-thickness and transverse deformation of prepreg stacks under compaction. 
The aim of the present study is to gain a better understanding of the way gaps and overlaps, which are introduced during auto-mated prepreg layup, evolve during processing and promote the development of .bre path defects and thickness variation. As in-situ testing and observation of the evolution of the ply structure in the course of processing is not possible, another advantage of the modelling is that it gives information on how the plies move in relation to each other during the layup consolidation and resin cure. In the .rst part of this paper the model for the consolidation of toughened prepreg under processing conditions [32] 
is coupled to an existing cure simulation model, which allows to describe the full cure cycle. Validation of the updated models is given for struc-turally simple but internally complex test cases, comparing .nite element (FE) predictions with experimental data. The second part of the paper focuses more closely on the processing of gaps and overlaps introduced during the layup process. 
2. Modelling tools 
The main novelty of the modelling approach is the use of the consolidation model for toughened prepreg presented in Belnoue et al. [32]. In order to be able to model the full cure cycle, the con-solidation model is here coupled to cure simulation models that are well established in the literature [33,34]. The models are brie.y summarized separately before their combination is presented in Section 2.3. The framework is then validated by simple test cases. The full list of the material parameters used throughout the study can be found at https://data.bris.ac.uk/data/dataset/x9rik93lx3tp 
1yeup4pe8kbti. 
2.1. Consolidation model 
The .ow-compaction model used here follows directly from Belnoue et al. [32] 
where more details are given. Only the main model assumptions and equations are summarised. One of the model＊s main feature is that it is capable of accounting for both squeezing and bleeding .ow. This is important for the modelling of toughened prepreg as they have been shown [31] 
to exhibit hybrid behavior between what would be expected from a thermo-plastic based prepreg system (e.g. the variation of the material response with the tape width and thickness) and what is usually observed for thermosetting systems (e.g. the existence of a com-paction limit). 
The model uses the general thermodynamical framework pro-posed by Limbert and Middleton [35] 
for transversely isotropic solids. Although the framework was .rst formulated for the descrip-tion of soft biological tissues (muscles), it is very relevant for the description of uncured prepreg as both materials are highly viscous and present one strong direction of anisotropy. It is assumed that the general thermodynamic potential, w, is additively decomposed into an elastic part (w e which is related here to the deformation of .bres) 
and a viscous part (w related to the .ow of matrix). Noting C the right Cauchy-Green deformation tensor, we can then express the second the Piola-Kirchhoff stress tensor as: 
where C denotes the derivative of C with respect to the time. 
The expression of the elastic potential follows from Bonet and Burton [10]. 
where 1 is the second-order unit tensor, I1 .1 : C, 
The material parameters a, b, k, lT and lL in Eq. (2) 
can all be expressed as a function of the engineering constants. N0 is the structural tensor which characterizes the local directional proper-ties of the material and is de.ned as N0 .n0 n0 (where n0 is the .bre orientation). 
The expression for w follows from a phenomenological model proposed in [32] 
which builds on the work by Rogers [36] 
and Kelly [37]. Based on experimental results, the existence of a transition mechanism between squeezing (typically at low temperature and low pressure) and bleeding .ow was postulated. This transition mechanism was thought to be related to what many authors have described in the past as locking, which corresponds to the point in time when the .bre bed reaches a con.guration that is such that it cannot deform transverse to the .bres and transverse to the load-ing direction anymore. It was further assumed that after locking a change of direction of the resin .ow between the .bres takes place, from transverse squeezing to bleeding. To ensure a smooth transi-tion between the two mechanisms, squeezing .ow theories were used in both cases. In other words, bleeding was mathematically represented as squeezing along the .bres. 
Another consequence of this idealisation was that, unlike tradi-tional .ow models for thermosets prepreg which use Darcy＊s .ow, the model assumed that the apparent viscosity of a piece of pre-preg subjected to pure compressive loading could be multiplica-tively decomposed into the product of a strain rate dependent term (assumed to behave as power law .uid) and a strain depen-dent term [37]. 
To capture the deformation of a prepregs with an anisotropic Stokes .ow, a model would have to be de.ned at sub-ply resolu-tion and a .ne mesh through thickness of the plies would have to be used [31,38]. This is not practical for the simulation at a com-ponent scale. To overcome this dif.culty a multiscale approach was used. As a result, the strain dependent term was further mul-tiplicative decomposed into a component pertaining to the macro-scale deformation of the tape and a term (at the micro scale) expressing the evolution the inter-.bre channels. All this develop-
ment .nally leads to the formulation of two expressions for w . Prior to locking, the transverse behavior of the material is con-
trolled by the viscous potential given in Eq. (3). After locking, w is expressed as in Eq. (4). In both these equations, J2 is the strain 
rate invariant de.ned as J2 .12 e1 : C2T. 
and 
are known a priori as they are linked to the geometry of the tape and of the unit cell. l0, w0 and h0 are the initial tape length (i.e. along the .bre direction), width and thickness respectively. Whilst vl and vf are the aspect ratios of a unit cell at locking and at the compaction limit respectively and d is the size of the .bres in the plane perpendicular to the .bre direction. Therefore, one of the most elegant aspects of the model is that it can describe the transverse behaviour of prepreg under processing conditions at constant temperature with 3 parameters only. The parameters a and b are two power law parameters con-trolling the behaviour of the rate dependent term of the apparent viscosity whilst the parameter k controls the size of the inter-.bre channels at the micro-scale. As shown in [32], these parame-ters can be easily determined by .tting straight lines through experimental data on a log每log plot of simple compaction tests performed on cruciform shaped samples. 
2.2. Cure simulation model 
A cure simulation model was developed and implemented using the commercial Finite Element Analysis code Abaqus/Stan-dard. The simulation addresses the heat transfer effects occurring during the cure process and allows for quanti.cation of the tem-perature and degree of cure evolution of the manufactured part. The model is three-dimensional and transient and has been applied to the Hexcel IM7/8552 prepreg material used in this study. The material properties depend on both the degree of cure and temper-ature and the material sub-models of cure kinetics, thermal con-ductivity and speci.c heat capacity were implemented in the user de.ned subroutine UMATHT. The heat transfer solution appropriate for simulating the cure process can be expressed by a three-dimensional energy balance combined with Fourier＊s heat conduction law and incorporating the exothermic heat generated due to the chemical reaction of the resin as follows [39]: 
@Ter; TT da 
qcp .r KrTer; TTtvr q HT e5T 
@t rdt 
Here q and cp are the density and the speci.c heat capacity of the composite, respectively, K is the thermal conductivity tensor, T the temperature, t and r the time and spatial coordinate and a the degree of cure. The rate of heat generation due to cure is expressed as the product of the resin volume fraction vr , the den-sity of the resin q , the total heat of reaction HT and the cure reac-
tion rate da=dt. 
The cure kinetics model used for the resin system of this study is a combination of a nth order model and an autocatalytic model [34]. The speci.c heat capacity is de.ned based on the rule of mix-tures for the relevant volume fraction of .bres and matrix whilst a geometry-based model is implemented to compute the thermal conductivity [34]. The .bres＊ speci.c heat capacity and thermal conductivity present a linear dependence on temperature [33], whilst the resin speci.c heat capacity and thermal conductivity depend on both degree of cure and temperature [34]. 
2.3. Thermo-mechanical model 
A coupled three-dimensional transient thermo-mechanical model was developed by coupling the consolidation model and the cure simulation model presented in Sections 2.1 
and 
2.2. Again, the coupled model was developed within the implicit FE package Abaqus/Standard. A staggered solution approach is adopted (see Fig. 
2). The heat transfer analysis is carried out .rst, followed by the mechanical analysis. As described in Section 2.2 
the heat trans-fer solution is coded within a UMATHT subroutine yielding the evolution of temperature and degree of cure of the laminate. This information is then passed to a UMAT subroutine where the mechanical model described in Section 2.1 
is coded. The coupling between the heat transfer solution and the consolidation model is carried out by expressing the consolidation phenomenological parameters a; b and k as a function of temperature [32]. The ther-mal and chemical strains occurring during the process cycle are taken into account by considering the thermal expansion [40] 
and cure shrinkage coef.cients [41] 
of the material. In addition, the fact that consolidation stops at gelation, is deployed numeri-cally by assigning a very large apparent viscosity gapp after the 
resin gels, i.e. when the instantaneous degree of cure a reaches the degree of cure at gelation agel. The degree of cure at gelation agel for the resin system of this study is 0.33 [41]. 
2.4. Model validation 
2.4.1. Cure simulation model validation 
To validate the cure simulation model, an 18 mm thick .at panel was cured within an autoclave. The .at panel was placed on a 10 mm aluminium tooling plate, bagged up and sealed before testing. A K-type thermocouple was mounted at the centre of the laminate in order to record the temperature evolution of the lam-inate during the process. The nominal cure pro.le [42] 
of the 8552 resin system was applied whilst the lay-up of the laminate was [0/90]37s. The cure of an 18 mm thick .at panel was then modelled using the developed cure simulation model and the results were compared with the experimental data. The widely accepted mate-rial properties of aluminium were applied to the tool. Hence, the thermal conductivity and the speci.c heat capacity were set at 120 [W/m/.C] and 960 [J/kg/.C], respectively, whereas the density was .xed at 2700 kg/m3. The heat convection coef.cients for the given autoclave were determined experimentally using lumped mass thermal analysis. It was shown that the heat convection coef-.cients of the top and bottom surfaces were 27.3 and 31.7 [W/m2/ K], respectively. In addition, adiabatic conditions were applied at the lateral walls of the laminate assuming no heat loss due to the high width to thickness ratio. 
Fig. 
3 
presents the temperature evolution and the degree of cure at the centre of the laminate. As it can been seen there is a very good agreement between the experimental data and simulation results implying that the cure simulation model developed is able to simulate the heat transfer phenomena occurring during the cure process of the material system of this study successfully. The tem-perature lag between the centre of the laminate and the autoclave air is due to the heat convection between the autoclave and the 
200 
1 applied pressure. Fig. 
5a depicts the heat transfer results for the two case studies. The evolution of both the degree of cure and tem-
180 
0.9 
perature is identical between the two con.gurations since the lay-
160 
0.8 up does not affect the heat transfer in the through-the thickness 140 
0.7 direction. In addition, there is no temperature and degree of cure 
Temperature at centre of laminate - experiment Autoclave air -experiment Temperature at centre of laminate - simula on Degree of cure at centre of laminate - simula on  
Temperature [C] 
Degreee of cure 
distribution within the laminates due to the fact that the laminates 
are relatively thin (2 mm). Furthermore, there is no heat loss 
between the hot plates and the laminates since they are in direct 
contact. In the beginning of the cycle consolidation effects domi-
nate, leading to a considerable thickness reduction until the thick-
ness reaches a plateau at the middle of the second ramp where the 
resin has gelled and cure shrinkage strains start to develop. This 
phenomenon is accurately captured by the model since the degree 
shown in Fig. 
5a. The increase of temperature during the second 
ramp results in a slight increase in thickness caused by thermal expansion effects followed by a slight decrease due to resin chem-ical shrinkage. During the second dwell there is no variation in thickness whereas there is a slight thickness reduction at the end of the cycle during cool-down caused by thermal contraction. 
Overall, there is a quite good agreement between the experimental 
Time [hr] 
Fig. 3. Cure of 18 mm thick IM7/8552 .at panel; evolution of temperature and degree of cure at the centre of the laminate. 
laminate as well as due to the low thermal conductivity in the 
through the thickness direction. 
2.4.2. Thermo-mechanical model validation 
data and the simulation results (see Fig. 
5b) implying that the cou-pled model is capable of capturing the thickness evolution during consolidation, accounting for the cessation of compaction at gel
A series of experiments were carried out in order to validate the and also thermal expansion effects. Furthermore, it is shown that developed thermo-mechanical model. The cruciform specimen the model can capture the layup size effects; the CP laminate developed in [31] 
was used for the layup and geometry. Two differ-shows less thickness reduction than the BP laminate due to the fact 
ent con.gurations of 16 plies laminates were studied; cross-ply (CP), [0/90]8 and blocked-ply (BP), [904/04/904/04] as shown in Fig. 
4a. The experimental testing was carried out using an Instron 8801 universal testing machine with temperature controlled hot plates transferring the temperature and pressure pro.le to the samples (see Fig. 
4b). The nominal temperature and pressure pro-.le from the manufacturers recommended cure cycle were applied. The coupled thermo-mechanical model presented in Section 2.3 
was used to model the compaction of the two samples and the results were compared with the experimental data. Prescribed temperature boundary conditions were applied to all the bottom and top nodes of the model representing the hot plates, whilst pre-scribed pressure was applied to all the top nodes representing the 
a) 
that the apparent viscosity is proportional to the width to thick-ness ratio rendering the CP laminate considerably more viscous [31]. 
3. Wrinkle formation 每 AFP gaps and overlaps 
3.1. Sample design and experimental procedure 
In order to validate further the .ow model and coupled approach and to gain a better understanding of the mechanisms involved in the consolidation of gaps and overlaps introduced dur-ing the Automated Fibre Placement lay-up process, a sample with a 
b) 

Cure Time [hr] Cure Time [hr] 
Fig. 5. (a) Heat transfer solution for cross ply and blocked ply laminate, (b) coupled solution for cross ply and blocked ply laminate. 
a) b) 

Fig. 6. Gaps and overlaps specimen (a) Ply book, (b) CT scan after vacuum consolidation, (c) Initial geometry for the FE analysis, obtained through forming of the plies initially laid-up ＆＆.at§ (see Fig. 
6a), (d) Realistic internal ply geometry in the FE model of the uncompacted sample after the forming phase. 
purposely severe combination of gaps and overlaps was designed and manufactured from broadgoods IM7/8552. As illustrated in Fig. 
6(a), the sample is a 100 mm 100 mm 6 mm with a cross-ply layup in which 3 defect areas are embedded in the 90 plies. Each of these defect areas is made of 5 gaps stacked on top of each other, directly above a stack of 5 overlaps. In the central defect region, the order of the gaps and overlaps regions are the opposite to the outer two regions. All the defects are 2 mm wide, which is of the same order of magnitude as defects introduced by an AFP process. Fig. 
6(b) shows a CT scan of the centre-line of a specimen that has been vacuum-bagged and then oven-cured for imaging purposes. 
The samples described above were submitted to two different types of thermomechanical loading. First, isothermal experiments were carried out at 70 C. The laminates were loaded at a rate of 
2.333 
kPa s 1 to 1 bar, 3 bars and 7 bars respectively and then maintained at these levels of pressure over 1 h. An Instron 8801 universal testing machine with temperature controlled hot plates was used to apply the temperature and pressure pro.le to the sam-ples. The obtained samples were then fully cured in an oven (with-out pressure) and micrographs were taken. The second test program used the same equipment to load the sample under the pressure and temperature cycle recommended by the manufac-turer [42], mimicking an autoclave curing process. 
3.2. 
Isothermal loading 
Although there is no composite manufacturing process that uses the load and temperature program applied in the isothermal tests, the levels of load and temperature were carefully chosen to be meaningful from a composite processing perspective. Thus, 70 C is close to the hot debulking temperature, 1 bar is equivalent to vacuum pressure, 3 bar is close to the pressure applied by an AFP roller and 7 bar is the autoclave pressure. These tests were carried out with 2 main objectives: 
to give further validation for the .ow model presented in Bel-noue et al. [32] 
for larger samples with more complex internal ply geometry to gain more understanding of the way gaps and overlaps con-solidate and interact during the consolidation process and the mechanisms involved. 
FE models of the tests described above were set up in Abaqus/ Standard. A ply-by-ply modelling approach was used. The neces-sity to start with an accurate geometry of the uncompacted speci-men led to the de.nition of a two-step modelling procedure. The .rst step consisted in the forming of the plies laid-up .at, as in Fig. 
6(a). As illustrated in Fig. 
6(c), this was done using a rigid plate that was moved down in order to progressively bring the plies into contact with each other. The 3 bumps observed on the sample top at the position of each of the defect areas (see Fig. 
6b), were repro-duced by placing 3 slits in the forming plate through which the material can penetrate. The good agreement between the FE model and a CT scan of a specimen that had been vacuum bagged and then oven cured with no pressure applied on the specimen (as can be seen comparing Fig. 
6(d) with Fig. 
6(b)) validated the choice of the forming procedure. The second part of the analysis consisted in the compaction of the samples between 2 rigid plates. In all the simulations, the material model for pre-preg was that from [32]. Custom-developed algorithms allowed for automatic mesh gener-ation and material property assignment from direct reading of the ply book. Single integration point elements with hourglass control were used, with one element per ply in the thickness direction. 
Fig. 7. Comparison of FE predictions with experimental results for the isothermal experiments. (a) Thickness vs Time evolution of the sample tested a 7 bar, (b) Experimental and predicted thicknesses of the samples tested at respectively 1 bar, 3 bar and 7 bar, (c) Micrograph of the defect area in the middle of the sample at 7 bar, (d) FE prediction for the internal ply geometry around a central defect region of the sample with a 7 bar applied pressure. 
As illustrated by Fig. 
7(a) and (b), the predicted thickness evolu-tion for the 3 levels of pressure compared very well with those measured experimentally. Similarly, good agreement is obtained when comparing samples＊ micrographs (Fig. 
7c) with the .nal ply geometry predicted by the model (Fig. 
7d). Both the ply thick-ness and the level of .lling of the gaps originally introduced in the sample are very well captured by the model. However, the pre-dicted wrinkle angle ( 15 in the sample tested at 7 bars) is more severe than what is observed in the micrographs ( 10). There are several possible explanations for this. Firstly, the model does not take account of the resin once it has bled out of the .bre bed into the gap. This potentially resists the development of the wrinkle. Secondly, as can be seen in Fig. 
6(b) and contrary to what is assumed in the model, the gaps (in the real samples) are not per-fectly aligned. This makes it more dif.cult for the wrinkle to develop. Lastly, as it has been shown in the context of dry .bre fab-ric forming [43], the bending stiffness can considerably affect the severity of the wrinkle. The present model was formulated for pre-diction of consolidation, with the value of the bending stiffness taken directly from Sj.lander et al. [44]. There a cantilever bending test was performed at 70 C on a piece of prepreg made from epoxy resin and IM .bres. The bending stiffness in an FE model of the test was adjusted until the predictions matched the experimental results. Sj.lander et al. [44] 
reported the dif.culty to calibrate this property properly. This suggests that, in the future, more effort should be dedicated to more clearly understanding the bending behavior of prepreg sheets under processing conditions. 
Fig. 
8, shows the initial con.guration and the .nal geometry predictions of the internal ply geometry around the central wrinkle for the 3 different pressure levels. As expected, most of the varia-tion of the thickness happens between 0 and 1 bar. The .gure high-lights the existence of two competing mechanisms in the way the gaps originally introduced within the laminates are .lled. Upon compaction, the gaps can either close due to the overlaps falling into the gaps (the wrinkle then becomes steeper) or they can be .lled laterally due to squeezing .ow (this prevents the wrinkle angle from increasing as can be seen at the position marked ＆a＊ on the sample loaded to 7 bars). Including capabilities for squeez-ing .ow into the model also allowed to take account of the higher compressibility of the overlapped areas. It can be clearly seen in the 7 bar sample that these regions are compacted nearly twice as much as the single plies. This was also observed on the micro-graphs of the samples (Fig. 
7c). 
3.3. Full cure cycle 
The second set of experiments, where the same apparatus was used to submit the sample to the manufacturer＊s recommended pressure and temperature cycle, was used to validate the two-step coupled thermo-mechanical model proposed in Section 2.3. The same material parameters as in Section 2.4.2 
and the same ini-tial mesh as Section 3.2 
were used. First, the temperature history at every gauss point in the model was calculated during the heat transfer analysis (which uses the UMATHT subroutine). This showed that, the temperature distribution through the thickness of the laminate was negligible with a slight temperature overshoot, in the order of two degrees at the beginning of the second dwell. The temperature .eld history was then given as an input to the mechanical analysis (which uses the UMAT subroutine). Fig. 
9(a) depicts the thickness evolution of the laminate, whilst Fig. 
9(b) presents a micrograph and the corresponding simulation result. It can be observed that the coupled model is able to predict the thickness evolution of the laminate during consolidation accu-rately. Comparison of the simulation results with the micrograph 
Fig. 8. Prediction for the internal ply geometry around the central wrinkle at different level of pressure. 
9(b) indicates that the coupled model predicts the morphology of the manufactured part successfully, including the .lling of the gaps as well as the formation and position of the wrin-kle. As with the isothermal experiments, the model slightly over-predicts the magnitude of the formed wrinkle. 
4. 
Although very informative about the mechanisms playing a role in the consolidation of gaps and overlaps, the study presented in the previous section was a rather simple and forced case compared to a real lay-up created by AFP. For example, the probability to see, 5 gaps facing exactly opposite to 5 overlaps is quite low. Moreover, in the industry AFP machines tends to be set-up so that they leave only gaps or only overlaps in the layup. The model＊s ability to pre-dict the effect of consolidation and curing on more realistic lay-ups with more complex distribution and a greater number of gaps and overlaps is presented. 
Two different lay-up con.gurations were modelled (see Fig. 
10). As for the previous section, a cross-ply con.guration was adopted. Both lay-ups had the same defect distribution but one of them only contained gaps whilst the other one only had overlaps. The defect width was 2 mm, which is consistent with the size of the defects 
Fig.10.Plybooksforthelayupswithacomplexdistributionofmanufacturingdefects.(a)100%overlaps每(b)100%gaps.a) 
mm7.4
b) 
gaps. 
introduced in an AFP process. The same modelling procedure as the one described in the previous section was used except that the laminate that contained only gaps did not need to be formed. The samples were assumed to be in.nitely long in the two in-plane directions therefore only a representative unit cell with fully constrained edges was modelled. Temperature and pressure boundary conditions were applied to simulate the samples being subjected to the cure cycle. It was assumed that the samples were consolidated between 2 aluminium plates, which were modelled as rigid bodies. The material parameters were the same as those used throughout the paper, given in Section 2. 
The results of the analysis are illustrated in Fig. 
11. Comparing the .nal internal geometry of the lay-up where only overlaps were introduced and the case with 100% gaps reveals signi.cant differ-ences between the 2 cases. The sample with 100% overlaps is about 
0.7 mm thicker. In addition, there is signi.cantly more .bre wavi-ness created in this sample. This is important from a design point of view as the level of .bre waviness can, as discussed in the intro-duction, impact the sample load carrying capacity greatly, whilst the sample thickness has implications for assembly tolerances. In the model there is a ＆void＊ formed at the top surface where the plies have deformed. In reality, this would be .lled by resin bleed, which can not be captured by the analysis. This may however be an artefact of the excessive ply bending, which was observed in the examples in Section 3. The difference in thickness observed between the two cases is partly due to the sample with embedded overlaps being originally thicker, as a result of the introduction of extra material in the overlapped regions. However as can be seen in Fig. 
11, it is also due to the samples with embedded gaps being able to compact more. This can mainly be explained by the fact that in the samples with embedded overlaps the plies are continu-ous (i.e. in.nitely long). It was shown in [31] 
that the higher the square of the ply thickness to width (i.e. dimension in the direction perpendicular to the .bre direction) ratio, the greater the amount of squeezing .ow and the more the material is able to compact. It can also be remarked that, contrary to what might be expected, there is almost no waviness in the samples with embedded gaps. This is because the samples gels very early on in the cure cycle, which results in the structure being ＆＆frozen§ and unable to deform any more. 
It is also interesting to remark that unlike what was observed in the case presented in Section 3 
where a sample with a deliberately bad combination of gaps and overlaps was analysed, lateral squeezing .ow does not seem to help to .ll up the gaps (see Fig. 
11b). This is because in the present case the ply thickness to width ratio is much smaller, thus limiting the ability of the mate-rial to squeeze. In this example, counterintuitively, the introduc-tion of gaps in the 90 plies compared to the overlaps has helped to increase the overall .bre volume fraction of the sample, since it is able to consolidate more easily. In the case of the overlaps, they cause the top tooling to be held up, preventing overall consol-idation of the un-overlapped regions. 
Finally, in order to demonstrate the model＊s ability to study dif-ferent manufacturing cases, an analysis where the hard top tooling was replaced by a soft vacuum bag was carried out (in practice a homogeneous pressure was applied to the top of the samples). The result of the analysis is illustrated in Fig. 
12. In the sample with embedded overlaps (see Fig. 
12(a)), soft tooling results in a lot less compaction than what was observed with hard tooling (see Fig. 
11 
(a)). This is the result of the applied pressure being non-homogeneously distributed when hard tooling is used. In fact, in the hard tooling case, the compacting plate is only in contact with the bumps at the surface of the sample. As a result, the force applied at the top of the tool is distributed over a much smaller area and the actual pressure applied on some parts of the lay-up is much greater than the external pressure applied to the rigid tool. This also results in much reduced waviness in the lay-up consoli-dated and cured under hard tooling. This is in good agreement with the observations made by Lan et al. [19,20] 
when studying the effect of tooling on the resistance to failure of samples containing gaps and overlaps. 
5. Conclusion 
This work has presented new insight into the formation of .bre path defects arising in AFP manufactured laminates, through experimental observations and predictive modelling. To achieve this a new multi-scale and multi-physics modelling framework for composite processing, based on a consolidation model pro-posed by Belnoue et al. [32] 
has been de.ned. Using a method pre-sented by Johnston et al. [33], the consolidation model was coupled to a heat transfer model for accurate description of the full cure cycle. Validation of the newly introduced models was .rst given for very simple test cases comparing FE predictions with experi-mental data. This model was then applied to analysis of the pro-cessing of specimens with gaps and overlaps introduced during the layup process and for the understanding of how these layup defects can evolve into .bre path defects. 
A specimen was designed with a purposely ＆＆bad§ combination of gaps and overlaps of realistic sizes. This magni.ed the effect of the different mechanisms participating in the gaps and overlaps consolidation and closure, thus facilitating their identi.cation. Samples were laid-up and processed in 2 different ways: isother-mally, subjected to 3 different pressure levels and following the manufacturer＊s recommended cure cycle (mimicking that of an autoclave). Micrographs of samples and measurement of the .nal thickness allowed further model validation. This highlighted the model＊s ability to capture the mechanisms leading to the .nal ply con.guration. Upon compaction, the gaps can either close due to the overlaps falling into the gaps or be .lled laterally due to squeezing .ow. Comparison between the model predictions and this set of experiments also highlighted the necessity for fur-ther investigation, in the future, at the effect of the ply bending stiffness on the wrinkle severity and to explore new methods to take this into account more accurately within the modelling frame-work proposed here. 
In the last part of the paper it was shown how the newly devel-oped numerical tool can be used to predict the evolution of the internal geometry and ply con.guration of components containing much more complex and realistic arrangements of gaps and over-laps. The emphasis has been put on predicting the effect of tooling on the samples＊ .nal internal geometry. It highlighted the effect of the plies＊ dimensions on the ability of the layup to compact. As much as 10% difference in the predicted thickness was obtained between the thickest specimen that contained the most severe waviness (100% overlaps with soft tooling) and the thinnest one (100% gaps with hard tooling). 
The new predictive capabilities developed here opens the way towards process optimisation and defect mitigation in parts man-ufactured by automated prepreg lay-up. For example, a sensitivity analysis on the material and processing parameters would help to limit the amount of part re-work and scrap by improving the gen-eral understanding of how manufacturing conditions affect the quality of the laminate. In addition, the modelling framework pre-sented in this study can be enhanced to predict residual stress and shape distortion occurring during the cure so that all the phenom-ena occurring during the manufacturing process of composites can be captured. It also holds potential to be used as part of a virtual framework for the prediction of the mechanical properties of engi-neering components made from composite materials, including the formation and effects of .bre path defects. 
Acknowledgements 
This work has been funded by the UK Engineering Physical Sciences Research Council (EPSRC) Centre for Innovative Manufac-turing in Composites project ＆＆Defect Generation Mechanisms in Thick and Variable Thickness Composite Parts 每 Understanding, Predicting and Mitigation§ (DefGen), [EP/I033513/1]. Data for the cure simulations models, including equations and materials parameters can be found at https://data.bris.ac.uk/data/dataset/ 
x9rik93lx3tp1yeup4pe8kbti. 
