Investigations on the material property changes of ultrasonic-vibration assisted aluminum alloy upsetting
Abstract
Numerous studies have shown the benefit of ultrasonic-vibration assisted metal forming. This benefit include a reduction in forming forces, which might be attributed to the superposition of stress, increased temperatures, the effects of interface friction, and energy absorption of dislocation. This study conducts a series of experiments and analyses to investigate the main mechanisms of a reduction in forming forces during ultrasonic-vibration assisted A6061-T6 aluminum alloy upsetting.
The findings of this research confirm that, under frictionless conditions, ultrasonic vibration still reduced forming forces, and ultrasonic vibration can increase the temperature of specimens and soften specimen surface during upsetting. From metallographic analyses and micro-hardness tests, the results reveal that energy absorption of dislocation was occurred during upsetting, which also contribute to the reduction of forming force.
This research concludes that the mechanisms of increased temperatures and energy absorption of dislocation can affect the material property and make a reduction in forming forces; however, the interface friction effect has nothing to do with a reduction in forming forces.
Highlights
The mechanisms of ultrasonic-vibration assisted upsetting were identified. Ultrasonic can reduce the material¡¯s flow stress when friction is negligible. Metal can absorb energy through heat energy, making a reduction in forming force. The grains have been refined when the ultrasonic-vibration is superimposed. The motions of dislocation were occurred, making a reduction in forming forces.
Keywords
Aluminum alloy Compression test Ultrasonic vibration Mechanism
1. Introduction
High-energy ultrasonic waves have been applied to a wide variety of uses, including ultrasonic welding, ultrasonic cutting, ultrasonic metal forming, and ultrasonic die-casting. The process of ultrasonic-vibration assisted metal forming applies ultrasonic energy to a die, which is then used to deform metal specimens. Interesting effects arise when ultrasonic vibration is applied to metal-forming processes, such as a decrease in friction between die and specimen, a reduction in forming forces, and changes in the microstructure of specimens during sheet metal forming. Blaha and Langenecker [1], [2] were the first to investigate the use of ultrasonic vibration in relation to the plasticity of metals. They superimposed high-frequency vibrations onto a static load during the tensile test of a zinc single crystal specimen. In their experiment, they observed a substantial reduction in yield stress and flow stress. In similar experiments, flow stress was clearly reduced in polycrystalline materials. The SAE1019 steel experiments showed that applying ultrasonic energy to a specimen increased its temperature, an effect related to the time and amount of ultrasonic energy applied. Microscopic observations showed that grain sizes of materials decreased when ultrasonic energy was applied. The transmission electron microscopy technique (TEM) was used to observe an increase in the density and movement of dislocations after the application of ultrasonic energy to materials.
Abramov [3] investigated the effect of the ultrasonic on the material micro-structural and mechanical properties. It showed that ultrasonically induced stress in NaCl and LiF crystals with the amplitude exceeding their yield strength enhances dislocation density. When the density of the dislocations is high enough, an alignment of dislocations occurs. Kempe [4] proposed three mechanisms by which dislocations absorb energy from vibrations to reduce flow stress: (1) a resonance mechanism, (2) a relaxation mechanism, and (3) a mechanism of simple hysteresis.
Hevill [5] attributed reductions in flow stress to a stress superposition mechanism involving the superposition of steady stress and alternating stress. Our previous study [6] proved that axial ultrasonic vibration could reduce the deformation resistance of materials during hot upsetting. We found that the effect of ultrasonic vibrations on hot upsetting could not be explained by a single simple mechanism, such as the effect of interface friction, the superposition of stress, or the absorption of ultrasonic vibration energy by dislocations.
Substantial research has been conducted on the changes in interface friction in ultrasonic-vibration assisted forming experiments and simulations [7], [8], [9], [10]. Huang et al. [11] investigated the benefits of applying the axial ultrasonic vibrations of forming tools to an upsetting process using plasticine as a model material to simulate hot metal. In their study, the application of ultrasonic vibration to die reduced the mean forming force during upsetting. The researchers concluded that the stress superposition effect and reductions in interface friction contributed to this phenomenon. Daud et al. [12], [13] performed ultrasonic-vibration assisted aluminum alloy compression and tension tests using a piezoelectric force transducer to measure the high-frequency vibration tension and compression force. Furthermore, a finite element model was constructed to describe the effects of superimposing ultrasonic vibration for compression and tension tests. The results indicated that oscillatory stress superposition and contact friction were insufficient to explain the effects of ultrasonic excitation in metal-forming processes.
As mentioned in the literatures, although ultrasonic-vibration assisted forming has been around for decades, the mechanism that induces these effects is still unclear. Explanations on the effects of ultrasonic vibration include the superposition of stress [1], [2], [5], [11], [12], [13], [14], increased temperatures [1], [2], [15], energy absorption of dislocation [3], [4], and the effects of interface friction [7], [8], [9], [10], [11], [12], [13]. These factors are usually coupled, which makes them difficult to understand. This study conducted a series of experiments and analyses to clearly investigate the main mechanisms of a reduction in forming forces during ultrasonic-vibration assisted A6061-T6 aluminum alloy upsetting. An extrapolated compression test removed the effects of interface friction between the specimens and the die, and a temperature measurement test explored the effects of increased temperatures using an IR thermometer. Finally, this study used metallographic analyses and micro-hardness tests to investigate the effects of energy absorption of dislocation during upsetting.
2. Ultrasonic-vibration assisted extrapolated compression test
To avoid the influence of friction on the measurement of the stress¨Cstrain data, Cook and Larke [16] utilized a method of the extrapolated compression test, which applied compression force to cylinder specimens with four different ratios of initial diameter to height (d0/h0) from 0.5 to 4 under identical loading conditions. If the diameter of the specimens is kept constant, the height of the specimens increases, resulting in a reduction in interface friction. When the height of specimens approaches infinity, interface friction becomes negligible. The deformation of the cylinder specimens therefore remains uniform during compressions (Fig. 1).
he compression strains obtained with different d0/h0 ratios are linear and can be extended to the origin point. When d0/h0 reaches 0, the specimen heights are infinite and the effect of interface friction in this strain data becomes negligible. Using this method, additional stain data can be obtained with different loading conditions when d0/h0 reaches 0. Based on these steps, stress¨Cstrain data under frictionless conditions can be derived.
2.1. Experimental procedure
The procedure for the high-temperature-extrapolated compression test is detailed as follows. The ultrasonic vibration system and a furnace were set up on a hot bench controlled by a microcomputer server, as shown in Fig. 2. The specimens were sprayed with MoS2 lubricant and placed between parallel dies. A 20 kg preload was applied to the specimens. The heating controller was turned on. When the designated temperature was reached, it was held constant for 10 min before the experiment began. Whenever loading reached 70 kg during an experiment, ultrasonic vibration was superimposed. After compression was complete, deformation of the cylindrical specimens was measured before the specimens were removed from the plates.
Table 1 shows the material properties and the high-temperature-extrapolated compression test conditions used in the experiment. The specimens used in this study were aluminum alloy A6061 with heights of 3 mm, 4 mm, and 6 mm (equivalent to d0/h0 of 2, 1.5, and 1, respectively) and fixed diameters of 6 mm. All specimens received T6 treatment, including solution treatment and artificial aging treatment, before the experiment. Fig. 3 shows the microstructure of the specimens with heat treatments before the experiment, and the grain size of the specimens was 22.285 ¦Ìm. Compression forces were set to 500, 800, 1100, 1100, 1500, and 1800 kg. A constant compression speed of 1 mm/min was maintained throughout the experiment. During the ultrasonic-vibration extrapolated compression test, the axial vibration frequency was maintained at 20 kHz and the amplitude was set to 5.6 ¦Ìm.
2.2. Experimental results and discussion
Fig. 4 plots the experimental strain vs. d0/h0 results of both the conventional compression (CC) test and the axial ultrasonic vibration compression (AUC) test. The environmental temperature was set to 25 ¡ãC. Fig. 4a shows the results from CC when compression loading of 500, 800, 1100, 1500, and 1800 kg were applied on specimens with d0/h0 ratios of 2, 1.5, and 1. Fig. 4a also shows compression strains under a 500 kg loading condition as 0.00187, 0.00191, and 0.00194 for specimens with d0/h0 ratios at 2, 1.5, and 1, respectively. It is therefore difficult to see the data clearly from this figure. The strain when d0/h0 = 0 can be extrapolated if the strains for specimens with different d0/h0 ratios have a linear relationship. This line can then be extended to the origin point. Fig. 4b shows the results from AUC when compression loading of 500, 800, 1100, and 1500 kg were applied to specimens with three different d0/h0 ratios. By the comparison between Fig. 4a and b, the engineering strain for AUC with the loading of 1500 kg is higher than that of CC with the loading of 1800 kg. It appears that ultrasonic vibration can significantly reduce the loading. Under these fictitious geometrical conditions with infinity of the height of specimens, no friction occurs and the stress value associated with the deformation is a function only of the material¡¯s resistance to flow.
Fig. 5, Fig. 6 plot the stress¨Cstrain curves obtained from Fig. 4 for three cylindrical specimens of different initial heights for CC and AUC. Higher loads are required for a higher d0/h0. The results show that, under the same loading conditions, flow stress decreased and compression strain increased when d0/h0 decreased. This was because interface friction was reduced when d0/h0 decreased and specimen height increased.
Fig. 7 shows a comparison of the stress¨Cstrain curves for CC and AUC under frictionless conditions. At the same loading of 1500 kg, true strain in AUC was 18.72% higher than that in CC. True stress in AUC, however, was lower by approximately 59.73 MPa. This shows that ultrasonic vibration can still effectively reduce material flow stress under frictionless conditions. Mechanisms other than friction must therefore be responsible for the reduction in flow stress caused by ultrasonic vibration.
Fig. 8 plots the stress¨Cstrain curves from extrapolated compression tests for AUC and high-temperature conventional compression (HCC). The true stress¨Cstrain curve for HCC at 150 ¡ãC is close to AUC¡¯s curve at 25 ¡ãC. The reduction in flow stress from increasing the temperature to 150 ¡ãC is comparable to the reduction caused by applying ultrasonic vibration. This result was in agreement with the literature [17], which indicated that the flow stress can be reduced when the temperature of the specimen was increased by pure heating, and also revealed that the specimen absorbed ultrasonic energy to cause the increased material temperatures and make a reduction in forming force. To clearly explore the effect of increased temperature, the temperature measurement tests were conducted.
3. Temperature measurements during ultrasonic-vibration assisted upsetting
3.1. Experimental conditions
During compression tests, materials vibrate at high speeds. Inserting a thermocouple into specimens to measure temperature may not lead to accurate measurements because of damage to the thermocouple caused by the heat generated from vibrating materials. To overcome this issue, an IR thermometer (Raytek MX4) and a film-type thermocouple (ANRITSU ST-24 K) were used to obtain mean and indirect measurements of temperature during AUC tests. Specially designed specimens with thin (0.2 mm) wings were made to allow attachment of the film-type thermocouple (Fig. 9).
3.2. Experimental results
Fig. 10 shows the measured temperatures for the AUC tests for specimens with d0/h0 ratios of 2, 1.5, and 1. The results show that ultrasonic vibration increased the temperature of the materials. Smaller specimens had higher temperatures. This shows that the ability of ultrasonic vibration to increase temperature during upsetting is related to the size of the specimen. Smaller specimens absorb more ultrasonic energy per unit of volume, which results in higher temperatures.
Temperatures measured using the film-type thermocouple exceeded those measured with the IR thermometer. This was because the ultrasonic-vibration assisted upsetting caused ultrasonic welding. The temperatures of the specimens increased both through absorption of ultrasonic energy and through the rubbing that occurred between specimens and the die. Temperatures at the interface were therefore higher than the overall temperatures of the specimens. Furthermore, the IR thermometer measured only mean temperatures at focal spots with diameters of 6 mm. The film-type thermocouple measured temperatures around the thin wings, which was near the interface areas for the specimens. The temperatures were therefore higher than the values measured by the IR thermometer. This phenomenon was in agreement with Statnikov¡¯s research [15]. The high-frequency impact on the surface is accompanied by quick surface local heating. The generated thermal energy may not heat up the whole sample, but has a significant influence on the surface and improve the surface quality. Metal can absorb energy from the ultrasonic vibration through thermoelastic energy conversion [2], [18], which could make a reduction in forming force.
Additionally, results show that temperatures increased rapidly during the initial vibration stages and then decreased with time. Fig. 10c and d shows that when d0/h0 = 1, the temperature of specimens under a compression load of 1100 kg was lower than that of specimens under 300 kg. This was because vibrating amplitude decreased as compression load increased, which reduced the output energy of the vibrations.
A discrepancy in temperature was found between indirectly measured temperatures (approximately 40¨C50 ¡ãC) and inferred temperatures (approximately 150¨C200 ¡ãC) from previous tests, which indicated that absorption of ultrasonic energy was not the only factor to increase the material temperatures. Other factors, such as stress superposition and dislocation activation, might also have been responsible. To clearly explore other mechanisms, metallographic analyses and micro-hardness tests were conducted.
4. Metallographic analyses and micro-hardness tests
A number of recent studies have explored the effects of ultrasonic energy on the microstructures of materials, which revealed that applying ultrasonic energy refined the grain sizes in these materials therefore improving their mechanical properties [19], [20], [21], [22], [23], [24]. Liu¡¯s research [24] indicated that the major approach to refine the grains of specimens for ultrasonic vibration is the motion of the dislocation.
To explore the effect of energy absorption of dislocation, metallographic analyses were conducted to investigate the grain size on specimen surface. A solution of hydrofluoric acid (10% hydrofluoric acid and 90% water) was used as an etchant. A surface area of one square inch was selected on each specimen for observation. These areas were magnified 100 times and shown on a microscope screen. Before the experiment, the grain size of the specimens was 22.285 ¦Ìm. With CC, specimens were subjected to compression forces of 500, 800, 1100, and 1500 kg, which resulted in grain sizes of 21.795, 21.496, 20.820, and 20.433 ¦Ìm, respectively. With AUC, specimens were subjected to compression forces of 500, 800, 1100, and 1500 kg, which resulted in grain sizes of 21.371, 20.523, 18.814, and 17.530 ¦Ìm, respectively. When ultrasonic vibration was superimposed on specimens, the grains on their surfaces were refined (Fig. 11, Fig. 12). The dislocation density increased during AUC, and these dislocations tangled together and the dislocation walls occurred. Finally, these dislocations walls changed into low or high angle grain boundaries, and then the original grains were refined into small grains and subgrains [3], [24]. Specimen absorbed the energy from the motion of dislocation to soften the martial and make a reduction in forming force [4], [25].
Micro-hardness tests were also conducted. Results showed that the forming process increased the specimens¡¯ surface strength because grains on the specimen surfaces were refined and the number of dislocation increased dramatically during ultrasonic-vibration superimposition (Fig. 13).
5. Conclusion
A series of experiments and analyses were conducted to explore and further understand the effects of ultrasonic vibration for the mechanical properties and microstructure changes of materials during ultrasonic-vibration assisted A6061-T6 aluminum alloy upsetting, and three mechanisms including the interface friction effect, the increased temperatures, and energy absorption of dislocation were clearly identified and analyzed.
The results of the extrapolated compression experiment showed that when friction is negligible, ultrasonic vibration can still effectively reduce material flow stress. Temperature measurement tests showed that metal can absorb energy from the ultrasonic vibration through thermoelastic energy conversion, which could make a reduction in forming force; however, heat energy was not the only factor. Furthermore, the results of metallographic analyses and micro-hardness tests indicated that energy absorption of dislocation was also occurred, which make a reduction in forming forces during upsetting.
Because of current limitations in measuring apparatuses, direct measurements of interface temperature could not be achieved. Instead of using real temperature measurements at the interface, this study relied on indirect and average local temperature measurements to understand the thermal effects of ultrasonic vibration. A more precise measuring method should be used in the future to accurately explore the relationship between ultrasonic vibration and temperature increase in specimens. Further investigation on the effects of the superposition of stress will be conducted to develop the full mechanisms of ultrasonic vibration upsetting.
A model to characterize acoustic softening during ultrasonic consolidation
Abstract
Ultrasonic consolidation (UC) is a solid state bonding process in which thin metal foils are bonded under the influence of ultrasonic vibration and pressure. Large parts can be made by placing foils side by side or by stacking layers to create thicker parts. Thermal and acoustic softening of metals during UC leads to increased plastic deformation and plays an important role in bond formation. In this work, a thermo-mechanical finite element model is developed to quantify the degree of thermal and acoustic softening occurring in Al 1100-0 foils during UC. The model uses experimentally measured temperatures and changes in the foil's geometry during UC to quantify the amount of thermal and acoustic softening. Acoustic softening is shown to reduce the yield stress of Al 1100-0 foils by up to 82%. In addition, thermal softening is found to be relatively minor, typically less than 5% of the total material softening. This method to quantify acoustic softening during UC allows for a better overall understanding of the bonding process and allows several aspects of the UC bonding process to be optimized and improved.
Keywords
Ultrasonic consolidation Acoustic softening Thermo-mechanical analysis Aluminum
1. Introduction
Ultrasonic consolidation (UC) is a bonding process in which two materials are joined under the influence of ultrasonic vibrations and pressure. A foil and a substrate or multiple foils are bonded together by a sonotrode that applies pressure and ultrasonic vibrations as shown in Fig. 1. Three adjustable process parameters influence UC weld properties; they are (i) the sonotrode's clamping force (Fc) in the negative z-direction, (ii) the sonotrode's oscillation amplitude (¦Ë) at a frequency (f) in the x-direction, and (iii) the sonotrode's speed (S) in the y-direction. A rough knurl pattern on the sonotrode prevents relative displacement at the sonotrode¨Cfoil interface and causes bonding to occur only at the foil¨Csubstrate interface.
UC is similar to the automated tape placement (ATP) process used for thermoplastic materials described in detail by Pitchumani et al. (1997). In their work, optimum ATP process conditions were identified in order to maximize the buildup of inter-layer bond strength during the process. A related work by Tierney and Gillespie (2006) investigated how the choice of process parameters in ATP leads to the development of intimate contact between parts and ultimately the final strength of the part. Similarly in UC, the choice of process parameters also influences the bond strength. UC bonds produced at low ¦Ë and Fc are generally weak. On the other hand, bonds produced at high ¦Ë and Fc can cause excessive deformations and misalignment in the part, resulting in poor bond quality. In order to create high strength UC bonds, it is essential to identify the proper weld parameters for a given material type and material geometry. This was shown by Kong et al. (2003) for Al 6061 where peel testing of UC bonds was conducted in order to determine a window of optimum UC process parameters for the material. A similar study was also done by Kong et al. (2004) for Al 3003.
UC is a low temperature solid-state bonding process in which thick parts can be fabricated by building up the thickness one layer at a time. There are many advantages in using UC over other welding techniques. Koellhoffer et al. (2011) measured the temperature of UC using an infrared camera and reported that temperatures during UC are typically less than 50% of the material's melting temperature. Yang et al. (2009) used a scanning electron microscope to examine the microstructure of UC bonds and found no evidence of melting, indicating that bonding is a solid-state process and that the materials would retain much of their original microstructure. The low temperature bonding process has the advantage of significantly reducing residual stresses in parts made with UC. The low temperatures of UC also allow thermally sensitive materials to be embedded into structures. A unique benefit to using UC over other welding techniques is that two dissimilar materials can be bonded during the process. This was shown by Obielodan et al. (2011) where the lap shear strength of commercially pure titanium and aluminum alloy 3003 dual-material structures was optimized.
The fundamental bonding mechanisms of UC are currently not fully understood; however, plastic deformation is accepted by many to be an important factor in bond formation during UC. de Vries (2004) investigated the mechanics of ultrasonic welding through mechanical modeling, a shear force sensor and an infrared camera. It was concluded that plastic deformation, heat generation, normal force and shear force all play an important role in bond formation. Kong et al. (2005) investigated the interface of UC bonds through optical microscopy and found that increased plastic deformation in Al 6061 foils leads to an increased linear weld density and therefore greater bond strength. In their work, linear weld density was determined by investigating a cross sectional cut of a UC bond and calculating the percent of the material that is bonded together. Janaki Ram et al. (2006) performed a similar study using Al 3003 foils and also found that UC process parameters that resulted in the largest amount of plastic deformation also had the greatest linear weld density. Yang et al. (2009) noted that plastic deformation at the interface during UC aids in the removal of oxide surface layers and facilitates bonding. Plastic deformation is an important part of the UC bonding process since it allows the two surfaces to come into intimate contact and aides in breaking apart oxide layers and contaminants. Only once the two materials are in intimate contact can bonding can occur. Plastic deformation during UC brings more material into intimate contact and increases the area over which bonding can take place. Thermal and acoustic material softening are an important part of the UC bonding process since they lead to increased plastic deformations in the foil and consequently could result in higher bond strengths.
Acoustic softening is the reduction in the apparent static stress necessary for plastic deformation in a material under the influence of ultrasonic energy. Langenecker (1966) performed tension tests on several materials under the influence of ultrasonic energy. It was found that acoustic softening of the material occurs immediately upon application of the ultrasonic energy and ends immediately upon removal of the ultrasonic energy. In addition, at a sufficient level of ultrasonic energy, the yield stress of a material can effectively be reduced to zero. Kirchner et al. (1984) performed quasi-static compression tests of Al 6061 and also reported a similar reduction in yield stress under the presence of ultrasonic irradiation. Izumi et al. (1966) have shown that acoustic softening occurs during compression testing of a wide range of materials including aluminum, copper, silver, steel, lead and magnesium under the influence of ultrasonic energy.
The fundamental mechanisms responsible for acoustic softening have been investigated by many researchers, but due to the complexity of the process, a clear consensus has not emerged. The following mechanisms have previously been suggested to explain the apparent reduction in yield stress due to acoustic softening: stress superimposition, volumetric energy absorption of dislocations and surface friction effects. Malygin (2000) investigated stress superimposition as an explanation of the experimentally observed acoustic softening. The stress superimposition theory states that oscillatory stresses induced by ultrasonic vibrations are added onto the static loading, making the material's yield stress appear lower during experimental measurements. Malygin (2000) concluded that the stress superimposition method adequately described the acoustic softening effect. Cai (2006) states that the stress superimposition method is not easy to verify experimentally since detailed dynamic stress profiles within acoustically softened specimens would be required. Cai (2006) investigated stress superimposition through numerical methods by modeling the dynamic stress profiles of metals under the influence of ultrasonic vibrations; however, this work did not consider the influence of energy absorption of dislocations on acoustic softening. The coefficient of friction between two surfaces is reduced under the influence of ultrasonic vibrations and can lead to a reduction in the force required for deformation to take place. Ashida and Aoyama (2007) studied the effects of ultrasonic vibration on press forming and found that the ultrasonic vibrations reduced the coefficient of friction at the surface between the sheet metal and the die.
Langenecker (1966) found that significantly less ultrasonic energy is needed to soften a material than thermal energy, suggesting that the ultrasonic energy must be absorbed primarily by the dislocations of metal grains, rather than uniformly throughout the material like thermal energy. Transmission electron microscopy (TEM) was used to observe the change in dislocation density and structure after the application of ultrasonic energy. Hansson and Tholen (1978) also used TEM to observe a change in dislocation microstructure of aluminum under the influence of ultrasonic energy. Langenecker (1966) hypothesized that localized short timescale heating may occur in a material as energy is absorbed by dislocations and that this localized heating may be responsible for the reduction in yield stress; however, Sriraman et al. (2011) monitored temperatures during UC with a thermocouple recording at 10 kHz and found no evidence of localized short timescale heating. Daud et al. (2007) compared a finite element model of acoustic softening during tension and compression tests of aluminum with experimental measurements. It was concluded that stress superimposition and interfacial surface effects do not account for the experimentally observed acoustic softening and that energy absorption at dislocations is an important mechanism of acoustic softening. Hung and Lin (2013) experimentally investigated the material property changes of aluminum during ultrasonic vibration and also concluded that energy absorption at dislocations is an important mechanism in acoustic softening.
The two process parameters that affect the amount of ultrasonic energy during UC ¨C and therefore the amount of acoustic softening ¨C are the oscillation amplitude (¦Ë) and the frequency (f). Few previous works have focused on quantifying acoustic softening over a wide range of process parameters. The previously mentioned acoustic softening study by Daud et al. (2007) was limited to only one amplitude and one frequency. Izumi et al. (1966) performed compression tests under the influence of ultrasonic irradiation and found a linear relationship between acoustic softening and the amplitude of vibrations. This relationship is not directly applicable to UC for two reasons: (i) ultrasonic vibrations in their study were parallel to the static loading direction, whereas in UC ultrasonic, vibrations are perpendicular to the static loading direction and (ii) the relative motion between the two surfaces during UC complicates the acoustic softening process since a portion of the input amplitude is absorbed by the material and the rest dissipates at the sliding surface. In addition, amplitudes only up to 20 ¦Ìm were studied by Izumi et al. (1966), whereas during UC, amplitudes over 40 ¦Ìm are often needed to create strong bonds. Yao et al. (2012) investigated the mechanism of acoustic softening in commercially pure aluminum using a model based on thermal activation theory and found good agreement with experimental measurements. Yao et al. (2012) also found a linear relationship between acoustic softening and oscillation amplitude during compression tests, but the study was limited to a narrow range of parameters: only amplitudes up to 6 ¦Ìm at a frequency of 9.6 kHz were studied.
Several previous numerical studies on the mechanics of UC have not taken into account acoustic softening. Without knowledge of the amount of acoustic softening during UC, stresses and plastic deformation cannot be accurately modeled. Doumanidis and Gao (2004) studied plastic deformation and stresses at the bond interface during ultrasonic welding through finite element modeling without including acoustic softening. Similarly, Elangovan et al. (2009) studied stress fields and volumetric heat generation due to plastic deformation during UC using a finite element model without accounting for acoustic softening during the process. Zhang and Li (2009) used thermo-mechanical modeling to study the mechanics of the bonding interface during UC. Acoustic softening was not accounted for in the model and it was concluded that only thermal softening at the interface was responsible for the material softening.
Acoustic softening during UC has been previously modeled by Siddiq and Ghassemieh (2008), but their work investigated a small range of process parameters and was validated using only a small set of experimental data by Langenecker (1966) that was not specific to UC. Pal and Stucker (2012) used a dislocation density based constitutive model in order to predict deformations of Al 3003-H18 foils during UC; however, additional experiments are required to test the validity of the model.
The vast majority of previous works on acoustic softening have investigated the mechanisms behind the experimentally observed softening in tension and compression specimens. There is limited information on a method to quantify acoustic softening during tension and compression tests and even less that is specifically related to UC. In addition, previous attempts to quantify acoustic softening have been limited to a small range of process conditions. A method to quantify acoustic softening over a wide range of process parameters is especially important for UC, since many different process parameters are used to create bonds with varying properties. The goal of this work is to evaluate in detail how various UC process parameters influence acoustic softening. A method to quantify acoustic softening under UC conditions is presented which will allow for a better overall understanding of the UC bonding process.
2. Model development
In order to model deformations in the Al 1100-0 foils used in this work during UC, the following mechanisms are included in the model: volumetric thermal softening due to bulk temperature increase, surface friction effects, strain hardening of the material, volumetric energy absorption of dislocations, stress superimposition of the ultrasonic vibrations and the clamping force.
Thermal softening during UC is caused by heat generated due to the friction at the foil¨Csubstrate interface and volumetric heat generation due to hysteresis energy loss during cyclic plastic deformation in the foil. Several previous studies have focused on frictional heat generation during UC. Elangovan et al. (2009) studied how the coefficient of friction at the foil¨Csubstrate interface influences UC process temperatures through thermal finite element modeling. Koellhoffer et al. (2011) used thermal finite element modeling and temperatures measured during UC to calculate the coefficient of friction during UC as a function of UC process parameters. In this current work, it is assumed that all work done by friction is uniformly dissipated as a heat flux over the area of contact between the surfaces. The coefficient of friction (¦Ì) during UC is a function of frictional heat generation (qfr), the horn's clamping force (Fc), the frequency (f) and the amplitude (¦Ë) as follows:
Previous works give some insight into the expected friction coefficient trends that are seen in UC. Naidu and Raman (2005) investigated the friction coefficient trends during ultrasonic fretting of aluminum and found the following trends: the coefficient of friction will decrease with increasing clamping force (Fc), increase with increasing amplitude (¦Ë), and decrease with increasing speed (S). These trends were also confirmed by Koellhoffer et al. (2011) specifically for UC.
In addition to friction, heat can be generated if the material plastically deforms during the UC process. Unlike friction, this is a volumetric phenomenon. Volumetric heat generation is typically considered to be small in comparison to frictional heat generation for typical process parameters in UC, as discussed by Koellhoffer et al. (2011). Volumetric heat generation during UC has been studied by Zhang and Li (2009), but this work did not account for the influence of acoustic softening on the yield stress of the material. In order to calculate volumetric heat generation, plastic dissipation (Wp) must first be calculated. Wp is related to the yield stress (¦Òy), the plastic strain rate (), the material volume (V) and time (t) as follows (Hodowany et al., 1999):
The expression for volumetric heat generation (qvol) is given in Eq. (3) and is related to the plastic dissipation (Wp), a material dependent heat conversion factor (¦Â) and time (t).
Hodowany et al. (1999) investigated the value of ¦Â in Eq. (3) specifically for aluminum in compression. It was found that ¦Â is a function of the amount of plastic strain and ranged from approximately ¦Â = 0.3 at low plastic strains up to ¦Â = 0.9 at higher plastic strains. Once acoustic softening during UC is quantified using the method described in this work, the contribution of volumetric heat generation to thermal softening can be investigated.
The relationship between yield stress, strain and temperature has been studied for Al 1100-0 in compression by Hockett (1967). Strain hardening behavior of Al 1100-0 is observed along with thermal softening. As reported by Hockett (1967), the stress¨Cstrain curve of Al 1100-0 at a given temperature can best be represented in the plastic region by the power law equation shown in Eq. (4). The power law model presented by Hockett (1967) and shown in Eq. (4) is modified in this work to account for acoustic softening during UC. A percent acoustic softening term (¦Î) is added to account for acoustic softening due to the volumetric energy absorption of dislocations in the material. For the purpose of this work, it is assumed that all material softening during UC is due to thermal and acoustic softening.
Due to the relatively thin foils used during UC, it is assumed that ¦Î is uniformly distributed throughout the material volume during the process. The following authors have made a similar assumption of a volume average in presenting experimental results of acoustic softening of thicker specimens (2¨C75 mm): Yao et al. (2012), Izumi et al. (1966), Kirchner et al. (1984) and Hansson and Tholen (1978). ¦Î ranges from zero to one depending on the amount of acoustic softening. At a value of ¦Î = 1, there is no acoustic softening in the material and it will deform normally. At a value of ¦Î = 0, there is sufficient ultrasonic energy to reduce the yield stress of the material to zero. For the purpose of this work, it is assumed that acoustic softening linearly affects the K term in the power law equation and that the n term remains unchanged by acoustic softening since K is primarily responsible for significant changes in ¦Òy.
In order to capture the effect of temperature on the yield stress of Al 1100-0, Hockett (1967) identified a series of constants for the power law model at different temperatures. These constants are shown in Table 1. Linear interpolation is used to calculate the values of K and n at temperatures between those listed in Table 1.
In order to investigate acoustic softening over the full range of process parameters and different foil geometries, it is useful to present results in a dimensionless form. First, a dimensionless amplitude (¦«) is defined according to Eq. (5) that is related to the peak-to-peak amplitude (¦Ë) and the initial foil thickness (H0). ¦« is related to the shear strain in the foil and in the extreme case where there is a sticking friction condition at the foil¨Csubstrate interface, ¦« would equal the shear strain in the foil.
In order to study and compare UC of foils with different initial widths (W0), it is useful to convert the clamping force (Fc) to a contact pressure (Pc). Pc is related to the clamping force (Fc), initial foil width (W0) and contact length (lc) according to Eq. (6). Here, lc is defined as the distance in the y-direction that the sonotrode and the foil are in contact during UC (see Fig. 1). The value of lc is a function of sonotrode radius and foil thickness. In this work, lc was measured to be 5.4 mm and assumed to be constant for all UC process parameters for the sonotrode and foil geometries that were investigated. It has been shown in Kelly (2012) that the assumption of a constant lc introduces minimal error to the acoustic softening (¦Î) calculation.
Finally, a dimensionless pressure (P) is defined below that is related to the contact pressure (Pc) and the yield stress of Al 1100-0 at 20 ¡ãC (¦Òy0 = 34 MPa) before its exposure to ultrasonic energy:
2.1. Thermo-mechanical finite element modeling
The amount of acoustic and thermal softening occurring during UC is quantified in this work using thermo-mechanical finite element modeling in Abaqus 6.9-2 by Dassault Systems (2009). The model accounts for volumetric thermal softening due to bulk temperature increase, surface friction effects, strain hardening of the material, volumetric energy absorption of dislocations, stress superimposition of the ultrasonic vibrations and the clamping force. The two material attributes that are needed in order to predict the temperature field and the deformation field as a function of the UC process parameters are the coefficient of friction between the foil and the substrate shown in Eq. (1) and a constitutive equation describing the material's yield stress behavior due to UC process parameters shown in Eq. (4). The friction coefficient can be determined independently by choosing a value of frictional heat (qfr) that when input into the thermal finite element model, predicts a steady state temperature that matches the experimentally measured steady state temperature. After the value of qfr that gives the best match between predicted temperatures and the experimentally measured temperatures is found, the friction coefficient between the foil and the substrate can be determined using Eq. (1). Once the friction coefficient (¦Ì) is established for a given set of UC process parameters, the acoustic softening parameter (¦Î) can be selected that results in a match between the experimentally measured change in the width of the foil (¦¤W/W0) and the predicted (¦¤W/W0) from a mechanical finite element model of the UC process. The procedure to quantify acoustic softening (¦Î) is summarized in Fig. 2.
2.1.1. Thermal finite element model
A 2D thermal finite element model in the x¨Cz plane, as shown in Fig. 1, is used to determine the frictional heat (qfr) that is required to match the predicted temperature with the experimental values measured during UC under a given set of process parameters. The thermal model, created in Abaqus 6.9-2, uses 4479 six-node quadratic triangular elements. The entire cross section of the foil and the substrate are modeled, but only a portion of the sonotrode is modeled to improve computational efficiency since the sonotrode is very large in comparison to the foil and the substrate. In order to ensure that the size of the modeled sonotrode region does not affect the temperature distribution, a sufficiently large portion of the sonotrode is modeled such that there is no heat flux through that boundary. A fixed temperature boundary condition is applied to the edges of the sonotrode and the edges of the substrate. All surfaces exposed to air have been modeled using a convection boundary condition with a convective heat transfer coefficient of h = 5 W/(m2 ¡ãC) and a T¡Þ = 20 ¡ãC. The value of h was varied in the normal range of natural convection (from 2 to 15 W/(m2 ¡ãC)) and it was found that it did not influence the temperature distribution in the foil due to the short time-scale of UC. Perfect thermal conductance is assumed at the sonotrode¨Cfoil and substrate¨Cfoil interfaces. The frictional heat (qfr) is applied at the substrate¨Cfoil interface. The thermal finite element model's boundary conditions are shown in Fig. 3.
Table 2 summarizes the material properties used in the thermal finite element analysis. The three required material properties are the thermal conductance (k), density (¦Ñ) and specific heat (c). The substrate is modeled using Al 3003-H14 properties, the sonotrode is modeled using Ti-6Al-4V properties and the foil is modeled using Al 1100-0 properties.
In this work, it is assumed that qfr is distributed evenly over the entire contact area between the foil and the substrate. Since qfr is a function of the UC process parameters, it must be calculated for each set of weld parameters. The measured steady-state temperature (Tss) during a weld is used in combination with the thermal model to determine the qfr generated during the weld. The thermal model is executed for 1.5 s (equal to the time it takes the sonotrode to travel down the length of the 60 mm weld at 40 mm/s). For each set of process parameters, an increasing value of heat input of qfr is imposed at the interface between the foil and the substrate until the error between the experimental steady state temperature and the predicted values from the thermal finite element model are minimized. This thermal model has been validated in Kelly (2012) where good agreement was found between experimentally measured and modeled temperatures in both the transient and steady-state temperature regions of the weld.
2.1.2. Mechanical finite element model
The 3D mechanical finite element model of UC used in this work was created in Abaqus 6.9-2 (Dassault Systems, 2009) and uses the explicit solver. The mechanical finite element model accounts for volumetric thermal softening of the bulk material due to temperature increase, changes in the surface friction effects during UC according to Eq. (1), strain hardening of the material, acoustic softening (¦Î) due to energy absorption of dislocations and the superimposition of the vibrational and clamping stresses. Fig. 4 summarizes the boundary conditions of the mechanical finite element model.
The sonotrode and substrate are modeled as rigid bodies since both materials have a much higher yield stress than the Al 1100-0 foil. The following boundary conditions apply to the sonotrode: ¦Ë in the x-direction at f = 20 kHz and Fc in the negative z-direction. The sonotrode is allowed zero displacement in the y-direction and is free to displace in the z-direction. Zero rotation is assigned to the sonotrode in the x-, y- and z-directions. The substrate is assigned an encastre boundary condition (zero displacement and rotation in all directions).
In Fig. 4(b), the Al 1100-0 foil is modeled with 10-node quadratic tetrahedron elements (C3D10M). In order to improve computational efficiency, only an 8 mm length of the foil is modeled rather than the full 60 mm weld length. Node-to-surface contact formulations are defined on the top and the bottom of the foil using the penalty contact method. Tangential contact between the sonotrode and the top of the foil is defined using a penalty friction formulation with ¦Ì = 1.0, preventing relative motion between the sonotrode and top of the foil. This sticking friction condition between the sonotrode and the foil is used because experimentally during UC there is no relative displacement between the two parts. The influence of ¦Ì at the sonotrode¨Cfoil interface was investigated on deformation in the x-direction. It was found that the results are not sensitive to a coefficient of friction ranging from 0.5 to 1.5 as well as a ¡°rough¡± contact formulation ¨C all yield the same result.
Tangential contact between the substrate and the bottom of the foil is also defined using a penalty friction formulation. The coefficient of friction used in the mechanical model at the foil¨Csubstrate interface is calculated using the thermal finite element model and Eq. (1). A zero y-displacement boundary condition is assigned to one edge of the foil since regions of the foil that the sonotrode has previously bonded will no longer be able to displace in the y-direction. The sonotrode speed (S) in the y-direction is ignored in the simulation since it has no effect on the amount of deformation in the x-direction in the foil.
In order to increase computational efficiency, the model is executed for a time period of only 0.001 s. This time period was chosen after running the model and observing the time required for the amount of plastic deformation in the foil to plateau. The time required for the plastic deformation to plateau varies with oscillation amplitude, clamping force and the material properties of the foil. Plastic dissipation (Wp) in the foil reaches a plateau when the applied normal and shear stresses imposed on the foil become equal to the strain-hardened yield stress of the material and has been investigated in more detail in Kelly (2012).
In order to quantify the amount of acoustic softening during a UC weld, the value of ¦Î in Eq. (4) is incrementally decreased until the error between the foil's width increase in the mechanical model and the experimental measurements for a given set of UC process parameters is minimized.
3. Equipment, materials and experiments
3.1. Equipment
3.1.1. Ultrasonic welder
The UC equipment used in this work is a seam welder custom built by AmTech. The welder's Ti-6Al-4V sonotrode is textured using an electric discharge machining (EDM) technique and has a diameter of 147 mm. Weld quality is controlled by adjusting the three process parameters shown in Fig. 1: the sonotrode's clamping force (Fc) ranging from 300 to 6000 N, peak-to-peak oscillation amplitude (¦Ë) ranging from 7 to 44 ¦Ìm and sonotrode speed (S) ranging from 0 to 300 mm/s. The available weld parameters allow a wide range of weld qualities to be produced. The frequency (f) of the welder is a constant 20 kHz, which is typical for many UC systems. The welder keeps the amplitude constant throughout the weld by automatically adjusting the power. The substrate, seen in Fig. 1, is held in place using a vise. The vise is used to reduce resonance effects in the substrate, which can occur with bolted substrates and leads to inconsistent weld quality and properties.
3.1.2. Infrared camera
An infrared (IR) camera is used to monitor temperatures during the UC process. There are several advantages in monitoring UC temperatures with an IR camera over other methods. The IR camera allows great flexibility in location and special resolution of measurements. Another method to measure UC temperatures, discussed by Sriraman et al. (2011), involves the use of thermocouples. In their work, weld temperatures were measured by placing a thermocouple and the bond interface during welding. Use of an IR camera has three distinct advantages when compared to using an thermocouple during UC: (i) one thermocouple is required for each data point, (ii) placing a thermocouple between weld materials will alter the weld properties at that location and (iii) the thermocouple cannot be reused afterwards.
The IR camera used in this work is a FLIR Thermovision Alert 194 and records images with a resolution of 320 ¡Á 240 pixels at 4 Hz. During each weld, the IR camera is placed 22 in. from the weld in the y-direction and 2 in. above the weld in the z-direction. Weld temperatures are recorded throughout the entire weld process and the data is used to identify thermally transient and steady-state regions along the length of the weld. This work focuses entirely on weld areas at steady-state temperatures.
IR camera recordings have been conducted at frequencies up to 35 kHz, but for this work it was found that a 4 Hz recording accurately captured the weld temperatures. An IR camera frequency of 4 Hz is not sufficient to capture the heat generation and temperature rise during one oscillation at 20 kHz; however, during UC each area of the foil experiences approximately 2000¨C3000 cycles (0.1¨C0.15 s) during which a steady-state temperature is reached. Fig. 5 shows that IR camera measurements of UC are independent of frequency in the range of 4 Hz¨C35 kHz.
3.2. Materials
The two materials used in this work are Al 1100-0 foils and Al 3003-H14 substrates. Since the yield stress of Al 1100-0 is much lower than the yield stress of Al 3003-H14, all plastic deformation occurs in the foil and allows finite element modeling of the process to be simplified. The materials were chosen over other aluminum alloys since both have a relatively thin oxide layer, which will reduce the possibility of inconsistent welds. Yang et al. (2009) have shown that the presence of significant oxide layers can weaken or prevent bonding via UC. Al 1100-0 was also chosen since its yield stress has been well characterized over a wide range of temperatures by Hockett (1967). The material, initial width (W0), and initial thickness (H0) of the substrates and the three foil geometries are shown in Table 3.
3.3. Experimental procedure
The following experimental data is required to quantify the amount of thermal and acoustic softening that occurs during UC using a thermo-mechanical finite element model: (i) IR camera temperature measurements and (ii) the change in the foil's width during UC processing. Temperature measurements are required to calculate thermal softening that occurs during UC and to calculate the coefficient of friction between the foil and the substrate. During the UC process, the IR camera records images at 4 Hz and the sonotrode speed (S) is 40 mm/s, so the IR camera records an image every 10 mm along the length of the weld in the y-direction. From this temperature data, a steady state weld temperature (Tss) region can be identified for any given set of UC process parameters. After UC, a digital image of each welded specimen was used to measure the average width increase in the location of Tss using Image J software (Ferreira and Rasband, 2011). The average UC processed foil width is calculated in Image J by dividing the area of the welded foil by the length of the Tss region in the y-direction. The foil's processed width is the sum of the original tape width (W0) and the increase in width after processing (¦¤W).
In this work, an array of UC process parameters was chosen that results in a wide range of bond qualities. Five amplitudes were chosen (¦Ë = 10, 18, 27, 32, 36 ¦Ìm) and five clamping pressures were chosen (Pc = 17.0, 28.3, 45.2, 62.0, 78.9 MPa). The sonotrode speed (S) is held constant at 40 mm/s for all sets of process parameters, since S will have no effect on acoustic softening. A total of 25 different combinations of UC process parameters were created using each combination of ¦Ë and Pc. Twenty-three of the 25 combinations of UC process parameters resulted in successful welding. Two sets of process parameters (36 ¦Ìm, 62.0 MPa) and (36 ¦Ìm, 78.9 MPa) did not successfully bond since the power required to produce these bonds exceeded the 3 kW maximum of the welder. Bonds made using the lowest ¦Ë could easily be separated by hand while bonds made at higher ¦Ë were sufficiently strong that they resulted in tensile failure in the base metal rather than failure at the bonded interface during peel testing. The full array of welds was produced using the three different foil cross sections shown in Table 3.
4. Results and discussion
4.1. Experimental results
A plot of the thermal development of a typical UC bond along the length of the weld (y-direction) is created according to the procedure described in Section 3.3 and is shown in Fig. 6. The maximum measured IR camera temperature for a typical weld is shown at each position along the weld length in Fig. 6 and a Tss region is identified from 30 to 60 mm along the length of the weld in the y-direction. The process is repeated for each set of UC process parameters and foil geometries and these results are shown in Fig. 7. The value of Tss in Fig. 7 ranges from 24 to 198 ¡ãC over the entire set of UC process parameters.
Fig. 8 shows an image used to calculate the average width increase (¦¤W/W0) using Image J after UC processing according to the procedure described in Section 3.3. Fig. 9 shows the experimentally measured ¦¤W/W0 for each combination of UC process parameters for the three foil geometries. All values of ¦¤W/W0 are taken for the region of Tss for each weld. The percentage increase of ¦¤W/W0 ranges from 1.3% to 24.5% over the investigated test arrays.
4.2. Model results
4.2.1. Coefficient of friction
Frictional heat generation (qfr) is calculated for each set of UC process parameters using the thermal finite element model and the Tss values shown in Fig. 7. Once qfr is determined for each combination of process parameters, the friction coefficient is calculated according to Eq. (1). The results of this calculation are shown in Fig. 10 for the 9.5 mm ¡Á 0.52 mm foil array. The trends found in this study ¨C decreasing coefficient of friction with increasing clamping force and increasing coefficient of friction with increasing amplitude ¨C agree with the previous studies by Naidu and Raman (2005) and Koellhoffer et al. (2011).
4.2.2. Acoustic softening
The coefficients of friction shown in Fig. 10 are used in the mechanical finite element model and acoustic softening (¦Î) is determined by minimizing the error between the experimentally measured ¦¤W/W0 and the predicted ¦¤W/W0 according to the procedure shown in Fig. 2. To highlight the importance of acoustic softening during UC, a few selected combinations of process parameters are shown in Fig. 11. The figure shows experimentally measured ¦¤W/W0 in addition to predicted ¦¤W/W0 from the finite element thermo mechanical model for three different amounts of acoustic softening (¦Î).
The mechanical finite element model accurately predicts the foil width increase without acoustic softening when ¦« = 0 and P = 3.40. This close agreement validates the Al 1100-0 material model and the boundary conditions employed in the mechanical model. It can clearly be seen that once ultrasonic energy is applied (¦« > 0), the material model with thermal softening and without acoustic softening (¦Î = 1) severely under predicts the foil's width increase. Fig. 11 clearly shows that the majority of material softening that occurs during UC is due to acoustic softening and can be quantified using the method described in this work. At an amplitude of ¦« = 0.062, the average acoustic softening of the five weld parameters tested is ¦Î = 0.20. All five combinations of UC process parameters fall into an acoustic softening range between ¦Î = 0.17 and ¦Î = 0.22.
Acoustic softening (¦Î) is determined for the full array of process parameters by minimizing the error between the predicted ¦¤W/W0 and experimentally measured values from the 9.5 mm ¡Á 0.52 mm foil array in Fig. 9 using the procedure shown in Fig. 2. Individual data points in Fig. 12 show the result of this acoustic softening (¦Î) calculation and result in the minimum error between experimentally measured and predicted ¦¤W/W0 for the 9.5 mm ¡Á 0.52 mm foil array. Acoustic softening (¦Î) of Al 1100-0 during UC is found to be a function of dimensionless amplitude (¦«) and dimensionless pressure (P) and can be expressed in a constitutive form as shown below:
Fig. 12 shows that ¦Î decreases with amplitude (¦«) until it approaches its minimum value of ¦Î0 = 0.177. The rate at which ¦Î approaches its minimum value is related to the dimensionless pressure (P): UC welds with a lower P approaches ¦Î0 at a lower ¦« than UC welds with higher values of P. It is also important to note that acoustic softening is a non-linear function of ¦« rather than ¦Ë; therefore, a thicker material will have a lower acoustic softening than a thinner material at a given oscillation amplitude according to Eq. (8). The acoustic softening model shown in Fig. 12 can be applied to any Al 1100-0 foil geometry during UC and this is shown in the following section where the acoustic softening model is validated using the 13.0 mm ¡Á 0.52 mm and 9.5 mm ¡Á 1.04 mm foil arrays.
In order to validate the mechanical model, it is necessary to check the modeled decrease in foil thickness (¦¤H/H0) against the experimentally measured ¦¤H/H0. This check is made once the value of ¦Î is solved for and error between the model and experimental ¦¤W/W0 is minimized. Once it is verified that the experimental and modeled deformations in the x- and z-directions match, it can be assumed that deformation in the y-direction will agree because of constant volume in plastic deformation. This comparison is shown in Fig. 13 for the 9.5 mm ¡Á 0.52 mm foil array.
4.3. Model validation
In this section, the acoustic softening model is validated using Tss and ¦¤W/W0 values from Fig. 7 and Fig. 9 for the 13.0 mm ¡Á 0.52 mm and 9.5 mm ¡Á 1.04 mm foil geometries. Acoustic softening values calculated using Eq. (8) are used as inputs to the thermo-mechanical finite element model and used to predict ¦¤W/W0 for the 13.0 mm ¡Á 0.52 mm and 9.5 mm ¡Á 1.04 mm foil arrays. Friction coefficients, shown in Fig. 10, are also input into the mechanical model. Fig. 14 compares experimental ¦¤W/W0 and predicted ¦¤W/W0 for the 13.0 mm ¡Á 0.52 mm foil array.
The average difference in ¦¤W/W0 between the experimental measurements and the model predictions in Fig. 14 is 0.5% and the maximum difference is 1.1%. Fig. 15 shows a similar comparison of experimental measurements and model predictions of ¦¤W/W0 for the 9.5 mm ¡Á 1.04 mm foil array. The average difference in ¦¤W/W0 between the experimental measurements and the model predictions in Fig. 15 is 0.7% and the maximum difference is 2.0%.
4.4. Model insights to characterize heat generation
The acoustic softening model that has been developed can be used to investigate several new aspects of UC. For example, the relative contributions of thermal and acoustic softening to the total material softening during UC can be investigated. Fig. 16 compares the relative contributions of thermal and acoustic softening on the stress¨Cstrain curve of Al 1100-0 during UC. The most significant acoustic softening is shown (¦Î0 = 0.177) in addition to the highest temperature (Tss = 198 ¡ãC) measured during the test arrays presented in this work. It can clearly be seen in Fig. 16 that acoustic softening is far more significant than thermal softening during UC. It can be seen that without the presence of acoustic softening, Al 1100-0 at 198 ¡ãC experiences a significant amount of softening (25.4%) when compared to room temperature properties. When ¦Î = 0.177, this same increase in temperature only increases material softening by 4.5%, compared to the 82.3% reduction due to acoustic softening. This shows that thermal softening plays a secondary role to acoustic softening during UC. Also, if thermal softening were ignored during the modeling process, a reasonable measurement of acoustic softening can still be made and the error should be no greater than 4.5% in the most extreme case.
The acoustic softening model also allows volumetric heating (qvol) to be calculated and compared to the heating contribution from frictional heating (qfr). qvol can be calculated using Eq. (3). In Eq. (3), Wp is calculated for each combination of UC process parameters using the mechanical finite element model and the time (t) is equal to the sonotrode speed (S = 40 mm/s) divided by the contact length (lc = 5.4 mm). In the case of ultrasonic consolidation, the exact value of ¦Â from Eq. (3) is not critical because even at a value of ¦Â = 0.9 (the upper limit for aluminum), the volumetric contribution to total heat generation is less than 5% for typical UC process parameters (¦« > 0.04). At ¦« < 0.04, little to no bonding of the materials takes place, so these processing conditions would not typically be used where strong bonding is required. Volumetric heat generation is small relative to frictional heat generation under process conditions where strong bonds are produced. In the extreme case, in which ¦« is low and P is high, the maximum contribution from volumetric heating is found and is equal to 18%. The ratio of volumetric heat generation to total heat generation for the full range of process parameters is shown in Fig. 17.
5. Summary and conclusions
Acoustic softening of Al 1100-0 foils during ultrasonic consolidation is quantified using the experimental and thermo-mechanical modeling process outlined in Fig. 2. A relationship between material thickness, UC process parameters (¦Ë and Fc) and acoustic softening has been identified. A wide range of UC process parameters were explored, ranging from parameters that create very weak bonds that can easily be peeled apart by hand to very strong welds that fail in tension in the base material during peel testing. Once acoustic softening is determined for a given material and geometry using the method described in this work, acoustic softening can be calculated for any other geometry of the same material.
For the UC process parameters investigated in this work, thermal softening alone does not account for the experimentally measured tape deformations. Acoustic softening (¦Î) during UC of Al 1100-0 foils is found to be very significant ¨C reducing the yield stress of the material up to 82%. On the other hand, thermal softening is a small contributor to the total material softening during UC (<5%). Friction is the dominant source of heat during UC. Volumetric heat generation accounts for less than 5% of all heat generated during UC process parameters that are typically chosen to produce strong bonds.
The model and experiments described in this work serve to connect the theoretical literature on acoustic softening with ultrasonic consolidation in a practical method that allows acoustic softening to be quantified during the process. Knowledge of how the yield stress of a material changes during UC will lead to a better understanding of how materials will come into intimate contact and when bonding will occur during the process.
Theoretical and experimental analyses of ultrasonic-assisted indentation forming of tube
Abstract
Indentation forming process is used for internal forming and sizing of thick-walled tubes working in high internal pressures. In this process, a mandrel with a diameter slightly larger than that of the tube is pressed and moved inside the tube, creating an internal profile. This article presents theoretically and experimentally influences of longitudinal ultrasonic vibration on this forming process. For this purpose, vibro-impact and continuous forming regimes have been investigated. Application of longitudinal ultrasonic vibration along the axis of the workpiece showed experimentally reduction of 15%¨C21% of axial forming forces and improvement of surface quality of the bore of the tube, while no effect on spring back of the formed zone was observed.
Keywords Indentation forming, ultrasonic-assisted forming, tube forming, slab analysis
Introduction
Indentation forming process is a simple way of little forming, ironing and sizing of internal surface of thick-walled tubes. The process is carried out by pressing and driving a profiled tool (mandrel) inside the tube. Since the diameter of the tool is slightly larger than that of the tube, the surface of the tube is formed and work-hardened.
In ultrasonic-assisted forming, ultrasonic vibration with a certain amplitude and frequency is applied to a part or whole of workpiece, die, tool, or combination of them in a certain direction or directions. This vibration can reduce friction, change strength of the material (if applied to workpiece) and change the loading mechanism and consequently reduce the forming force. In addition, better surface quality, tool life enhancement, more uniform dislocation distribution, internal stress concentration reduction and dimensional and geometric stability are the expected results of employment of ultrasonic vibration. Due to the likely advantages, ultrasonic-assisted forming has been investigated during the past years.
Blaha and Langenecker 1 studied the effects of applying ultrasonic vibration on the process of forming in 1955. Their research showed the softening effect of superimposed ultrasonic vibration on zinc single crystals, which were undergoing a tensile test. In 1957, Nevill and Brotzen 2 investigated the effect of mechanical vibration in the frequency range of 15¨C80?kHz on the tensile elongation of low-carbon steel wires. Their experiments showed that for a variety of amplitudes, the reduction in stress was proportional to the amplitude of vibration. In 1966, Pohlman and Lehfeldt 3 studied the influence of ultrasonic vibration on plastic forming of metals. The result of an experiment on a polycrystalline copper specimen subjected to tensile deformation, while impulses of 20-kHz ultrasound were intermittently superimposed, showed that the force drop due to ultrasonic vibration was only seen in the plastic part of stress¨Cstrain curve and not in the elastic part. They concluded that application of ultrasound under suitable conditions can considerably reduce both the external and internal forces required to overcome friction forces to form plastically metallic specimens.
Atanasiu 4 investigated tube drawing by axial ultrasonic oscillation of the plug in 1980. He found that ultrasonic affects the yield limit and causes the coefficient of friction and the coefficient of viscosity to decrease. It was also observed that the reduction in drawing force is diminished at greater drawing speeds. 4 Siegert and Ulmer 5 in 2001 investigated the superimposing of ultrasonic waves on the dies in tube and wire drawing. The results showed that longitudinal oscillation of the die at ultrasonic frequencies in the range of 20¨C22?kHz parallel to the drawing direction could reduce the friction. The reduction of the drawing force was found to be mainly a function of the ultrasonic amplitude. It was also found that increased drawing speed could decrease the drawing force reduction, perhaps due to decrease in oscillations per unit of length. Murakawa and Jin 6 in 2001 investigated the applicability of ultrasonic radial vibrated dies in the wire drawing process. They reported that radial ultrasonic vibration application is very effective in increasing the critical drawing speed, and it is approximately 10 times as fast as that for axial ultrasonic vibration application, making the radial vibration more productive than the axial vibration.
Hung and Hung 7 in 2005 studied the influence of ultrasonic vibration on hot upsetting of aluminum alloy. Their experimental results indicated that ultrasonic vibration could considerably reduce the compressive forces needed during hot upsetting. The reducing effect on compressive forces decreased while the temperature increased. The strain rate did not significantly affect the reducing effect on compressive forces.
In 2007, Mousavi et al. 8 studied the effects of applying ultrasonic vibration on the die during the extrusion process. They showed that the extrusion force and the material flow stress would be reduced by applying the ultrasonic vibration if the extrusion speed was below a critical speed. In addition, it was found that applying the ultrasonic vibration had no significant effect on the equivalent plastic strain of the material. A larger reduction in average extrusion force was obtained by either reducing the extrusion speed or increasing the amplitude of vibration. In 2009, Hung and Chiang 9 investigated the influence of ultrasonic vibration on double backward-extrusion of aluminum alloy. The results showed that under the effect of ultrasonic vibration, the forming force would be decreased because the ultrasonic vibration increased the temperature of the workpiece.
Susan et al. 10 in 2010 researched mechanical characteristics of stainless steel tubes in the process of drawing under different drawing speeds while using ultrasonic vibration. They obtained decreased mechanical resistance, much greater extension and increase of drawing surface quality while using ultrasonic vibration.
In 2011, Pazand and Feizi 11 presented application of artificial neural networks in investigations on the effects of ultrasonic vibration on the extrusion process. Siddiq and Sayed 12 in 2012 presented a computational study of ultrasonic-assisted manufacturing processes including sheet metal forming, upsetting and wire drawing. A fully variational porous plasticity model was modified to include ultrasonic softening effects and then utilized to account for instantaneous softening when ultrasonic energy was applied during deformation. Shan et al. 13 in 2012 presented a new mathematical model of the antifriction effect on wire drawing with ultrasonic. The results pointed out that the antifriction effect of ultrasonic had observable effects on the drawing force reduction, while the drawing speed had no visible effect on the drawing force.
In this article, theoretical relations of required axial force in indentation forming process are derived under the condition of longitudinal ultrasonic vibration superimposed on the tool in axial direction. In addition, the ratio of tool speed to vibration speed is investigated. By using these relations, effects of amplitude and frequency of vibration on axial forming force can be extracted. To prove the effects experimentally, longitudinal ultrasonic vibration is applied to the workpiece along the axis, and the results are presented.
Analysis of indentation forming process
The employed model for analysis of indentation forming is shown in Figure 1. Slab method is utilized for analysis of indentation forming without ultrasonic vibration. This method is also known as the free-body equilibrium approach or the force balance method. 14
In this process, the tool geometry consists of two parts, forging and sizing. In the forging zone, the intended internal profile of the tube is formed, and the tube inner diameter, and consequently, its outer diameter, is enlarged. In the sizing zone, both inner and outer diameters of the tube reach their final size and surface finishing is done.
The force balance method is applied on slab elements in forging and sizing zones separately under the following assumptions.
The force required for tube forming without ultrasonic vibration is as follows 14
Longitudinal ultrasonic vibration of the tool in axial direction
In order to study the effects of vibration on the indentation forming process, the tool is axially exposed to ultrasonic longitudinal vibration. The rheological model of material is used for this analysis. This reflects its real elastic, viscous and plastic properties without consideration of hardening. In addition, it aids to describe the forming process while ultrasonic is used. This approach explains the physical mechanisms of ultrasonic effects on the processes of plastic deformation. Moreover, impulsive and continuous regimes of loading have been considered in this analysis. 15 For continuous regime of deformation (Figure 2(a)), the contact between the tool and tube is always preserved and there is no separation, while for impulsive regime, there is a cutoff and separation for a short time (Figure 2(b)). For v¡Ýa¦Ø , continuous regime is present. To make impulsive loading, two conditions of 2a>Ft/Ftk0k0 and v<a¦Ø must be established in a period. Based on theoretical relations, in impulsive loading and at low displacement speeds (v) , the force is applied to the tube at less than half of the period. While in the continuous loading, it is variable and uninterrupted during a period. Dynamic equation (4) explains the relation between the force (F), displacement (u) and tool speed (u?) of the tool. In the equations, parameters, ltube,um,¦¤,k0 and Ft are undeformed length of the tube, longitudinal maximum movement in a period of vibration (total displacement due to continuous forced feed of the tool + vibration amplitude), initial gap distance between the tool and the tube, static stiffness coefficient (a function of ltube ) and conventional tube axial forming force (yield force), respectively. It is noteworthy to mention that ¦¤<0 denotes the initial interference between the tool and the tube.
In equation (5), a,¦Øandv represent the vibration amplitude (m), vibration angular frequency (Hz) and tool constant feed-speed (m/s), respectively. Since the axial force applied by the tool is considered to be impulsive, by simplification and integration of equation (4) and utilizing the impact theory, the required average forming force given to the tool in the presence of axial longitudinal ultrasonic vibration of the tool is obtained as follows.
The required values of parameters in experimental tests have been shown in Table 1. The tube material was DIN CK45 (EN 1.1191), and its properties have been obtained by experiment (see Figure 3 and Table 2). After substituting these values in equations (1) and (6), the required forming force with and without ultrasonic in different vibration amplitudes and speed ratios (v/a¦Ø) can be obtained. The axial force of forming has been shown in Figure 4.
Experimental setup
To verify the theoretical relations with and without employment of ultrasonic vibration, CK45 steel tubes, with the specification given in Table 1, were formed by using tungsten carbide indentation tool shown in Figure 5.
A setup consisting of a forming machine body, a hydraulic jack for tool feed, a tool feed driving rod, fixed and movable constraining holders, ultrasonic system and dynamometer were prepared. Figure 6(a) shows a picture of the whole system assembled and schematic representation of the system.
Machine body was fabricated from a welded structure with some adjustable bolted parts for holding and supporting the dynamometer, ultrasonic system assembly, workpiece tube and tool driving system. The constraining holders were made from thick squared steel blocks to prevent misaligning of the workpiece tube and the tool feed system during forming process. Ultrasonic system consisted of a 2-kW power supply made by MPI of Switzerland, 16 a designed and fabricated 3-kW piezoelectric ultrasonic head for longitudinal vibration (Figure 6(b)) and a titanium matching and titanium connecting part between the ultrasonic head and the workpiece for transferring ultrasonic vibration to the work tube (not to the tool). In addition, the titanium connecting part was fixed on its vibration node to support and bear the axial forming force. The fixing body holding the titanium connecting part was standing on a dynamometer system from Kistler (Type 9255B), 17 which can measure normal compressive forces up to 40 kN. The dynamometer was further supported by an adjustable strong welded square connected to the machine body by bolts.
The dynamometer is connected through interfacing unit to a computer, and the forces exerted on the dynamometer in three normal directions can be monitored and drawn against time by using DynoWare software. The MPI ultrasonic power supply to the transducer is also connected to a computer and is driven and controlled by LabVIEW from National Instrument. The amplitude and power of vibration can be adjusted in the software, and the resonance frequency of the whole system assembly is automatically measured and readjusted every 10 ms by the software/hardware of power supply.
Effect of ultrasonic power given to the workpiece on the forming force
If vibration frequency and tool feed-speed are constant, period of imposed plastic deformation exerted by the tool on the workpiece tube decreases within a period of vibration if vibration amplitude is increased. This is proved in Figure 7 in which three experimental graphs are presented. When no ultrasonic is used, the force curve stands at highest level along the whole length of the tube. At 956 W power of ultrasonic, it stands lower, and at 1705?W, it is the lowest. Decrease of 21% of axial force could be achieved in these tests. Theoretical graphs of 0, 5 (¦Í/a¦Ø=0.018) and 10??m (¦Í/a¦Ø=0.0092) ultrasonic vibration amplitudes, given to the tool, show a good conformity with experimental ones in which vibration is given to the workpiece. This conformity takes place in continuous deformation regimes. Under discontinuous impulsive deformation of theoretical force calculation (amplitude > 27.7??m), there is much further decrease in axial force down to half of the axial force without ultrasonic. Figure 8 presents three graphs of axial forming forces: one without ultrasonic over the whole length of the tube (NU) and two without ultrasonic at about half of the length of the workpiece tube and then switching the ultrasonic on at powers 1264?W (excitation frequency of 25,681?Hz) and 1973?W (excitation frequencies of 24,897?Hz) for the remaining length of the tube. As it is clear, axial forming force starts to decrease at the switching time of ultrasonic. In addition, more decrease is earned at higher power levels.
Effect of ultrasonic vibration of the tube workpiece on the bore dimension
The formed tube has two inner diameters (Figure 9(a)): base and formed circle diameters. The former is approximately equal to the initial inner diameter of the tube, and the latter is close to the tool sizing area diameter. It must be mentioned that before doing the tests, tubes were cleaned by ultrasonic cleaner for 10¨C15 min and then cleaned using alcohol and maintained in silica gel for moisture control. Two methods have been used to measure the tube bore dimensions.
Video measuring machine (VMM) operation is based on identification of dark and light boundaries; therefore, sharper edges of measuring surface improve the measurement accuracy. For using this method, small pieces of sample were cut carefully normal to the axis of the tube, and they were ground finely with burrs removed by emery cloth and then washed and dried before measurement. The pieces were placed on the VMM (KIM-CU Series; ARCS, Taiwan) table, and through adjusting its focal point and lighting, four suitable points on formed circle were selected to determine the diameter.
Percentage of spring back can be calculated from the following relation
Table 3 shows the value of spring back for the formed circle obtained by VMM with and without ultrasonic application. Ultrasonic power for the test was set at 1186 W with frequency of 25,613 Hz at 1260 mm/min tool feed-speed.
in the explained measuring method, human error in selecting the points (dark and light boundaries) is very effective; the calculated percentage value may be affected. To have more secure results, an alternative measuring method was also used as follows.
The coordinate measuring machine (CMM) used in this study (Bridge Type; Leader Metrology Company, China, 2009) 18 has Renishaw PH10T head and Renishaw TP20 probe with 2 mm diameter stylus (Figure 9(c)). The ruby tip of stylus makes contact with the inner surface of the samples prepared from the tube, and by selecting four sample points around the formed circle, the circle diameter was determined. Measurements were performed three times in five sections, and the results given in Table 3 are the average of 15 repetitions with ¡À0.2% of coefficient of variation. The measurement results of this method are also shown in Table 3. It is obvious that there is no significant effect from ultrasonic vibration on the spring back of the tube.
Effect of axial ultrasonic vibration given to the workpiece on the inner surface quality of the formed tube
In order to investigate the effect of ultrasonic vibration on the inner surface quality of the formed zone, two sections of a tube in which ultrasonic vibration was applied for only the second half of its length were cut: one section at the middle of the place without application of ultrasonic and one section at the middle of the place formed under ultrasonic vibration. The pieces separated from the tube were halved along the axis in order to be able to see the deformed bore surface of the tube. Before viewing, the pieces were washed and dried. Figure 10 shows the surface quality of the two sections by using the optical measuring device (VMM). This figure reveals that the presence of ultrasonic vibration improves inner surface quality of the formed tube. In fact, creating a homogeneous surface pattern is one of the advantages of ultrasonic vibration. Although the surface of the tool was fully polished to mirror finish, traces of scratches are evident in Figure 10(a) in the form of vertical lines. Ultrasonic vibration significantly reduces these traces.
Results and discussion
Based on the findings of Figure 4, vibration amplitude and the ratio of tool feed-speed to amplitude of vibration speed have a significant influence on mechanism of process and on the forming force. Theoretical study shows that increase of the latter cause decrease of ultrasonic effect and thereby increasing forming forces. At unity, the effect of ultrasonic vibration is completely eliminated and the forming force will be equal to operation without ultrasonic.
Experimental results with and without axial longitudinal ultrasonic vibration of the tube and theoretical results of applying axial longitudinal ultrasonic vibration to the tool in various amplitudes of vibration (0, 5, 10, 27.8, 40 and 60??m) have been shown in Figure 11. According to the theoretical relations, in the vibro-impact regime, the axial forming force reduction is approximately 50% (5602 N in the presence of ultrasonic vibration of 60??m amplitude). In continuous regime of 10??m amplitude, the theoretical reduction is about 18% (9589?N). In experimental tests, maximum axial forming force reduction is about 21% (9249?N) when ultrasonic vibration of 1705?W power at frequency of 24,748?Hz and tool feed-speed of 840?mm/min is used. Therefore, comparing the experimental and theoretical results, it shows that the loading regime is continuous in experimental tests, that is, during the ultrasonic vibration, tool and tube are in continuous contact in the whole vibration period and no separation occurs.
It should be mentioned that during experimental tests, the longitudinal vibration has been applied to the workpiece, and the tool was forced to go ahead by a separated rod (no pulling was possible by the rod). In practice, due to limitations of tool feed-speed in the present experimental equipment, it was not possible to investigate the effect of the speed ratio (tool feed-speed over vibration speed amplitude) on the forming force. The amount of experimental axial forming force reduction in the presence of ultrasonic vibration was determined about 2¨C3?kN (15%¨C21%). It is expected that there was no separation between the tool and the workpiece and forming was of continuous type, these effects can be attributed to two dominant phenomena.
Nature of indentation forming process when the workpiece tube is longitudinally vibrated
In present experimental studies, feed force was transferred to the tool by a disconnected feeding rod pushing the tool only in one direction (ahead) inside the workpiece tube along the whole length of the tube.
By considering v<a¦Ø in our experimental condition, two forming situations could exist. The first, if the workpiece spring back was higher than twice the amplitude of vibration (2a<Ft/k0) , then the contact and the pressure force in the forging zone between the tool and the workpiece would not be interrupted and thereby the contact between the feed-rod and the tool was not interrupted either, whether the vibration given to the tool or given to the workpiece. In this case, only a little back move from the tool happens in response to elastic spring back of the workpiece.
The second, if the workpiece spring back was lower than twice the amplitude of longitudinal vibration given to the workpiece (2a>Ft/k0) , then although the tool was not separated from the workpiece (because of the lack of solid permanent connection of the tool and feed-rod), the rod could probably be separated from the tool in a half of a period and the rod tip could hammer and strike on the back of the tool in every vibration period.
It must be noted again that in our real case, the longitudinal vibration was given to the workpiece and the tool was driven forward by a disconnected rod, which was touching the back of the tool and pushing the tool ahead. Under this condition, even at low spring backs of the workpiece, the tool does not separate from the workpiece under any circumstances, but the hammering effect of the feeding rod still can occur if the condition of (2a>Ft/k0) is reached.
In our experiments, comparison of theoretical and experimental results (Figures 4 and 7) shows that the condition of 2a<Ft/k0 exists and no separation between the tool and the workpiece tube and the tool and the pushing rod occurs.
Effects of frictional forces, acoustic softening, local temperature rise at discontinuities and material structural changes
Unfortunately, the force monitoring system (Kistler, Type 9255B) has a maximum response speed of 1?kHz, and it is not able to show the force variation within a cycle as the variation frequency is much higher (about 25?kHz). Therefore, it has an averaging effect on the force. Otherwise, the said mechanisms in part 1 could be analyzed experimentally.
Conclusion
In this article, theory of ultrasonic-assisted indentation forming process (axial longitudinal vibration given to the tool) has been studied, and the corresponding analytical relations have been derived. In addition, for comparison purposes, experimental tests have been performed under axial longitudinal ultrasonic vibration given to the workpiece. According to the theoretical and experimental findings, the following conclusions can be made.
1-饶勇超
1-The high-throughput highway to computational materials design

ABSTRACT
High-throughput computational materials design is an emerging area of materials science. By combining advanced thermodynamic and electronic-structure methods with intelligent data mining and database construction, and exploiting the power of current supercomputer architectures, scientists generate, manage and analyse enormous data repositories for the discovery of novel materials. In this Review we provide a current snapshot of this rapidly evolving field, and highlight the challenges and opportunities that lie ahead.

INTRODUCTION
Every technology is intimately related to a particular materials set. The steam engines that powered the industrial revolution in the eighteenth century were made of steel and, information and communication technologies are underpinned by silicon. Once a material is chosen for a given technology, it gets locked with it because of the investments associated with establishing large-scale production lines. This means that changing the materials set in an established technology is a rare event and must be considered as a revolution. Moreover, the initial choice of a material is absolutely crucial for the long-lasting success of a technological sector. Importantly, recent times have seen a surge of new technological niches, each one of them potentially looking for a different materials set. Thus, the pressure on the development of new materials is becoming formidable. These should score on many counts. They should be tailored on the specific property that the technology is based on, they often should be compatible with other technologies, should not contain toxic elements, and, if needed in large quantities, should be made of cheap raw materials. As such, searching for materials is a multi-dimensional problem where many boxes should be ticked at the same time.
Although the demand for materials is endlessly growing, experimental discovery is bound by high costs and time-consuming procedures of synthesis. Is there another way? Indeed, this is the burgeoning area of computational materials science called 'high-throughput' (HT) computational materials design. It is based on the marriage between computational quantum-mechanical–thermodynamic approaches and a multitude of techniques rooted in database construction and intelligent data mining. The concept is simple yet powerful: create a large database containing the calculated thermodynamic and electronic properties of existing and hypothetical materials, and then intelligently interrogate the database in the search of materials with the desired properties. Clearly, the entire construct should be validated by reality, namely the existing materials must be predicted correctly and the hypothetical ones should eventually be made. Such a reality check feeds back to the theory to construct better databases and increase predictive power.
The HT experimental approach was pioneered over a hundred years ago by Edison and Ciamician, but with the advent of efficient and accurate theoretical tools and inexpensive computers, its computational counterpart has become a viable path for tackling materials design. Thus, in the past decade computational HT materials research has emerged following the impetus of experimental HT approaches In the literature, HT materials research is often confused with the combinatorial evaluation of materials properties. Although a few attempts have been made to clearly define the two concepts, the distinction is not yet rigorous. Here we define HT as the throughput of data that is way too high to be produced or analysed by the researcher's direct intervention, and must therefore be performed automatically: HT implies an automatic flow from ideas to results. The confusion of HT with combinatorial approaches is thus resolved. The latter, in fact, specifies how the degrees of freedom are investigated, whereas HT strictly defines the overwhelming and automatic flow of the investigations.
The practical implementation of computational HT is highly non-trivial. The method is employed in three strictly connected steps: (i) virtual materials growth: thermodynamic and electronic structure calculations of materials; (ii) rational materials storage: systematic storage of the information in database repositories; (iii) materials characterization and selection: data analysis aimed at selecting novel materials or gaining new physical insights.
High-throughput is often known for the large databases it generates (for example, the AFLOWLIB.org consortium and the Materials Project. Here we posit that all three HT stages are highly necessary, but that the last one is the most challenging and important. In fact, it is the step that allows one to extract the information and, as such, it requires a deep understanding of the physical problem at hand. The intelligent search of a database is performed by means of 'descriptors'. These are empirical quantities, not necessarily observables, connecting the calculated microscopic parameters (for example, formation and defect energies, atomic environments, band structure, density of states or magnetic moments) to macroscopic properties of the materials (for example, mobility, susceptibility or critical temperatures). In other words, the descriptor is the language with which the researcher speaks to the database, and thus the heart of any effective HT implementation. In Table 1 we illustrate examples of 
recently introduced descriptors.
Once a good descriptor is identified, the search for better materials within the repository can be performed intrinsically or extrinsically, depending on whether the optimum solutions are already included in the set of calculations or not. Intrinsic searches include just step (iii), require only fast descriptors, and may employ various informatics techniques. Examples of previous such searches include the scanning of better cathode materials, and the uncovering of unknown compounds, novel topological insulators16 or thermoelectric materials. Extrinsic searches involve all three steps, because the search for an optimal solution includes iterations leading to an expansion of the repository.
An important component of extrinsic HT computational research is a scheme capable of using the evaluation of descriptors on existing database entries to guide new calculations not yet included in the database. Examples of such schemes published in the literature comprise evolutionary and genetic algorithms7,8, data mining of spectral decompositions3 and Bayesian probabilities10, refinement and optimization by cluster expansion and structure map analysis. Neural networks35,36 and support vector machines have also been utilized in a few cases. These methods may sometimes be used to bypass step (iii) of the HT analysis, that is, the formulation of a physically meaningful descriptor, so that a search can still be implemented even with only a superficial understanding of the physical problem.

RESULT
Areas of current application
Following the general framework outlined above, we describe in this section a few specific examples of computational HT studies reported in the literature, ordered by increasing degree of complexity.
Thermodynamics for the identification of binary and ternary compounds. The identification of stable structures is the first step in the design of materials with various specific functionalities. The proper descriptor of alloy stability, the formation enthalpy, is the simplest example of a parameter used for HT materials development.
Alloys are the workhorse material of many important technological applications. Thus, finding new and improved alloys could be transformative in some areas and would have a substantial economic impact. When improving an existing alloy or designing a new one, scientists rely on databases of alloy thermodynamics and phase diagrams (for example, the Massalski's Binary Alloy Phase Diagrams38 and the Villars's et al. Pauling File). Although the utility of these repositories is tremendous, they could be of even greater use if they were more complete. Experimental completeness is difficult to achieve due to the vast combination space and because experimentation is often difficult: it requires high temperatures or pressures, very long equilibration processes, or may involve hazardous, highly reactive, poisonous or radioactive materials. Computational compilation of the properties of materials is more feasible and will lead to much more complete repositories. Examples that demonstrate this are the almost simultaneos prediction and experimental verification of the previously unknown C11b structure of the Pd2Ti compound9,40, the verification by Niu et al. of an earlier prediction42 that the CrB4 compound, thought for 40 years to have an oI10 structure, is actually more stable in an oP10 structure, and the simultaneous synthesis and solution, by an ab initio evolutionary search, of an unexpectedly complex tI56 crystal structure of CaB6 .
In alloy design, the targets of the formation enthalpy descriptor are stable phases. The HT ab initio method explores the phase stability landscape of alloys by calculating the descriptor for a large number of possible structures. An HT code must perform these calculations automatically, transform the structures into standard forms that are the easiest to calculate, and automatically set the necessary k-point grid densities, basis-set energy cutoffs and relaxation cycles with a convergence tolerance of the order of a few meV per atom. It should also respond automatically to calculation failures, due to insufficient hardware resources or runtime errors of the ab initio calculation itself. These are among the most difficult challenges in HT database generation that have only recently been overcome (ref. 44 gives details about how this automatic data generation is implemented in the AFLOW HT framework). The initial search is performed on a set of known crystal structures, of all lattice types, spanning the entire composition range of the investigated systems3,9. In advanced HT studies this set includes hundreds of structures per system44. In subsequent steps, the search is often aided by data-mining and optimization techniques that refine and accelerate the structure screening. They include a variety of different approaches: for example, cluster expansion with exhaustive evaluation or genetic search algorithms on fixed-lattice systems46,47,48, and evolutionary algorithms for off-lattice structures in mixtures with fixed stoichiometries. These screening and optimization techniques are continuously being improved and adapted for implementation in HT frameworks. The search concludes with the automatic construction of the Gibbs free-energy curve for each system from the minimum-energy structures at various component concentrations.
As of this writing, the largest computational alloy database, the Binary Alloy Project hosted in the AFLOWLIB.org consortium repository24, contains the formation enthalpies for hundreds of thousands of intermetallic structures comprising all the transition metal systems and many other intermetallics. The same framework is also being used to generate similar data for ternary alloys. This information overlaps much of the experimental phase-diagram databases and complements them where the data is partial or missing.
By using HT, massive analyses become possible. Figure 1 illustrates the capacity of HT in dealing with all of the 435 d-electron binary intermetallics (elements ordered by Pettifor's scale54). The top left triangle in Fig. 1 shows the ordering tendency of the mixtures, defined as the maximum temperature at which the entropic term of an ideal solid solution is equal to the formation energy of the mixture, calculated ab initio. This is a measure of the strength of a mixture to oppose disorder. Curtarolo et al.24 demonstrated that HT is capable of reproducing the existence of stable ordered structures, or lack thereof, in 80% of the comparisons: in HT, 65% of the existing binaries are found to be compound forming, whereas in only 58% of the systems compounds have been experimentally reported. The bottom right triangle shows this comparison. In cases of agreement between experiments and calculations on the existence of compounds, HT has been found to reproduce the experimental structure ∼96.7% of the time (equation). The database has been used to extensively study several alloy classes, for example, platinum group metals, Mg- and Hf-alloys and to pinpoint particular missing features of well-known systems (for example, kinetic acceleration in Fe–V alloys59). The availability of low-temperature HT data opens new avenues for high-temperature Monte Carlo simulations, and ultimately for the automatic determination of phase diagrams.
Solar materials. Photovoltaic (PV) cells are specialized semiconductor diodes that convert light into direct electric current. Typically, they consist of a transparent conductive oxide layer (such as In-doped SnO2), an anti-reflection coating (such as Si3N4), a p-type doped crystalline Si semiconductor layer with some n-type dopants diffused at the top, and a metallic electrode layer (typically Al), each layer stacked on top of each other. Although such monocrystalline silicon solar cells can convert a useful amount of terrestrial solar energy into electrical energy60, their construction requires energy-intensive manufacturing at high temperatures (400–1,400 °C) as well as numerous lithographic processes to employ light-trapping techniques. Hence, these conventional cells are too expensive to replace non-renewable energy sources. Indeed, since the introduction of Si solar cells in the 1950s, the search for alternative light-absorbing materials has been an active area of research.
Despite the importance of PV materials, the commonly used materials, such as Si, GaAs and CuInSe2, have been discovered accidentally, and been incrementally improved over the years. A systematic analysis of the over 150,000 entries of the inorganic crystal structure database (ICSD) could provide, given the appropriate search parameters, previously undiscovered materials with the appropriate characteristics: semiconductors with strong optical absorption coefficients, a bandgap of ∼1.3 eV (the Shockley–Queisser criterion), low cost and, ideally, compatibility with existing technologies. Only recently, the in silico screening of PV materials has been attempted with HT techniques: the real difficulty is the characterization of the proper descriptor for the identification of the candidate materials. Yu and Zunger introduced the concept of 'spectroscopic limited maximum efficiency (SLME)', a descriptor combining bandgap, shape of absorption spectra and material-dependent non-radiative recombination losses (all intrinsic materials properties) that were used to tackle the ICSD database63 (Fig. 2). With their procedure, they were able to identify a set of high SLME materials, including the best already known thin-film solar absorbers, such as CuInSe2, CuGaSe2 and CuInS2, and others that have been found experimentally to be feasible solar absorbers but are much less studied. Interestingly, they were able to identify high SLME materials away from a 1:1:2 stoichiometry (for example, Cu7TlS4, Cu3TlS2 and Cu3TlSe2). Although Tl-containing materials might be unfavourable in practice because of the high toxicity of Tl in the +1 oxidation state, these results suggest that replacing Tl with non-toxic elements could be a viable route to novel high SLME materials.
Water photosplitting. Castelli et al. screened a large class of oxide and oxynitride materials (5,400 semiconducting compounds in the cubic perovskite structure covering 52 metals) for optimal solar-light capture in photoelectrochemical cells. Their HT approach, based on the screening of candidate materials by looking at criteria for stability and for the size and position of the bandgap, identified ten oxides and five oxynitrides that are well known in the water-splitting community, and predicted nine new combinations for further experimental investigation. In this study, the descriptor was a combination of materials properties (the gene), which include (i) an appropriate bandgap; (ii) well-positioned band edges relative to the water redox levels; (iii) high mobilities, allowing electrons and holes to reach the surface and reduce or oxidize the targets before recombining; and (iv) chemical and structural stability under irradiation. Two of these descriptors (i and iv) are displayed together in Fig. 3, highlighting the process of materials gene construction.
Carbon capture and gas storage. Lin et al. recently proposed another application of HT to energy and environment: the screening of materials for large-scale carbon dioxide capture and sequestration (CCS) in power plants, where carbon dioxide should be captured at its source for subsequent storage in non-atmospheric reservoirs. Capture materials and processes that reduce the parasitic energy imposed by CCS are of extreme interest not only for their industrial impact but also for long-term management of climate change. A complex process such as CCS requires the introduction of composite descriptors that must go beyond single material properties. In their study, Lin et al. introduced a complex metric, the 'parasitic energy', to identify the optimal process conditions for each material. This descriptor is predicted by the minimization of the electric load imposed on a power plant by temperature–pressure swing capture processes. Previous investigations were limited to only a handful of materials and/or single property descriptors. Lin et al. were able to screen hundreds of thousands of zeolite and zeolitic imidazolate framework structures, and identify many different structures that have the potential to reduce the parasitic energy of CCS by 30 to 40% compared with near-term technologies.
Another interesting example is the work by Wilmer et al. on the screening of metal–organic frameworks (MOFs) for natural gas storage. In this study, the methane-storage capacity of 137,953 hypothetical MOFs was calculated, and over 300 systems were identified to have a potentially better capacity than any known material. One of such predictions, a methyl-functionalized MOF, was also experimentally verified69. In another study, Alapati et al. scanned over 100 dehydrogenation reactions on experimentally known compounds, and uncovered several new reactions for potential hydrogen-storage materials. Experimental confirmation of the predictions for the LiNH2:MgH2 system yielded an 8 wt% capacity and an enthalpy of reaction within about 10% of the predicted value. This particular system had been experimentally studied previously for a different ratio of constituents. The new promising composition was suggested by the large-scale computational scan.
Nuclear detection and scintillators. The design of new scintillator materials for γ-ray nuclear detection has recently attracted much interest due to the potential industrial and security applications. Large-scale experimental studies are already feasible72 and accentuate the necessity of guidance by HT computations.
Computational predictions were first proposed by Ortiz et al., using a highly accurate version of the full potential linear muffin-tin orbital method. They parameterized ∼22,000 compounds from the ICSD database63, and implemented a data-mining reduction, based on electronic structure considerations, to extract 136 potential novel scintillators. Using HT methods, Setyawan et al. later proposed a solution to the 'non-proportionality' puzzle, a long-standing problem in the field of scintillator materials. In ref, they introduced a 'non-proportionality descriptor', the mismatch between effective masses of the carriers, me (mh), near the bottom (top) of the conduction (valence) bands,
which correlates with the non-proportionality response for oxides and semiconductors (non-proportionality defined as NP ≡ Y10keV/Y662keV where Yi denotes the photon light yield resulting from γ radiation with energy E in keV). The explanation of the correlation  NP led to new scientific understanding of the role of the electron–hole mismatch, free or coupled as an exciton, in the spatial distribution around — or in the migration to — a recombination centre (usually a defect or a dopant). The correlation was experimentally verified with the application of hydrostatic pressure, which modifies the effective masses through the induced strain in the crystalline cell, and increases or decreases the non-proportionality depending on the electro–elastic response of the material. In ref. , the descriptor was further optimized by integrating it with theoretical light yield and photo-attenuation length, so that an extensive dataset of compounds could be analysed.
Topological insulators. Quantities directly accessible from the band structure (energy gaps, effective masses and so on) may be used as descriptors or combined in composite descriptors, such as those recently developed to search for novel topological insulator77 materials16. In a first study, Lin et al. searched for ternary thermoelectric Heusler compounds and demonstrated that although most of the well-known ones, such as TiNiSn and LuNiBi, are topologically trivial, the distorted LnPtSb-type compounds (such as LnPtBi or LnPdBi, Ln = fn lanthanides), belonging to the half-Heusler subclass, are topologically non-trivial and could provide a platform for the realization of multifunctional topological devices.
In a more recent study, Yang et al. searched the quantum materials repository AFLOWLIB.org and automatically discovered 28 topological insulators (some of them already known) in five different symmetry families by defining a variational descriptor (Table 1) that represents the topological robustness or feasibility of the candidate system. The newly discovered materials included peculiar ternary halides, Cs(Sn,Pb,Ge)(Cl,Br,I)3, which could hardly have been anticipated without HT means. The robustness descriptor combines information on the energy versus strain variations of band structures with and without spin–orbit coupling. This is an example where the definition of the descriptor combines advanced electronic-structure data with geometrical modifications to obtain the essential gene that is associated with the required property (Table 1).
Piezoelectrics. High-throughput techniques have also been applied to the search of materials with large piezoelectric coefficients, a class of systems with many technological applications. In particular, the discovery of the anomalously large piezoelectric effect in lead zirconate titanate suggests that other materials of the same family could display similar properties. The large piezoelectric response observed in the solid solution between lead titanate and lead zirconate relies on the formation of a morphotropic phase boundary (MPB) between tetragonal and rhombohedral distortions of the perovskite structure. Calculations of the relative stability of those phases is crucial for predicting the formation of the MPB and the potential of materials to exhibit large electromechanical couplings. In a systematic search across all the possible 3,969 ABO3 compositions, Armiento et al.79 developed simple descriptors (and other criteria) to screen viable phase-diagram end-points (Table 1). These descriptors and criteria were used to extract a set of 49 compositions that can be seen as the fundamental building blocks of isovalent alloys for compounds forming MPBs, suitable for high piezoelectric performance. Among those, they identified three primary composition groups, (Sn,Pb)(Zr,Hf,Ti)O3, (Ba,Sr,Ca)(Zr,Hf,Ti)O3 and (Li,Na,K,Rb,Cs)(Ta,Nb)O3, which coincide with three known materials with MPBs.
More recently, Roy et al. screened the ICSD for half-Heusler semiconductor compounds, looking for previously unrecognized piezoelectric systems. In their search, they scanned 987 candidate combinations for insulating character and structural, dielectric and piezoelectric properties, and identified a number of promising systems.
Thermoelectric materials. The efficiency of direct thermoelectric energy conversion devices is directly related to the dimensionless thermoelectric figures of merit (ZT) of their p- and n-type components. ZT is defined as TσS2/κ, where σ is the electric conductivity, S is the Seebeck coefficient, κ is the zero current thermal conductivity and T is the temperature81,82. The ZT depends on temperature and on doping. Thus, for each operation temperature there is a different set of compounds that display the best ZT, once their doping level has been optimized. In this context ZT would be an ideal candidate for an HT descriptor.
An early example of HT was provided by Madsen83, who reported an automated search for new thermoelectric materials among 570 Sb-containing compounds in the ICSD database63. The study suggested the Zintl compound LiZnSb as a potentially interesting n-type thermoelectric. Subsequent experiments84 found good agreement between the ab initio calculated transport properties for the p-type material and the measured ones, and concluded that p-type LiZnSb is not a good thermoelectric. However, it was not possible to assess the n-type material experimentally. Using a comparable approach, Yang et al.85 screened the thermoelectric properties of 36 half-Heusler compounds.
In addition to HT calculations of bulk crystalline compounds, an immense unexplored territory lies in the field of nanostructured materials. A potential area of development for thermoelectric systems is the case of nano-granular materials. In a recent study, Wang et al.15 compared the thermoelectric power factors (σS2) of more than 3,000 compounds in the ICSD database. Figure 4 indicates the compounds that are ranked as worthy of further study. Some potentially good solutions captured by HT would be missed by traditional approaches that typically explore compounds similar to known good ones. In addition to predicting new materials, the HT analysis also pinpoints interesting phenomena. Using principal component analysis on the obtained data, the authors established correlations between the power factor and other structural or electronic properties of the compounds. In particular, it was found that, at a given grain size, higher power factors are more likely to occur in sintered compounds with large charge 
carrier effective masses and bandgaps, and with a large number of inequivalent atoms in the unit cell. The first two rules were already known from previous models86, but the HT analysis was able to obtain this trend through an automated procedure. The third finding, relating power factor and unit-cell size, is new for electronic properties. A similar correlation has been known for thermal conductivity, stating that larger unit cells are associated with lower thermal conductivities. Together, these rules for electronic and thermal transport indicate that higher ZTs are to be expected in materials with complex unit cells.
Materials for catalysis. The search for new solid catalysts is probably the most complex problem tackled ab initio87,88. The complexity stems from the numerous microscopic variables affecting catalysis, such as reactants' energies and transition states on the surface through all the steps of the catalytic process. Although there are several examples of catalytic processes with a complete ab initio characterization89,90,91,92,93,94, an HT screening of a large number of processes in this manner is at present far beyond current capabilities. A significant advance has been made in medium-throughput computational screening for catalysts with increased activity and improved selectivity, representing the foundation on which a future fully HT approach could be developed.
As in the previous examples of HT screening for new materials, the search for improved catalysts relies on simple descriptors. However, because the catalysis problem is very complex, these descriptors are reaction specific and their identification requires preliminary analysis of the reaction steps. This is in contrast to the previous examples where a general descriptor of the sought-for functionality is heuristically deduced from the underlying physics and directly employed to screen for candidate materials.
Computational design studies of catalysts carried out in recent years reflect this complexity and include a few principal steps. First, the elementary steps of the reaction, including all the intermediate chemical species, are identified. The energetics of each step, including dissociation and adsorption energies and reaction barriers are evaluated for a number of transition metal surfaces by density functional theory (DFT) calculations, and the active sites, usually a kink or step on the surface at which the activation energies are lowest, are also determined for the specific reaction. Next, correlations are identified between the adsorption energies of the different adsorbates and intermediate species (scaling relations100), and between these adsorption energies and their associated transition-state activation energies (Brønsted–Evans–Polanyi relations87,88,101). The determination of these relations is of central importance for computational catalyst design, as it allows one to model the catalytic reaction in terms of the minimal number of independent chemical parameters, the descriptors of the process. These descriptors are usually the adsorption energies of the main components of the reactants, which are much easier to evaluate by DFT calculations than activation energies. They can therefore be implemented as practical descriptors of catalytic processes involving dissociation of simple molecules on a metal surface102. It is important to note that each active surface site defines a different set of these relations and therefore it is crucial to identify those that lead to the highest reaction rate and the best catalyst.
Once the appropriate descriptor or descriptors have been identified, they are related to measured or calculated total reaction rates for the various metals87,96, leading to volcano-shaped plots where the optimal catalyst can be identified as the one lying closest to the top of the volcano. These volcano plots emerge due to a fundamental concept in catalysis, the Sabatier principle, stating that for an efficient catalysis the interaction between the surface and reactant should not be too strong nor too weak103. When interactions are too weak, reactants will not bind to the catalyst and reaction will not take place. In contrast, if the binding is too strong the catalyst will get blocked, poisoned by reactants or intermediates, or the products will fail to dissociate. Using the volcano plot, the optimal catalyst may be found from a much larger set of candidates than the transition metals used to construct it, because for these candidates the evaluation requires the calculation of only a small number of 
descripters. Additional considerations, such as the estimated structural stability and selectivity at the appropriate chemical environment, and cost of the candidates, can be included in the search87,88,96.
Several descriptor-based searches for new heterogeneous catalysts have been reported in recent years, each screening a few dozen metal surfaces and surface alloys for various chemical reactions. Figure 5 shows the results of one of these studies (Greeley et al.96): the volcano plot (Fig. 5a) and the optimum active surface alloys for a hydrogen evolution reaction (Fig. 5b). Furthermore, among the predictions made by Studt et al.107, NiZn has been experimentally verified to have better selectivity than the traditional Ag–Pt catalyst at a fraction of the cost, because it contains no precious metals.
Battery materials for energy storage. Efficient energy storage, with high capacity and long cycling lifetime, is one of the main issues in the development of sustainable clean-energy technologies. Lithium-ion batteries are the state of the art in this field. They operate by the transfer of lithium ions from a high chemical potential anode to a low chemical potential cathode through an ion-conducting electron-insulating electrolyte while the electrons flow through an external circuit. Recharging is performed by applying an external potential, forcing the Li ions to migrate back from the cathode to the anode. Almost all anodes currently in use are based on graphite, due to its Li storage capacity, and cycling and safety characteristics109. The electrolytes are solutions of Li salts in organic solvents. Several recent attempts have been made to enhance the anode capacity without degrading cycling and safety, using MoSi2-based compounds. However, the main route taken by current research to improve Li-ion 
batteries is to seek new cathode materials with superior properties. Finding new battery components experimentally might be impractical due to the very large chemical space and the difficulty of the experiments. High-throughput computational research can make a difference, but as pointed out by Ceder et al., a few physical parameters have to be taken into account in addition to structure stability. The search for better materials is a complicated multistep process, with constraints in terms of safety, toxicity, weight, capacity, charge and discharge rates, recyclability and cost28,114.
The search consists of three steps. First, it starts with a set of candidate chemistries, namely, chemical compositions containing high concentrations of Li ions, a redox active metal, and oxygen or oxide ions (carbonate, borate, phosphate, silicate or arsenate) that could stabilize a rechargeable Li-containing compound. The chemistry defines the electron activity that can be achieved during the delithiation process and essentially determines the capacity of the battery. Computational screening of the multi-component system is then carried out to identify stable crystallographic structures. This is performed by ab initio calculations of the total energies of a large number of structures, chosen with ad hoc algorithms10,115 to parameterize the thermodynamic stability of the systems. This step has been fully implemented in an HT framework.
Second, to achieve good cycling behaviour the structures should facilitate intercalation of Li ions, for example, reversible host–guest chemistry where the ions migrate through layers or tunnels in the host structure with minimal changes in the host itself113. The energy density of a battery is determined by the capacity — defined by the chemical process — and the voltage profile, which can be related to the Gibbs free-energy difference between different charged states of the cathode material. Good candidates should exhibit the highest possible voltage, compatible with the limitations of electrolyte stability, usually less than 5 V. Ceder shows an example of the relationship between chemical potential and material resilience to a reducing environment for several classes of compounds. Considerable advances have been made towards a full HT implementation of this step, but it has not been achieved yet. Standard DFT exchange-correlation functionals — for example, the local density approximation (LDA) and the 
various forms of generalized gradient approximations (GGA) — may lead to large errors in voltage predictions in redox processes that involve electron transfer between different orbitals, and more advanced and precise hybrid functionals are currently too computationally expensive for HT116. The alternative, and less expensive, DFT+U method is usually implemented. So far, the most extensive screening of cathode materials through voltage prediction has been carried out by Hautier et al.120 for over 600 phosphate compounds selected out of more than 4,000 structure total-energy calculations. Smaller studies were done on polyanion compounds, with just over 200 different compositions of the sidorenkite structure121, and on 64 tavorite structured materials.
Third, following the screening for voltage and cycling potential, the remaining candidates are evaluated for charge and discharge rates. As illustrated by Ceder et al.27, the rate-limiting processes, which are the ionic and electronic conductivities, are evaluated through ab initio calculations of the diffusion barriers for ion migration and of the availability of charge carriers and their mobility in the polaronic intercalation compounds of the electrodes (the most promising new cathode materials are electrical insulators such as borates, phosphates and silicates). This step of evaluation, the most computationally expensive, is carried out at the end of the screening process and it has not yet been implemented in HT. Indeed, Mueller et al.122 calculated lithium-diffusion activation energies for just three structures out of the 64 tavorite compounds proposed, and no lithium-diffusion or polaron-migration calculations were reported in the larger studies120,121. A future implementation of an HT framework for this step would require a definition of an efficient descriptor for these transport properties, which is currently non-existent.
Outlook

Outlook
The computational HT approach, although at a good level of maturity, is still far from being a magic box for materials discovery. Several important properties and classes of materials have not been addressed in this framework, and further algorithm implementations, repositories and data-mining interfaces, are necessary. Here we give important examples of materials and property challenges to be addressed, and an overview of the possible evolution of the framework.
Stability at finite temperatures. Ab initio calculations of metallic systems often predict ordered compounds that may be difficult to fabricate due to entropy and kinetic effects. The assessment of the thermal stability of structures and the practicality of their realization requires accurate estimation of the various entropic contributions (for instance, configurational, vibrational and magnetic) of the competing phases predicted by HT calculations. For specific sets of structures sharing a common parent lattice, this can be accomplished by high-temperature Monte Carlo simulations. However, a general HT ab initio framework for this problem is still lacking.
Thermoelectrics. Design of better thermoelectrics requires a better estimation of the carrier lifetimes. The standard approximation of a constant relaxation time is not satisfactory. However, recent work has shown that it is possible to predict carrier lifetimes and electronic mobility with good accuracy, fully from first principles. Also, the calculation of the lattice thermal conductivity can now be accurately calculated ab initio. The definition of a simpler descriptor that can speed up massive HT computation remains a considerable challenge. For bulk systems, a possible approach is to relate the lattice thermal conductivity to the Debye temperature and Grüneisen parameter that can be estimated by the 'GIBBS' isothermal–isobaric approach132. For nanostructured materials, an additional challenge is the evaluation of conductivities at interfaces, where the transport problem couples with the difficulty of predicting nanoscale geometries. Here the solution lies in a 
quantum-transport treatment that has only recently been considered for HT applications.
Magnetic materials. Although magnetism can be found in a multitude of materials (several thousands), the choice of magnets available for mainstream applications is much more limited (around two dozen)134. There are two main reasons for such limited diversity. First, any standard application, regardless of the particular technology it concerns, needs to operate in the temperature range between −50 °C and +120 °C, which requires the magnet to have a critical ordering temperature, TC, of at least 550 K. Unfortunately, there are only a few hundred magnets with such a high critical temperature. The second reason is that magnetic materials need to satisfy additional physical constraints for each specific application. Thus, for instance, energy-related technologies (electric turbines, electric motors and so on) require large magnetic energy densities, whereas magnetic sensors often need sensitive magnetic–electric responses.
Only by exploiting HT techniques can one explore the possibility of synthesizing new high-performance magnetic materials, and search into large materials classes that are known to be populated by high-temperature magnets. Particular classes of interest are the intermetallic ternary materials, such as the Heusler compounds. A simple combinatorial calculation gives an upper limit for the number of possible Heusler compounds (including half-Heusler) of about 230,000. Among these, about 1,500 are known and have been synthesized in the past. However, there is still a significant number for which a synthetic strategy has not been designed. Particularly important would be the development of permanent magnets without critical elements (so as to counteract the current 'rare-earth crisis'), or of magnets whose properties are specifically targeted to electronics applications, such as magnetic random access memories. Finally, HT technologies can be used to design entire magnetic heterostructures, and the case of tunnel
magnetoresistance devices seem particularly attractive.
Heusler alloys. This Review includes a few references to research on Heusler alloys. In addition to the reviewed topics (thermoelectricity, topological insulators, piezoelectricity), such systems have drawn general attention in the computational materials community: chemical stability was investigated by the Zunger group, and bandgap and lattice constants were computed, for optoelectronic applications, by Gruhn. A repository of Heusler alloys' calculations would thus be useful for scientific and industrial applications. This is an undergoing task of the AFLOWLIB.org consortium. By combining the wealth of binary intermetallics and the parameterization of all the possible Heusler alloy combinations (full-[AlCu2Mn], half-[AgAsMg] and anti-[CuLi2Sn]), one can rapidly determine the thermodynamic stability and the appropriate electronic-structure features.
Alloy theory at the nanoscale. The extension of the HT framework to predict alloy stability at the nanoscale has great technological implications, especially for catalysis. Many phenomena are chemically dependent on the stable surface of the catalyst, and the proper parameterization of the surface stability (energy) and surface tension (stress) will greatly help the development of new catalysts (for example, the size-dependent phase transitions in Fe–C mixtures have been shown to be responsible for deactivation in nanotube growth).
Catalysis. Considerable improvements are necessary to advance to a more comprehensive HT framework for computational catalyst design. One direction is related to the current limitations of DFT calculations in treating non-metallic surfaces, electronic bandgaps and excited electronic states, and the chemistry of atoms and molecules on such surfaces in various environments143. Improvements in this field would be needed to extend the current framework from transition metals to other useful materials families, such as oxides, sulphides, nitrides and zeolites. Furthermore, for the development of catalysts with long lifetimes, the thermodynamics of the catalyst–product mixture must be elucidated, especially at the nanoscale where the quest for more active surface dictates size reduction and, as a consequence, possible size-induced thermokinetic deactivation.
On the HT conceptual level, developing a systematic methodology to determine appropriate descriptors for an as extensive as possible variety of catalytic properties would be of crucial importance. It is not yet apparent how this challenge could be met, but it is clear that it is the key for the implementation of truly HT computational catalyst discovery and design, concomitant with the necessary experimental validation.
Battery technologies. Additional advancements would be needed to expand energy-storage materials research beyond the current scope of Li-ion batteries. For example, a variety of conversion electrodes, where transition metal binary phases react with lithium, potentially possess much higher energy density than intercalation devices. The wealth of such compounds with different degrees of covalence and transition metal oxidation states could enable the tuning of operation voltages, for positive or negative electrodes, and the possibility of selecting low-cost and environmentally friendly materials145. Likewise, batteries employing a higher valence cation such as magnesium or aluminum, which could have considerably increased capacity with reduced weight and volume109,146, have not yet been considered in computational studies.
Algorithms and repositories. To be effective, the wealth of calculations produced by HT needs to be open-domain, shared in online repositories and equipped with effective search capabilities. Examples are the AFLOWLIB.org consortium, the Materials Project25, the Computational Materials Repository, The Electronic Structure Project74, and the Carnegie Mellon's Alloy Database148. For efficiency, the repositories should be integrated so that they can share information through standardized calculation and communication protocols. This will bring the HT field beyond the monolithic, undistributed approach of each single research group, and allow a better use of the results generated by the entire community. Furthermore, another computational frontier will be the implementation of 'materials daemons', ad hoc artificial-intelligence codes implementing the descriptors and autonomously crawling across the various linked databases, scanning and directing calculations until optimum materials are found.

Summary
In this Review, we described the milestones that have been reached with HT computational materials research. We proposed a comprehensive framework, in which they can be conceptually classified, to address the increasing demands of modern technology. For the many other achievements winking at the horizon, crucial components still need to be put in place: efficient HT codes, open and distributed networks of repositories, fast and effective descriptors, and strategies to transfer knowledge to practical implementations. This is an adventurous journey where players will race on a highway without speed limits.

2-Machine learning in materials informatics: recent applications and prospects

ABSTRACT
Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods—due to the cost, time or effort involved—but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints, also 
referred to as “descriptors”, may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative—extending into new materials spaces—provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven “materials informatics” strategies undertaken in the last decade, with particular emphasis on the fingerprint or descriptor choices. The review also identifies some challenges the community is facing and those that should be overcome in the near future.

INTRODUCTION
When a new situation is encountered, cognitive systems (including humans) have a natural tendency to make decisions based on past similar encounters. When the new situation is distinctly different from those encountered in the past, errors in judgment may occur and lessons may be learned. The sum total of such past scenarios, decisions made and the lessons learned may be viewed collectively as “experience”, “intuition” or even as “common sense”. Ideally, depending on the intrinsic capability of the cognitive system, its ability to make decisions should progressively improve as the richness of scenarios encountered increases.
In recent decades, the artificial intelligence (AI) and statistics communities have made these seemingly vague notions quantitative and mathematically precise. These efforts have resulted in practical machines that learn from past experiences (or “examples”). Classic exemplars of such machine learning approaches include facial, fingerprint or object recognition systems, machines that can play sophisticated games such as chess, Go or poker, and automation systems such as in robotics or self-driving cars. In each of these cases, a large data set of past examples is required, e.g., images and their identities, configuration of pieces in a board game and the best moves, and scenarios encountered while driving and the best actions.
On the surface, it may appear as though the “data-driven” approach for determining the best decision or answer when a new situation or problem is encountered is radically different from approaches based on fundamental science in which predictions are made by solving equations that govern the pertinent phenomena. But viewed differently, is not the scientific process itself—which begins with observations, followed by intuition, then construction of a quantitative theory that explains the observations, and subsequently, refinement of the theory based on new observations—the ultimate culmination of such data-driven inquiries?
For instance, consider how the ancient people from India and Sri Lanka figured out, through persistent tinkering, the alloying elements to add to iron to impede its tendency to rust, using only their experience and creativity3,4 (and little “steel science”, which arose from this empiricism much later)—an early example of the reality and power of “chemical intuition.” Or, more recently, over the last century, consider the enormously practical Hume–Rothery rules to determine the solubility tendency of one metal in another,5 the Hall–Petch studies that have led to empirical relationships between grain sizes and mechanical strength (not just for metals but for ceramics as well),6,7 and the group contribution approach to predict complex properties of organic and polymeric materials based just on the identity of the chemical structure,8 all of which arose from data-driven pursuits (although they were not called as such), and later rationalized using physical principles. It would thus be fair to say that data
either directly or indirectly—drives the creation of both complex fundamental and simple empirical scientific theories. Figure 1 charts the timeline for some classic historical and diverse examples of data-driven efforts.
In more modern times, in the last decade or so, thanks to the implicit or explicit acceptance of the above notions, the “data-driven”, “machine learning”, or “materials informatics” paradigms (with these terms used interchangeably by the community) are rapidly becoming an essential part of the materials research portfolio. The availability of robust and trustworthy in silico simulation methods and systematic synthesis and characterization capabilities, although time-consuming and sometimes expensive, provides a pathway to generate at least a subset of the required critical data in a targeted and organized manner (e.g., via “high-throughput” experiments or computations). Indeed, such efforts are already underway, which have lead to the burgeoning of a number of enormously useful repositories such as NOMAD (http://nomad-coe.eu), Materials Project (http://materialsproject.org), Aflowlib (http://www.aflowlib.org), and OQMD (http://oqmd.org). Mining or learning from these resources or other reliable 
extant data can lead to the recognition of previously unknown correlations between properties, and the discovery of qualitative and quantitative rules—also referred to as surrogate models—that can be used to predict material properties orders of magnitude faster and cheaper, and with reduced human effort than required by the benchmark simulation or experimental methods utilized to create the data in the first place.
With excitement and opportunities come challenges. Questions constantly arise as to what sort of materials science problems are most appropriate for, or can benefit most from, a data-driven approach. A satisfactory understanding of this aspect is essential before one makes a decision on using machine learning methods for their problem of interest. Perhaps the most dangerous aspect of data-driven approaches is the unwitting application of machine learning models to cases that fall outside the domain of prior data. A rich and largely uncharted area of inquiry is to recognize when such a scenario ensues, and to be able to quantify the uncertainties of the machine learning predictions especially when models veer out-of-domain. Solutions for handling these perilous situations may open up pathways for adaptive learning models that can progressively improve in quality through systematic infusion of new data—an aspect critical to the further burgeoning of machine learning within the hard sciences.
This article attempts to provide an overview of some of the recent successful data-driven materials research strategies undertaken in the last decade, and identifies challenges that the community is facing and those that should be overcome in the near future.

RESULTS
Elements of machine learning (within materials science)
Regardless of the specific problem under study, a prerequisite for machine learning is the existence of past data. Thus, either clean, curated and reliable data corresponding to the problem under study should already be available, or an effort has to be put in place upfront for the creation of such data. An example data set may be an enumeration of a variety of materials that fall within a well-defined chemical class of interest and a relevant measured or computed property of those materials (see Fig. 2a). Within the machine learning parlance, the former, i.e., the material, is referred to as “input”, and the latter, i.e., the property of interest, is referred to as the “target” or “output.” A learning problem (Fig. 2b) is then defined as follows: Given a {materials → property} data set, what is the best estimate of the property for a new material not in the original data set? Provided that there are sufficient examples, i.e., that the data set is sufficiently large, and provided that the new material falls 
within the same chemo-structural class as the materials in the original data set, we expect that it should be possible to make such an estimate. Ideally, uncertainties in the prediction should also be reported, which can give a sense of whether the new case is within or outside the domain of the original data set.
All data-driven strategies that attempt to address the problem posed above are composed of two distinct steps, both aimed at satisfying the need for quantitative predictions. The first step is to represent numerically the various input cases (or materials) in the data set. At the end of this step, each input case would have been reduced to a string of numbers (or “fingerprints”; see Fig. 2c). This is such an enormously important step, requiring significant expertise and knowledge of the materials class and the application, i.e., “domain expertise”, that we devote a separate Section to its discussion below.
The second step establishes a mapping between the fingerprinted input and the target property, and is entirely numerical in nature, largely devoid of the need for domain knowledge. Both the fingerprinting and mapping/learning steps are schematically illustrated in Fig. 2. Several algorithms, ranging from elementary (e.g., linear regression) to highly sophisticated (kernel ridge regression, decision trees, deep neural networks), are available to establish this mapping and the creation of surrogate prediction models. While some algorithms provide actual functional forms that relate input to output (e.g., regression based schemes), others do not (e.g., decision trees). Moreover, the amount of available data may also dictate the choice of learning algorithms. For instance, tens to thousands of data points may be adequately handled using regression algorithms such as kernel ridge regression or gaussian process regression, but the availability of much larger data sets (e.g., hundreds of thousands or 
millions) may warrant deep neural networks, simply due to considerations of favorable scalability of the prediction models with data set size. In the above discussion, it was implicitly assumed that the target property is a continuous quantity (e.g., bulk modulus, band gap, melting temperature, etc.). Problems can also involve discrete targets (e.g., crystal structure, specific structural motifs, etc.), which are referred to as classification problems. At this point, it is worth mentioning that the learning problem as described above for the most part involving a mapping between the fingerprints and target properties is referred to as “supervised learning”; “unsupervised learning”, on the other hand, involves using just the fingerprints to recognize patterns in the data (e.g., for classification purposes or for reduction of the dimensionality of the fingerprint vector).
Throughout the learning process, it is typical (and essential) to adhere to rigorous statistical practices. Central to this are the notions of cross-validation and testing on unseen data, which attempt to ensure that a learning model developed based on the original data set can truly handle a new case without falling prey to the perils of “overfitting”. Indeed, it should be noted here that some of the original and most successful applications of machine learning, including statistical treatments and practices such as regularization and cross-validation, were first introduced into materials research in the field of alloy theory, cluster expansions and lattice models.16,17,18,19,20,21,22,23,24 These ideas, along with machine learning techniques such as compressive sensing, are further taking shape within the last decade.25,26
Machine learning should be viewed as the sum total of the organized creation of the initial data set, the fingerprinting and learning steps, and a necessary subsequent step (discussed at the end of this article) of progressive and targeted new data infusion, ultimately leading to an expert recommendation system that can continuously and adaptively improve.
Hierarchy of fingerprints or descriptors
We now elaborate on what is perhaps the most important component of the machine learning paradigm, the one that deals with the numerical representation of the input cases or materials. A numerical representation is essential to make the prediction scheme quantitative (i.e., moving it away from the “vague” notions alluded to in the first paragraph of this article). The choice of the numerical representation can be effectively accomplished only with adequate knowledge of the problem and goals (i.e., domain expertise or experience), and typically proceeds in an iterative manner by duly considering aspects of the material that the target property may be correlated with. Given that the numerical representation serves as the proxy for the real material, it is also referred to as the fingerprint of the material or its descriptors (in machine learning parlance, it is also referred to as the feature vector).
Depending on the problem under study and the accuracy requirements of the predictions, the fingerprint can be defined at varying levels of granularity. For instance, if the goal is to obtain a high-level understanding of the factors underlying a complex phenomenon—such as the mechanical or electrical strength of materials, catalytic activity, etc.—and prediction accuracy is less critical, then the fingerprint may be defined at a gross level, e.g., in terms of the general attributes of the atoms the material is made up of, other potentially relevant properties (e.g., the band gap) or higher-level structural features (e.g., typical grain size). On the other hand, if the goal is to predict specific properties at a reasonable level of accuracy across a wide materials chemical space—such as the dielectric constant of an insulator or the glass transition temperature of a polymer—the fingerprint may have to include information pertaining to key atomic-level structural fragments that may control these properties. If extreme (chemical) accuracy in predictions is demanded—such as total energies and atomic forces, precise identification of structural features, space groups or phases—the fingerprint has to be fine enough so that it is able to encode details of atomic-level structural information with sub-Angstrom-scale resolution. Several examples of learning based on this hierarchy of fingerprints or descriptors are provided in subsequent Sections.
The general rule of thumb is that finer the fingerprint, greater is the expected accuracy, and more laborious, more data-intensive and less conceptual is the learning framework. A corollary to the last point is that rapid coarse-level initial screening of materials should generally be targeted using coarser fingerprints.
Regardless of the specific choice of representation, the fingerprints should also be invariant to certain transformations. Consider the facial recognition scenario. The numerical representation of a face should not depend on the actual placement location of the face in an image, nor should it matter whether the face has been rotated or enlarged with respect to the examples the machine has seen before. Likewise, the representation of a material should be invariant to the rigid translation or rotation of the material. If the representation is fine enough that it includes atomic position information, permutation of like atoms should not alter the fingerprint. These invariance properties are easy to incorporate in coarser fingerprint definitions but non-trivial in fine-level descriptors. Furthermore, ensuring that a fingerprint contains all the relevant components (and only the relevant components) for a given problem requires careful analysis, for example, using unsupervised learning algorithms. For these 
reasons, construction of a fingerprint for a problem at hand is not always straightforward or obvious.
Examples of learning based on gross-level property-based descriptors
Two historic efforts in which gross-level descriptors were utilized to create surrogate models (although they were not couched under those terms) have lead to the Hume–Rothery rules5 and Hall–Petch relationships6,7 (Fig. 1). The former effort may be viewed as a classification exercise in which the target is to determine whether a mixture of two metals will form a solid solution; the gross-level descriptors considered were the atomic sizes, crystal structures, electronegativities, and oxidation states of the two metal elements involved. In the latter example, the strength of a polycrystalline material is the target property, which was successfully related to the average grain size; specifically a linear relationship was found between the strength and the reciprocal of the square root of the average grain size. While a careful manual analysis of data gathered from experimentation was key to developing such rules in the past, modern machine learning and data mining approaches provide powerful pathways for such knowledge discovery, especially when the dependencies are multivariate and highly nonlinear.
To identify potential nonlinear multivariate relationships efficiently, one may start from a moderate number of potentially relevant primary descriptors (e.g., electronegativity, E, ionic radius, R, etc.), and create millions or even billions of compound descriptors by forming algebraic combinations of the primary descriptors (e.g., E/R 2, R log(E), etc.); see Fig. 3a, b. This large space of nonlinear mathematical functions needs to be “searched” for a subset that is highly correlated with the target property. Dedicated methodological approaches to accomplish such a task have emerged from recent work in genetic programing,27 compressed sensing,28,29 and information science.30
One such approach—based on the least absolute shrinkage and selection operator (LASSO)—was recently demonstrated to be highly effective for determining key physical factors that control a complex phenomenon through identification of simple empirical relationships.28,29 An example of such complex behavior is the tendency of insulators to fail when subjected to extreme electric fields. The critical field at which this failure occurs in a defect-free material—referred to as the intrinsic electrical breakdown field—is related to the balance between energy gained by charge carriers from the electric field to the energy lost due to collisions with phonons. The intrinsic breakdown field may be computed from first principles by treatment of electron-phonon interactions, but this computation process is enormously laborious. Recently, the breakdown field was computed from first principles using density functional theory (DFT) for a benchmark set of 82 binary octet insulators.31 This data set included alkali metal halides, transition metal halides, alkaline earth metal chalcogenides, transition metal oxides, and group III, II–VI, I–VII semiconductors. After validating the theoretical results by comparing against available experimental data, this data set was used to build simple predictive phenomenological surrogate models of dielectric breakdown using LASSO as well as other advanced machine learning schemes. The general flow of the LASSO-based procedure, starting from the primary descriptors considered (Fig. 3a), is charted in Fig. 3b. The trained and validated surrogate models were able to reveal key correlations and analytical relationships between the breakdown field and other easily accessible material properties such as the band gap and the phonon cutoff frequency. Figure 3c shows the agreement between such a discovered analytical relationship and the DFT results (spanning three orders of magnitude) for the benchmark data set of 82 insulators, as well as for four new ones that were not included in the original 
training data set.
The phenomenological model was later employed to systematically screen and identify perovskite compounds with high breakdown strength. The purely machine learning based screening revealed that boron-containing compounds are of particular interest, some of which were predicted to exhibit remarkable intrinsic breakdown strength of ~1 GV/m (see Fig. 3d). These predictions were subsequently confirmed using first principles computations.32
The LASSO-based and related schemes have also been shown to be enormously effective at predicting the preferred crystal structures of materials. In a pioneering study that utilized the LASSO-based approach, Ghiringelli and co-workers were able to classify binary octet insulators into tendencies for the formation of rock salt versus zinc blende structures. More recently, Bialon and co-workers34 aimed to classify 64 different prototypical crystal structures formed by AxBy type compounds, where A and B are sp-block and transition metal elements, respectively. After searching over a set of 1.7 × 105 non-linear descriptors formed by physically meaningful functions of primary coarse-level descriptors such as band-filling, atomic volume, and different electronegativity scales of the sp and d elements, the authors were able to find a set of three optimal descriptors. A three-dimensional structure-map—built on the identified descriptor set—was used to classify 2105 experimentally known training examples available from the Pearson’s Crystal Database35 with an 86% probability of predicting the correct crystal structure. Likewise, Oliynyk and co-workers recently used a set of elemental descriptors to train a machine-learning model, built on a random forest algorithm,36 with an aim to accelerate the search for Heusler compounds. After training the model on available crystallographic data from Pearson’s Crystal Database35 and the ASM Alloy Phase Diagram Database the model was used to evaluate the probabilities at which compounds with the formula AB2C will adopt Heusler structures. This approach was exceptionally successful in distinguishing between Heusler and non-Heusler compounds (with a true positive rate of 94%), including the prediction of unknown compounds and flagging erroneously assigned entries in the literature and in crystallographic databases. As a proof of concept, 12 novel predicted candidates (Gallides with formulae MRu2Ga and RuM2Ga, where M = Ti, V, Cr, Mn, Fe, and Co) were synthesized and confirmed to be Heusler compounds. One point to be cautious about when creating an enormous number of compound descriptors (starting from a small initial set of primary descriptors) is model interpretability. Efforts must be taken to ensure that the final set of shortlisted descriptors (e.g., the output of the LASSO process) is stable, i.e., the same or similar set of compound descriptors is obtained during internal cross-validation steps, lest the process becomes a victim of the “curse of dimensionality.”
Yet another application of the gross-level descriptors relate to the prediction of the band gap of insulators. Rajan and co-workers have used experimentally available band gaps of ABC2 chalcopyrite compounds to train regression models with electronegativity, atomic number, melting point, pseudopotential radii, and the valence for each of the A, B, and C elements as features. Just using the gross-level elemental features, the developed machine learning models were able to predict the experimental band gaps with moderate accuracy. In a different study, Pilania and co-workers41 used a database consisting of computed band gaps of ~1300 AA′BB′O6 type double perovskites to train a kernel ridge regression (KRR) machine learning model, a scheme that allows for nonlinear relationships based on measures of (dis)similarity between fingerprints, for efficient predictions of the band gaps. A set of descriptors with increasing complexity was identified by searching across a large portion of the feature 
space using LASSO, with ≥ 1.2 million compound descriptors created from primary elemental features such as electronegativities, ionization potentials, electronic energy levels, and valence orbital radii of the constituent atomic species. One of the most important chemical insights that emerged from this effort was that the band gap in the double perovskites is primarily controlled (and therefore effectively learned) by the lowest occupied energy levels of the A-site elements and electronegativities of the B-site elements.
Other successful attempts of using gross-level descriptors include the creation of surrogate models for the estimation of formation enthalpies,43,44,45 free energies,46 defect energetics,47 melting temperatures,48,49 mechanical properties,50,51,52 thermal conductivity,53 catalytic activity,54,55 and radiation damage resistance.56 Efforts are also underway for the identification of novel shape memory alloys, improved piezoelectrics,58 MAX phases,59 novel perovskite60 and double perovskite halides,43,60 CO2 capture materials,61 and potential candidates for water splitting.62
Emerging materials informatics tools also offer tremendous potential and new avenues for mining for structure-property-processing linkages from aggregated and curated materials data sets.63 While a large fraction of such efforts in the current literature has considered relatively simple definitions of the material that included mainly the overall chemical composition of the material, Kalidindi and co-workers have recently proposed a new materials data science framework known as Materials Knowledge Systems68,69 that explicitly accounts for the complex hierarchical material structure in terms of n-point spatial correlations (also frequently referred to as n-point statistics). Further adopting the n-point statistics as measures to quantify materials microstructure, a flexible computational framework has been developed to customize toolsets to understand structure-property-processing linkages in materials science.
Examples of learning based on molecular fragment-level descriptors
The next in the hierarchy of descriptor types are those that encode finer details than those captured by the gross-level properties. Within this class, materials are described in terms of the basic building blocks they are made of. The origins of “block-level” or “molecular fragment” based descriptors can be traced back to cheminformatics, which is a field of theoretical chemistry that deals with correlating properties such as biological activity, physio-chemical properties and reactivity with molecular structure and fragments,71,72,73 leading up to what is today referred to as quantitative structure activity/property relationships (QSAR/QSPR).
Within materials science, specifically, within polymer science, the notions underlying QSAR/QSPR ultimately led to the successful group contribution methods.8 Van Krevelen and co-workers studied the properties of polymers and discovered that they were strongly correlated to the chemical structure (i.e., nature of the polymer repeat unit, end groups, etc.) and the molecular weight distribution. They observed that polymer properties such as glass transition temperature, solubility parameter and bulk modulus (which were, and still are, difficult to compute using traditional computational methods) were correlated with the presence of chemical groups and combinations of different groups in the repeat unit. Based on a purely data-driven approach, they developed an “atomic group contribution method” to express various properties as a linear weighted sum of the contribution (called atomic group parameter) from every atomic group that constituted the repeat unit. These groups could be units like CH2, C6H4, CH2-CO, etc., that make up the polymer. It was also noticed that factors such as the presence of aromatic rings, long side chains and cis/trans conformations influence the properties, prompting their introduction into the group additivity scheme. For instance, a CH2 group attached to an aromatic ring would have a different atomic group parameter than a CH2 group attached to an aliphatic group. In this fashion, nearly all the important contributing factors were taken into account, and linear empirical relationships were devised for thermal, elastic and other polymer properties. However, widespread usage of these surrogate models is still restricted because (1) the definition of atomic groups is somewhat ad hoc, and (2) the target properties are assumed to be linearly related to the group parameters.
Modern data-driven methods have significantly improved on these earlier ideas with regards to both issues mentioned above. Recently, in order to enable the accelerated discovery of polymer dielectrics, hundreds of polymers built from a chemically allowed combination of seven possible basic units, namely, CH2, CO, CS, O, NH, C6H4, and C4H2S, were considered, inclusive of van der Waals interactions,80 and a set of properties relevant for dielectric applications, namely, the dielectric constant and band gap, were computed using DFT.74,81 These polymers were then fingerprinted by keeping track of the occurrence of a fixed set of molecular fragments in the polymers in terms of their number fractions.81,82 A particular molecular fragment could be a triplet of contiguous blocks such as –NH–CO–CH2– (or, at a finer level, a triplet of contiguous atoms, such as C4–O2–C3 or C3–N3–H1, where X n represents an n-fold coordinated X atom).83,84 All possible triplets were considered (some examples are shown 
in Fig. 4a), and the corresponding number fractions in a specific order formed the fingerprint of a particular polymer (see Fig. 4b). This procedure provides a uniform and seamless pathway to represent all polymers within this class, and the procedure can be indefinitely generalized by considering higher order fragments (i.e., quadruples, quintuples, etc., of atom types). Furthermore, relationships between the fingerprint and properties have been established using the KRR learning algorithm; a schematic of how this algorithm works is shown in Fig. 4c. The capability of this scheme for dielectric constant and band gap predictions is portrayed in Fig. 4d. These predictive tools are available online (Fig. 4e) and are constantly being updated.85 The power of such modern data-driven molecular fragment-based learning approaches (like its group contribution predecessor) lies in the realization that any type of property related to the molecular structure—whether computable using DFT (e.g., band gap, dielectric 
constant) or measurable experimentally (e.g., glass transition temperature, dielectric loss)—can be learned and predicted.
The molecular fragment-based representation is not restricted to polymeric materials. Novel compositions of AxByOz ternary oxides and their most probable crystal structures have been predicted using a probabilistic model built on an experimental crystal structure database.86 The descriptors used in this study are a combination of the type of crystal structure (spinel, olivine, etc.) and the composition information, i.e., the elements that constitute the compound. Likewise, surrogate machine learning models have been developed for predicting the formation energies of AxByOz ternary compounds using only compositional information as descriptors, trained on a data set of 15,000 compounds from the Inorganic Crystal Structure Database.44 Using this approach, 4500 new stable materials have been discovered. Finally, surrogate models have been developed for predicting the formation energies of elpasolite crystals with the general formula A2BCD6, based mainly on compositional information. The descriptors used take into account the periodic table row and column of elements A, B, C, and D that constitute the compound (although this fingerprint could have been classified as a gross-level one, we choose to place this example in the present Section as the prototypical structure of the elpasolite was implicitly assumed in this work and fingerprint). Important correlations and trends were revealed between atom types and the energies; for example, it was found that the preferred element for the D site is F, and that for the A and B sites are late group II elements.43
Examples of learning based on sub-Angstrom-level descriptors
We now turn to representing materials at the finest possible scale, such that the fingerprint captures precise details of atomic configurations with high fidelity. Such a representation is useful in many scenarios. For instance, one may attempt to connect this fine-scale fingerprint directly with the corresponding total potential energy with chemical accuracy, or with structural phases/motifs (e.g., crystal structure or the presence/absence of a stacking fault). The former capability can lead to purely data-driven accelerated atomistic computational methods, and the latter to refined and efficient on-the-fly characterization schemes.
“Chemical accuracy” specifically refers to potential energy and reaction enthalpy predictions with errors of < 1 kcal/mol, and atomic force predictions (the input quantity for molecular dynamics, or MD, simulations) with errors of < 0.05 eV/Å. Chemical accuracy is key to enable reliable MD simulations (or for precise identification of the appropriate structural phases or motifs), and is only possible with fine-level fingerprints that offer sufficiently high configurational resolution, more than those in the examples encountered thus far.
The last decade has seen spectacular activity and successes in the general area of data-driven atomistic computations. All modern atomistic computations use either some form of quantum mechanical scheme (e.g., DFT) or a suitably parameterized semi-empirical method to predict the properties of materials, given just the atomic configuration. Quantum mechanical methods are versatile, i.e., they can be used to study any material, in principle. However, they are computationally demanding, as complex differential equations governing the behavior of electrons are solved for every given atomic configuration. Systems involving at most about 1000 atoms can be simulated routinely in a practical setting today. In contrast, semi-empirical methods use prior knowledge about interatomic interactions under known conditions and utilize parameterized analytical equations to determine properties such as the total potential energies, atomic forces, etc. These semi-empirical force fields are several orders of magnitude faster than quantum mechanical methods, and are the choice today for routinely simulating systems containing millions to billions of atoms, as well as the dynamical evolution of systems at nonzero temperatures (using the MD method) at timescales of nanoseconds to milliseconds. However, a major drawback of traditional semi-empirical force fields is that they lack versatility, i.e., they are not transferable to situations or materials for which the original functional forms and parameterizations do not apply.
Machine learning is rapidly bridging the chasm between the two extremes of quantum mechanical and semi-empirical methods, and has offered surrogate models that combine the best of both worlds. Rather than resort to specific functional forms and parameterizations adopted in semi-empirical methods (the aspects that restrict their versatility), machine learning methods use an {atomic configuration → property} data set, carefully prepared, e.g., using DFT, to make interpolative predictions of the property of a new configuration at speeds several orders of magnitude faster than DFT. Any material for which adequate reference DFT computations may be performed ahead of time can be handled using such a machine learning scheme. Thus, the lack of versatility issue of traditional semi-empirical approach and the time-intensive nature of quantum mechanical calculations are simultaneously addressed, while also preserving quantum mechanical and chemical accuracy.
The primary challenge though has been the creation of suitable fine-level fingerprinting schemes for materials, as these fingerprints are required to be strictly invariant with respect to arbitrary translations, rotations, and exchange of like atoms, in addition to being continuous and differentiable (i.e., “smooth”) with respect to small variations in atomic positions. Several candidates, including those based on symmetry functions, bispectra of neighborhood atomic densities, Coulomb matrices (and its variants), smooth overlap of atomic positions (SOAP),93,94,95,96 and others,97,98 have been proposed. Most fingerprinting approaches use sophisticated versions of distribution functions (the simplest one being the radial distribution function) to represent the distribution of atoms around a reference atom, as qualitatively captured in Fig. 5a. The Coulomb matrix is an exception, which elegantly represents a molecule, with the dimensionality of the matrix being equal to the total number of atoms 
in the molecule. Although questions have arisen with respect to smoothness considerations and whether the representation is under/over-determined (depending on whether the eigenspectrum or the entire matrix is used as the fingerprint),93 this approach has been shown to be able to predict various molecular properties accurately.
Figure 5b also shows a general schema typically used in the construction of machine learning force fields, to be used in MD simulations. Numerous learning algorithms—ranging from neural networks, KRR, Gaussian process regression (GPR), etc.—have been utilized to accurately map the fingerprints to various materials properties of interest. A variety of fingerprinting schemes, as well as learning schemes that lead up to force fields have been recently reviewed. One of the most successful and widespread machine learning force field schemes to date is the one by Behler and co-workers,87 which uses symmetry function fingerprints mapped to the total potential energy using a neural network. Several applications have been studied, including surface diffusion, liquids, phase equilibria in bulk materials, etc. This approach is also quite versatile in that multiple elements can be considered. Bispectra based fingerprints combined with GPR learning schemes have lead to Gaussian approximation potentials,87,90 which have also been demonstrated to provide chemical accuracy, versatility and efficiency.
A new development within the area of machine learning force fields is to learn and predict the atomic forces directly; the total potential energy is determined through appropriate integration of the forces along a reaction coordinate or MD trajectory.105 These approaches are inspired by Feynman’s original idea that it should be possible to predict atomic forces given just the atomic configuration, without going through the agency of the total potential energy.106 An added attraction of this perspective is that the atomic force can be uniquely assigned to an individual atom, while the potential energy is a global property of the entire system (partitioning the potential energy to atomic contributions does not have a formal basis). Mapping atomic fingerprints to purely atomic properties can thus lead to powerful and accurate prescriptions. Figure 5c, for instance, compares the atomic forces at the core of an edge dislocation in Al, predicted using a machine learning force prediction 
recipe called AGNI, with the DFT forces for the same atomic configuration. Also shown are forces predicted using the embedded atom method (EAM), a popular classical force field, for the same configuration. EAM tends to severely under-predict large forces while the machine learning scheme predicts forces with high fidelity (neither EAM nor the machine learning force field were explicitly trained on dislocation data). This general behavior is consistent with recent detailed comparisons of EAM with machine learning force fields.107 It is worth noting that although this outlook of using atomic forces data during force field development is reminiscent of the “force-matching” approach of Ercolessi and Adams,108 this new development is distinct from that approach in that it attempts to predict the atomic force given just the atomic configuration.
Another notable application of fine-level fingerprints has been in the use of the electronic charge density itself as the representation to learn various properties82 or density functionals, thus going to the very heart of DFT. While these efforts are in a state of infancy—as they have dealt with mainly toy problems and learning the kinetic energy functional—such efforts have great promise as they attempt to integrate machine learning methods within DFT (all other DFT-related informatics efforts so far have utilized machine learning external to DFT).
Fine-level fingerprints have also been used to characterize structure in various settings. Within a general crystallographic structure refinement problem, one has to estimate the structural parameters of a system, i.e., the unit cell parameters (a, b, c, α, β, and γ) that best fit measured X-ray diffraction (XRD) data. Using a Bayesian learning approach and a Markov chain Monte Carlo algorithm to sample multiple combinations of possible structural parameters for the case of Si, Fancher and co-workers not only accurately determined the estimates of the structural parameters, but also quantified the associated uncertainty (thus going beyond the conventional Rietveld refinement method).
Unsupervised learning using fine-level fingerprints (and clustering based on these fingerprints) has led to the classification of materials based on their phases or structural characteristics.11,12 Using the XRD spectrum itself as the fingerprint, high-throughput XRD measurements for various compositional spreads have been used to automate the creation of phase diagrams. Essentially, features of the XRD spectra are used to distinguish between phases of a material as a function of composition. Likewise, on the computational side, the SOAP fingerprints have been effectively used to distinguish between different allotropes of materials, as well as different motifs that emerge during the course of a MD simulation (see Fig. 5d for an example).
Critical steps going forward
Quantifying the uncertainties of predictions
Given that machine learning predictions are inherently statistical in nature, uncertainties must be expected in the predictions. Moreover, predictions are typically and ideally interpolative between data points corresponding to previously seen data. To what extent a new case for which a prediction needs to be made falls in or out of the domain of the original data set (i.e., to what extent the predictions are interpolative or extrapolative) may be quantified using the predicted uncertainty. While strategies are available to prescribe prediction uncertainties, these ideas have been explored only to a limited extent within materials science.57,118 Bayesian methods (e.g., Gaussian process regression)15 provide a natural pathway for estimating the uncertainty of the prediction in addition to the prediction itself. This approach assumes that a Gaussian distribution of models fit the available data, and thus a distribution of predictions may be made. The mean and variance of these predictions—the natural outcomes 
of Bayesian approaches—are the most likely predicted value and the uncertainty of the prediction, respectively, within the spectrum of models and the fingerprint considered. Other methods may also be utilized to estimate uncertainties, but at significant added cost. A straightforward and versatile scheme is bootstrapping, in which different (but small) subsets of the data are randomly excluded, and several prediction models are developed based on these closely related but modified data sets. The mean and variance of the predictions from these bootstrapped models provide the property value and expected uncertainty. Essentially, this approach attempts to probe how sensitive the model is with respect to slight “perturbations” to the data set. Another related methodology is to explicitly consider a variety of closely related models, e.g., neural networks or decision trees with slightly different architectures, and to use the distribution of predictions to estimate uncertainty.89
Adaptive learning and design
Uncertainty quantification has a second important benefit. It can be used to continuously and progressively improve a prediction model, i.e., render it a truly learning model. Ideally, the learning model should adaptively and iteratively improve by asking questions such as “what should be the next new material system to consider or include in the training set that would lead to an improvement of the model or the material?” This may be accomplished by balancing the tradeoffs between exploration and exploitation.118,120 That is, at any given stage of an iterative learning process, a number of new candidates may be predicted to have certain properties with uncertainties. The tradeoff is between exploiting the results by choosing to perform the next computation (or experiment) on the material predicted to have the optimal target property or further improving the model through exploration by performing the calculation (or experiment) on a material where the predictions have the largest uncertainties. This can be 
done rigorously by adopting well-established information theoretic selector frameworks such as the knowledge gradient. In the initial stages of the iterative process, it is desired to “explore and learn” the property landscape. As the machine learning predictions improve and the associated uncertainties shrink, the adaptive design scheme allows one to gradually move away from exploration towards exploitation. Such an approach, schematically portrayed in Fig. 6a, enables one to systematically expand the training data towards a target chemical space, where materials with desired functionality are expected to reside.
Some of the first examples of using adaptive design for targeted materials discovery include identification of shape memory alloys with low thermal hysteresis57 and accelerated search for BaTiO3-based piezoelectrics with optimized morphotropic phase boundary.58 In the first example, Xue and co-workers57 employed the aforementioned adaptive design framework to find NiTi-based shape memory alloys that may display low thermal hysteresis. Starting from a limited number of 22 training examples and going through the iterative process 9 times, 36 predicted compositions were synthesized and tested from a potential space of ~800,000 compound possibilities. It was shown that 14 out of these 36 new compounds were better (i.e., had a smaller thermal hysteresis) than any of the 22 compounds in the original data set. The second successful demonstration of the adaptive design approach combined informatics and Landau–Devonshire theory to guide experiments in the design of lead-free piezoelectrics.58 Guided by predictions 
from the machine learning model, an optimized solid solution, (Ba0.5Ca0.5)TiO3–Ba(Ti0.7Zr0.3)O3, with piezoelectric properties was synthesized and characterized to show better temperature reliability than other BaTiO3-based piezoelectrics in the initial training data.
The materials science community is just beginning to explore and utilize the plethora of available information theoretic algorithms to mine and learn from data. The usage of an algorithm is driven largely by need, as it should. One such need is to be able to learn and predict vectorial quantities. Examples include functions, such as the electronic or vibrational density of states (which are functions of energy or frequency). Although, the target property in these cases may be viewed as a set of scalar quantities at each energy or frequency (for a given structure) to be learned and predicted independently, it is desirable to learn and predict the entire function simultaneously. This is because the value of the function at a particular energy or frequency is correlated to the function values at other energy or frequency values. Properly learning the function of interest requires machine learning algorithms that can handle vectorial outputs. Such algorithms are indeed available, and if exploited can lead to prediction schemes of the electronic structure for new configurations of atoms. Another class of examples where vector learning is appropriate includes cases where the target property is truly a vector (e.g., atomic force) or a tensor (e.g., stress). In these cases, the vector or tensor transforms in a particular way as the material itself is transformed, e.g., if it is rotated (in the examples of functions discussed above, the vectors, i.e., the functions, are invariant to any unitary transformation of the material). These truly vectorial or tensorial target property cases will thus have to be handled with care, as has been done recently using vector learning and covariant kernels.102
Another algorithm that is beginning to show value within material science falls under multi-fidelity learning. This learning method can be used when a property of interest can be computed at several levels of fidelities, exhibiting a natural hierarchy in both computational cost and accuracy. A good materials science example is the band gap of insulators computed at an inexpensive lower level of theory, e.g., using a semilocal electronic exchange-correlation functional (the low-fidelity value), and the band gap computed using an more accurate, but expensive, approach, e.g., using a hybrid exchange-correlation functional (the high-fidelity value). A naive approach in such a scenario can be to use a low-fidelity property value as a feature in a machine learning model to predict the corresponding higher fidelity value. However, using low-fidelity estimates as features strictly requires the low-fidelity data for all materials for which predictions are to be made using the trained model. This can be particularly challenging and extremely computationally demanding when faced with a combinatorial problem that targets exploring vast chemical and configurational spaces. A multi-fidelity co-kriging framework, on the other hand, can seamlessly combine inputs from two or more levels of fidelities to make accurate predictions of the target property for the highest fidelity. Such an approach, schematically represented in Fig. 6b, requires high-fidelity training data only on a subset of compounds for which low-fidelity training data is available. More importantly, the trained model can make efficient highest-fidelity predictions even in the absence of the low-fidelity data for the prediction set compounds. While multi-fidelity learning is routinely used in several fields to address computationally challenging engineering design problems,125,126 it is only beginning to find applications in materials informatics.
Finally, machine learning algorithms may also lead to strategies for making the so-called “inverse design” of materials possible. Inverse design refers to the paradigm whereby one seeks to identify materials that satisfy a target set of desired properties (in this parlance, the “forward” process refers to predicting the properties of a given material). Within the machine learning context, although the backward process of going from a desired set of properties to the appropriate fingerprints is straightforward, the process of inverting the fingerprint to actual physically and chemically meaningful materials continues to be a major hurdle. Two strategies that are adopted to achieve inverse design within the context of machine learning involves either inverting the desired properties to only fingerprints that correspond to physically realizable materials (through imposition of constraints that fingerprint components are required to satisfy),83,127 or adopting schemes such as the genetic algorithm or simulated annealing to determine iteratively a population of materials that meet the given target property requirements. Despite these developments, true inverse design continues to remain a challenge (although materials design through adaptive learning discussed above appears to have somewhat mitigated this challenge).

CONCLUSION
Decisions on when to use machine learning
Perhaps the most important question that plagues new researchers eager to use data-driven methods is whether their problem lends itself to such methods. Needless to say, the existence of past reliable data, or efforts devoted to its generation for at least a subset of the critical cases in a uniform and controlled manner, is a prerequisite for the adoption of machine learning. Even so, the question is the appropriateness of machine learning for the problem at hand. Ideally, data-driven methods should be aimed at (1) properties very difficult or expensive to compute or measure using traditional methods, (2) phenomena that are complex enough (or nondeterministic) that there is no hope for a direct solution based on solving fundamental equations, or (3) phenomena whose governing equations are not (yet) known, providing a rationale for the creation of surrogate models. Such scenarios are replete in the social, cognitive and biological sciences, explaining the pervasive applications of data-driven methods in such domains. Materials science examples ideal for studies using machine learning methods include properties such as the glass transition temperature of polymers, dielectric loss of polycrystalline materials over a wide frequency and temperature range, mechanical strength of composites, failure time of engineering materials (e.g., due to electrical, mechanical or thermal stresses), friction coefficient of materials, etc., all of which involve the inherent complexity of materials, i.e., their polycrystalline or amorphous nature, multi-scale geometric architectures, the presence of defects of various scales and types, and so on.
Machine learning may also be used to eliminate redundancies underlying repetitive but expensive operations, especially when interpolations in high-dimensional spaces are required, such as when properties across enormous chemical and/or configurational spaces are desired. An example of the latter scenario, i.e., an immense configurational space, is encountered in first principles molecular dynamics simulations, when atomic forces are evaluated repetitively (using expensive quantum mechanical schemes) for myriads of very similar atomic configurations. The area of machine learning force fields has burgeoned to meet this need. Yet another setting where large chemical and configurational spaces are encountered is the emerging domain of high-throughput materials characterization, where on-the-fly predictions are required to avoid data accumulation bottlenecks. Although materials informatics efforts so far have largely focused on model problems and the validation of the general notion of data-driven discovery, 
active efforts are beginning to emerge that focus on complex real-world materials applications, strategies to handle situations inaccessible to traditional materials computations, and the creation of adaptive prediction frameworks (through adequate uncertainty quantification) that build efficiencies within rational materials design efforts.

3-Machine Learning-Assisted Discovery of Solid Li-Ion Conducting Materials

ABSTRACT
We discover many new crystalline solid materials with fast single crystal Li ion conductivity at room temperature, discovered through density functional theory simulations guided by machine learning-based methods. The discovery of new solid Li superionic conductors is of critical importance to the development of safe all-solid-state Li-ion batteries. With a predictive universal structure–property relationship for fast ion conduction not well understood, the search for new solid Li ion conductors has relied largely on trial-and-error computational and experimental searches over the last several decades. In this work, we perform a guided search of materials space with a machine learning (ML)-based prediction model for material selection and density functional theory molecular dynamics (DFT-MD) simulations for calculating ionic conductivity. These materials are screened from over 12 000 experimentally synthesized and characterized candidates with very diverse structures and compositions. When compared to a 
random search of materials space, we find that the ML-guided search is 2.7 times more likely to identify fast Li ion conductors, with at least a 44 times improvement in the log-average of room temperature Li ion conductivity. The F1 score of the ML-based model is 0.50, 3.5 times better than the F1 score expected from completely random guesswork. In a head-to-head competition against six Ph.D. students working in the field, we find that the ML-based model doubles the F1 score of human experts in its ability to identify fast Li-ion conductors from atomistic structure with a 1000-fold increase in speed, clearly demonstrating the utility of this model for the research community. In addition to having high predicted Li-ion conductivity, all materials reported here lack transition metals to enhance stability against reduction by the Li metal anode and are predicted to exhibit low electronic conduction, high stability against oxidation, and high thermodynamic stability, making them promising candidates for 
solid-state electrolyte applications on these several essential metrics.

INTRODUCTION
All-solid-state Li-ion batteries (SSLIBs) hold promise as safer, longer lasting, and more energy dense alternatives to today’s commercialized LIBs with liquid electrolytes. However, the design of SSLIBs remains a challenge, with the principal technological bottleneck in realizing these devices being the solid electrolyte. A high performance solid electrolyte material must satisfy several criteria simultaneously: it must possess fast Li-ion conduction, negligible electronic conduction, a wide electrochemical window, robust chemical stability against side reactions with the electrodes, and high mechanical rigidity to suppress dendritic growth on the anode. The material should also be cheap and easy to manufacture. Given these many constraints, searching for promising new materials that satisfy all requirements through the trial-and-error searches has yielded slow progress.
The earliest efforts to discover fast Li-ion conducting solids began in the 1970s(1) and have continued to present. More recently, density functional theory (DFT) simulation has enabled high-throughput computational searches, essentially automating the process of guess-and-check.(2,3) Across these four decades, only several solids with liquid-level Li conduction (>10–2 S/cm) at room temperature (RT) have been identified, including notably Li10GeP2S12 (17 mS/cm)(4) and Li7P3S11 (25 mS/cm).(5) This slow progress suggests that continuing in the guess-and-check paradigm of decades past is unlikely to quickly yield the material innovations we need to unlock the high energy density, high cycle life, and unquestionably safe energy storage devices of the future.
Leveraging atomic and electronic structure data from the Materials Project database,(6) we have screened all 12 000+ Li-containing materials for thermodynamic phase stability, low electronic conduction, high electrochemical stability, and no transition metals (to enhance stability against reduction). We also compiled information on the estimated raw materials cost and the earth abundance of the elemental constituents of each material. This identifies 317 materials that may be strong candidate electrolyte materials if they are also fast ion conductors.
Following the guess-and-check paradigm, one would begin to test these materials for fast ion conduction at random, or according to his/her best scientific intuition. To identify the subset of these materials most likely to exhibit fast ionic conductivity, we have instead developed(7) a machine learning (ML)-based model for predicting the likelihood Psuperionic that an arbitrary material exhibits fast Li-ion conduction at RT, based only on features xi derived from the atomistic structure of the unit cell. Throughout this work, we define superionic conductivity to be greater than 0.1 mS/cm, based on the approximate minimum electrolyte ionic conductivity required for battery applications. Experimental reports of ionic conductivity for several dozen materials ranging over 10 orders of magnitude were used to train the model. This data-driven predictor takes the form of a logistic function, , where
Here, LLB is the average Li–Li bond number (number of Li neighbors) per Li; SBI is the average sublattice bond ionicity; AFC is the average coordination of the anions in the anion framework; LASD is the average equilibrium Li–anion separation distance in angstroms; and LLSD is the average equilibrium Li–Li separation distance in angstroms. Since this model does not require any electronic structure information, it is >5 orders of magnitude faster to evaluate than a DFT simulation of conductivity.
Screening this list of 317 candidate materials identifies 21 crystalline compounds that are predicted to be fast ion conductors with robust structural and electrochemical stability, representing a 99.8% reduction in the entire space of known Li-containing materials. One of these 21 materials, LiCl, has been reported to exhibit poor RT Li conduction (∼10–9 S/cm),(8) making it a known false positive prediction. Another material, Li3InCl6, has been reported to have a RT conductivity of approximately 1 mS/cm, making it a correct model prediction.(9) Very little is reported in the literature regarding the remaining 19 materials to our knowledge.
In this work, we perform DFT molecular dynamics (DFT-MD) calculations(10,11) on the promising candidate materials identified by our screening procedure, finding evidence of superionic RT Li conduction in eight and marginal RT Li conduction in two. As a control, we then perform DFT-MD on a similar number of materials drawn at random from the same population of 317. We quantify the increase in research efficiency offered by our ML-based model by comparing the improvements in experimental outcomes against the random case. We consider 41 unique materials in total. We find our ML-guided search offers approximately 3–4× improvement in predictive power for fast Li ion conductors over random guesswork depending on the metric, while on average the predicted RT Li ionic conductivity of any simulated material is over 44× higher.
As a further test of the model’s efficacy, we provided the same list of materials to a group of six graduate students working in the field and asked them to identify the best ion conductors. We found the F1 score of the model outperformed the F1 score of the intuition of the students by approximately two times, while each prediction was made approximately 1000 times faster. This result suggests ML-based approaches to materials selection may provide significant acceleration over the guess-and-check research paradigm of the past. Furthermore, these results provide confidence in our data-driven superionic prediction model, as well as compelling evidence in the promise of machine learning-based approaches to materials discovery.

METHODS
We first perform DFT-MD on the 19 most promising new candidate materials for solid electrolyte applications that are all predicted to be fast ion conductors by our ML-based model eq 1. LiCl and Li3InCl6 were not simulated due to the existence of conductivity data in the literature.(8,9) In order to accelerate Li diffusion to a computationally tractable time scale, we initially seek an upper bound by performing MD at elevated temperature and removing one Li atom per computational cell to introduce a small concentration of Li vacancies to enhance conduction and minimize the number of false negatives identified. We simulate large supercells in order to minimize the effect of the periodic boundary conditions. All materials were initially simulated at T = 900 K; if melting is observed, the simulation is restarted at increasingly lower temperatures until no melting is observed. The vacancy concentration ranges from 3 to 17% depending on the unit cell. The simulation temperatures, computational cell size, and Li 
vacancy concentrations are provided in the Supporting Information, Table S1.
We simulate the candidate materials for a range of times on the tens to hundreds of picoseconds time scale; see Table S1. To calculate ionic diffusivity, which may be isotropic, we compute the average of the diagonal elements of the second rank Li diffusivity tensor (or equivalently, one-third of the trace). We denote this as ⟨Dii⟩, where the average is taken over the three elements ii = {xx, yy, zz}. We first evaluate the mean squared displacement (MSD) of the Li atoms ⟨Δr2⟩ over time (starting at t = 0) and apply the following formula:
To probe for melting, we also calculate the MSD of the sublattice atoms and assume melting if sustained, nonzero diffusivity is observed in both Li and the sublattice (the smallest diffusivity that can be resolved through DFT-MD at 900 K is approximately 0.01 Å2/ps). To assess the degree of convergence in ⟨Dii⟩, we compute the standard deviation in diffusivities when measured from different starting times in the MSD data. We compute the slope of the MSD data for every starting time from t = 0 to up to 75% of the total run time, in 100 fs increments. We compute one standard deviation above and below the mean diffusivity across all starting times to represent the approximate upper and lower limits of the distribution of diffusivities one may observe under these simulation conditions. The diffusivity as measured from t = 0 and the mean diffusivity across all starting times are not necessarily equivalent, and thus the upper and lower uncertainties are not necessarily symmetric around the value predicted from  
Ionic transport in crystalline solids is modeled here as a stochastic phenomenon governed by a Boltzmann (Arrhenius) factor that is exponential in a single energy barrier between equilibrium sites, Ea:
All-solid-state Li-ion batteries (SSLIBs) hold promise as safer, longer lasting, and more energy dense alternatives to today’s commercialized LIBs with liquid electrolytes. However, the design of SSLIBs remains a challenge, with the principal technological bottleneck in realizing these devices being the solid electrolyte. A high performance solid electrolyte material must satisfy several criteria simultaneously: it must possess fast Li-ion conduction, negligible electronic conduction, a wide electrochemical window, robust chemical stability against side reactions with the electrodes, and high mechanical rigidity to suppress dendritic growth on the anode. The material should also be cheap and easy to manufacture. Given these many constraints, searching for promising new materials that satisfy all requirements through the trial-and-error searches has yielded slow progress.
The earliest efforts to discover fast Li-ion conducting solids began in the 1970s(1) and have continued to present. More recently, density functional theory (DFT) simulation has enabled high-throughput computational searches, essentially automating the process of guess-and-check.(2,3) Across these four decades, only several solids with liquid-level Li conduction (>10–2 S/cm) at room temperature (RT) have been identified, including notably Li10GeP2S12 (17 mS/cm)(4) and Li7P3S11 (25 mS/cm).(5) This slow progress suggests that continuing in the guess-and-check paradigm of decades past is unlikely to quickly yield the material innovations we need to unlock the high energy density, high cycle life, and unquestionably safe energy storage devices of the future.
Leveraging atomic and electronic structure data from the Materials Project database,(6) we have screened all 12 000+ Li-containing materials for thermodynamic phase stability, low electronic conduction, high electrochemical stability, and no transition metals (to enhance stability against reduction). We also compiled information on the estimated raw materials cost and the earth abundance of the elemental constituents of each material. This identifies 317 materials that may be strong candidate electrolyte materials if they are also fast ion conductors.
Following the guess-and-check paradigm, one would begin to test these materials for fast ion conduction at random, or according to his/her best scientific intuition. To identify the subset of these materials most likely to exhibit fast ionic conductivity, we have instead developed(7) a machine learning (ML)-based model for predicting the likelihood Psuperionic that an arbitrary material exhibits fast Li-ion conduction at RT, based only on features xi derived from the atomistic structure of the unit cell. Throughout this work, we define superionic conductivity to be greater than 0.1 mS/cm, based on the approximate minimum electrolyte ionic conductivity required for battery applications. Experimental reports of ionic conductivity for several dozen materials ranging over 10 orders of magnitude were used to train the model. This data-driven predictor takes the form of a logistic function, , where
Here, LLB is the average Li–Li bond number (number of Li neighbors) per Li; SBI is the average sublattice bond ionicity; AFC is the average coordination of the anions in the anion framework; LASD is the average equilibrium Li–anion separation distance in angstroms; and LLSD is the average equilibrium Li–Li separation distance in angstroms. Since this model does not require any electronic structure information, it is >5 orders of magnitude faster to evaluate than a DFT simulation of conductivity.
Screening this list of 317 candidate materials identifies 21 crystalline compounds that are predicted to be fast ion conductors with robust structural and electrochemical stability, representing a 99.8% reduction in the entire space of known Li-containing materials. One of these 21 materials, LiCl, has been reported to exhibit poor RT Li conduction (∼10–9 S/cm),(8) making it a known false positive prediction. Another material, Li3InCl6, has been reported to have a RT conductivity of approximately 1 mS/cm, making it a correct model prediction.(9) Very little is reported in the literature regarding the remaining 19 materials to our knowledge.
In this work, we perform DFT molecular dynamics (DFT-MD) calculations(10,11) on the promising candidate materials identified by our screening procedure, finding evidence of superionic RT Li conduction in eight and marginal RT Li conduction in two. As a control, we then perform DFT-MD on a similar number of materials drawn at random from the same population of 317. We quantify the increase in research efficiency offered by our ML-based model by comparing the improvements in experimental outcomes against the random case. We consider 41 unique materials in total. We find our ML-guided search offers approximately 3–4× improvement in predictive power for fast Li ion conductors over random guesswork depending on the metric, while on average the predicted RT Li ionic conductivity of any simulated material is over 44× higher.
As a further test of the model’s efficacy, we provided the same list of materials to a group of six graduate students working in the field and asked them to identify the best ion conductors. We found the F1 score of the model outperformed the F1 score of the intuition of the students by approximately two times, while each prediction was made approximately 1000 times faster. This result suggests ML-based approaches to materials selection may provide significant acceleration over the guess-and-check research paradigm of the past. Furthermore, these results provide confidence in our data-driven superionic prediction model, as well as compelling evidence in the promise of machine learning-based approaches to materials discovery.
We first perform DFT-MD on the 19 most promising new candidate materials for solid electrolyte applications that are all predicted to be fast ion conductors by our ML-based model eq 1. LiCl and Li3InCl6 were not simulated due to the existence of conductivity data in the literature.(8,9) In order to accelerate Li diffusion to a computationally tractable time scale, we initially seek an upper bound by performing MD at elevated temperature and removing one Li atom per computational cell to introduce a small concentration of Li vacancies to enhance conduction and minimize the number of false negatives identified. We simulate large supercells in order to minimize the effect of the periodic boundary conditions. All materials were initially simulated at T = 900 K; if melting is observed, the simulation is restarted at increasingly lower temperatures until no melting is observed. The vacancy concentration ranges from 3 to 17% depending on the unit cell. The simulation temperatures, computational cell size, and Li 
vacancy concentrations are provided in the Supporting Information, Table S1.
We simulate the candidate materials for a range of times on the tens to hundreds of picoseconds time scale; see Table S1. To calculate ionic diffusivity, which may be isotropic, we compute the average of the diagonal elements of the second rank Li diffusivity tensor (or equivalently, one-third of the trace). We denote this as ⟨Dii⟩, where the average is taken over the three elements ii = {xx, yy, zz}. We first evaluate the mean squared displacement (MSD) of the Li atoms ⟨Δr2⟩ over time (starting at t = 0) and apply the following formula:
To probe for melting, we also calculate the MSD of the sublattice atoms and assume melting if sustained, nonzero diffusivity is observed in both Li and the sublattice (the smallest diffusivity that can be resolved through DFT-MD at 900 K is approximately 0.01 Å2/ps). To assess the degree of convergence in ⟨Dii⟩, we compute the standard deviation in diffusivities when measured from different starting times in the MSD data. We compute the slope of the MSD data for every starting time from t = 0 to up to 75% of the total run time, in 100 fs increments. We compute one standard deviation above and below the mean diffusivity across all starting times to represent the approximate upper and lower limits of the distribution of diffusivities one may observe under these simulation conditions. The diffusivity as measured from t = 0 and the mean diffusivity across all starting times are not necessarily equivalent, and thus the upper and lower uncertainties are not necessarily symmetric around the value predicted from 
Ionic transport in crystalline solids is modeled here as a stochastic phenomenon governed by a Boltzmann (Arrhenius) factor that is exponential in a single energy barrier between equilibrium sites, Ea:
Ionic hopping becomes exponentially less likely as the energy barrier increases, and at 900 K (kBT = 78 meV) hopping may not happen on the tens to hundreds of picoseconds time scale if the barrier is much above 0.4 eV (e–0.4/0.078 = 0.006). Thus, high temperature MD simulations are not well-suited to predict a numerical value for ionic conductivity in medium- to high barrier systems because the statistical behavior is not captured on typical DFT simulation time scales. Numerical values can only be predicted through MD in low barrier systems, and even then are still approximate due to the stochastic nature of hopping. Our approach is most suited for high throughput calculations in which the main goal is identifying whether or not materials are superionic conductors, i.e., for making binary predictions of superionic vs nonsuperionic conduction.
From the diffusivity ⟨Dii⟩, we convert to the average of the diagonal elements of the ionic conductivity tensor ⟨σii⟩ via the Einstein relation:(12)
where n is the concentration of Li atoms and q is the charge on the Li atoms. We measure the effective Li charge q using the Bader charge analysis methods of Henkelman et al. Given the linear relationship between conductivity and diffusivity, we compute the spread of possible conductivities by applying eq 4 to the computed bounds in diffusivity as well.
It stands to reason that fast Li conduction will not be observed at RT if it is not observed in these favorable conditions. Our approach therefore should not identify false negatives (materials with poor conduction at high temperature but fast conduction at RT), although there is a risk of identifying false positives (materials with fast conduction at high temperature but poor conduction at RT). To ensure favorable scaling to room temperature and guard against the identification of false positives, we simulate DFT-MD again at alternate temperatures if significant Li diffusion has been observed at the initial simulation temperature. The line connecting the two or three diffusivities on an Arrhenius plot of log10(σ) versus inverse temperature is employed here to extrapolate down to RT without direct calculation of Ea. This assumes Arrhenius scaling applies and there are no structural phase changes or nonstructural superionic transitions (where conductivity changes abruptly as new ionic pathways become 
energetically accessible or inaccessible without any significant sublattice rearrangement) between the simulation temperatures and RT. The spread of possible values in the RT conductivity extrapolations is computed by extrapolating along the upper and lower limits of the deviations in the high temperature conductivity calculations. Using typical values for Li concentration, Bader charge, and site hopping distance, we estimate that the RT conductivity is likely to be 10–9 S/cm or lower if no Li diffusion is observed on the simulation time scale at 900 K (see Supporting Information, Section S1 for calculation).
A flowchart describing this process is shown in Figure 1. This extrapolation scheme requires less computational expense than simulating each material at several different temperatures right away and is the most tractable way for us to quantify the ionic conductivity of the materials studied here on a reasonable time scale. Screening for Li superionic RT diffusion by two-temperature extrapolation has been leveraged in recent work to search sulfide-based compositional spaces, for example.
We perform DFT calculations with the Vienna Ab Initio Simulation Package (VASP)(18) with the generalized gradient approximation (GGA) of Perdew–Burke–Ernzerhof (PBE)(19) and the projector augmented wave (PAW)(20) method. We use the pseudopotentials and plane wave cutoff energy (520 eV for all structures) as recommended by the Materials Project. The VASP input files are generated using the pymatgen.io.vasp.sets module of Pymatgen. Given the large unit cells (and DFT Kohn–Sham bandgaps exceeding 1 eV for all these materials), we use a gamma-point only k-mesh. The pseudopotentials are given in Table S1.
The ultimate metric of the utility of materials selection models is the improvement in likelihood of successfully identifying positive examples over the background probability of these materials. To quantify the superiority of our model to completely random guesswork, we must know the likelihood of discovering a superionic material by chance with no scientific intuition involved. In 1978, Huggins and Rabenau questioned “whether there is anything fundamentally different in such materials, or whether they merely exhibit extreme values of ‘normal’ behavior.”(23) Superionic conductors may have remarkably low diffusion barriers, but how do they compare to the distribution of barriers across the space of all the candidate electrolyte materials?
Experience would tell us that the likelihood of chance discoveries is low, given that only a handful of fast Li conductors have been discovered since the search began nearly 40 years ago. However, we are probing for superionic conduction in a very specific way: we are pulling from a potentially biased group of 317 known materials in the Materials Project database, introducing Li vacancies, and simulating finite computational cells at high temperature. Therefore, this question is best answered by doing a straight-across comparison of high temperature DFT-MD simulations for a similar number of structures chosen at random from the same population.
To accomplish this, we perform a control experiment where we simulate 21 structures chosen uniformly at random from among the 317 materials that satisfy all prerequisite screening criteria (band gap > 1 eV, predicted oxidation potential > 4 V, energy above convex hull = 0 eV, no transition metals) and simulate MD under the same procedure. These 21 structures and their predicted superionic likelihoods according to the ML model are the following: LiLa2SbO6 (0%), Li6UO6 (7.7%), LiInF4 (0.6%), LiBiF4 (0.2%), Li6Ho(BO3)3 (10.1%), RbLiB4O7 (4%), Li4Be3As3ClO12 (0.3%), Li6TeO6 (6.4%), Li3Pr2(BO3)3 (9.2%), NaLiS (36%), LiSbO3 (2.4%), LiCaGaF6 (0.0%), Li2Te2O5 (15.8%), LiNO3 (8.3%), Ba4Li(SbO4)3 (0.0%), Rb2Li2SiO4 (4.5%), NaLi2PO4 (0.0%), Cs4Li2(Si2O5)3 (0.1%), RbLi(H2N)2 (24.5%), Cs2LiTlF6 (0.0%), and LiSO3F (100%). We chose 29 structures in total but removed eight (Na2LiNF6, CsLi2(HO)3, LiU4P3O20, Li3Nd2H6(N3O10)3, Li3La2H6(N3O10)3, Li3P11(H3N)17, Li4H3BrO3, and Li2Pr(NO3)5) due to either melting at 900 K or due to extremely slow or failed electronic convergence, in some cases due to electronic bandgap closure during the simulation. Of the remaining 21 randomly chosen materials we successfully simulate, 20 have a negative ML-based superionic prediction (below <50%) so we do not expect them to conduct. The only material of these 21 random materials that is predicted by our ML-based algorithm to conduct is LiSO3F. The computational parameters used in these randomly chosen simulations are also provided in Table S1. This set of randomly drawn materials also provides a test set to explore the ML model performance versus the predictive power of the intuition of human experts; we explore this in Section III, Subsection v.
The ML-chosen materials and randomly chosen materials exhibit notably different distributions in their compositions. For example, compositions containing oxygen are heavily represented in the randomly chosen materials (71.4% of materials) but poorly represented in the ML-chosen materials (14.3%). The former is closely aligned with the distribution of oxygen-containing compositions in the broader pool of 317 candidates (70.0%) that satisfy the constraints including high oxidation potential, which we expect given the randomly chosen materials are drawn from this pool. Similarly, sulfur-containing compositions make up 9.2% of the 317 candidates and 9.5% of the randomly chosen candidates but 57.1% of the ML-chosen materials, indicating a clear preference by the ML model for sulfides over oxides when ionic conductivity is the primary consideration. We note that this work is focused primarily on finding the best specific target compounds; a follow-up study(24) will focus on the identification of promising new 
target systems.
In this work we report a total of 4.3 ns of molecular dynamics simulation, with a mean simulation time of 74.9 ps and mean computational cell size of 99.1 atoms. The total volume of data reported here corresponds to approximately 330 000 GPU-hours of simulation.

RESULTS
The computed ionic conductivities for the ML-chosen candidate materials and the randomly chosen materials are provided in the Arrhenius diagram on Figure 2a. The computed high temperature diffusivities, ionic conductivities, average Li Bader charge, and extrapolated RT ionic conductivities with predicted deviations (conductivities measured from alternate starting time points) of these candidates are given in Table 1. We do not calculate results for one of the ML-chosen candidates, Li2GePbS4, because of problems with electronic
We simulate 20 materials as identified by the ML model. For each of the simulated materials, we provide the computed values for diffusivity, conductivity, and average Li Bader charge and the extrapolated RT ionic conductivity values. Here, the diffusivity ⟨Dii⟩ represents the average of the diagonal elements of the diffusivity tensor. In parentheses we provide the diffusivities corresponding to one standard deviation above and below the mean when measuring the diffusivity from varying times. We extrapolate these alternate diffusivities to RT to compute expected deivations in the RT conductivity predictions. The second-to-right column gives the ML-based model prediction for superionic likelihood, and the rightmost column communicates whether the ML-based model gave a correct prediction for each material. Eight materials have an extrapolated RT ionic conductivity of 10–4 S/cm or higher, two are below but near this threshold, and ten did not show any Li conduction even at high temperature. Li3InCl6 and LiCl were not simulated (“N/S”) because conductivity values could be found in the literature.
bLi2HIO was only simulated at 400 K due to melting at higher temperatures. This extrapolation to RT assumes a diffusion barrier of 0.35 eV, a typical diffusion barrier in fast conducting systems.
CsLi2BS3 could not be simulated at lower temperatures and thus the extrapolated ionic conductivity at RT was taken to be of the same order of magnitude as that of Li3BS3.(25)
Our simulations show 10 of the simulated candidate materials exhibit significant Li conduction at high temperature. After applying Arrhenius scaling and extrapolating to RT, eight of these materials are predicted to exhibit superionic conductivity (>10–4 S/cm) under the simulated vacancy concentrations: Li5B7S13, CsLi2BS3, LiMgB3(H9N)2, Li2B2S5, Li3ErCl6, Li3InCl6, Li2HIO, and LiSO3F. Two materials, Sr2LiCBr3N2 and LiErSe2, conduct well at high temperature, but their extrapolated RT conductivities are below 10–4 S/cm at RT.
Zhu et al. report(3) that RT superionic conductors will typically exhibit a simulated Li diffusivity of at least 0.01 Å2/ps at 800 K. Of these promising materials that were simulated at 900 K, all have diffusivities significantly above 0.01 Å2/ps. The two marginal conductors are above this limit by a factor of two to three, while the best conductors are above this limit by a factor of 10 or more. This provides additional confidence in their fast RT ionic conductivity.
The rightmost column of Table 1 indicates whether the DFT evidence suggests the superionic prediction of the ML model was correct. In total, 8 materials are observed to show Li conduction of 10–4 S/cm or above at RT, 2 materials are near this threshold, and 10 fall far below.
The results of the simulation on the randomly selected materials are listed in Table 2 and plotted in Figure 2b. Of the 21 randomly chosen and simulated materials, five structures demonstrated measurable Li conduction at 900 K: LiSO3F, LiSbO3, Li6Ho(BO3)3, Li2Te2O5, and RbLi(H2N)2, with Li diffusivities at 900 K of 0.3, 0.1, 0.05, 0.04, and 0.02 Å2/ps, respectively. The remaining 16 structures showed no observable Li conduction on the time scale of tens to hundreds of picoseconds. See Supporting Information for computational parameters and simulation times.
We simulate 21 materials as chosen at random and provide the computed values for diffusivity ⟨Dii⟩ (average of diagonal elements in the diffusivity tensor), conductivity, and average Li Bader charge and the extrapolated RT ionic conductivity values. In parentheses, we provide the diffusivities corresponding to one standard deviation above and below the mean when measuring the diffusivity from varying start times. We extrapolate along these alternate diffusivities to RT to compute expected deviations in the RT conductivity predictions. The second-to-right column gives the ML-based model prediction for superionic likelihood, and the rightmost column communicates whether the ML-based model gave a correct prediction for each material. The Bader charge on the Li atoms in the control group is taken to be +1.0 for computational efficiency. Additional randomly chosen materials that melted at 900 K or whose simulations did not converge were discarded and are not listed here. The success rate of random selection is 
computed to be 3/21 = 14.3%.
The five materials that exhibited diffusion at 900 K were simulated again at alternate temperatures. After constructing the conductivity extrapolation, the conductivities in three of these materials (LiSO3F, Li6Ho(BO3)3, and LiSbO3) were found to extrapolate to above 10–4 S/cm at room temperature. The remaining two, however, showed zero diffusion during simulation at 700 K. Using typical values for the Li concentration and Bader charge, we estimate that zero Li diffusion after 50 ps of simulation at 700 K likely corresponds to an ionic conductivity of 10–9 S/cm or less. Therefore, the RT conductivity of these two materials is predicted to be far below 10–4 S/cm when the assumed 700 K conductivity is factored into the extrapolation.
Between the ML-guided search and the random search, we identify 12 materials total that are predicted by our DFT-MD simulations to exhibit significant Li ion conduction at RT (and the DFT-MD of Zevgolis et al.(9) for Li3InCl6). Aside from Li3InCl6 and Li6Ho(BO3)3, none of these materials have reported DFT or experimental conductivity values in the literature to our knowledge. Due to our additional screening steps, these materials also satisfy several other critical criteria beyond fast ionic conductivity to make them useful as solid-state electrolytes in Li-ion batteries: all materials possess DFT-predicted band gaps greater than 1 eV to ensure limited electronic conduction, are free from transition metal elements which may easily reduce in contact with a Li metal anode, exhibit high predicted electrochemical stability against oxidation by cathodes (>4 V vs Li/Li+), and sit on the convex hull of their phase diagrams to ensure robust structural (phase) stability.
Two of the materials discovered here (Li2B2S5 and Li5B7S13) belong to the Li–B–S system, which appears to be a promising class of materials for realizing fast conducting, electrochemically stable, low mass solid electrolyte materials. The RT ionic conductivity of Li5B7S13 in particular is predicted to be an exceptional 74 mS/cm, six times higher than the material Li10GeP2S12 (12 mS/cm),(4) one of the best known Li-ion conductors. CsLi2BS3 is a Cs-substituted isomorph of Li3BS3, a fast conductor also in the Li–B–S family.(25) These three phases were initially identified and characterized as phases observed within Li–B–S (and Cs–Li–B–S) glasses.(26−28) The glassy Li–B–S–I system was reported to be an exceptional Li ion conductor in 1981,(29) but to our knowledge these phases within the crystalline Li–B–S space have not been studied as solid electrolytes.
Li3ErCl6, an Er-substituted analogue to the fast-conducting Li3InCl6, appears to be a promising Li conductor with a predicted RT ionic conductivity of 3 × 10–4 S/cm, although the high atomic mass and low earth abundance of Er makes it a less attractive candidate for battery applications. Li3ErCl6 was first studied and characterized in 1996 as a potential fast Li ion conductor,(30) but to our knowledge the ionic conductivity was not reported. We find another Er-based compound appears to be a marginal RT Li ion conductor, LiErSe2. LiErSe2 is a Li-intercalated two-dimensional layered material that was initially studied in 1987 for its phase transitions and magnetic properties.
We discover three nitride-based fast ion conductor materials with quite complex compositions: Sr2LiCBr3N2, RbLi(H2N)2, and LiMgB3(H9N)2. Sr2LiCBr3N2 is a solid-state carbodiimide material that was first characterized in 2005;(32) RbLi(H2N)2 was originally synthesized in 1918(33) and subsequently characterized in 2002;(34) and LiMgB3(H9N)2 was initially synthesized and studied in 2014 as a candidate solid-state hydrogen storage material.(35) The exotic stoichiometries of these materials underscores the utility of the ML-based model, as it seems unlikely that such complex materials would be discovered by chance to be fast ion conductors. A potential drawback in using these materials with many elements as solid electrolytes is that the electrochemical stability is likely to be low, given the high number of stable interfacial phases that could form from a subset of these elements.
We discover four oxygen-based materials predicted to be fast RT ion conductors: Li2HIO, LiSO3F, Li6Ho(BO3)3, and LiSbO3. These materials may be particularly promising candidates for further experimental studies, given that oxide materials can often be synthesized in atmospheric conditions. LiSbO3, LiSO3F, and Li2HIO were first studied in 1954, 1977, and 1994, respectively,(36−38) but to our knowledge have not been studied as a candidate solid electrolyte material. Li6Ho(BO3)3 was first characterized(39) in 1977, and experimental work from 1982 reported it to have poor RT ionic conductivity.(40) If the experimental observation is assumed to reflect the “ground truth” of ionic conductivity, this gives Li6Ho(BO3)3 the interesting distinction of having been accurately predicted to be a poor ion conductor by the ML model (which was trained on experimental data) but incorrectly predicted to be a good ion conductor by DFT-MD. It is also possible that the bulk ionic conductivity of the material is fast, which is 
accurately modeled by DFT-MD, but it becomes a poor ion conductor under the experimental conditions of ref (40), e.g., due to defects, stoichiometric variations, or microstructural effects. The calculations performed here suggest that this material warrants a revisit of the experimental results.
iv. Model Performance
Quantifying the accuracy of the original ML-based model should be done based on its performance on a randomly sampled test set, not on a test set consisting only of positive predictions. The 21 randomly chosen materials that were simulated here provide such a set. On this set, the model correctly predicts the DFT-validated material label 19/21 times, yielding an overall predictive accuracy of 90.5%. This value is similar to the 90.0% model accuracy predicted via leave-one-out cross-validation in the original model building process.(7) Only one material (LiSO3F) was predicted to be a superionic conductor by the ML-based model, and our DFT simulation confirmed the prediction was correct; this gives a model precision of 1.0 on this particular test set. Of the three materials that DFT-MD found to be RT superionic conductors (LiSO3F, LiSbO3, Li6Ho(BO3)3), only one was accurately predicted by the ML model; this gives a model recall of 1/3 = 0.333 on the test set. Taken together, these values give the model an F1 
score of 0.50. For comparison, the baseline F1 score of fast ion conductors on the test set (i.e., the F1 score of completely random guesswork) is simply the fraction of positive examples in the test set, which gives a baseline F1 score of 0.143. Therefore, the F1 score of the ML model is 0.50/0.143 = 3.5x higher than the F1 score associated with completely random trial-and-error.
As an alternative means of quantifying the expected improvement in experimental outcomes of Li conductivity measurements in the ML-guided versus random searches, we average the predicted RT ionic conductivities across all randomly chosen materials and all ML-chosen materials. Given that ionic conductivity varies over many orders of magnitude, we average the base-10 logarithm of conductivity. This gives a log-average RT ionic conductivity of the randomly chosen materials of 6.8 × 10–6 mS/cm, while the log-average conductivity in the ML chosen materials is 3.0 × 10–4 mS/cm, 44 times higher. This comparison assumes the average RT ionic conductivity in the materials with no observed Li motion is 10–6 mS/cm; as discussed in Supporting Information Section S1, this value likely serves as an upper bound in most cases on the conductivity in these materials, and thus the 44× improvement in conductivity is a lower bound on the true improvement the model offers, e.g., if the average conductivity of the nonconductors is 
taken to be 10–9 mS/cm, the improvement in conductivity increases to over 350×.
As an additional performance metric, we compare the likelihood of discovering a RT superionic conductor under the ML-guided screening versus random searching. The random search yields 3 fast conductors and 18 nonconductors. This yields a baseline superionic probability of 3/21 = 14.3%. To make a fair comparison to the ML-guided case where materials that melted at 900 K were not discarded, we do not consider in the statistics those ML-chosen materials that melted at 900 K. Two of the eight ML-chosen RT superionic conductors melted at 900 K (LiMgB3(H9N)2 and Li2HIO). Removing these two materials from the count, the ML model identifies six fast conductors, two marginal conductors, and ten nonconductors. Counting the marginal cases as one-half, this gives an ML-guided conductor discovery rate of 7/18 = 38.9%. Therefore, applying the ML-based model to screen candidate materials before experimentation yields an expected 0.389/0.143 = 2.7x improvement in discovery rate. In Figure 3 we provide a histogram showing the ionic conductivity distribution in the ML-chosen and randomly chosen materials; the ML model discovers over five times more materials with ionic conductivity above 1 mS/cm, while the number of nondiffusive materials falls from 86% to 56%. We refer the reader to the Supporting Information Section S2 for further discussion.
We perform a test of the model’s speed and predictive power against human intuition, which is likely to identify fast conducting materials at a higher rate than the background distribution of ion conductors. Several design principles for predicting superionic diffusion have been reported recently,(41−43) but human-guided search efforts may or may not take these principles into account. To understand the precision of human intuition, we polled six Ph.D. students in the Department of Materials Science & Engineering at Stanford University to test their ability to predict superionic conduction in the 21 randomly chosen materials. The students are all actively engaged in research involving ion conductors. The students were allowed to take as much time as necessary to make predictions and could access all structural information about the candidate materials provided on the Materials Project database or any other source. This included structure (CIF) files, space and point groups, and properties like formation 
energy and band gap. This poll sought to test how quickly and accurately the students would naturally predict superionic conduction in the absence of eq 1, and so they were not given access to any of the pretabulated features employed in eq 1, nor were they encouraged to make predictions quickly. After making predictions, we calculated their average precision and recall on the data set; the average precision was 0.25 and the average recall was 0.222, giving an overall average F1 score of 0.235. The baseline F1 score for random guessing is 0.143. The students took approximately one minute to make each individual prediction. For comparison, the ML model made each prediction in approximately one millisecond.
In Figure 4 we compare the performance in speed and F1 score of the Ph.D. students and the ML model. The ML model exhibits more than double the F1 score of the students and is more than 3 orders of magnitude faster. As a reference, we provide the performance of DFT-MD, which we assume has an F1 score of 1.0 and requires approximately 2 weeks to make a reliable prediction.
While DFT-MD is taken here to be ground truth, the logistic regression model utilized in this work is trained on experimental measurements, where grain boundaries, contaminants, and other uncharacterized factors may have altered the result. This may mean the model is best suited to guide experimental searches, as the model has a built-in bias toward expected results under experimental conditions. In contrast, our DFT-MD simulates conductivity in single crystal bulk systems with small Li vacancy concentrations. Although DFT-MD is well-suited to make predictions under these conditions, its predictions have potential to deviate from the ground truth for the ML model; for example, the conflicting DFT-MD and experimental reports for RT Li ion conductivity in Li6Ho(BO3)3 between this work and ref.(40) This work predicts RT ion conductivity of approximately 5.1 mS/cm, while ref (40) reports RT ion conductivity orders of magnitude lower (see Section III, Subsection iii). It is possible that the model may recognize 
that experimental efforts on Li6Ho(BO3)3 are likely to yield a poor conductor even though the single crystal conductivity is fast. We look forward to future experimental reports of the ionic conductivities of some of the materials presented here, to both advance the state of the art in ion conductors and to further quantify the performance of the data driven approach.

CONCLUSION
Guided by machine learning methods, we discover many new solid materials with predicted superionic Li ion conduction (≥10–4 S/cm) at room temperature: Li5B7S13, Li2B2S5, Li3ErCl6, LiSO3F, Li3InCl6, Li2HIO, LiMgB3(H9N)2, and CsLi2BS3. Two additional materials show marginal RT conduction: Sr2LiCBr3N2 and LiErSe2. One of these materials, Li5B7S13, has a DFT-MD predicted RT Li conductivity (74 mS/cm) many times larger than the fastest known Li ion conductors. A search over randomly chosen materials identifies two additional materials with promising predicted RT ionic conductivities: Li6Ho(BO3)3, and LiSbO3. In addition to high ionic conductivities, all these materials have high band gaps, high thermodynamic stability, and no transition metals, making them promising candidates for solid-state electrolytes in SSLIBs. These materials represent many exciting new candidates for solid electrolytes in SSLIBs, and we encourage subsequent experimental investigations into the properties of these materials.
Compared to the machine learning-guided search, the control experiment of searching for fast ion conductors through random guesses yields significantly lower quality results. Screening with our ML-based model improves the likelihood of superionic discovery by nearly three times, with a more than 44× improvement in log-average of RT ionic conductivity. Additionally, the model outperforms the intuition of Ph.D. students actively working on ion conductors by more than a factor of 2. This is a significant acceleration beyond trial-and-error research and provides confidence in our ML-based approach to materials screening. These results are summarized in Table 3. Such data-driven models are expected to improve with every new data point, and with the data we report here we expect to drive significant improvements to ML-based models for Li conduction.
Here we provide the performance metrics (precision, recall, F1 score) and list of discovered fast Li ion conducting materials for the three models explored in this work. The results of the Ph.D. student screening represent the average performance of six students on the same test set of randomly chosen materials as used to compute the “random selection” statistics. The machine learning-based approach to predicting ion conductivity from a small data set of 40 materials significantly outperforms both random chance and the average polled Ph.D. student. This highlights the promise of applying machine learning-based approaches to materials screening before performing computationally expensive simulation techniques like DFT-MD or time consuming experimental tests.
The improvement over random guessing provided by our ML-based model for predicting Li ion conductivity underscores the importance of thorough data reporting, centralized data collection, and careful data analysis for materials. Although significant improvements have been made in the last several years thanks to incentives provided by the Materials Genome Initiative, there are still many materials properties, especially experimentally expensive properties like ionic conductivity, without any comprehensive data repository. This work demonstrates that learning on even small sets of materials data (40 samples) can offer a significant advantage in screening efforts. To that end, we encourage efforts to continue centralizing and learning on diverse types of data on materials.
